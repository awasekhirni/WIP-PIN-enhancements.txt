-- =============================================
-- PostgreSQL Schema for Enterprise Risk Management with 11 Modules
-- Version: 4.0
-- Copyright 2025 All rights reserved Î² ORI Inc.Canada
-- Created: 2025-05-29
-- Last Updated: 2025-08-09
--Author: Awase Khirni Syed
-- Description: Comprehensive schema for Enterprise Risk Management
--              with advanced features for data governance, analytics, and compliance
-- Enable essential extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pgcrypto";
CREATE EXTENSION IF NOT EXISTS "hstore";


-- Set timezone and other configurations
SET TIMEZONE = 'UTC';
SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SET check_function_bodies = false;
SET client_min_messages = warning;
SET row_security = off;

-- ========================================================================
-- SECTION 2: CORE SCHEMA STRUCTURE
-- ========================================================================
CREATE SCHEMA IF NOT EXISTS core;
CREATE SCHEMA IF NOT EXISTS rcm;


-- Enable UUID extension
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- Enable citext for case-insensitive username/email comparisons
CREATE EXTENSION IF NOT EXISTS citext;

-- =================== Core Entities =================== --

-- Users Table with citext for case-insensitive fields
CREATE TABLE core.users (
  user_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  username VARCHAR(50) NOT NULL UNIQUE,
  email VARCHAR(100) NOT NULL UNIQUE,
  password_hash TEXT NOT NULL,
  first_name VARCHAR(50),
  last_name VARCHAR(50),
  department VARCHAR(50),
  position VARCHAR(50),
  is_active BOOLEAN DEFAULT TRUE,
  last_login TIMESTAMP,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  gdpr_consent BOOLEAN DEFAULT FALSE,
  gdpr_consent_date TIMESTAMP,
  data_retention_period INT DEFAULT 365, -- days
  CONSTRAINT email_check CHECK (email ~* '^[A-Za-z0-9._%-]+@[A-Za-z0-9.-]+[.][A-Za-z]+$')

);

-- Roles Table
CREATE TABLE core.roles (
    role_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(100) UNIQUE NOT NULL,
    description TEXT,
    parent_role_id UUID REFERENCES roles(role_id),
    is_template BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata JSONB
);

-- Permissions Table
CREATE TABLE core.permissions (
    permission_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(100) UNIQUE NOT NULL,
    description TEXT,
    category VARCHAR(100),
    scope VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata JSONB
);

-- Permission Groups
CREATE TABLE core.permission_groups (
    group_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(100) UNIQUE NOT NULL,
    description TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata JSONB
);

-- Join table between permission groups and permissions
CREATE TABLE core.permission_group_permissions (
    group_id UUID NOT NULL REFERENCES permission_groups(group_id),
    permission_id UUID NOT NULL REFERENCES permissions(permission_id),
    PRIMARY KEY (group_id, permission_id)
);

-- User Role Assignments
CREATE TABLE core.user_roles (
    user_id UUID NOT NULL REFERENCES users(user_id),
    role_id UUID NOT NULL REFERENCES roles(role_id),
    assigned_by UUID REFERENCES users(user_id),
    assigned_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP,
    PRIMARY KEY (user_id, role_id),
    metadata JSONB
);

-- Role Permission Mapping
CREATE TABLE core.role_permissions (
    role_id UUID NOT NULL REFERENCES roles(role_id),
    permission_id UUID NOT NULL REFERENCES permissions(permission_id),
    granted_by UUID REFERENCES users(user_id),
    granted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (role_id, permission_id),
    metadata JSONB
);

-- User Groups
CREATE TABLE core.user_groups (
    group_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(100) UNIQUE NOT NULL,
    description TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata JSONB
);

-- User Group Memberships
CREATE TABLE core.user_group_members (
    group_id UUID NOT NULL REFERENCES user_groups(group_id),
    user_id UUID NOT NULL REFERENCES users(user_id),
    joined_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (group_id, user_id),
    metadata JSONB
);

-- Role Assignment to User Groups
CREATE TABLE core.group_role_assignments (
    group_id UUID NOT NULL REFERENCES user_groups(group_id),
    role_id UUID NOT NULL REFERENCES roles(role_id),
    assigned_by UUID REFERENCES users(user_id),
    assigned_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP,
    PRIMARY KEY (group_id, role_id),
    metadata JSONB
);

-- =================== Privilege Escalation =================== --

-- Temporary Privileges
CREATE TABLE core.temporary_privileges (
    temp_priv_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID NOT NULL REFERENCES users(user_id),
    permission_id UUID NOT NULL REFERENCES permissions(permission_id),
    reason TEXT,
    approved_by UUID REFERENCES users(user_id),
    start_time TIMESTAMP NOT NULL,
    end_time TIMESTAMP NOT NULL,
    status VARCHAR(20) CHECK (status IN ('pending', 'approved', 'rejected', 'expired')) DEFAULT 'pending',
    requested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata JSONB
);

-- Approval Workflow
CREATE TABLE core.privilege_approvals (
    approval_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    temp_priv_id UUID NOT NULL REFERENCES temporary_privileges(temp_priv_id),
    approver_id UUID REFERENCES users(user_id),
    action VARCHAR(20) CHECK (action IN ('approved', 'rejected')),
    comment TEXT,
    approved_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Notification Logs
CREATE TABLE core.notification_logs (
    notification_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    event_type VARCHAR(50),
    user_id UUID REFERENCES users(user_id),
    message TEXT,
    delivered BOOLEAN DEFAULT FALSE,
    sent_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Trigger-based real-time notifications via pg_notify
CREATE OR REPLACE FUNCTION core.notify_on_notification()
RETURNS TRIGGER AS $$
BEGIN
    PERFORM pg_notify('notifications', json_build_object(
        'notification_id', NEW.notification_id,
        'event_type', NEW.event_type,
        'user_id', NEW.user_id,
        'message', NEW.message,
        'sent_at', NEW.sent_at
    )::text);
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER core.trg_notify_on_insert
AFTER INSERT ON notification_logs
FOR EACH ROW
EXECUTE FUNCTION notify_on_notification();

-- =================== Logging & Auditing =================== --

-- Access Logs
CREATE TABLE core.access_logs (
    log_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID NOT NULL REFERENCES users(user_id),
    resource_type VARCHAR(100),
    resource_id VARCHAR(255),
    action VARCHAR(50),
    success BOOLEAN,
    ip_address INET,
    user_agent TEXT,
    accessed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Audit Logs
CREATE TABLE core.audit_logs (
    audit_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    entity_type VARCHAR(50),
    entity_id UUID,
    operation VARCHAR(20) CHECK (operation IN ('create', 'update', 'delete')),
    old_value JSONB,
    new_value JSONB,
    performed_by UUID REFERENCES users(user_id),
    performed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- =================== ABAC Support =================== --

-- Attribute Definitions
CREATE TABLE core.abac_attributes (
    attribute_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(100) UNIQUE NOT NULL,
    description TEXT,
    data_type VARCHAR(50)
);

-- Policy Rules
CREATE TABLE core.abac_policies (
    policy_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(100) UNIQUE NOT NULL,
    description TEXT,
    condition_expression TEXT NOT NULL,
    effect VARCHAR(10) CHECK (effect IN ('allow', 'deny')) NOT NULL,
    enabled BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Link Policies to Roles
CREATE TABLE core.abac_policy_roles (
    policy_id UUID NOT NULL REFERENCES abac_policies(policy_id),
    role_id UUID NOT NULL REFERENCES roles(role_id),
    PRIMARY KEY (policy_id, role_id)
);

-- =================== Row-Level Security (RLS) =================== --

-- Example: Secure access to a sensitive table based on user's role
ALTER TABLE core.access_logs ENABLE ROW LEVEL SECURITY;

-- You can create dynamic policies per table as needed
-- Sample policy function:
-- CREATE POLICY access_log_owner_only ON access_logs
--     FOR SELECT
--     USING (user_id = current_user_id());

-- =================== Reporting Materialized Views =================== --

-- Materialized view for role usage reporting
CREATE MATERIALIZED VIEW core.mv_role_usage AS
SELECT r.name AS role_name, COUNT(ur.user_id) AS user_count
FROM roles r
LEFT JOIN user_roles ur ON r.role_id = ur.role_id
GROUP BY r.name;

-- Materialized view for compliance reporting
CREATE MATERIALIZED VIEW core.mv_compliance_report AS
SELECT u.username, r.name AS role_name, p.name AS permission_name
FROM users u
JOIN user_roles ur ON u.user_id = ur.user_id
JOIN roles r ON ur.role_id = r.role_id
JOIN role_permissions rp ON r.role_id = rp.role_id
JOIN permissions p ON rp.permission_id = p.permission_id;

-- Refresh materialized views periodically
CREATE OR REPLACE FUNCTION core.refresh_materialized_views()
RETURNS VOID AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY mv_role_usage;
    REFRESH MATERIALIZED VIEW CONCURRENTLY mv_compliance_report;
END;
$$ LANGUAGE plpgsql;

-- =================== Stored Procedures =================== --

-- Assign a role to a user
CREATE OR REPLACE PROCEDURE core.assign_user_to_role(
    p_user_id UUID,
    p_role_id UUID,
    p_assigned_by UUID,
    p_expires_at TIMESTAMP DEFAULT NULL
)
AS $$
BEGIN
    INSERT INTO core.user_roles (user_id, role_id, assigned_by, expires_at)
    VALUES (p_user_id, p_role_id, p_assigned_by, p_expires_at);

    -- Log notification
    INSERT INTO core.notification_logs (event_type, user_id, message)
    VALUES ('role_assigned', p_user_id, 'You were assigned a new role.');
END;
$$ LANGUAGE plpgsql;

-- Request temporary privilege
CREATE OR REPLACE PROCEDURE core.request_temporary_privilege(
    p_user_id UUID,
    p_permission_id UUID,
    p_reason TEXT,
    p_start_time TIMESTAMP,
    p_end_time TIMESTAMP
)
AS $$
BEGIN
    INSERT INTO core.temporary_privileges (
        user_id, permission_id, reason, start_time, end_time
    ) VALUES (
        p_user_id, p_permission_id, p_reason, p_start_time, p_end_time
    );

    -- Log notification
    INSERT INTO core.notification_logs (event_type, user_id, message)
    VALUES ('privilege_requested', p_user_id, 'Your privilege escalation request has been submitted.');
END;
$$ LANGUAGE plpgsql;

-- Approve a temporary privilege
CREATE OR REPLACE PROCEDURE core.approve_temporary_privilege(
    p_temp_priv_id UUID,
    p_approver_id UUID,
    p_action VARCHAR(20),
    p_comment TEXT
)
AS $$
DECLARE
    v_user_id UUID;
BEGIN
    UPDATE core.temporary_privileges
    SET status = p_action
    WHERE temp_priv_id = p_temp_priv_id
    RETURNING user_id INTO v_user_id;

    INSERT INTO core.privilege_approvals (
        temp_priv_id, approver_id, action, comment
    ) VALUES (
        p_temp_priv_id, p_approver_id, p_action, p_comment
    );

    -- Notify user
    INSERT INTO core.notification_logs (event_type, user_id, message)
    VALUES ('privilege_approved', v_user_id, 'Your privilege escalation was approved.');
END;
$$ LANGUAGE plpgsql;

-- Check if user has a specific permission
CREATE OR REPLACE FUNCTION core.user_has_permission(p_user_id UUID, p_permission_id UUID)
RETURNS BOOLEAN AS $$
DECLARE
    perm_count INTEGER;
BEGIN
    SELECT COUNT(*) INTO perm_count
    FROM user_roles ur
    JOIN role_permissions rp ON ur.role_id = rp.role_id
    WHERE ur.user_id = p_user_id AND rp.permission_id = p_permission_id
      AND (ur.expires_at IS NULL OR ur.expires_at > NOW());

    RETURN perm_count > 0;
END;
$$ LANGUAGE plpgsql;

-- Log access attempt
CREATE OR REPLACE PROCEDURE core.log_access_attempt(
    p_user_id UUID,
    p_resource_type VARCHAR(100),
    p_resource_id VARCHAR(255),
    p_action VARCHAR(50),
    p_success BOOLEAN,
    p_ip_address INET,
    p_user_agent TEXT
)
AS $$
BEGIN
    INSERT INTO core.access_logs (
        user_id, resource_type, resource_id, action, success, ip_address, user_agent
    ) VALUES (
        p_user_id, p_resource_type, p_resource_id, p_action, p_success, p_ip_address, p_user_agent
    );
END;
$$ LANGUAGE plpgsql;

-- =================== Views =================== --

-- View to show all user roles including expiration
CREATE OR REPLACE VIEW core.vw_user_roles AS
SELECT u.username, r.name AS role_name, ur.expires_at, ur.assigned_at
FROM user_roles ur
JOIN users u ON ur.user_id = u.user_id
JOIN roles r ON ur.role_id = r.role_id;

-- View to show all role permissions
CREATE OR REPLACE VIEW core.vw_role_permissions AS
SELECT r.name AS role_name, p.name AS permission_name, p.category, p.scope
FROM role_permissions rp
JOIN roles r ON rp.role_id = r.role_id
JOIN permissions p ON rp.permission_id = p.permission_id;

-- View to show all active temporary privileges
CREATE OR REPLACE VIEW core.vw_active_temp_privileges AS
SELECT tp.temp_priv_id, u.username, p.name AS permission_name, tp.start_time, tp.end_time
FROM temporary_privileges tp
JOIN users u ON tp.user_id = u.user_id
JOIN permissions p ON tp.permission_id = p.permission_id
WHERE tp.status = 'approved' AND tp.end_time > NOW();

-- =================== Indexes for Performance =================== --

CREATE INDEX idx_users_username ON core.users(username);
CREATE INDEX idx_roles_parent ON core.roles(parent_role_id);
CREATE INDEX idx_user_roles_user ON core.user_roles(user_id);
CREATE INDEX idx_user_roles_role ON core.user_roles(role_id);
CREATE INDEX idx_role_permissions_role ON core.role_permissions(role_id);
CREATE INDEX idx_access_logs_user ON core.access_logs(user_id);
CREATE INDEX idx_audit_logs_entity ON core.audit_logs(entity_type, entity_id);

-- =================== Security Functions =================== --

-- Enforce TLS/SSL at application level; not enforced here but recommended
-- Two-factor authentication should be handled at login layer
-- Integration with SSO/LDAP/AD can be done via external services/APIs



-- =============================================
-- Table: risk_categories
-- Description: Classification of risk types
-- =============================================
CREATE TABLE core.risk_categories (
    category_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    category_name VARCHAR(100) NOT NULL UNIQUE,
    description TEXT,
    parent_category_id UUID REFERENCES erp_core.risk_categories(category_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- =============================================
-- Table: risk_register
-- Description: Core risk registry
-- =============================================
CREATE TABLE core.risk_register (
    risk_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    risk_name VARCHAR(200) NOT NULL,
    description TEXT NOT NULL,
    category_id UUID REFERENCES erp_core.risk_categories(category_id),
    owner_id UUID REFERENCES erp_core.users(user_id),
    inherent_impact INT CHECK (inherent_impact BETWEEN 1 AND 5),
    inherent_likelihood INT CHECK (inherent_likelihood BETWEEN 1 AND 5),
    residual_impact INT CHECK (residual_impact BETWEEN 1 AND 5),
    residual_likelihood INT CHECK (residual_likelihood BETWEEN 1 AND 5),
    risk_score DECIMAL(5,2) GENERATED ALWAYS AS (
        CASE
            WHEN residual_impact IS NOT NULL AND residual_likelihood IS NOT NULL
            THEN (residual_impact * residual_likelihood)
            WHEN inherent_impact IS NOT NULL AND inherent_likelihood IS NOT NULL
            THEN (inherent_impact * inherent_likelihood)
            ELSE NULL
        END
    ) STORED,
    status VARCHAR(20) CHECK (status IN ('Open', 'Mitigated', 'Accepted', 'Transferred', 'Closed')),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by UUID REFERENCES erp_core.users(user_id),
    updated_by UUID REFERENCES erp_core.users(user_id)
);

-- =============================================
-- Table: risk_assessments
-- Description: Risk assessment records
-- =============================================
CREATE TABLE core.risk_assessments (
    assessment_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    risk_id UUID REFERENCES erp_core.risk_register(risk_id) ON DELETE CASCADE,
    assessment_date DATE NOT NULL,
    assessor_id UUID REFERENCES erp_core.users(user_id),
    assessment_method VARCHAR(50) CHECK (assessment_method IN ('Qualitative', 'Quantitative', 'Hybrid')),
    assessment_notes TEXT,
    next_assessment_date DATE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- =============================================
-- Table: key_risk_indicators
-- Description: Key Risk Indicators tracking
-- =============================================
CREATE TABLE core.key_risk_indicators (
    kri_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    kri_name VARCHAR(200) NOT NULL,
    description TEXT,
    risk_id UUID REFERENCES erp_core.risk_register(risk_id),
    measurement_unit VARCHAR(50) NOT NULL,
    target_value DECIMAL(10,2),
    threshold_min DECIMAL(10,2),
    threshold_max DECIMAL(10,2),
    current_value DECIMAL(10,2),
    data_source VARCHAR(200),
    refresh_frequency VARCHAR(50) CHECK (refresh_frequency IN ('Daily', 'Weekly', 'Monthly', 'Quarterly', 'Real-time')),
    owner_id UUID REFERENCES erp_core.users(user_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- =============================================
-- Table: kri_measurements
-- Description: Historical KRI measurements
-- =============================================
CREATE TABLE core.kri_measurements (
    measurement_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    kri_id UUID REFERENCES erp_core.key_risk_indicators(kri_id) ON DELETE CASCADE,
    measured_value DECIMAL(10,2) NOT NULL,
    measurement_date TIMESTAMP NOT NULL,
    notes TEXT,
    measured_by UUID REFERENCES erp_core.users(user_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- =============================================
-- Table: risk_mitigation_actions
-- Description: Actions to mitigate risks
-- =============================================
CREATE TABLE core.risk_mitigation_actions (
    action_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    risk_id UUID REFERENCES erp_core.risk_register(risk_id) ON DELETE CASCADE,
    action_description TEXT NOT NULL,
    action_owner_id UUID REFERENCES erp_core.users(user_id),
    due_date DATE,
    completion_date DATE,
    status VARCHAR(20) CHECK (status IN ('Not Started', 'In Progress', 'Completed', 'Delayed', 'Cancelled')),
    effectiveness_rating INT CHECK (effectiveness_rating BETWEEN 1 AND 5),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by UUID REFERENCES erp_core.users(user_id),
    updated_by UUID REFERENCES erp_core.users(user_id)
);

-- =============================================
-- Table: incidents
-- Description: Risk incident tracking
-- =============================================
CREATE TABLE core.incidents (
    incident_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    incident_name VARCHAR(200) NOT NULL,
    description TEXT NOT NULL,
    risk_id UUID REFERENCES erp_core.risk_register(risk_id),
    incident_date TIMESTAMP NOT NULL,
    detected_date TIMESTAMP NOT NULL,
    resolved_date TIMESTAMP,
    severity VARCHAR(20) CHECK (severity IN ('Low', 'Medium', 'High', 'Critical')),
    status VARCHAR(20) CHECK (status IN ('Open', 'Investigating', 'Resolved', 'Closed')),
    root_cause TEXT,
    impact_assessment TEXT,
    lessons_learned TEXT,
    reporter_id UUID REFERENCES erp_core.users(user_id),
    assigned_to UUID REFERENCES erp_core.users(user_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);



-- =============================================
-- Table: risk_dashboards
-- Description: Saved dashboard configurations
-- =============================================
CREATE TABLE analytics.risk_dashboards (
    dashboard_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    dashboard_name VARCHAR(200) NOT NULL,
    description TEXT,
    layout_config JSONB NOT NULL,
    is_default BOOLEAN DEFAULT FALSE,
    created_by UUID REFERENCES erp_core.users(user_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);


-- =============================================
-- Table: reports
-- Description: Report definitions and history
-- =============================================
CREATE TABLE analytics.reports (
    report_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    report_name VARCHAR(200) NOT NULL,
    description TEXT,
    report_type VARCHAR(50) CHECK (report_type IN ('Risk', 'Compliance', 'Incident', 'Third Party', 'Executive')),
    frequency VARCHAR(50) CHECK (frequency IN ('Ad-hoc', 'Daily', 'Weekly', 'Monthly', 'Quarterly', 'Annual')),
    last_generated TIMESTAMP,
    next_generation TIMESTAMP,
    created_by UUID REFERENCES erp_core.users(user_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- =============================================
-- Table: report_parameters
-- Description: Parameters for report generation
-- =============================================
CREATE TABLE analytics.report_parameters (
    parameter_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    report_id UUID REFERENCES erp_analytics.reports(report_id) ON DELETE CASCADE,
    parameter_name VARCHAR(100) NOT NULL,
    parameter_type VARCHAR(50) NOT NULL,
    default_value TEXT,
    is_required BOOLEAN DEFAULT TRUE,
    display_order INT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- =============================================
-- Table: report_executions
-- Description: Report execution history
-- =============================================
CREATE TABLE analytics.report_executions (
    execution_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    report_id UUID REFERENCES erp_analytics.reports(report_id),
    execution_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    executed_by UUID REFERENCES erp_core.users(user_id),
    parameters JSONB,
    status VARCHAR(20) CHECK (status IN ('Pending', 'Running', 'Completed', 'Failed')),
    result_location TEXT,
    error_message TEXT,
    duration_seconds INT
);

-- Organizations/Departments
CREATE TABLE core.organizations (
    org_id SERIAL PRIMARY KEY,
    org_name VARCHAR(255) NOT NULL,
    org_code VARCHAR(50) UNIQUE NOT NULL,
    parent_org_id INTEGER REFERENCES core.organizations(org_id),
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Users/Employees
CREATE TABLE core.users (
    user_id SERIAL PRIMARY KEY,
    username VARCHAR(100) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    first_name VARCHAR(100) NOT NULL,
    last_name VARCHAR(100) NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- User-Organization mapping
CREATE TABLE core.user_org_mapping (
    mapping_id SERIAL PRIMARY KEY,
    user_id INTEGER NOT NULL REFERENCES core.users(user_id),
    org_id INTEGER NOT NULL REFERENCES core.organizations(org_id),
    role VARCHAR(100) NOT NULL,
    is_primary BOOLEAN DEFAULT FALSE,
    UNIQUE(user_id, org_id)
);

-- Documents repository
CREATE TABLE core.documents (
    doc_id SERIAL PRIMARY KEY,
    doc_name VARCHAR(255) NOT NULL,
    doc_type VARCHAR(100) NOT NULL,
    file_path VARCHAR(512) NOT NULL,
    file_size INTEGER NOT NULL,
    upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    uploaded_by INTEGER REFERENCES core.users(user_id),
    is_active BOOLEAN DEFAULT TRUE
);

-- =============================================
-- Table: regulations
-- Description: Regulatory requirements
-- =============================================
CREATE TABLE compliance.regulations (
    regulation_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    regulation_name VARCHAR(200) NOT NULL,
    short_name VARCHAR(50),
    jurisdiction VARCHAR(100),
    description TEXT,
    effective_date DATE,
    last_updated DATE,
    website_url TEXT,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- =============================================
-- Table: compliance_requirements
-- Description: Specific requirements from regulations
-- =============================================
CREATE TABLE compliance.compliance_requirements (
    requirement_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    regulation_id UUID REFERENCES erp_compliance.regulations(regulation_id) ON DELETE CASCADE,
    requirement_code VARCHAR(50) NOT NULL,
    description TEXT NOT NULL,
    category VARCHAR(100),
    implementation_guidance TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (regulation_id, requirement_code)
);

-- =============================================
-- Table: controls
-- Description: Controls to meet requirements
-- =============================================
CREATE TABLE compliance.controls (
    control_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    control_name VARCHAR(200) NOT NULL,
    description TEXT,
    control_type VARCHAR(50) CHECK (control_type IN ('Preventive', 'Detective', 'Corrective', 'Compensating')),
    frequency VARCHAR(50) CHECK (frequency IN ('Continuous', 'Daily', 'Weekly', 'Monthly', 'Quarterly', 'Annual', 'Event-driven')),
    owner_id UUID REFERENCES erp_core.users(user_id),
    is_automated BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- =============================================
-- Table: requirement_controls
-- Description: Mapping requirements to controls
-- =============================================
CREATE TABLE compliance.requirement_controls (
    requirement_id UUID REFERENCES erp_compliance.compliance_requirements(requirement_id) ON DELETE CASCADE,
    control_id UUID REFERENCES erp_compliance.controls(control_id) ON DELETE CASCADE,
    mapping_notes TEXT,
    mapped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    mapped_by UUID REFERENCES erp_core.users(user_id),
    PRIMARY KEY (requirement_id, control_id)
);

-- =============================================
-- Table: control_assessments
-- Description: Control effectiveness assessments
-- =============================================
CREATE TABLE compliance.control_assessments (
    assessment_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    control_id UUID REFERENCES erp_compliance.controls(control_id) ON DELETE CASCADE,
    assessment_date DATE NOT NULL,
    assessed_by UUID REFERENCES erp_core.users(user_id),
    effectiveness VARCHAR(20) CHECK (effectiveness IN ('Effective', 'Partially Effective', 'Ineffective', 'Not Assessed')),
    assessment_notes TEXT,
    next_assessment_date DATE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);


-- =============================================
-- Table: data_dictionary
-- Description: Business glossary and data definitions
-- =============================================
CREATE TABLE compliance.data_dictionary (
    term_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    term_name VARCHAR(200) NOT NULL,
    definition TEXT NOT NULL,
    business_meaning TEXT,
    data_type VARCHAR(50),
    allowed_values TEXT,
    related_terms TEXT[],
    steward_id UUID REFERENCES erp_core.users(user_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (term_name)
);


-- =============================================
-- Table: data_lineage
-- Description: Data lineage tracking
-- =============================================
CREATE TABLE compliance.data_lineage (
    lineage_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    source_entity VARCHAR(200) NOT NULL,
    source_attribute VARCHAR(200) NOT NULL,
    target_entity VARCHAR(200) NOT NULL,
    target_attribute VARCHAR(200) NOT NULL,
    transformation_logic TEXT,
    frequency VARCHAR(50),
    last_execution TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- =============================================
-- Table: data_quality_rules
-- Description: Data quality validation rules
-- =============================================
CREATE TABLE compliance.data_quality_rules (
    rule_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    rule_name VARCHAR(200) NOT NULL,
    description TEXT,
    entity_name VARCHAR(200) NOT NULL,
    attribute_name VARCHAR(200) NOT NULL,
    rule_expression TEXT NOT NULL,
    severity VARCHAR(20) CHECK (severity IN ('Critical', 'High', 'Medium', 'Low')),
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- =============================================
-- Table: data_quality_results
-- Description: Data quality validation results
-- =============================================
CREATE TABLE compliance.data_quality_results (
    result_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    rule_id UUID REFERENCES erp_compliance.data_quality_rules(rule_id) ON DELETE CASCADE,
    execution_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    entity_id TEXT,
    expected_value TEXT,
    actual_value TEXT,
    status VARCHAR(20) CHECK (status IN ('Pass', 'Fail', 'Error')),
    notes TEXT,
    resolved BOOLEAN DEFAULT FALSE,
    resolved_by UUID REFERENCES erp_core.users(user_id),
    resolved_at TIMESTAMP,
    resolution_notes TEXT
);
-- ========================================================================
-- SECTION 3: THIRD PARTY RISK MANAGMENT
-- ========================================================================
-- Module 1: Third-Party Risk Management (TPRM)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------



-- Create schemas for logical separation
CREATE SCHEMA IF NOT EXISTS tprm;
CREATE SCHEMA IF NOT EXISTS kpi; --kpi framework schema
CREATE SCHEMA IF NOT EXISTS rf; -- resilience framework schema
CREATE SCHEMA IF NOT EXISTS auditcore; -- audit core schema


-- Third-party vendors basic information
CREATE TABLE tprm.third_parties (
    party_id SERIAL PRIMARY KEY,
    legal_name VARCHAR(255) NOT NULL,
    dba_name VARCHAR(255),
    tax_id VARCHAR(50),
    registration_number VARCHAR(100),
    legal_entity_type VARCHAR(50),
    incorporation_date DATE,
    incorporation_country VARCHAR(100),
    industry_sector VARCHAR(100),
    primary_business_activity TEXT,
    company_website VARCHAR(255),
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_tax_id UNIQUE (tax_id)
);

-- Vendor contact information (1-to-many with third_parties)
CREATE TABLE tprm.party_contacts (
    contact_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id) ON DELETE CASCADE,
    contact_type VARCHAR(50) NOT NULL, -- 'primary', 'billing', 'technical', etc.
    first_name VARCHAR(100) NOT NULL,
    last_name VARCHAR(100) NOT NULL,
    job_title VARCHAR(100),
    email VARCHAR(255) NOT NULL,
    phone VARCHAR(50),
    mobile VARCHAR(50),
    is_primary BOOLEAN DEFAULT FALSE,
    notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Contract information with third parties
CREATE TABLE tprm.contracts (
    contract_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    contract_name VARCHAR(255) NOT NULL,
    contract_reference VARCHAR(100) NOT NULL,
    description TEXT,
    start_date DATE NOT NULL,
    end_date DATE,
    auto_renewal BOOLEAN DEFAULT FALSE,
    contract_value NUMERIC(15, 2),
    currency VARCHAR(3) DEFAULT 'USD',
    contract_type VARCHAR(50) NOT NULL, -- 'services', 'software', 'consulting', etc.
    responsible_manager VARCHAR(255),
    contract_status VARCHAR(50) DEFAULT 'active', -- 'active', 'terminated', 'expired'
    document_location VARCHAR(255),
    risk_level VARCHAR(20), -- 'low', 'medium', 'high', 'critical'
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_contract_reference UNIQUE (contract_reference)
);

-- Risk assessment framework
CREATE TABLE tprm.risk_assessments (
    assessment_id SERIAL PRIMARY KEY,
    assessment_name VARCHAR(255) NOT NULL,
    description TEXT,
    assessment_framework VARCHAR(100) NOT NULL, -- 'ISO 27001', 'NIST', 'Custom', etc.
    version VARCHAR(50) NOT NULL,
    effective_date DATE NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Assessment questions/criteria
CREATE TABLE tprm.assessment_questions (
    question_id SERIAL PRIMARY KEY,
    assessment_id INTEGER NOT NULL REFERENCES risk_assessments(assessment_id) ON DELETE CASCADE,
    section VARCHAR(100) NOT NULL,
    question_number VARCHAR(20) NOT NULL,
    question_text TEXT NOT NULL,
    guidance TEXT,
    weight INTEGER NOT NULL DEFAULT 1,
    risk_domain VARCHAR(100) NOT NULL, -- 'cybersecurity', 'financial', 'compliance', etc.
    is_critical BOOLEAN DEFAULT FALSE,
    sort_order INTEGER NOT NULL,
    CONSTRAINT unique_question_number UNIQUE (assessment_id, question_number)
);

-- Actual assessments conducted on third parties
CREATE TABLE tprm.party_assessments (
    party_assessment_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    contract_id INTEGER REFERENCES contracts(contract_id),
    assessment_id INTEGER NOT NULL REFERENCES risk_assessments(assessment_id),
    assessment_date DATE NOT NULL,
    conducted_by VARCHAR(255) NOT NULL,
    next_assessment_date DATE,
    overall_score NUMERIC(5, 2),
    overall_risk_level VARCHAR(20), -- 'low', 'medium', 'high', 'critical'
    status VARCHAR(50) DEFAULT 'draft', -- 'draft', 'in-progress', 'completed', 'approved'
    notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Assessment responses
CREATE TABLE tprm.assessment_responses (
    response_id SERIAL PRIMARY KEY,
    party_assessment_id INTEGER NOT NULL REFERENCES party_assessments(party_assessment_id) ON DELETE CASCADE,
    question_id INTEGER NOT NULL REFERENCES assessment_questions(question_id),
    response_value VARCHAR(50) NOT NULL, -- 'yes', 'no', 'partial', 'n/a'
    score NUMERIC(5, 2) NOT NULL,
    comments TEXT,
    evidence_reference VARCHAR(255),
    supporting_document VARCHAR(255),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_assessment_question UNIQUE (party_assessment_id, question_id)
);

-- Issues identified during assessments
CREATE TABLE tprm.assessment_issues (
    issue_id SERIAL PRIMARY KEY,
    party_assessment_id INTEGER NOT NULL REFERENCES party_assessments(party_assessment_id) ON DELETE CASCADE,
    question_id INTEGER REFERENCES assessment_questions(question_id),
    issue_title VARCHAR(255) NOT NULL,
    description TEXT NOT NULL,
    severity VARCHAR(20) NOT NULL, -- 'low', 'medium', 'high', 'critical'
    category VARCHAR(100) NOT NULL,
    due_date DATE,
    status VARCHAR(50) DEFAULT 'open', -- 'open', 'in-progress', 'resolved', 'closed'
    resolution_date DATE,
    resolution_notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- KPI definitions
CREATE TABLE tprm.kpi_definitions (
    kpi_id SERIAL PRIMARY KEY,
    kpi_name VARCHAR(255) NOT NULL,
    description TEXT,
    category VARCHAR(100) NOT NULL, -- 'performance', 'financial', 'security', etc.
    measurement_unit VARCHAR(50) NOT NULL,
    target_value NUMERIC(15, 2),
    target_direction VARCHAR(10) NOT NULL, -- 'higher', 'lower', 'equal'
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_kpi_name UNIQUE (kpi_name)
);

-- KRM (Key Risk Metric) definitions
CREATE TABLE tprm.krm_definitions (
    krm_id SERIAL PRIMARY KEY,
    krm_name VARCHAR(255) NOT NULL,
    description TEXT,
    risk_domain VARCHAR(100) NOT NULL,
    measurement_unit VARCHAR(50) NOT NULL,
    threshold_value NUMERIC(15, 2),
    threshold_direction VARCHAR(10) NOT NULL, -- 'higher', 'lower', 'equal'
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_krm_name UNIQUE (krm_name)
);

-- KPI measurements for third parties
CREATE TABLE tprm.kpi_measurements (
    measurement_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    contract_id INTEGER REFERENCES contracts(contract_id),
    kpi_id INTEGER NOT NULL REFERENCES kpi_definitions(kpi_id),
    measurement_date DATE NOT NULL,
    measured_value NUMERIC(15, 2) NOT NULL,
    reporting_period VARCHAR(20) NOT NULL, -- 'daily', 'weekly', 'monthly', 'quarterly'
    data_source VARCHAR(255),
    notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_kpi_measurement UNIQUE (party_id, contract_id, kpi_id, measurement_date, reporting_period)
);

-- KRM measurements for third parties
CREATE TABLE tprm.krm_measurements (
    measurement_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    contract_id INTEGER REFERENCES contracts(contract_id),
    krm_id INTEGER NOT NULL REFERENCES krm_definitions(krm_id),
    measurement_date DATE NOT NULL,
    measured_value NUMERIC(15, 2) NOT NULL,
    reporting_period VARCHAR(20) NOT NULL, -- 'daily', 'weekly', 'monthly', 'quarterly'
    data_source VARCHAR(255),
    notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_krm_measurement UNIQUE (party_id, contract_id, krm_id, measurement_date, reporting_period)
);

-- Compliance requirements
CREATE TABLE tprm.compliance_requirements (
    requirement_id SERIAL PRIMARY KEY,
    regulation_standard VARCHAR(255) NOT NULL, -- 'GDPR', 'HIPAA', 'SOX', etc.
    requirement_code VARCHAR(100) NOT NULL,
    description TEXT NOT NULL,
    category VARCHAR(100) NOT NULL,
    jurisdiction VARCHAR(100),
    effective_date DATE,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_requirement_code UNIQUE (regulation_standard, requirement_code)
);

-- Third-party compliance mappings
CREATE TABLE tprm.party_compliance (
    compliance_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    contract_id INTEGER REFERENCES contracts(contract_id),
    requirement_id INTEGER NOT NULL REFERENCES compliance_requirements(requirement_id),
    compliance_status VARCHAR(50) NOT NULL, -- 'compliant', 'non-compliant', 'in-progress', 'exempt'
    evidence_reference VARCHAR(255),
    last_verified_date DATE,
    next_verification_date DATE,
    notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_party_compliance UNIQUE (party_id, contract_id, requirement_id)
);

-- Financial health indicators
CREATE TABLE tprm.financial_indicators (
    indicator_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    reporting_date DATE NOT NULL,
    revenue NUMERIC(15, 2),
    profit NUMERIC(15, 2),
    assets NUMERIC(15, 2),
    liabilities NUMERIC(15, 2),
    equity NUMERIC(15, 2),
    credit_rating VARCHAR(50),
    credit_score INTEGER,
    financial_health_score INTEGER,
    data_source VARCHAR(255),
    notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_financial_report UNIQUE (party_id, reporting_date)
);

-- Incident tracking
CREATE TABLE tprm.incidents (
    incident_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    contract_id INTEGER REFERENCES contracts(contract_id),
    incident_date TIMESTAMP WITH TIME ZONE NOT NULL,
    discovery_date TIMESTAMP WITH TIME ZONE NOT NULL,
    report_date TIMESTAMP WITH TIME ZONE NOT NULL,
    incident_type VARCHAR(100) NOT NULL,
    severity VARCHAR(20) NOT NULL, -- 'low', 'medium', 'high', 'critical'
    description TEXT NOT NULL,
    impact TEXT,
    root_cause TEXT,
    remediation_actions TEXT,
    status VARCHAR(50) DEFAULT 'open', -- 'open', 'investigating', 'resolved', 'closed'
    resolution_date TIMESTAMP WITH TIME ZONE,
    financial_impact NUMERIC(15, 2),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- View for high-risk third parties
CREATE VIEW tprm.high_risk_parties AS
SELECT
    tp.party_id,
    tp.legal_name,
    tp.industry_sector,
    pa.overall_risk_level,
    pa.assessment_date,
    COUNT(ai.issue_id) FILTER (WHERE ai.severity = 'critical') AS critical_issues,
    COUNT(ai.issue_id) FILTER (WHERE ai.severity = 'high') AS high_issues,
    c.contract_name,
    c.contract_status
FROM
    third_parties tp
JOIN
    party_assessments pa ON tp.party_id = pa.party_id
LEFT JOIN
    assessment_issues ai ON pa.party_assessment_id = ai.party_assessment_id
LEFT JOIN
    contracts c ON pa.contract_id = c.contract_id
WHERE
    pa.overall_risk_level IN ('high', 'critical')
    AND pa.status = 'completed'
GROUP BY
    tp.party_id, tp.legal_name, tp.industry_sector,
    pa.overall_risk_level, pa.assessment_date, c.contract_name, c.contract_status;

-- View for third-party performance dashboard
CREATE VIEW tprm.party_performance_dashboard AS
SELECT
    tp.party_id,
    tp.legal_name,
    tp.industry_sector,
    c.contract_name,
    c.contract_status,
    COUNT(DISTINCT km.measurement_id) AS kpi_measurements_count,
    AVG(CASE WHEN kd.target_direction = 'higher' THEN
            CASE WHEN km.measured_value >= kd.target_value THEN 1 ELSE 0 END
        WHEN kd.target_direction = 'lower' THEN
            CASE WHEN km.measured_value <= kd.target_value THEN 1 ELSE 0 END
        ELSE
            CASE WHEN km.measured_value = kd.target_value THEN 1 ELSE 0 END
        END) AS kpi_target_achievement_rate,
    COUNT(DISTINCT rm.measurement_id) AS krm_measurements_count,
    AVG(CASE WHEN rd.threshold_direction = 'higher' THEN
            CASE WHEN rm.measured_value > rd.threshold_value THEN 1 ELSE 0 END
        WHEN rd.threshold_direction = 'lower' THEN
            CASE WHEN rm.measured_value < rd.threshold_value THEN 1 ELSE 0 END
        ELSE 0 END) AS krm_threshold_breach_rate,
    MAX(fi.financial_health_score) AS latest_financial_score,
    COUNT(DISTINCT i.incident_id) FILTER (WHERE i.severity IN ('high', 'critical')) AS severe_incidents_count
FROM
    third_parties tp
LEFT JOIN
    contracts c ON tp.party_id = c.party_id
LEFT JOIN
    kpi_measurements km ON tp.party_id = km.party_id AND (c.contract_id IS NULL OR km.contract_id = c.contract_id)
LEFT JOIN
    kpi_definitions kd ON km.kpi_id = kd.kpi_id
LEFT JOIN
    krm_measurements rm ON tp.party_id = rm.party_id AND (c.contract_id IS NULL OR rm.contract_id = c.contract_id)
LEFT JOIN
    krm_definitions rd ON rm.krm_id = rd.krm_id
LEFT JOIN
    financial_indicators fi ON tp.party_id = fi.party_id
LEFT JOIN
    incidents i ON tp.party_id = i.party_id AND (c.contract_id IS NULL OR i.contract_id = c.contract_id)
GROUP BY
    tp.party_id, tp.legal_name, tp.industry_sector, c.contract_name, c.contract_status;

-- View for compliance status by regulation
CREATE VIEW tprm.compliance_status_by_regulation AS
SELECT
    cr.regulation_standard,
    cr.requirement_code,
    cr.description,
    COUNT(pc.compliance_id) FILTER (WHERE pc.compliance_status = 'compliant') AS compliant_count,
    COUNT(pc.compliance_id) FILTER (WHERE pc.compliance_status = 'non-compliant') AS non_compliant_count,
    COUNT(pc.compliance_id) FILTER (WHERE pc.compliance_status = 'in-progress') AS in_progress_count,
    COUNT(pc.compliance_id) AS total_count,
    ROUND(COUNT(pc.compliance_id) FILTER (WHERE pc.compliance_status = 'compliant') * 100.0 /
          NULLIF(COUNT(pc.compliance_id), 0), 2) AS compliance_percentage
FROM
    compliance_requirements cr
LEFT JOIN
    party_compliance pc ON cr.requirement_id = pc.requirement_id
GROUP BY
    cr.regulation_standard, cr.requirement_code, cr.description
ORDER BY
    cr.regulation_standard, cr.requirement_code;

-- View for upcoming assessments and renewals
CREATE VIEW tprm.upcoming_activities AS
SELECT
    tp.party_id,
    tp.legal_name,
    'Assessment' AS activity_type,
    pa.assessment_date AS start_date,
    pa.next_assessment_date AS due_date,
    pa.overall_risk_level,
    c.contract_name
FROM
    third_parties tp
JOIN
    party_assessments pa ON tp.party_id = pa.party_id
LEFT JOIN
    contracts c ON pa.contract_id = c.contract_id
WHERE
    pa.status = 'completed'
    AND pa.next_assessment_date BETWEEN CURRENT_DATE AND (CURRENT_DATE + INTERVAL '90 days')

UNION ALL

SELECT
    tp.party_id,
    tp.legal_name,
    'Contract Renewal' AS activity_type,
    c.start_date,
    c.end_date AS due_date,
    NULL AS overall_risk_level,
    c.contract_name
FROM
    third_parties tp
JOIN
    contracts c ON tp.party_id = c.party_id
WHERE
    c.end_date BETWEEN CURRENT_DATE AND (CURRENT_DATE + INTERVAL '90 days')
    AND c.contract_status = 'active'
ORDER BY
    due_date;

--Materialized views
-- Materialized view for third-party risk summary (refreshed daily)
CREATE MATERIALIZED VIEW tprm.mv_party_risk_summary AS
SELECT
    tp.party_id,
    tp.legal_name,
    tp.industry_sector,
    COUNT(DISTINCT c.contract_id) AS active_contracts,
    MAX(pa.overall_risk_level) AS highest_risk_level,
    MAX(pa.assessment_date) AS latest_assessment_date,
    COUNT(DISTINCT ai.issue_id) FILTER (WHERE ai.severity = 'critical' AND ai.status != 'closed') AS open_critical_issues,
    COUNT(DISTINCT ai.issue_id) FILTER (WHERE ai.severity = 'high' AND ai.status != 'closed') AS open_high_issues,
    COUNT(DISTINCT i.incident_id) FILTER (WHERE i.severity IN ('high', 'critical')
        AND i.incident_date >= (CURRENT_DATE - INTERVAL '12 months')) AS severe_incidents_12m,
    MAX(fi.financial_health_score) AS latest_financial_score,
    ROUND(AVG(CASE WHEN krm.measured_value > kd.threshold_value AND kd.threshold_direction = 'higher' THEN 1
              WHEN krm.measured_value < kd.threshold_value AND kd.threshold_direction = 'lower' THEN 1
              ELSE 0 END), 2) AS risk_metric_breach_rate
FROM
    third_parties tp
LEFT JOIN
    contracts c ON tp.party_id = c.party_id AND c.contract_status = 'active'
LEFT JOIN
    party_assessments pa ON tp.party_id = pa.party_id AND pa.status = 'completed'
LEFT JOIN
    assessment_issues ai ON pa.party_assessment_id = ai.party_assessment_id AND ai.status != 'closed'
LEFT JOIN
    incidents i ON tp.party_id = i.party_id
LEFT JOIN
    financial_indicators fi ON tp.party_id = fi.party_id
LEFT JOIN
    krm_measurements krm ON tp.party_id = krm.party_id
LEFT JOIN
    krm_definitions kd ON krm.krm_id = kd.krm_id
GROUP BY
    tp.party_id, tp.legal_name, tp.industry_sector
WITH DATA;

-- Create index on the materialized view
CREATE INDEX idx_mv_party_risk_summary_party_id ON mv_party_risk_summary(party_id);
CREATE INDEX idx_mv_party_risk_summary_risk_level ON mv_party_risk_summary(highest_risk_level);

-- Materialized view for KPI performance trends (refreshed weekly)
CREATE MATERIALIZED VIEW tprm.mv_kpi_performance_trends AS
SELECT
    tp.party_id,
    tp.legal_name,
    kd.kpi_id,
    kd.kpi_name,
    kd.category,
    DATE_TRUNC('month', km.measurement_date) AS month,
    AVG(km.measured_value) AS avg_value,
    kd.target_value,
    kd.target_direction,
    COUNT(km.measurement_id) AS measurement_count,
    CASE WHEN kd.target_direction = 'higher' THEN
            AVG(CASE WHEN km.measured_value >= kd.target_value THEN 1 ELSE 0 END)
        WHEN kd.target_direction = 'lower' THEN
            AVG(CASE WHEN km.measured_value <= kd.target_value THEN 1 ELSE 0 END)
        ELSE
            AVG(CASE WHEN km.measured_value = kd.target_value THEN 1 ELSE 0 END)
        END AS target_achievement_rate
FROM
    kpi_measurements km
JOIN
    kpi_definitions kd ON km.kpi_id = kd.kpi_id
JOIN
    third_parties tp ON km.party_id = tp.party_id
WHERE
    km.measurement_date >= (CURRENT_DATE - INTERVAL '24 months')
GROUP BY
    tp.party_id, tp.legal_name, kd.kpi_id, kd.kpi_name, kd.category,
    DATE_TRUNC('month', km.measurement_date), kd.target_value, kd.target_direction
WITH DATA;

-- Create index on the materialized view
CREATE INDEX idx_mv_kpi_trends_party_id ON mv_kpi_performance_trends(party_id);
CREATE INDEX idx_mv_kpi_trends_kpi_id ON mv_kpi_performance_trends(kpi_id);
CREATE INDEX idx_mv_kpi_trends_month ON mv_kpi_performance_trends(month);

--stored procedures
-- Procedure to calculate and update assessment scores
CREATE OR REPLACE PROCEDURE tprm.update_assessment_score(
    p_party_assessment_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_total_score NUMERIC;
    v_max_score NUMERIC;
    v_weighted_score NUMERIC;
    v_risk_level VARCHAR(20);
    v_critical_issues INTEGER;
BEGIN
    -- Calculate weighted score
    SELECT
        SUM(ar.score * aq.weight),
        SUM(aq.weight)
    INTO
        v_weighted_score, v_max_score
    FROM
        assessment_responses ar
    JOIN
        assessment_questions aq ON ar.question_id = aq.question_id
    WHERE
        ar.party_assessment_id = p_party_assessment_id;

    -- Calculate percentage score
    IF v_max_score > 0 THEN
        v_total_score := (v_weighted_score / v_max_score) * 100;
    ELSE
        v_total_score := 0;
    END IF;

    -- Count critical issues
    SELECT COUNT(*)
    INTO v_critical_issues
    FROM assessment_issues
    WHERE party_assessment_id = p_party_assessment_id
    AND severity = 'critical'
    AND status != 'closed';

    -- Determine risk level based on score and critical issues
    IF v_critical_issues > 0 THEN
        v_risk_level := 'critical';
    ELSIF v_total_score < 50 THEN
        v_risk_level := 'high';
    ELSIF v_total_score < 75 THEN
        v_risk_level := 'medium';
    ELSE
        v_risk_level := 'low';
    END IF;

    -- Update the assessment record
    UPDATE party_assessments
    SET
        overall_score = v_total_score,
        overall_risk_level = v_risk_level,
        updated_at = CURRENT_TIMESTAMP
    WHERE
        party_assessment_id = p_party_assessment_id;

    COMMIT;
END;
$$;

-- Procedure to add a new third party with initial assessment
CREATE OR REPLACE PROCEDURE tprm.add_new_third_party(
    p_legal_name VARCHAR(255),
    p_industry_sector VARCHAR(100),
    p_contact_first_name VARCHAR(100),
    p_contact_last_name VARCHAR(100),
    p_contact_email VARCHAR(255),
    p_contact_phone VARCHAR(50),
    p_assessment_framework VARCHAR(100),
    OUT p_party_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_contact_id INTEGER;
    v_assessment_id INTEGER;
    v_party_assessment_id INTEGER;
BEGIN
    -- Insert the third party
    INSERT INTO third_parties (legal_name, industry_sector)
    VALUES (p_legal_name, p_industry_sector)
    RETURNING party_id INTO p_party_id;

    -- Add primary contact
    INSERT INTO party_contacts (
        party_id,
        contact_type,
        first_name,
        last_name,
        email,
        phone,
        is_primary
    )
    VALUES (
        p_party_id,
        'primary',
        p_contact_first_name,
        p_contact_last_name,
        p_contact_email,
        p_contact_phone,
        TRUE
    )
    RETURNING contact_id INTO v_contact_id;

    -- Find the latest active assessment framework
    SELECT assessment_id INTO v_assessment_id
    FROM risk_assessments
    WHERE assessment_framework = p_assessment_framework
    AND is_active = TRUE
    ORDER BY effective_date DESC
    LIMIT 1;

    IF v_assessment_id IS NULL THEN
        RAISE EXCEPTION 'No active assessment framework found for %', p_assessment_framework;
    END IF;

    -- Create initial assessment
    INSERT INTO party_assessments (
        party_id,
        assessment_id,
        assessment_date,
        conducted_by,
        status
    )
    VALUES (
        p_party_id,
        v_assessment_id,
        CURRENT_DATE,
        'System',
        'draft'
    )
    RETURNING party_assessment_id INTO v_party_assessment_id;

    COMMIT;
END;
$$;

-- Procedure to refresh materialized views
CREATE OR REPLACE PROCEDURE tprm.refresh_tprm_materialized_views()
LANGUAGE plpgsql
AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY mv_party_risk_summary;
    REFRESH MATERIALIZED VIEW CONCURRENTLY mv_kpi_performance_trends;
    COMMIT;
END;
$$;

-- Procedure to escalate overdue issues
CREATE OR REPLACE PROCEDURE tprm.escalate_overdue_issues()
LANGUAGE plpgsql
AS $$
BEGIN
    -- Update severity for high issues overdue by 30+ days
    UPDATE assessment_issues
    SET severity = 'critical',
        updated_at = CURRENT_TIMESTAMP
    WHERE severity = 'high'
    AND status = 'open'
    AND due_date < (CURRENT_DATE - INTERVAL '30 days');

    -- Update severity for medium issues overdue by 60+ days
    UPDATE assessment_issues
    SET severity = 'high',
        updated_at = CURRENT_TIMESTAMP
    WHERE severity = 'medium'
    AND status = 'open'
    AND due_date < (CURRENT_DATE - INTERVAL '60 days');

    COMMIT;
END;
$$;

-- Procedure to get third-party risk profile
CREATE OR REPLACE FUNCTION tprm.get_party_risk_profile(p_party_id INTEGER)
RETURNS TABLE (
    legal_name VARCHAR(255),
    industry_sector VARCHAR(100),
    active_contracts BIGINT,
    latest_assessment_date DATE,
    overall_risk_level VARCHAR(20),
    overall_score NUMERIC(5, 2),
    open_critical_issues BIGINT,
    open_high_issues BIGINT,
    severe_incidents_12m BIGINT,
    financial_health_score INTEGER,
    risk_metric_breach_rate NUMERIC(5, 2)
LANGUAGE plpgsql
AS $$
BEGIN
    RETURN QUERY
    SELECT
        prs.legal_name,
        prs.industry_sector,
        prs.active_contracts,
        prs.latest_assessment_date,
        prs.highest_risk_level,
        pa.overall_score,
        prs.open_critical_issues,
        prs.open_high_issues,
        prs.severe_incidents_12m,
        prs.latest_financial_score,
        prs.risk_metric_breach_rate
    FROM
        mv_party_risk_summary prs
    LEFT JOIN
        party_assessments pa ON prs.party_id = pa.party_id
            AND pa.assessment_date = prs.latest_assessment_date
    WHERE
        prs.party_id = p_party_id;
END;
$$;

--usage
-- Add a new third party with initial assessment
CALL add_new_third_party(
    'Acme Corporation',
    'Manufacturing',
    'John',
    'Doe',
    'john.doe@acme.com',
    '+1-555-123-4567',
    'ISO 27001',
    NULL
);

-- Update an assessment score after entering responses
CALL update_assessment_score(123);

-- Get risk profile for a third party
SELECT * FROM get_party_risk_profile(456);

-- Refresh materialized views (typically scheduled)
CALL refresh_tprm_materialized_views();

-- View high-risk parties
SELECT * FROM high_risk_parties ORDER BY overall_risk_level DESC;

-- Check compliance status by regulation
SELECT * FROM compliance_status_by_regulation
WHERE regulation_standard = 'GDPR';

-- Table for TPRM program governance
CREATE TABLE tprm.tprm_program (
    program_id SERIAL PRIMARY KEY,
    program_name VARCHAR(255) NOT NULL,
    version VARCHAR(50) NOT NULL,
    implementation_date DATE NOT NULL,
    maturity_level VARCHAR(50),
    next_review_date DATE,
    policy_document VARCHAR(255),
    risk_appetite_statement TEXT,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Table for business continuity and resilience
CREATE TABLE tprm.business_continuity_plans (
    bcp_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    plan_name VARCHAR(255) NOT NULL,
    plan_type VARCHAR(100) NOT NULL, -- 'pandemic', 'disaster', 'cyber', etc.
    plan_document VARCHAR(255),
    last_test_date DATE,
    next_test_date DATE,
    recovery_time_objective NUMERIC(10,2), -- in hours
    recovery_point_objective NUMERIC(10,2), -- in hours
    test_results TEXT,
    approval_status VARCHAR(50) DEFAULT 'draft',
    approved_by VARCHAR(255),
    approved_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Table for incident management
CREATE TABLE tprm.incident_management (
    incident_mgmt_id SERIAL PRIMARY KEY,
    incident_id INTEGER REFERENCES incidents(incident_id),
    playbook_used VARCHAR(255),
    response_team VARCHAR(255),
    containment_time INTERVAL,
    eradication_time INTERVAL,
    recovery_time INTERVAL,
    lessons_learned TEXT,
    follow_up_actions TEXT,
    status VARCHAR(50) DEFAULT 'open',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Table for training and awareness
CREATE TABLE tprm.training_programs (
    training_id SERIAL PRIMARY KEY,
    program_name VARCHAR(255) NOT NULL,
    target_audience VARCHAR(100) NOT NULL, -- 'internal', 'vendors', 'both'
    training_type VARCHAR(100) NOT NULL, -- 'onboarding', 'refresher', 'specialized'
    frequency VARCHAR(50), -- 'annual', 'quarterly', 'adhoc'
    delivery_method VARCHAR(100), -- 'online', 'in-person', 'hybrid'
    completion_criteria TEXT,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Table for training participation
CREATE TABLE tprm.training_participation (
    participation_id SERIAL PRIMARY KEY,
    training_id INTEGER NOT NULL REFERENCES training_programs(training_id),
    party_id INTEGER REFERENCES third_parties(party_id),
    user_id INTEGER, -- References internal users table (not shown in this schema)
    participation_date DATE NOT NULL,
    completion_date DATE,
    score NUMERIC(5,2),
    feedback TEXT,
    certificate_issued BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Table for risk quantification
CREATE TABLE tprm.risk_quantification (
    quantification_id SERIAL PRIMARY KEY,
    party_id INTEGER REFERENCES third_parties(party_id),
    model_name VARCHAR(255) NOT NULL,
    model_type VARCHAR(100) NOT NULL, -- 'VaR', 'scenario', 'heatmap', etc.
    calculation_date DATE NOT NULL,
    risk_value NUMERIC(15,2),
    risk_currency VARCHAR(3) DEFAULT 'USD',
    confidence_level NUMERIC(5,2), -- percentage
    time_horizon VARCHAR(50), -- '30d', '1y', '5y'
    assumptions TEXT,
    validation_status VARCHAR(50) DEFAULT 'draft',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Table for emerging risks
CREATE TABLE tprm.emerging_risks (
    risk_id SERIAL PRIMARY KEY,
    risk_name VARCHAR(255) NOT NULL,
    risk_category VARCHAR(100) NOT NULL, -- 'technology', 'geopolitical', 'climate', etc.
    description TEXT NOT NULL,
    potential_impact TEXT,
    likelihood VARCHAR(20), -- 'low', 'medium', 'high', 'critical'
    time_frame VARCHAR(50), -- 'short', 'medium', 'long'
    monitoring_indicators TEXT,
    mitigation_strategies TEXT,
    owner VARCHAR(255),
    status VARCHAR(50) DEFAULT 'identified',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Table for TPRM program governance
CREATE TABLE tprm.tprm_program (
    program_id SERIAL PRIMARY KEY,
    program_name VARCHAR(255) NOT NULL,
    version VARCHAR(50) NOT NULL,
    implementation_date DATE NOT NULL,
    maturity_level VARCHAR(50),
    next_review_date DATE,
    policy_document VARCHAR(255),
    risk_appetite_statement TEXT,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Table for business continuity and resilience
CREATE TABLE tprm.business_continuity_plans (
    bcp_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    plan_name VARCHAR(255) NOT NULL,
    plan_type VARCHAR(100) NOT NULL, -- 'pandemic', 'disaster', 'cyber', etc.
    plan_document VARCHAR(255),
    last_test_date DATE,
    next_test_date DATE,
    recovery_time_objective NUMERIC(10,2), -- in hours
    recovery_point_objective NUMERIC(10,2), -- in hours
    test_results TEXT,
    approval_status VARCHAR(50) DEFAULT 'draft',
    approved_by VARCHAR(255),
    approved_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Table for incident management
CREATE TABLE tprm.incident_management (
    incident_mgmt_id SERIAL PRIMARY KEY,
    incident_id INTEGER REFERENCES incidents(incident_id),
    playbook_used VARCHAR(255),
    response_team VARCHAR(255),
    containment_time INTERVAL,
    eradication_time INTERVAL,
    recovery_time INTERVAL,
    lessons_learned TEXT,
    follow_up_actions TEXT,
    status VARCHAR(50) DEFAULT 'open',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Table for training and awareness
CREATE TABLE tprm.training_programs (
    training_id SERIAL PRIMARY KEY,
    program_name VARCHAR(255) NOT NULL,
    target_audience VARCHAR(100) NOT NULL, -- 'internal', 'vendors', 'both'
    training_type VARCHAR(100) NOT NULL, -- 'onboarding', 'refresher', 'specialized'
    frequency VARCHAR(50), -- 'annual', 'quarterly', 'adhoc'
    delivery_method VARCHAR(100), -- 'online', 'in-person', 'hybrid'
    completion_criteria TEXT,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Table for training participation
CREATE TABLE tprm.training_participation (
    participation_id SERIAL PRIMARY KEY,
    training_id INTEGER NOT NULL REFERENCES training_programs(training_id),
    party_id INTEGER REFERENCES third_parties(party_id),
    user_id INTEGER, -- References internal users table (not shown in this schema)
    participation_date DATE NOT NULL,
    completion_date DATE,
    score NUMERIC(5,2),
    feedback TEXT,
    certificate_issued BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Table for risk quantification
CREATE TABLE tprm.risk_quantification (
    quantification_id SERIAL PRIMARY KEY,
    party_id INTEGER REFERENCES third_parties(party_id),
    model_name VARCHAR(255) NOT NULL,
    model_type VARCHAR(100) NOT NULL, -- 'VaR', 'scenario', 'heatmap', etc.
    calculation_date DATE NOT NULL,
    risk_value NUMERIC(15,2),
    risk_currency VARCHAR(3) DEFAULT 'USD',
    confidence_level NUMERIC(5,2), -- percentage
    time_horizon VARCHAR(50), -- '30d', '1y', '5y'
    assumptions TEXT,
    validation_status VARCHAR(50) DEFAULT 'draft',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Table for emerging risks
CREATE TABLE tprm.emerging_risks (
    risk_id SERIAL PRIMARY KEY,
    risk_name VARCHAR(255) NOT NULL,
    risk_category VARCHAR(100) NOT NULL, -- 'technology', 'geopolitical', 'climate', etc.
    description TEXT NOT NULL,
    potential_impact TEXT,
    likelihood VARCHAR(20), -- 'low', 'medium', 'high', 'critical'
    time_frame VARCHAR(50), -- 'short', 'medium', 'long'
    monitoring_indicators TEXT,
    mitigation_strategies TEXT,
    owner VARCHAR(255),
    status VARCHAR(50) DEFAULT 'identified',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Enhanced contracts table with additional fields
ALTER TABLE tprm.contracts ADD COLUMN subcontractor_oversight BOOLEAN DEFAULT FALSE;
ALTER TABLE tprm.contracts ADD COLUMN right_to_audit BOOLEAN DEFAULT FALSE;
ALTER TABLE tprm.contracts ADD COLUMN termination_clauses TEXT;
ALTER TABLE tprm.contracts ADD COLUMN data_ownership_terms TEXT;
ALTER TABLE tprm.contracts ADD COLUMN force_majeure_provisions TEXT;
ALTER TABLE tprm.contracts ADD COLUMN liability_terms TEXT;
ALTER TABLE tprm.contracts ADD COLUMN ip_protection_terms TEXT;

-- Enhanced compliance_requirements table
ALTER TABLE tprm.compliance_requirements ADD COLUMN regulation_category VARCHAR(100);
ALTER TABLE tprm.compliance_requirements ADD COLUMN jurisdiction VARCHAR(100);
ALTER TABLE tprm.compliance_requirements ADD COLUMN effective_date DATE;
ALTER TABLE tprm.compliance_requirements ADD COLUMN review_frequency VARCHAR(50);

-- Enhanced incidents table
ALTER TABLE tprm.incidents ADD COLUMN incident_category VARCHAR(100);
ALTER TABLE tprm.incidents ADD COLUMN containment_time INTERVAL;
ALTER TABLE tprm.incidents ADD COLUMN root_cause_analysis TEXT;
ALTER TABLE tprm.incidents ADD COLUMN lessons_learned TEXT;


-----
--KPI Framework tables
-- KPI Framework
CREATE TABLE kpi.kpi_framework (
    framework_id SERIAL PRIMARY KEY,
    framework_name VARCHAR(255) NOT NULL,
    version VARCHAR(50) NOT NULL,
    effective_date DATE NOT NULL,
    review_frequency VARCHAR(50) NOT NULL,
    owner VARCHAR(255) NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- KPI Categories
CREATE TABLE kpi.kpi_categories (
    category_id SERIAL PRIMARY KEY,
    category_name VARCHAR(255) NOT NULL,
    description TEXT,
    weight NUMERIC(5,2),
    framework_id INTEGER REFERENCES kpi_framework(framework_id),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Detailed KPI Definitions (enhanced from previous kpi_definitions)
CREATE TABLE kpi.kpi_definitions (
    kpi_id SERIAL PRIMARY KEY,
    kpi_name VARCHAR(255) NOT NULL,
    description TEXT,
    category_id INTEGER REFERENCES kpi_categories(category_id),
    measurement_unit VARCHAR(50) NOT NULL,
    measurement_method TEXT NOT NULL,
    data_source VARCHAR(255) NOT NULL,
    reporting_frequency VARCHAR(50) NOT NULL,
    target_value NUMERIC(15,2),
    min_value NUMERIC(15,2),
    max_value NUMERIC(15,2),
    target_direction VARCHAR(10) NOT NULL, -- 'higher', 'lower', 'equal'
    is_critical BOOLEAN DEFAULT FALSE,
    threshold_breach_actions TEXT,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_kpi_name UNIQUE (kpi_name)
);

-- KPI Measurement Periods
CREATE TABLE kpi.kpi_periods (
    period_id SERIAL PRIMARY KEY,
    period_name VARCHAR(255) NOT NULL,
    start_date DATE NOT NULL,
    end_date DATE NOT NULL,
    is_open BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Enhanced KPI Measurements (from previous kpi_measurements)
CREATE TABLE kpi.kpi_measurements (
    measurement_id SERIAL PRIMARY KEY,
    kpi_id INTEGER NOT NULL REFERENCES kpi_definitions(kpi_id),
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    contract_id INTEGER REFERENCES contracts(contract_id),
    period_id INTEGER REFERENCES kpi_periods(period_id),
    measurement_date DATE NOT NULL,
    measured_value NUMERIC(15,2) NOT NULL,
    data_source VARCHAR(255),
    data_quality_score NUMERIC(5,2),
    notes TEXT,
    status VARCHAR(50) DEFAULT 'draft', -- 'draft', 'submitted', 'approved'
    approved_by VARCHAR(255),
    approved_date TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_kpi_measurement UNIQUE (kpi_id, party_id, contract_id, period_id)
);

-------------
--Resilience Framework
--------
-- Resilience Framework
CREATE TABLE rf.resilience_framework (
    framework_id SERIAL PRIMARY KEY,
    framework_name VARCHAR(255) NOT NULL,
    version VARCHAR(50) NOT NULL,
    based_on_standard VARCHAR(255), -- 'ISO 22301', 'NIST', etc.
    effective_date DATE NOT NULL,
    review_frequency VARCHAR(50) NOT NULL,
    owner VARCHAR(255) NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Resilience Domains
CREATE TABLE rf.resilience_domains (
    domain_id SERIAL PRIMARY KEY,
    domain_name VARCHAR(255) NOT NULL,
    description TEXT,
    framework_id INTEGER REFERENCES resilience_framework(framework_id),
    weight NUMERIC(5,2),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Key Resilience Metrics (enhanced from previous krm_definitions)
CREATE TABLE rf.krm_definitions (
    krm_id SERIAL PRIMARY KEY,
    krm_name VARCHAR(255) NOT NULL,
    description TEXT,
    domain_id INTEGER REFERENCES resilience_domains(domain_id),
    risk_domain VARCHAR(100) NOT NULL,
    measurement_unit VARCHAR(50) NOT NULL,
    measurement_method TEXT NOT NULL,
    data_source VARCHAR(255) NOT NULL,
    reporting_frequency VARCHAR(50) NOT NULL,
    threshold_value NUMERIC(15,2),
    threshold_direction VARCHAR(10) NOT NULL, -- 'higher', 'lower', 'equal'
    breach_consequences TEXT,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_krm_name UNIQUE (krm_name)
);

-- KRM Measurements (enhanced from previous krm_measurements)
CREATE TABLE rf.krm_measurements (
    measurement_id SERIAL PRIMARY KEY,
    krm_id INTEGER NOT NULL REFERENCES krm_definitions(krm_id),
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    contract_id INTEGER REFERENCES contracts(contract_id),
    period_id INTEGER REFERENCES kpi_periods(period_id),
    measurement_date DATE NOT NULL,
    measured_value NUMERIC(15,2) NOT NULL,
    data_source VARCHAR(255),
    data_quality_score NUMERIC(5,2),
    notes TEXT,
    status VARCHAR(50) DEFAULT 'draft', -- 'draft', 'submitted', 'approved'
    approved_by VARCHAR(255),
    approved_date TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_krm_measurement UNIQUE (krm_id, party_id, contract_id, period_id)
);

-- Resilience Assessments
CREATE TABLE rf.resilience_assessments (
    assessment_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    framework_id INTEGER NOT NULL REFERENCES resilience_framework(framework_id),
    assessment_date DATE NOT NULL,
    overall_score NUMERIC(5,2),
    maturity_level VARCHAR(50),
    conducted_by VARCHAR(255) NOT NULL,
    next_assessment_date DATE,
    status VARCHAR(50) DEFAULT 'draft',
    notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Resilience Assessment Scores
CREATE TABLE rf.resilience_assessment_scores (
    score_id SERIAL PRIMARY KEY,
    assessment_id INTEGER NOT NULL REFERENCES resilience_assessments(assessment_id) ON DELETE CASCADE,
    domain_id INTEGER NOT NULL REFERENCES resilience_domains(domain_id),
    score NUMERIC(5,2) NOT NULL,
    strengths TEXT,
    weaknesses TEXT,
    improvement_opportunities TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);


-----
-- View for Vendor Risk Profile (enhanced)
CREATE OR REPLACE VIEW tprm.vendor_risk_profile AS
SELECT
    tp.party_id,
    tp.legal_name,
    tp.industry_sector,
    COUNT(DISTINCT c.contract_id) AS active_contracts,
    pa.overall_risk_level,
    pa.overall_score,
    pa.assessment_date AS last_assessment_date,
    COUNT(DISTINCT ai.issue_id) FILTER (WHERE ai.severity = 'critical' AND ai.status != 'closed') AS open_critical_issues,
    COUNT(DISTINCT ai.issue_id) FILTER (WHERE ai.severity = 'high' AND ai.status != 'closed') AS open_high_issues,
    COUNT(DISTINCT i.incident_id) FILTER (WHERE i.severity IN ('high', 'critical')
        AND i.incident_date >= (CURRENT_DATE - INTERVAL '12 months')) AS severe_incidents_12m,
    fi.financial_health_score AS latest_financial_score,
    (SELECT COUNT(*) FROM business_continuity_plans bcp
     WHERE bcp.party_id = tp.party_id AND bcp.approval_status = 'approved') AS approved_bcp_count,
    (SELECT COUNT(*) FROM compliance_requirements cr
     JOIN party_compliance pc ON cr.requirement_id = pc.requirement_id
     WHERE pc.party_id = tp.party_id AND pc.compliance_status = 'non-compliant') AS non_compliant_items,
    (SELECT COUNT(*) FROM training_participation tp2
     WHERE tp2.party_id = tp.party_id
     AND tp2.completion_date >= (CURRENT_DATE - INTERVAL '12 months')) AS training_completions_12m,
    (SELECT STRING_AGG(er.risk_name, ', ')
     FROM emerging_risks er
     JOIN risk_quantification rq ON er.risk_id = rq.risk_id
     WHERE rq.party_id = tp.party_id) AS emerging_risks
FROM
    third_parties tp
LEFT JOIN
    contracts c ON tp.party_id = c.party_id AND c.contract_status = 'active'
LEFT JOIN
    party_assessments pa ON tp.party_id = pa.party_id AND pa.status = 'completed'
LEFT JOIN
    assessment_issues ai ON pa.party_assessment_id = ai.party_assessment_id AND ai.status != 'closed'
LEFT JOIN
    incidents i ON tp.party_id = i.party_id
LEFT JOIN
    financial_indicators fi ON tp.party_id = fi.party_id
GROUP BY
    tp.party_id, tp.legal_name, tp.industry_sector,
    pa.overall_risk_level, pa.overall_score, pa.assessment_date,
    fi.financial_health_score;

-- View for KPI Performance Dashboard (enhanced)
CREATE OR REPLACE VIEW kpi.kpi_performance_dashboard AS
SELECT
    tp.party_id,
    tp.legal_name,
    kd.kpi_id,
    kd.kpi_name,
    kc.category_name,
    km.measurement_date,
    km.measured_value,
    kd.target_value,
    kd.target_direction,
    CASE
        WHEN kd.target_direction = 'higher' AND km.measured_value >= kd.target_value THEN 'achieved'
        WHEN kd.target_direction = 'lower' AND km.measured_value <= kd.target_value THEN 'achieved'
        WHEN kd.target_direction = 'equal' AND km.measured_value = kd.target_value THEN 'achieved'
        ELSE 'not achieved'
    END AS target_status,
    kd.threshold_breach_actions,
    c.contract_name,
    c.contract_status,
    km.data_quality_score,
    km.status AS measurement_status
FROM
    kpi_measurements km
JOIN
    kpi_definitions kd ON km.kpi_id = kd.kpi_id
JOIN
    kpi_categories kc ON kd.category_id = kc.category_id
JOIN
    third_parties tp ON km.party_id = tp.party_id
LEFT JOIN
    contracts c ON km.contract_id = c.contract_id
ORDER BY
    tp.legal_name, kc.category_name, kd.kpi_name, km.measurement_date DESC;

-- View for Resilience Maturity Assessment
CREATE OR REPLACE VIEW rf.resilience_maturity_view AS
SELECT
    tp.party_id,
    tp.legal_name,
    ra.assessment_date,
    ra.overall_score,
    ra.maturity_level,
    rd.domain_name,
    ras.score AS domain_score,
    ras.strengths,
    ras.weaknesses,
    rf.framework_name,
    rf.based_on_standard
FROM
    resilience_assessments ra
JOIN
    resilience_assessment_scores ras ON ra.assessment_id = ras.assessment_id
JOIN
    resilience_domains rd ON ras.domain_id = rd.domain_id
JOIN
    resilience_framework rf ON ra.framework_id = rf.framework_id
JOIN
    third_parties tp ON ra.party_id = tp.party_id
ORDER BY
    tp.legal_name, ra.assessment_date DESC, rd.domain_name;

--materialized views
-- Materialized View for Vendor Risk Summary (enhanced)
CREATE MATERIALIZED VIEW tprm.mv_vendor_risk_summary AS
SELECT
    vrp.*,
    (SELECT COUNT(*) FROM kpi_measurements km
     JOIN kpi_definitions kd ON km.kpi_id = kd.kpi_id
     WHERE km.party_id = vrp.party_id
     AND km.measurement_date >= (CURRENT_DATE - INTERVAL '3 months')
     AND ((kd.target_direction = 'higher' AND km.measured_value < kd.target_value)
       OR (kd.target_direction = 'lower' AND km.measured_value > kd.target_value))) AS kpi_breaches_last_quarter,
    (SELECT COUNT(*) FROM krm_measurements rm
     JOIN krm_definitions rd ON rm.krm_id = rd.krm_id
     WHERE rm.party_id = vrp.party_id
     AND rm.measurement_date >= (CURRENT_DATE - INTERVAL '3 months')
     AND ((rd.threshold_direction = 'higher' AND rm.measured_value > rd.threshold_value)
       OR (rd.threshold_direction = 'lower' AND rm.measured_value < rd.threshold_value))) AS krm_breaches_last_quarter,
    (SELECT STRING_AGG(DISTINCT i.incident_type, ', ')
     FROM incidents i
     WHERE i.party_id = vrp.party_id
     AND i.incident_date >= (CURRENT_DATE - INTERVAL '12 months')) AS incident_types,
    (SELECT MAX(rq.risk_value)
     FROM risk_quantification rq
     WHERE rq.party_id = vrp.party_id
     AND rq.model_type = 'VaR') AS value_at_risk
FROM
    vendor_risk_profile vrp
WITH DATA;

-- Create index on the materialized view
CREATE INDEX idx_mv_vendor_risk_summary_party_id ON tprm.mv_vendor_risk_summary(party_id);
CREATE INDEX idx_mv_vendor_risk_summary_risk_level ON tprm.mv_vendor_risk_summary(overall_risk_level);

-- Materialized View for TPRM Program Metrics
CREATE MATERIALIZED VIEW tprm.mv_tprm_program_metrics AS
SELECT
    DATE_TRUNC('month', CURRENT_DATE) AS reporting_month,
    (SELECT COUNT(*) FROM third_parties WHERE is_active = TRUE) AS active_vendors,
    (SELECT COUNT(*) FROM third_parties WHERE is_active = FALSE) AS inactive_vendors,
    (SELECT COUNT(*) FROM contracts WHERE contract_status = 'active') AS active_contracts,
    (SELECT COUNT(*) FROM party_assessments
     WHERE assessment_date >= (CURRENT_DATE - INTERVAL '12 months')) AS assessments_last_12m,
    (SELECT COUNT(*) FROM party_assessments
     WHERE status = 'completed'
     AND next_assessment_date < CURRENT_DATE) AS overdue_assessments,
    (SELECT COUNT(*) FROM assessment_issues
     WHERE status = 'open'
     AND severity IN ('high', 'critical')) AS open_critical_issues,
    (SELECT COUNT(*) FROM incidents
     WHERE incident_date >= (CURRENT_DATE - INTERVAL '12 months')
     AND severity IN ('high', 'critical')) AS severe_incidents_12m,
    (SELECT AVG(overall_score) FROM party_assessments
     WHERE status = 'completed'
     AND assessment_date >= (CURRENT_DATE - INTERVAL '3 months')) AS avg_assessment_score,
    (SELECT COUNT(*) FROM kpi_measurements km
     JOIN kpi_definitions kd ON km.kpi_id = kd.kpi_id
     WHERE km.measurement_date >= (CURRENT_DATE - INTERVAL '3 months')
     AND ((kd.target_direction = 'higher' AND km.measured_value < kd.target_value)
       OR (kd.target_direction = 'lower' AND km.measured_value > kd.target_value))) AS kpi_breaches_last_quarter,
    (SELECT COUNT(*) FROM krm_measurements rm
     JOIN krm_definitions rd ON rm.krm_id = rd.krm_id
     WHERE rm.measurement_date >= (CURRENT_DATE - INTERVAL '3 months')
     AND ((rd.threshold_direction = 'higher' AND rm.measured_value > rd.threshold_value)
       OR (rd.threshold_direction = 'lower' AND rm.measured_value < rd.threshold_value))) AS krm_breaches_last_quarter,
    (SELECT COUNT(*) FROM business_continuity_plans
     WHERE approval_status = 'approved') AS approved_bcps,
    (SELECT COUNT(*) FROM training_participation
     WHERE completion_date >= (CURRENT_DATE - INTERVAL '12 months')) AS training_completions_12m
WITH DATA;

-- Create index on the materialized view
CREATE INDEX idx_mv_tprm_program_metrics_month ON tprm.mv_tprm_program_metrics(reporting_month);


--stored procedure
-- Procedure to calculate and update KPI achievement status
CREATE OR REPLACE PROCEDURE kpi.update_kpi_achievement_status(
    p_measurement_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_target_direction VARCHAR(10);
    v_measured_value NUMERIC(15,2);
    v_target_value NUMERIC(15,2);
    v_achievement_status VARCHAR(20);
BEGIN
    -- Get the measurement details
    SELECT
        kd.target_direction,
        km.measured_value,
        kd.target_value
    INTO
        v_target_direction,
        v_measured_value,
        v_target_value
    FROM
        kpi_measurements km
    JOIN
        kpi_definitions kd ON km.kpi_id = kd.kpi_id
    WHERE
        km.measurement_id = p_measurement_id;

    -- Determine achievement status
    IF v_target_value IS NULL THEN
        v_achievement_status := 'not applicable';
    ELSIF v_target_direction = 'higher' AND v_measured_value >= v_target_value THEN
        v_achievement_status := 'achieved';
    ELSIF v_target_direction = 'lower' AND v_measured_value <= v_target_value THEN
        v_achievement_status := 'achieved';
    ELSIF v_target_direction = 'equal' AND v_measured_value = v_target_value THEN
        v_achievement_status := 'achieved';
    ELSE
        v_achievement_status := 'not achieved';
    END IF;

    -- Update the measurement record
    UPDATE kpi_measurements
    SET
        achievement_status = v_achievement_status,
        updated_at = CURRENT_TIMESTAMP
    WHERE
        measurement_id = p_measurement_id;

    COMMIT;
END;
$$;

-- Procedure to conduct a full vendor risk assessment
CREATE OR REPLACE PROCEDURE tprm.conduct_vendor_risk_assessment(
    p_party_id INTEGER,
    p_assessment_framework VARCHAR(100),
    p_conducted_by VARCHAR(255),
    OUT p_party_assessment_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_assessment_id INTEGER;
    v_question_id INTEGER;
    v_default_response VARCHAR(50);
    v_default_score NUMERIC(5,2);
BEGIN
    -- Find the latest active assessment framework
    SELECT assessment_id INTO v_assessment_id
    FROM risk_assessments
    WHERE assessment_framework = p_assessment_framework
    AND is_active = TRUE
    ORDER BY effective_date DESC
    LIMIT 1;

    IF v_assessment_id IS NULL THEN
        RAISE EXCEPTION 'No active assessment framework found for %', p_assessment_framework;
    END IF;

    -- Create the assessment record
    INSERT INTO party_assessments (
        party_id,
        assessment_id,
        assessment_date,
        conducted_by,
        next_assessment_date,
        status
    )
    VALUES (
        p_party_id,
        v_assessment_id,
        CURRENT_DATE,
        p_conducted_by,
        CURRENT_DATE + INTERVAL '1 year',
        'in-progress'
    )
    RETURNING party_assessment_id INTO p_party_assessment_id;

    -- Create default responses for all questions
    FOR v_question_id IN
        SELECT question_id FROM assessment_questions WHERE assessment_id = v_assessment_id
    LOOP
        -- Set default response based on question criticality
        SELECT
            CASE WHEN is_critical THEN 'no' ELSE 'n/a' END,
            CASE WHEN is_critical THEN 0 ELSE NULL END
        INTO
            v_default_response,
            v_default_score
        FROM
            assessment_questions
        WHERE
            question_id = v_question_id;

        INSERT INTO assessment_responses (
            party_assessment_id,
            question_id,
            response_value,
            score,
            comments
        )
        VALUES (
            p_party_assessment_id,
            v_question_id,
            v_default_response,
            v_default_score,
            'Default response - to be completed'
        );
    END LOOP;

    COMMIT;
END;
$$;

-- Procedure to calculate resilience maturity score
CREATE OR REPLACE PROCEDURE rf.calculate_resilience_maturity(
    p_assessment_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_total_weight NUMERIC(10,2);
    v_weighted_score NUMERIC(10,2);
    v_overall_score NUMERIC(5,2);
    v_maturity_level VARCHAR(50);
BEGIN
    -- Calculate total weight and weighted score
    SELECT
        SUM(rd.weight),
        SUM(ras.score * rd.weight)
    INTO
        v_total_weight,
        v_weighted_score
    FROM
        resilience_assessment_scores ras
    JOIN
        resilience_domains rd ON ras.domain_id = rd.domain_id
    WHERE
        ras.assessment_id = p_assessment_id;

    -- Calculate overall score
    IF v_total_weight > 0 THEN
        v_overall_score := (v_weighted_score / v_total_weight);
    ELSE
        v_overall_score := 0;
    END IF;

    -- Determine maturity level
    IF v_overall_score >= 4 THEN
        v_maturity_level := 'Advanced';
    ELSIF v_overall_score >= 3 THEN
        v_maturity_level := 'Intermediate';
    ELSIF v_overall_score >= 2 THEN
        v_maturity_level := 'Basic';
    ELSE
        v_maturity_level := 'Initial';
    END IF;

    -- Update the assessment record
    UPDATE resilience_assessments
    SET
        overall_score = v_overall_score,
        maturity_level = v_maturity_level,
        updated_at = CURRENT_TIMESTAMP
    WHERE
        assessment_id = p_assessment_id;

    COMMIT;
END;
$$;

-- Procedure to generate TPRM report
CREATE OR REPLACE PROCEDURE generate_tprm_report(
    p_report_type VARCHAR(50), -- 'executive', 'operational', 'vendor'
    p_party_id INTEGER DEFAULT NULL
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_report_id INTEGER;
    v_report_text TEXT;
BEGIN
    -- Create report record
    INSERT INTO tprm_reports (
        report_type,
        party_id,
        generated_by,
        generation_date
    )
    VALUES (
        p_report_type,
        p_party_id,
        CURRENT_USER,
        CURRENT_DATE
    )
    RETURNING report_id INTO v_report_id;

    -- Generate report based on type
    IF p_report_type = 'executive' THEN
        SELECT
            'TPRM Executive Report - ' || TO_CHAR(CURRENT_DATE, 'YYYY-MM-DD') || E'\n\n' ||
            'Active Vendors: ' || (SELECT COUNT(*) FROM third_parties WHERE is_active = TRUE) || E'\n' ||
            'High/Critical Risk Vendors: ' || (SELECT COUNT(*) FROM mv_vendor_risk_summary WHERE overall_risk_level IN ('high', 'critical')) || E'\n' ||
            'Open Critical Issues: ' || (SELECT COUNT(*) FROM assessment_issues WHERE severity = 'critical' AND status != 'closed') || E'\n' ||
            'Severe Incidents (12m): ' || (SELECT COUNT(*) FROM incidents WHERE severity IN ('high', 'critical')
                                          AND incident_date >= (CURRENT_DATE - INTERVAL '12 months')) || E'\n' ||
            'Average Assessment Score: ' || (SELECT ROUND(AVG(overall_score), 2) FROM party_assessments
                                           WHERE status = 'completed'
                                           AND assessment_date >= (CURRENT_DATE - INTERVAL '3 months')) || E'\n' ||
            'Compliance Rate: ' || (SELECT ROUND(COUNT(*) FILTER (WHERE compliance_status = 'compliant') * 100.0 /
                                               NULLIF(COUNT(*), 0), 2)
                                   FROM party_compliance) || '%' || E'\n' ||
            'KPI Achievement Rate: ' || (SELECT ROUND(COUNT(*) FILTER (WHERE achievement_status = 'achieved') * 100.0 /
                                                   NULLIF(COUNT(*), 0), 2)
                                           FROM kpi_measurements
                                           WHERE measurement_date >= (CURRENT_DATE - INTERVAL '3 months')) || '%'
        INTO v_report_text;

    ELSIF p_report_type = 'vendor' AND p_party_id IS NOT NULL THEN
        SELECT
            'Vendor Risk Report - ' || legal_name || ' - ' || TO_CHAR(CURRENT_DATE, 'YYYY-MM-DD') || E'\n\n' ||
            'Industry Sector: ' || industry_sector || E'\n' ||
            'Risk Level: ' || overall_risk_level || E'\n' ||
            'Assessment Score: ' || overall_score || E'\n' ||
            'Last Assessment Date: ' || last_assessment_date || E'\n' ||
            'Open Critical Issues: ' || open_critical_issues || E'\n' ||
            'Open High Issues: ' || open_high_issues || E'\n' ||
            'Severe Incidents (12m): ' || severe_incidents_12m || E'\n' ||
            'Financial Health Score: ' || latest_financial_score || E'\n' ||
            'Approved BCPs: ' || approved_bcp_count || E'\n' ||
            'Non-Compliant Items: ' || non_compliant_items || E'\n' ||
            'Training Completions (12m): ' || training_completions_12m || E'\n' ||
            'Emerging Risks: ' || emerging_risks
        INTO v_report_text
        FROM vendor_risk_profile
        WHERE party_id = p_party_id;
    ELSE
        RAISE EXCEPTION 'Invalid report type or missing vendor ID';
    END IF;

    -- Update report with content
    UPDATE tprm_reports
    SET
        report_content = v_report_text,
        status = 'completed',
        completion_date = CURRENT_TIMESTAMP
    WHERE
        report_id = v_report_id;

    COMMIT;
END;
$$;

----
-- Documentation for features added and missing
-- Vendor Risk Assessment & Due Diligence: Fully supported through risk_assessments, assessment_questions, party_assessments, and assessment_responses tables.
--
-- Contractual Risk Management: Enhanced contracts table with additional fields and linked to party_assessments.
--
-- Ongoing Monitoring & Reporting: Supported through kpi_measurements, krm_measurements, and various monitoring views.
--
-- Cybersecurity & Data Protection: Covered in assessment_questions with cybersecurity domains and incidents tracking.
--
-- Business Continuity & Resilience: New business_continuity_plans table and resilience framework tables.
--
-- Governance, Policies & Frameworks: New tprm_program table and enhanced policy tracking.
--
-- Risk Quantification & Analytics: New risk_quantification and emerging_risks tables.
--
-- Technology & Tools: Supported through various tracking fields and integration points.
--
-- Training & Awareness: New training_programs and training_participation tables.
--
-- Incident Management & Response: Enhanced incidents table and new incident_management table.
--
-- Risk Culture & Stakeholder Engagement: Supported through training tables and survey tracking.
--
-- Vendor Offboarding & Exit Strategy: Covered in contracts status and exit clauses.
--
-- Legal, Regulatory & Compliance Oversight: Enhanced compliance_requirements and party_compliance tables.
--
-- Performance Evaluation & Continuous Improvement: Supported through KPI/KRM frameworks and assessment tracking.
--
-- Emerging Risks & Innovation: New emerging_risks table and integration with risk quantification.
--
-- Missed Features in Initial Schema
--
-- The initial schema missed several important aspects that have now been addressed:
--
-- Business Continuity Planning: Added business_continuity_plans table.
--
-- Resilience Maturity Tracking: Added resilience framework tables.
--
-- Training Management: Added training_programs and training_participation tables.
--
-- Incident Response Details: Added incident_management table.
--
-- Risk Quantification: Added risk_quantification table for advanced modeling.
--
-- Emerging Risks: Added emerging_risks table.
--
-- TPRM Program Governance: Added tprm_program table.
--
-- Detailed Contract Clauses: Enhanced contracts table with specific clause tracking.
--
-- more to do awase
--
-- Data Migration: Plan for migrating existing data to the enhanced schema, especially for new tables.
--
-- Indexing Strategy: Review query patterns and add appropriate indexes beyond those already defined.
--
-- API Layer: Develop a comprehensive API layer to interact with this schema rather than direct database access.
--
-- Scheduled Jobs: Set up jobs to refresh materialized views and run maintenance procedures.
--
-- Audit Trail: Consider adding comprehensive audit logging for critical tables.
--
-- Data Retention Policy: Implement policies for archiving old assessments and measurements.

-- need to think of moving it to data governance schema is it the right place?
-- revisit and analyze this
CREATE TABLE data_migrations (
    migration_id SERIAL PRIMARY KEY,
    migration_name VARCHAR(255) NOT NULL,
    description TEXT,
    source_system VARCHAR(100),
    source_table VARCHAR(100),
    target_table VARCHAR(100) NOT NULL,
    record_count INTEGER,
    status VARCHAR(50) DEFAULT 'pending',
    started_at TIMESTAMP WITH TIME ZONE,
    completed_at TIMESTAMP WITH TIME ZONE,
    duration INTERVAL,
    error_message TEXT,
    created_by VARCHAR(100) DEFAULT CURRENT_USER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Procedure to log migration start
CREATE OR REPLACE PROCEDURE log_migration_start(
    p_migration_name VARCHAR(255),
    p_description TEXT,
    p_source_system VARCHAR(100),
    p_source_table VARCHAR(100),
    p_target_table VARCHAR(100)
LANGUAGE plpgsql
AS $$
DECLARE
    v_migration_id INTEGER;
BEGIN
    INSERT INTO data_migrations (
        migration_name,
        description,
        source_system,
        source_table,
        target_table,
        status,
        started_at
    )
    VALUES (
        p_migration_name,
        p_description,
        p_source_system,
        p_source_table,
        p_target_table,
        'in-progress',
        CURRENT_TIMESTAMP
    )
    RETURNING migration_id INTO v_migration_id;

    RAISE NOTICE 'Migration % started with ID %', p_migration_name, v_migration_id;
END;
$$;

-- Procedure to log migration completion
CREATE OR REPLACE PROCEDURE log_migration_complete(
    p_migration_id INTEGER,
    p_record_count INTEGER,
    p_status VARCHAR(50) DEFAULT 'completed'
)
LANGUAGE plpgsql
AS $$
BEGIN
    UPDATE data_migrations
    SET
        status = p_status,
        record_count = p_record_count,
        completed_at = CURRENT_TIMESTAMP,
        duration = CURRENT_TIMESTAMP - started_at
    WHERE
        migration_id = p_migration_id;

    RAISE NOTICE 'Migration ID % completed with status %', p_migration_id, p_status;
END;
$$;

-- Example vendor data migration procedure
CREATE OR REPLACE PROCEDURE migrate_vendor_data()
LANGUAGE plpgsql
AS $$
DECLARE
    v_migration_id INTEGER;
    v_count INTEGER;
BEGIN
    -- Log migration start
    CALL log_migration_start(
        'Legacy Vendor Data Migration',
        'Migration of vendor data from legacy system to TPRM schema',
        'Legacy_ERP',
        'VENDOR_MASTER',
        'third_parties',
        v_migration_id
    );

    -- Perform migration (example - adjust based on actual source schema)
    INSERT INTO third_parties (
        legal_name,
        dba_name,
        tax_id,
        industry_sector,
        is_active,
        created_at,
        updated_at
    )
    SELECT
        vendor_name,
        trading_name,
        tax_identifier,
        industry_type,
        active_flag,
        created_date,
        modified_date
    FROM
        legacy.vendor_master
    WHERE
        vendor_type = 'THIRD_PARTY';

    GET DIAGNOSTICS v_count = ROW_COUNT;

    -- Log completion
    CALL log_migration_complete(v_migration_id, v_count);

    COMMIT;
EXCEPTION WHEN OTHERS THEN
    CALL log_migration_complete(v_migration_id, 0, 'failed: ' || SQLERRM);
    RAISE EXCEPTION 'Vendor data migration failed: %', SQLERRM;
END;
$$;

---
-- Performance indexes for assessments
CREATE INDEX idx_assessment_questions_assessment_id ON tprm.assessment_questions(assessment_id);
CREATE INDEX idx_assessment_responses_question_id ON tprm.assessment_responses(question_id);
CREATE INDEX idx_party_assessments_party_id ON tprm.party_assessments(party_id);
CREATE INDEX idx_party_assessments_status ON tprm.party_assessments(status);

-- Performance indexes for incidents
CREATE INDEX idx_incidents_party_id ON tprm.incidents(party_id);
CREATE INDEX idx_incidents_severity ON tprm.incidents(severity);
CREATE INDEX idx_incidents_incident_date ON tprm.incidents(incident_date);

-- Performance indexes for KPI/KRM measurements
CREATE INDEX idx_kpi_measurements_kpi_id ON kpi.kpi_measurements(kpi_id);
CREATE INDEX idx_kpi_measurements_measurement_date ON kpi.kpi_measurements(measurement_date);
CREATE INDEX idx_krm_measurements_krm_id ON rf.krm_measurements(krm_id);
CREATE INDEX idx_krm_measurements_measurement_date ON rf.krm_measurements(measurement_date);

-- Composite indexes for common query patterns
CREATE INDEX idx_contracts_party_status ON tprm.contracts(party_id, contract_status);
CREATE INDEX idx_assessment_issues_party_severity ON tprm.assessment_issues(party_id, severity, status);


CREATE OR REPLACE VIEW tprm.api_vendor_detail AS
SELECT
    tp.party_id,
    tp.legal_name,
    tp.dba_name,
    tp.industry_sector,
    tp.is_active,
    pa.overall_risk_level,
    pa.overall_score,
    pa.assessment_date AS last_assessment_date,
    (SELECT COUNT(*) FROM contracts c WHERE c.party_id = tp.party_id AND c.contract_status = 'active') AS active_contracts,
    (SELECT COUNT(*) FROM assessment_issues ai
     JOIN party_assessments pa ON ai.party_assessment_id = pa.party_assessment_id
     WHERE pa.party_id = tp.party_id AND ai.status = 'open') AS open_issues,
    (SELECT COUNT(*) FROM incidents i WHERE i.party_id = tp.party_id
     AND i.incident_date >= (CURRENT_DATE - INTERVAL '12 months')) AS incidents_12m,
    (SELECT json_agg(json_build_object(
        'contact_id', pc.contact_id,
        'name', pc.first_name || ' ' || pc.last_name,
        'email', pc.email,
        'phone', pc.phone,
        'is_primary', pc.is_primary
    )) FROM party_contacts pc WHERE pc.party_id = tp.party_id) AS contacts,
    (SELECT json_agg(json_build_object(
        'contract_id', c.contract_id,
        'contract_name', c.contract_name,
        'start_date', c.start_date,
        'end_date', c.end_date,
        'status', c.contract_status
    )) FROM contracts c WHERE c.party_id = tp.party_id) AS contracts
FROM
    third_parties tp
LEFT JOIN
    party_assessments pa ON tp.party_id = pa.party_id AND pa.status = 'completed'
ORDER BY
    tp.legal_name;


CREATE OR REPLACE VIEW tprm.api_assessment_detail AS
    SELECT
        pa.party_assessment_id,
        pa.party_id,
        tp.legal_name,
        ra.assessment_name,
        ra.assessment_framework,
        pa.assessment_date,
        pa.overall_score,
        pa.overall_risk_level,
        pa.status,
        (SELECT json_agg(json_build_object(
            'section', aq.section,
            'question_number', aq.question_number,
            'question_text', aq.question_text,
            'response_value', ar.response_value,
            'score', ar.score,
            'comments', ar.comments,
            'is_critical', aq.is_critical
        )) FROM assessment_responses ar
        JOIN assessment_questions aq ON ar.question_id = aq.question_id
        WHERE ar.party_assessment_id = pa.party_assessment_id
        ORDER BY aq.sort_order) AS responses,
        (SELECT json_agg(json_build_object(
            'issue_id', ai.issue_id,
            'issue_title', ai.issue_title,
            'severity', ai.severity,
            'status', ai.status,
            'due_date', ai.due_date
        )) FROM assessment_issues ai
        WHERE ai.party_assessment_id = pa.party_assessment_id) AS issues
    FROM
        party_assessments pa
    JOIN
        third_parties tp ON pa.party_id = tp.party_id
    JOIN
        risk_assessments ra ON pa.assessment_id = ra.assessment_id;

-- business case: to assess financial stability of vendors for capturing intelligence with reference supply chain disruptions.
-- Proactive monitoring helps mitigate risks from vendor bankruptcies or financial distress
CREATE TABLE tprm.vendor_financial_statements (
            statement_id SERIAL PRIMARY KEY,
            party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
            statement_date DATE NOT NULL,
            statement_type VARCHAR(50) NOT NULL, -- 'annual', 'quarterly', 'interim'
            revenue NUMERIC(15,2),
            net_income NUMERIC(15,2),
            total_assets NUMERIC(15,2),
            total_liabilities NUMERIC(15,2),
            current_ratio NUMERIC(10,2),
            debt_to_equity NUMERIC(10,2),
            document_reference VARCHAR(255),
            verification_status VARCHAR(50) DEFAULT 'unverified',
            verified_by VARCHAR(255),
            verified_date TIMESTAMP WITH TIME ZONE,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
            CONSTRAINT unique_vendor_statement UNIQUE (party_id, statement_date, statement_type)
        );

CREATE TABLE tprm.vendor_credit_ratings (
            rating_id SERIAL PRIMARY KEY,
            party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
            rating_agency VARCHAR(100) NOT NULL, -- 'Moody's', 'S&P', 'Fitch'
            rating_value VARCHAR(20) NOT NULL,
            outlook VARCHAR(20), -- 'positive', 'stable', 'negative'
            rating_date DATE NOT NULL,
            next_review_date DATE,
            rationale TEXT,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
        );




  CREATE MATERIALIZED VIEW tprm.mv_vendor_financial_health AS
        SELECT
            tp.party_id,
            tp.legal_name,
            fs.statement_date AS latest_financial_date,
            fs.current_ratio,
            fs.debt_to_equity,
            cr.rating_value AS credit_rating,
            cr.outlook AS credit_outlook,
            CASE
                WHEN cr.rating_value IN ('AAA','AA+','AA','AA-','A+','A','A-') THEN 'low'
                WHEN cr.rating_value IN ('BBB+','BBB','BBB-') THEN 'medium'
                WHEN cr.rating_value IN ('BB+','BB','BB-','B+','B','B-') THEN 'high'
                WHEN cr.rating_value IN ('CCC+','CCC','CCC-','CC','C','D') THEN 'critical'
                ELSE 'unrated'
            END AS financial_risk_level,
            (SELECT COUNT(*)
             FROM vendor_financial_statements
             WHERE party_id = tp.party_id
             AND statement_date >= CURRENT_DATE - INTERVAL '2 years') AS statements_last_2y,
            (SELECT MIN(current_ratio)
             FROM vendor_financial_statements
             WHERE party_id = tp.party_id
             AND statement_date >= CURRENT_DATE - INTERVAL '2 years') AS min_current_ratio_2y
        FROM
            third_parties tp
        LEFT JOIN
            (SELECT DISTINCT ON (party_id) *
             FROM vendor_financial_statements
             ORDER BY party_id, statement_date DESC) fs ON tp.party_id = fs.party_id
        LEFT JOIN
            (SELECT DISTINCT ON (party_id) *
             FROM vendor_credit_ratings
             ORDER BY party_id, rating_date DESC) cr ON tp.party_id = cr.party_id
        WITH DATA;




  CREATE OR REPLACE PROCEDURE tprm.assess_financial_risk(
          p_party_id INTEGER
      )
      LANGUAGE plpgsql
      AS $$
      DECLARE
          v_risk_level VARCHAR(20);
          v_recommendation TEXT;
          v_current_ratio NUMERIC(10,2);
          v_debt_to_equity NUMERIC(10,2);
          v_credit_rating VARCHAR(20);
      BEGIN
          -- Get latest financial metrics
          SELECT
              current_ratio,
              debt_to_equity,
              (SELECT rating_value FROM vendor_credit_ratings
               WHERE party_id = p_party_id
               ORDER BY rating_date DESC LIMIT 1)
          INTO
              v_current_ratio,
              v_debt_to_equity,
              v_credit_rating
          FROM
              vendor_financial_statements
          WHERE
              party_id = p_party_id
          ORDER BY
              statement_date DESC
          LIMIT 1;

          -- Determine risk level
          IF v_credit_rating IN ('AAA','AA+','AA','AA-','A+','A','A-') THEN
              v_risk_level := 'low';
              v_recommendation := 'Continue normal monitoring cycle';
          ELSIF v_credit_rating IN ('BBB+','BBB','BBB-') AND v_current_ratio > 1.5 AND v_debt_to_equity < 1 THEN
              v_risk_level := 'medium';
              v_recommendation := 'Increase monitoring frequency to quarterly';
          ELSIF v_credit_rating IN ('BB+','BB','BB-','B+','B','B-') OR
                (v_current_ratio < 1 OR v_debt_to_equity > 2) THEN
              v_risk_level := 'high';
              v_recommendation := 'Require additional financial guarantees and monthly reporting';
          ELSIF v_credit_rating IN ('CCC+','CCC','CCC-','CC','C','D') THEN
              v_risk_level := 'critical';
              v_recommendation := 'Initiate contingency planning and alternative vendor identification';
          ELSE
              v_risk_level := 'unrated';
              v_recommendation := 'Request updated financial information and credit rating';
          END IF;

          -- Update vendor risk profile
          UPDATE third_parties
          SET
              financial_risk_level = v_risk_level,
              financial_recommendation = v_recommendation,
              updated_at = CURRENT_TIMESTAMP
          WHERE
              party_id = p_party_id;

          -- Log the assessment
          INSERT INTO tprm.risk_assessment_log (
              party_id,
              assessment_type,
              risk_level,
              details,
              recommendation
          )
          VALUES (
              p_party_id,
              'financial',
              v_risk_level,
              json_build_object(
                  'current_ratio', v_current_ratio,
                  'debt_to_equity', v_debt_to_equity,
                  'credit_rating', v_credit_rating
              ),
              v_recommendation
          );

          COMMIT;
      END;
      $$;

--business case: to assess global operations may require constant monitoring for country specific risks that may impact vendor performance and continuity
CREATE TABLE tprm.country_risk_assessments (
    assessment_id SERIAL PRIMARY KEY,
    country_code CHAR(2) NOT NULL,
    country_name VARCHAR(100) NOT NULL,
    assessment_date DATE NOT NULL,
    political_risk_score NUMERIC(5,2) CHECK (political_risk_score BETWEEN 0 AND 100),
    economic_risk_score NUMERIC(5,2) CHECK (economic_risk_score BETWEEN 0 AND 100),
    legal_risk_score NUMERIC(5,2) CHECK (legal_risk_score BETWEEN 0 AND 100),
    operational_risk_score NUMERIC(5,2) CHECK (operational_risk_score BETWEEN 0 AND 100),
    overall_risk_score NUMERIC(5,2) CHECK (overall_risk_score BETWEEN 0 AND 100),
    risk_level VARCHAR(20) NOT NULL, -- 'low', 'medium', 'high', 'critical'
    source VARCHAR(100) NOT NULL,
    summary TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_country_assessment UNIQUE (country_code, assessment_date)
);

CREATE TABLE tprm.vendor_geographic_exposure (
    exposure_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    country_code CHAR(2) NOT NULL,
    exposure_type VARCHAR(50) NOT NULL, -- 'hq', 'operations', 'suppliers', 'customers'
    revenue_percentage NUMERIC(5,2) CHECK (revenue_percentage BETWEEN 0 AND 100),
    employee_count INTEGER,
    critical_operations BOOLEAN DEFAULT FALSE,
    last_verified_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE VIEW tprm.vendor_geopolitical_risk AS
SELECT
    tp.party_id,
    tp.legal_name,
    vge.country_code,
    cra.country_name,
    cra.overall_risk_score,
    cra.risk_level,
    vge.exposure_type,
    vge.revenue_percentage,
    vge.critical_operations,
    CASE
        WHEN cra.risk_level = 'critical' AND vge.critical_operations = TRUE THEN 'critical'
        WHEN cra.risk_level = 'critical' OR (cra.risk_level = 'high' AND vge.critical_operations = TRUE) THEN 'high'
        WHEN cra.risk_level = 'high' OR (cra.risk_level = 'medium' AND vge.revenue_percentage > 20) THEN 'medium'
        ELSE 'low'
    END AS exposure_risk_level,
    CASE
        WHEN cra.risk_level = 'critical' AND vge.critical_operations = TRUE THEN 'Immediate action required - develop contingency plans'
        WHEN cra.risk_level = 'critical' THEN 'Identify alternative sources and reduce exposure'
        WHEN cra.risk_level = 'high' AND vge.critical_operations = TRUE THEN 'Develop risk mitigation strategies'
        WHEN cra.risk_level = 'high' THEN 'Increase monitoring and review contracts for force majeure'
        ELSE 'Monitor for changes in country risk'
    END AS risk_mitigation
FROM
    third_parties tp
JOIN
    vendor_geographic_exposure vge ON tp.party_id = vge.party_id
JOIN
    (SELECT DISTINCT ON (country_code) *
     FROM country_risk_assessments
     ORDER BY country_code, assessment_date DESC) cra ON vge.country_code = cra.country_code
ORDER BY
    tp.legal_name,
    CASE
        WHEN cra.risk_level = 'critical' THEN 1
        WHEN cra.risk_level = 'high' THEN 2
        WHEN cra.risk_level = 'medium' THEN 3
        ELSE 4
    END,
    vge.revenue_percentage DESC;


CREATE OR REPLACE PROCEDURE tprm.update_geopolitical_risk_scores()
  LANGUAGE plpgsql
  AS $$
  BEGIN
      -- Update country risk scores from external API (conceptual example)
      -- In practice, this would call an external API and process the results
      INSERT INTO country_risk_assessments (
          country_code,
          country_name,
          assessment_date,
          political_risk_score,
          economic_risk_score,
          legal_risk_score,
          operational_risk_score,
          overall_risk_score,
          risk_level,
          source,
          summary
      )
      SELECT
          c.country_code,
          c.country_name,
          CURRENT_DATE,
          -- These would come from API calls in a real implementation
          random() * 100, -- political_risk_score
          random() * 100, -- economic_risk_score
          random() * 100, -- legal_risk_score
          random() * 100, -- operational_risk_score
          random() * 100, -- overall_risk_score
          CASE
              WHEN random() * 100 > 80 THEN 'critical'
              WHEN random() * 100 > 60 THEN 'high'
              WHEN random() * 100 > 30 THEN 'medium'
              ELSE 'low'
          END,
          'Geopolitical Risk API',
          'Automated risk assessment based on current geopolitical indicators'
      FROM
          (SELECT DISTINCT country_code, country_name
           FROM vendor_geographic_exposure) c;

      -- Flag high-risk vendors for review
      UPDATE third_parties
      SET
          geopolitical_risk_flag = EXISTS (
              SELECT 1 FROM vendor_geopolitical_risk vgr
              WHERE vgr.party_id = third_parties.party_id
              AND vgr.exposure_risk_level IN ('high', 'critical')
          ),
          updated_at = CURRENT_TIMESTAMP
      WHERE
          EXISTS (
              SELECT 1 FROM vendor_geopolitical_risk vgr
              WHERE vgr.party_id = third_parties.party_id
              AND vgr.exposure_risk_level IN ('high', 'critical')
          );

      COMMIT;
  END;
  $$;



--ESG compliance tracking
-- business case: to increase regulatory and stakeholder demands required to track vendors ESG performance
CREATE TABLE tprm.esg_frameworks (
    framework_id SERIAL PRIMARY KEY,
    framework_name VARCHAR(255) NOT NULL,
    version VARCHAR(50) NOT NULL,
    publisher VARCHAR(255) NOT NULL,
    effective_date DATE NOT NULL,
    description TEXT,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.esg_indicators (
    indicator_id SERIAL PRIMARY KEY,
    framework_id INTEGER NOT NULL REFERENCES esg_frameworks(framework_id),
    indicator_code VARCHAR(50) NOT NULL,
    indicator_name VARCHAR(255) NOT NULL,
    category VARCHAR(50) NOT NULL, -- 'environmental', 'social', 'governance'
    subcategory VARCHAR(100),
    measurement_unit VARCHAR(50),
    target_value NUMERIC(15,2),
    is_critical BOOLEAN DEFAULT FALSE,
    description TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_indicator_code UNIQUE (framework_id, indicator_code)
);

CREATE TABLE tprm.vendor_esg_assessments (
    assessment_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    framework_id INTEGER NOT NULL REFERENCES esg_frameworks(framework_id),
    assessment_date DATE NOT NULL,
    conducted_by VARCHAR(255) NOT NULL,
    next_assessment_date DATE,
    overall_score NUMERIC(5,2),
    overall_rating VARCHAR(50), -- 'excellent', 'good', 'fair', 'poor'
    status VARCHAR(50) DEFAULT 'draft',
    notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.vendor_esg_scores (
    score_id SERIAL PRIMARY KEY,
    assessment_id INTEGER NOT NULL REFERENCES vendor_esg_assessments(assessment_id) ON DELETE CASCADE,
    indicator_id INTEGER NOT NULL REFERENCES esg_indicators(indicator_id),
    score NUMERIC(5,2) NOT NULL,
    evidence_reference VARCHAR(255),
    comments TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_assessment_indicator UNIQUE (assessment_id, indicator_id)
);


---materialized views
CREATE MATERIALIZED VIEW tprm.mv_vendor_esg_performance AS
SELECT
    tp.party_id,
    tp.legal_name,
    ef.framework_name,
    vea.assessment_date,
    vea.overall_score,
    vea.overall_rating,
    COUNT(DISTINCT ves.score_id) AS indicators_assessed,
    ROUND(AVG(CASE WHEN ei.category = 'environmental' THEN ves.score ELSE NULL END), 2) AS env_score,
    ROUND(AVG(CASE WHEN ei.category = 'social' THEN ves.score ELSE NULL END), 2) AS social_score,
    ROUND(AVG(CASE WHEN ei.category = 'governance' THEN ves.score ELSE NULL END), 2) AS gov_score,
    (SELECT STRING_AGG(ei.indicator_code, ', ')
     FROM vendor_esg_scores ves2
     JOIN esg_indicators ei2 ON ves2.indicator_id = ei2.indicator_id
     WHERE ves2.assessment_id = vea.assessment_id
     AND ves2.score < (ei2.target_value * 0.7)) AS underperforming_indicators
FROM
    third_parties tp
JOIN
    vendor_esg_assessments vea ON tp.party_id = vea.party_id
JOIN
    esg_frameworks ef ON vea.framework_id = ef.framework_id
JOIN
    vendor_esg_scores ves ON vea.assessment_id = ves.assessment_id
JOIN
    esg_indicators ei ON ves.indicator_id = ei.indicator_id
WHERE
    vea.status = 'completed'
GROUP BY
    tp.party_id, tp.legal_name, ef.framework_name, vea.assessment_date, vea.overall_score, vea.overall_rating
WITH DATA;


---
CREATE OR REPLACE PROCEDURE calculate_esg_rating(
    p_assessment_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_avg_score NUMERIC(5,2);
    v_critical_failures INTEGER;
    v_rating VARCHAR(50);
BEGIN
    -- Calculate average score
    SELECT AVG(ves.score) INTO v_avg_score
    FROM vendor_esg_scores ves
    WHERE ves.assessment_id = p_assessment_id;

    -- Count critical indicator failures
    SELECT COUNT(*) INTO v_critical_failures
    FROM vendor_esg_scores ves
    JOIN esg_indicators ei ON ves.indicator_id = ei.indicator_id
    WHERE ves.assessment_id = p_assessment_id
    AND ei.is_critical = TRUE
    AND ves.score < (ei.target_value * 0.7);

    -- Determine overall rating
    IF v_critical_failures > 0 THEN
        v_rating := 'poor';
    ELSIF v_avg_score >= 85 THEN
        v_rating := 'excellent';
    ELSIF v_avg_score >= 70 THEN
        v_rating := 'good';
    ELSIF v_avg_score >= 50 THEN
        v_rating := 'fair';
    ELSE
        v_rating := 'poor';
    END IF;

    -- Update assessment record
    UPDATE vendor_esg_assessments
    SET
        overall_score = v_avg_score,
        overall_rating = v_rating,
        updated_at = CURRENT_TIMESTAMP
    WHERE
        assessment_id = p_assessment_id;

    -- Generate improvement recommendations for poor performers
    IF v_rating = 'poor' THEN
        INSERT INTO vendor_improvement_plans (
            party_id,
            plan_type,
            trigger_assessment_id,
            priority,
            description,
            due_date,
            status
        )
        VALUES (
            (SELECT party_id FROM vendor_esg_assessments WHERE assessment_id = p_assessment_id),
            'esg',
            p_assessment_id,
            'high',
            'Address critical ESG failures identified in assessment ' || p_assessment_id,
            CURRENT_DATE + INTERVAL '90 days',
            'pending'
        );
    END IF;

    COMMIT;
END;
$$;

CREATE OR REPLACE PROCEDURE tprm.generate_esg_report(
    p_framework_id INTEGER,
    p_min_rating VARCHAR(50) DEFAULT 'poor'
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_report_id INTEGER;
    v_framework_name VARCHAR(255);
BEGIN
    -- Get framework name
    SELECT framework_name INTO v_framework_name
    FROM esg_frameworks
    WHERE framework_id = p_framework_id;

    IF NOT FOUND THEN
        RAISE EXCEPTION 'ESG framework with ID % not found', p_framework_id;
    END IF;

    -- Create report record
    INSERT INTO tprm_reports (
        report_type,
        report_parameters,
        generated_by,
        generation_date
    )
    VALUES (
        'esg',
        json_build_object('framework_id', p_framework_id, 'min_rating', p_min_rating),
        CURRENT_USER,
        CURRENT_DATE
    )
    RETURNING report_id INTO v_report_id;

    -- Generate report content (simplified example)
    UPDATE tprm_reports
    SET
        report_content = (
            SELECT
                'ESG Performance Report - ' || v_framework_name || ' - ' || TO_CHAR(CURRENT_DATE, 'YYYY-MM-DD') || E'\n\n' ||
                'Vendors Assessed: ' || COUNT(DISTINCT v.party_id) || E'\n' ||
                'Average Score: ' || ROUND(AVG(v.overall_score), 2) || E'\n' ||
                'Rating Distribution:' || E'\n' ||
                '  Excellent: ' || COUNT(DISTINCT CASE WHEN v.overall_rating = 'excellent' THEN v.party_id END) || E'\n' ||
                '  Good: ' || COUNT(DISTINCT CASE WHEN v.overall_rating = 'good' THEN v.party_id END) || E'\n' ||
                '  Fair: ' || COUNT(DISTINCT CASE WHEN v.overall_rating = 'fair' THEN v.party_id END) || E'\n' ||
                '  Poor: ' || COUNT(DISTINCT CASE WHEN v.overall_rating = 'poor' THEN v.party_id END) || E'\n\n' ||
                'Vendors Requiring Improvement:' || E'\n' ||
                (SELECT STRING_AGG(v2.legal_name, E'\n' ORDER BY v2.overall_score)
                 FROM mv_vendor_esg_performance v2
                 WHERE v2.framework_name = v_framework_name
                 AND v2.overall_rating = 'poor')
            FROM
                mv_vendor_esg_performance v
            WHERE
                v.framework_name = v_framework_name
        ),
        status = 'completed',
        completion_date = CURRENT_TIMESTAMP
    WHERE
        report_id = v_report_id;

    COMMIT;
END;
$$;

--- AI powered risk prediction
-- business case: to leverage machine learning to predict vendor risk before they materialize based on the historical patterns
CREATE TABLE tprm.risk_prediction_models (
    model_id SERIAL PRIMARY KEY,
    model_name VARCHAR(255) NOT NULL,
    model_type VARCHAR(100) NOT NULL, -- 'financial', 'operational', 'compliance'
    version VARCHAR(50) NOT NULL,
    deployment_date TIMESTAMP WITH TIME ZONE NOT NULL,
    accuracy_score NUMERIC(5,4),
    precision_score NUMERIC(5,4),
    recall_score NUMERIC(5,4),
    features_used TEXT[] NOT NULL,
    model_metadata JSONB,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.vendor_risk_predictions (
    prediction_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    model_id INTEGER NOT NULL REFERENCES risk_prediction_models(model_id),
    prediction_date TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    risk_type VARCHAR(100) NOT NULL,
    prediction_score NUMERIC(10,6) NOT NULL,
    risk_level VARCHAR(20) NOT NULL, -- 'low', 'medium', 'high', 'critical'
    confidence_interval_lower NUMERIC(10,6),
    confidence_interval_upper NUMERIC(10,6),
    explanation TEXT,
    actionable_insights TEXT,
    reviewed BOOLEAN DEFAULT FALSE,
    reviewed_by VARCHAR(255),
    review_notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

--materilaized view
CREATE MATERIALIZED VIEW tprm.mv_vendor_risk_predictions AS
SELECT
    tp.party_id,
    tp.legal_name,
    rp.risk_type,
    rp.prediction_score,
    rp.risk_level,
    rp.prediction_date,
    rm.model_name,
    rm.accuracy_score,
    rp.explanation,
    rp.actionable_insights,
    (SELECT COUNT(*)
     FROM incidents i
     WHERE i.party_id = tp.party_id
     AND i.incident_date >= rp.prediction_date - INTERVAL '3 months'
     AND i.incident_date <= rp.prediction_date + INTERVAL '3 months'
     AND i.severity IN ('high', 'critical')) AS actual_incidents_6m,
    CASE
        WHEN (SELECT COUNT(*)
              FROM incidents i
              WHERE i.party_id = tp.party_id
              AND i.incident_date >= rp.prediction_date - INTERVAL '3 months'
              AND i.incident_date <= rp.prediction_date + INTERVAL '3 months'
              AND i.severity IN ('high', 'critical')) > 0
             AND rp.risk_level IN ('high', 'critical') THEN 'true positive'
        WHEN (SELECT COUNT(*)
              FROM incidents i
              WHERE i.party_id = tp.party_id
              AND i.incident_date >= rp.prediction_date - INTERVAL '3 months'
              AND i.incident_date <= rp.prediction_date + INTERVAL '3 months'
              AND i.severity IN ('high', 'critical')) = 0
             AND rp.risk_level IN ('high', 'critical') THEN 'false positive'
        WHEN (SELECT COUNT(*)
              FROM incidents i
              WHERE i.party_id = tp.party_id
              AND i.incident_date >= rp.prediction_date - INTERVAL '3 months'
              AND i.incident_date <= rp.prediction_date + INTERVAL '3 months'
              AND i.severity IN ('high', 'critical')) > 0
             AND rp.risk_level IN ('low', 'medium') THEN 'false negative'
        ELSE 'true negative'
    END AS prediction_accuracy
FROM
    vendor_risk_predictions rp
JOIN
    third_parties tp ON rp.party_id = tp.party_id
JOIN
    risk_prediction_models rm ON rp.model_id = rm.model_id
WHERE
    rp.prediction_date >= CURRENT_DATE - INTERVAL '12 months'
WITH DATA;


CREATE OR REPLACE PROCEDURE trpm.generate_risk_predictions(
    p_model_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_model RECORD;
    v_party RECORD;
    v_prediction_score NUMERIC(10,6);
    v_risk_level VARCHAR(20);
    v_explanation TEXT;
    v_insights TEXT;
BEGIN
    -- Get model details
    SELECT * INTO v_model FROM risk_prediction_models WHERE model_id = p_model_id;

    IF NOT FOUND THEN
        RAISE EXCEPTION 'Risk prediction model with ID % not found', p_model_id;
    END IF;

    -- For each vendor, generate prediction (simplified example)
    -- In practice, this would call a machine learning model API
    FOR v_party IN SELECT party_id, legal_name FROM third_parties WHERE is_active = TRUE LOOP
        -- Simulate prediction (replace with actual model call)
        v_prediction_score := random();

        -- Determine risk level
        IF v_prediction_score > 0.8 THEN
            v_risk_level := 'critical';
            v_explanation := 'High probability of risk events based on historical patterns and current indicators';
            v_insights := 'Recommend increased monitoring and review of contingency plans';
        ELSIF v_prediction_score > 0.6 THEN
            v_risk_level := 'high';
            v_explanation := 'Elevated risk indicators detected';
            v_insights := 'Recommend additional due diligence and risk mitigation actions';
        ELSIF v_prediction_score > 0.3 THEN
            v_risk_level := 'medium';
            v_explanation := 'Moderate risk indicators present';
            v_insights := 'Monitor for changes in risk profile';
        ELSE
            v_risk_level := 'low';
            v_explanation := 'No significant risk indicators detected';
            v_insights := 'Continue standard monitoring';
        END IF;

        -- Store prediction
        INSERT INTO tprm.vendor_risk_predictions (
            party_id,
            model_id,
            risk_type,
            prediction_score,
            risk_level,
            explanation,
            actionable_insights
        )
        VALUES (
            v_party.party_id,
            p_model_id,
            v_model.model_type,
            v_prediction_score,
            v_risk_level,
            v_explanation,
            v_insights
        );
    END LOOP;

    -- Update model performance metrics
    UPDATE tprm.risk_prediction_models
    SET
        accuracy_score = (
            SELECT
                COUNT(*) FILTER (WHERE prediction_accuracy = 'true positive' OR prediction_accuracy = 'true negative')::NUMERIC /
                NULLIF(COUNT(*), 0)
            FROM
                mv_vendor_risk_predictions
            WHERE
                model_id = p_model_id
        ),
        precision_score = (
            SELECT
                COUNT(*) FILTER (WHERE prediction_accuracy = 'true positive')::NUMERIC /
                NULLIF(COUNT(*) FILTER (WHERE prediction_accuracy IN ('true positive', 'false positive')), 0)
            FROM
                mv_vendor_risk_predictions
            WHERE
                model_id = p_model_id
        ),
        recall_score = (
            SELECT
                COUNT(*) FILTER (WHERE prediction_accuracy = 'true positive')::NUMERIC /
                NULLIF(COUNT(*) FILTER (WHERE prediction_accuracy IN ('true positive', 'false negative')), 0)
            FROM
                mv_vendor_risk_predictions
            WHERE
                model_id = p_model_id
        ),
        updated_at = CURRENT_TIMESTAMP
    WHERE
        model_id = p_model_id;

    COMMIT;
END;
$$;

CREATE OR REPLACE PROCEDURE tprm.flag_high_risk_predictions()
LANGUAGE plpgsql
AS $$
BEGIN
    -- Flag vendors with critical predictions for immediate review
    UPDATE third_parties
    SET
        ai_risk_flag = TRUE,
        ai_risk_last_flag_date = CURRENT_DATE,
        updated_at = CURRENT_TIMESTAMP
    WHERE
        party_id IN (
            SELECT DISTINCT party_id
            FROM vendor_risk_predictions
            WHERE risk_level IN ('high', 'critical')
            AND prediction_date >= CURRENT_DATE - INTERVAL '7 days'
            AND reviewed = FALSE
        );

    -- Create review tasks for risk team
    INSERT INTO risk_review_tasks (
        task_type,
        party_id,
        priority,
        description,
        due_date,
        status
    )
    SELECT
        'ai_risk_review',
        vrp.party_id,
        CASE WHEN vrp.risk_level = 'critical' THEN 'critical' ELSE 'high' END,
        'Review AI-generated risk prediction for ' || tp.legal_name || ': ' || vrp.explanation,
        CURRENT_DATE + CASE WHEN vrp.risk_level = 'critical' THEN INTERVAL '2 days' ELSE INTERVAL '7 days' END,
        'pending'
    FROM
        vendor_risk_predictions vrp
    JOIN
        third_parties tp ON vrp.party_id = tp.party_id
    WHERE
        vrp.risk_level IN ('high', 'critical')
        AND vrp.prediction_date >= CURRENT_DATE - INTERVAL '7 days'
        AND vrp.reviewed = FALSE;

    COMMIT;
END;
$$;


--business case: supply chain dependency mapping
-- organizaitons need to visualize multi-tier supplier relationships to identify single point of failure and concentration risks in their supply chains
CREATE TABLE tprm.supply_chain_nodes (
    node_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    node_name VARCHAR(255) NOT NULL,
    node_type VARCHAR(50) NOT NULL, -- 'tier1', 'tier2', 'raw-material', 'logistics'
    criticality VARCHAR(20) NOT NULL, -- 'critical', 'high', 'medium', 'low'
    geographic_location VARCHAR(100),
    is_single_source BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.supply_chain_relationships (
    relationship_id SERIAL PRIMARY KEY,
    parent_node_id INTEGER NOT NULL REFERENCES supply_chain_nodes(node_id),
    child_node_id INTEGER NOT NULL REFERENCES supply_chain_nodes(node_id),
    relationship_type VARCHAR(50) NOT NULL, -- 'direct-supplier', 'subcontractor', 'logistics'
    dependency_strength NUMERIC(3,2) CHECK (dependency_strength BETWEEN 0 AND 1),
    alternative_sources INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT no_self_relationship CHECK (parent_node_id <> child_node_id),
    CONSTRAINT unique_relationship UNIQUE (parent_node_id, child_node_id, relationship_type)
);

CREATE TABLE tprm.supply_chain_risk_scenarios (
    scenario_id SERIAL PRIMARY KEY,
    scenario_name VARCHAR(255) NOT NULL,
    scenario_type VARCHAR(100) NOT NULL, -- 'geopolitical', 'natural-disaster', 'financial'
    description TEXT NOT NULL,
    likelihood VARCHAR(20) NOT NULL, -- 'rare', 'unlikely', 'possible', 'likely', 'certain'
    potential_impact VARCHAR(20) NOT NULL, -- 'low', 'medium', 'high', 'critical'
    affected_nodes JSONB, -- Array of node IDs
    mitigation_strategy TEXT,
    last_simulated_date TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE MATERIALIZED VIEW tprm.mv_supply_chain_network AS
WITH RECURSIVE chain_tree AS (
    SELECT
        n.node_id,
        n.party_id,
        n.node_name,
        n.node_type,
        n.criticality,
        1 AS tier_level,
        ARRAY[n.node_id] AS path,
        p.legal_name AS party_name
    FROM
        supply_chain_nodes n
    JOIN
        third_parties p ON n.party_id = p.party_id
    WHERE
        n.node_type = 'tier1'

    UNION ALL

    SELECT
        c.node_id,
        c.party_id,
        c.node_name,
        c.node_type,
        c.criticality,
        p.tier_level + 1,
        p.path || c.node_id,
        tp.legal_name AS party_name
    FROM
        supply_chain_nodes c
    JOIN
        supply_chain_relationships r ON c.node_id = r.child_node_id
    JOIN
        chain_tree p ON r.parent_node_id = p.node_id
    JOIN
        third_parties tp ON c.party_id = tp.party_id
)
SELECT
    ct.node_id,
    ct.party_id,
    ct.party_name,
    ct.node_name,
    ct.node_type,
    ct.tier_level,
    ct.criticality,
    (SELECT COUNT(*) FROM supply_chain_relationships WHERE parent_node_id = ct.node_id) AS downstream_dependencies,
    (SELECT COUNT(*) FROM supply_chain_relationships WHERE child_node_id = ct.node_id) AS upstream_dependencies,
    (SELECT STRING_AGG(tp.legal_name, ', ' ORDER BY tp.legal_name)
    FROM supply_chain_relationships scr
    JOIN supply_chain_nodes scn ON scr.parent_node_id = scn.node_id
    JOIN third_parties tp ON scn.party_id = tp.party_id
    WHERE scr.child_node_id = ct.node_id) AS suppliers,
    (SELECT STRING_AGG(tp.legal_name, ', ' ORDER BY tp.legal_name)
    FROM supply_chain_relationships scr
    JOIN supply_chain_nodes scn ON scr.child_node_id = scn.node_id
    JOIN third_parties tp ON scn.party_id = tp.party_id
    WHERE scr.parent_node_id = ct.node_id) AS customers,
    (SELECT COUNT(*) FROM supply_chain_nodes WHERE party_id = ct.party_id AND criticality = 'critical') AS critical_nodes_by_vendor,
    ct.path
FROM
    chain_tree ct
WITH DATA;


CREATE OR REPLACE PROCEDURE tprm.analyze_supply_chain_vulnerabilities()
LANGUAGE plpgsql
AS $$
DECLARE
    v_single_source_count INTEGER;
    v_critical_nodes_without_alternatives INTEGER;
BEGIN
    -- Identify single source dependencies
    SELECT COUNT(*) INTO v_single_source_count
    FROM supply_chain_nodes
    WHERE is_single_source = TRUE;

    -- Identify critical nodes without alternatives
    SELECT COUNT(*) INTO v_critical_nodes_without_alternatives
    FROM supply_chain_nodes n
    WHERE n.criticality IN ('critical', 'high')
    AND NOT EXISTS (
        SELECT 1 FROM supply_chain_relationships r
        WHERE r.parent_node_id = n.node_id
        AND r.alternative_sources > 0
    );

    -- Generate vulnerability report
    INSERT INTO risk_reports (
        report_type,
        generated_at,
        findings,
        recommendations
    )
    VALUES (
        'supply-chain-vulnerability',
        CURRENT_TIMESTAMP,
        json_build_object(
            'single_source_dependencies', v_single_source_count,
            'critical_nodes_without_alternatives', v_critical_nodes_without_alternatives,
            'deepest_tier_level', (SELECT MAX(tier_level) FROM mv_supply_chain_network)
        ),
        json_build_object(
            'high_priority', CASE WHEN v_critical_nodes_without_alternatives > 0
                                THEN 'Identify and qualify alternative sources for critical dependencies'
                                ELSE NULL END,
            'medium_priority', CASE WHEN v_single_source_count > 5
                                 THEN 'Develop contingency plans for single source dependencies'
                                 ELSE NULL END,
            'general', 'Conduct regular supply chain mapping exercises to maintain visibility'
        )
    );

    COMMIT;
END;
$$;

CREATE OR REPLACE PROCEDURE tprm.simulate_supply_chain_disruption(
    p_node_id INTEGER,
    p_scenario_type VARCHAR(100)
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_affected_nodes INTEGER[];
    v_critical_paths INTEGER;
BEGIN
    -- Find all dependent nodes (downstream)
    WITH RECURSIVE dependency_tree AS (
        SELECT node_id FROM supply_chain_nodes WHERE node_id = p_node_id
        UNION
        SELECT r.child_node_id
        FROM supply_chain_relationships r
        JOIN dependency_tree d ON r.parent_node_id = d.node_id
    )
    SELECT ARRAY_AGG(node_id) INTO v_affected_nodes FROM dependency_tree;

    -- Count critical paths affected
    SELECT COUNT(*) INTO v_critical_paths
    FROM mv_supply_chain_network
    WHERE node_id = ANY(v_affected_nodes)
    AND criticality IN ('critical', 'high');

    -- Store simulation results
    INSERT INTO supply_chain_risk_scenarios (
        scenario_name,
        scenario_type,
        description,
        likelihood,
        potential_impact,
        affected_nodes,
        mitigation_strategy,
        last_simulated_date
    )
    VALUES (
        'Disruption at Node ' || p_node_id,
        p_scenario_type,
        'Simulated disruption affecting ' || array_length(v_affected_nodes, 1) || ' nodes',
        'possible',
        CASE WHEN v_critical_paths > 3 THEN 'critical'
             WHEN v_critical_paths > 0 THEN 'high'
             ELSE 'medium' END,
        v_affected_nodes,
        CASE
            WHEN v_critical_paths > 3 THEN 'Immediate activation of business continuity plans and alternative sourcing'
            WHEN v_critical_paths > 0 THEN 'Prioritize affected critical paths for mitigation'
            ELSE 'Monitor for secondary impacts' END,
        CURRENT_TIMESTAMP
    );

    -- Flag affected vendors for review
    UPDATE third_parties
    SET supply_chain_risk_flag = TRUE
    WHERE party_id IN (
        SELECT DISTINCT party_id
        FROM supply_chain_nodes
        WHERE node_id = ANY(v_affected_nodes)
    );

    COMMIT;
END;
$$;


-- continuous monitoring integration
-- business case: real-time monitoring of vendors digital footprints for early detection of security incidents, financial distress signals, and reputation changes
CREATE TABLE tprm.monitoring_configurations (
    config_id SERIAL PRIMARY KEY,
    party_id INTEGER REFERENCES third_parties(party_id),
    monitor_type VARCHAR(50) NOT NULL, -- 'dark-web', 'financial', 'reputation', 'availability'
    is_active BOOLEAN DEFAULT TRUE,
    frequency_minutes INTEGER NOT NULL,
    alert_thresholds JSONB NOT NULL,
    last_scan TIMESTAMP WITH TIME ZONE,
    next_scan TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.monitoring_findings (
    finding_id SERIAL PRIMARY KEY,
    config_id INTEGER NOT NULL REFERENCES monitoring_configurations(config_id),
    detected_at TIMESTAMP WITH TIME ZONE NOT NULL,
    severity VARCHAR(20) NOT NULL, -- 'info', 'low', 'medium', 'high', 'critical'
    finding_type VARCHAR(100) NOT NULL,
    description TEXT NOT NULL,
    raw_data JSONB,
    reference_url VARCHAR(512),
    is_false_positive BOOLEAN DEFAULT FALSE,
    reviewed_by VARCHAR(255),
    review_notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.monitoring_services (
    service_id SERIAL PRIMARY KEY,
    service_name VARCHAR(255) NOT NULL,
    service_type VARCHAR(100) NOT NULL,
    api_endpoint VARCHAR(512),
    auth_config JSONB, -- Stores API keys, credentials securely
    last_successful_connection TIMESTAMP WITH TIME ZONE,
    status VARCHAR(20) DEFAULT 'active',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);


CREATE VIEW tprm.vw_monitoring_alerts AS
SELECT
    mf.finding_id,
    mc.monitor_type,
    tp.party_id,
    tp.legal_name,
    mf.detected_at,
    mf.severity,
    mf.finding_type,
    mf.description,
    CASE
        WHEN mf.severity = 'critical' AND mf.is_false_positive = FALSE THEN 'immediate'
        WHEN mf.severity = 'high' AND mf.is_false_positive = FALSE THEN 'urgent'
        ELSE 'review'
    END AS response_priority,
    ms.service_name AS detection_source,
    mf.reviewed_by,
    mf.is_false_positive
FROM
    monitoring_findings mf
JOIN
    monitoring_configurations mc ON mf.config_id = mc.config_id
JOIN
    third_parties tp ON mc.party_id = tp.party_id
LEFT JOIN
    monitoring_services ms ON mc.monitor_type = ms.service_type
WHERE
    mf.detected_at >= CURRENT_DATE - INTERVAL '7 days'
ORDER BY
    response_priority DESC,
    mf.detected_at DESC;

CREATE VIEW tprm.vw_vendor_digital_footprint AS
SELECT
    tp.party_id,
    tp.legal_name,
    (SELECT COUNT(*) FROM monitoring_findings mf
     JOIN monitoring_configurations mc ON mf.config_id = mc.config_id
     WHERE mc.party_id = tp.party_id
     AND mf.detected_at >= CURRENT_DATE - INTERVAL '30 days') AS findings_30d,
    (SELECT COUNT(*) FROM monitoring_findings mf
     JOIN monitoring_configurations mc ON mf.config_id = mc.config_id
     WHERE mc.party_id = tp.party_id
     AND mf.severity IN ('high', 'critical')
     AND mf.is_false_positive = FALSE
     AND mf.detected_at >= CURRENT_DATE - INTERVAL '30 days') AS critical_findings_30d,
    (SELECT STRING_AGG(DISTINCT mc.monitor_type, ', ')
     FROM monitoring_configurations mc
     WHERE mc.party_id = tp.party_id
     AND mc.is_active = TRUE) AS active_monitors,
    (SELECT MAX(mf.detected_at)
     FROM monitoring_findings mf
     JOIN monitoring_configurations mc ON mf.config_id = mc.config_id
     WHERE mc.party_id = tp.party_id) AS last_finding_date
FROM
    third_parties tp
WHERE
    EXISTS (SELECT 1 FROM monitoring_configurations WHERE party_id = tp.party_id AND is_active = TRUE);

------job executions
      CREATE TABLE job_executions (
                  execution_id SERIAL PRIMARY KEY,
                  job_id INTEGER NOT NULL REFERENCES scheduled_jobs(job_id),
                  started_at TIMESTAMP WITH TIME ZONE NOT NULL,
                  completed_at TIMESTAMP WITH TIME ZONE,
                  status VARCHAR(50) NOT NULL, -- 'running', 'completed', 'failed'
                  execution_log TEXT,
                  created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
              );

  ---scheduled jobs
  CREATE TABLE scheduled_jobs (
              job_id SERIAL PRIMARY KEY,
              job_name VARCHAR(255) NOT NULL,
              description TEXT,
              job_type VARCHAR(100) NOT NULL, -- 'procedure', 'script', 'maintenance'
              job_schedule VARCHAR(100) NOT NULL, -- cron expression
              procedure_name VARCHAR(255),
              script_path VARCHAR(255),
              is_active BOOLEAN DEFAULT TRUE,
              last_run_at TIMESTAMP WITH TIME ZONE,
              next_run_at TIMESTAMP WITH TIME ZONE,
              created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
              updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
          );
        -- Procedure to register a new job
        CREATE OR REPLACE PROCEDURE register_job(
            p_job_name VARCHAR(255),
            p_description TEXT,
            p_job_type VARCHAR(100),
            p_job_schedule VARCHAR(100),
            p_procedure_name VARCHAR(255) DEFAULT NULL,
            p_script_path VARCHAR(255) DEFAULT NULL
        )
        LANGUAGE plpgsql
        AS $$
        BEGIN
            INSERT INTO scheduled_jobs (
                job_name,
                description,
                job_type,
                job_schedule,
                procedure_name,
                script_path,
                next_run_at
            )
            VALUES (
                p_job_name,
                p_description,
                p_job_type,
                p_job_schedule,
                p_procedure_name,
                p_script_path,
                -- Calculate next run time from cron expression (simplified example)
                CASE
                    WHEN p_job_schedule = '0 2 * * *' THEN CURRENT_DATE + INTERVAL '1 day' + INTERVAL '2 hours'
                    WHEN p_job_schedule = '0 * * * *' THEN CURRENT_DATE + INTERVAL '1 hour'
                    ELSE CURRENT_TIMESTAMP + INTERVAL '1 day'
                END
            );

            COMMIT;
        END;
        $$;

        -- Procedure to execute a job
        CREATE OR REPLACE PROCEDURE execute_job(
            p_job_id INTEGER
        )
        LANGUAGE plpgsql
        AS $$
        DECLARE
            v_job RECORD;
            v_execution_id INTEGER;
            v_start_time TIMESTAMP WITH TIME ZONE := CURRENT_TIMESTAMP;
            v_result TEXT;
        BEGIN
            -- Get job details
            SELECT * INTO v_job FROM scheduled_jobs WHERE job_id = p_job_id;

            IF NOT FOUND THEN
                RAISE EXCEPTION 'Job with ID % not found', p_job_id;
            END IF;

            -- Log execution start
            INSERT INTO job_executions (
                job_id,
                started_at,
                status
            )
            VALUES (
                p_job_id,
                v_start_time,
                'running'
            )
            RETURNING execution_id INTO v_execution_id;

            -- Execute the job based on type
            IF v_job.job_type = 'procedure' AND v_job.procedure_name IS NOT NULL THEN
                BEGIN
                    EXECUTE 'CALL ' || v_job.procedure_name;
                    v_result := 'Procedure executed successfully';
                EXCEPTION WHEN OTHERS THEN
                    v_result := 'Error executing procedure: ' || SQLERRM;
                END;
            ELSIF v_job.job_type = 'script' AND v_job.script_path IS NOT NULL THEN
                -- In a real implementation, this would execute an external script
                v_result := 'Script execution would run here: ' || v_job.script_path;
            ELSE
                v_result := 'Unsupported job type or missing procedure/script';
            END IF;

            -- Update execution log
            UPDATE job_executions
            SET
                completed_at = CURRENT_TIMESTAMP,
                status = CASE WHEN v_result LIKE 'Error%' THEN 'failed' ELSE 'completed' END,
                execution_log = v_result
            WHERE
                execution_id = v_execution_id;

            -- Update job with last run time and calculate next run
            UPDATE scheduled_jobs
            SET
                last_run_at = v_start_time,
                next_run_at = CASE
                    WHEN job_schedule = '0 2 * * *' THEN CURRENT_DATE + INTERVAL '1 day' + INTERVAL '2 hours'
                    WHEN job_schedule = '0 * * * *' THEN CURRENT_DATE + INTERVAL '1 hour'
                    ELSE CURRENT_TIMESTAMP + INTERVAL '1 day'
                END,
                updated_at = CURRENT_TIMESTAMP
            WHERE
                job_id = p_job_id;

            COMMIT;
        END;
        $$;


  CREATE OR REPLACE PROCEDURE tprm.execute_vendor_monitoring()
      LANGUAGE plpgsql
      AS $$
      DECLARE
          v_config RECORD;
          v_service RECORD;
          v_result JSONB;
          v_finding_count INTEGER := 0;
      BEGIN
          -- Process each active monitoring configuration
          FOR v_config IN SELECT * FROM monitoring_configurations
                         WHERE is_active = TRUE
                         AND (next_scan IS NULL OR next_scan <= CURRENT_TIMESTAMP)
          LOOP
              -- Get service configuration
              SELECT * INTO v_service FROM monitoring_services
              WHERE service_type = v_config.monitor_type
              AND status = 'active';

              IF FOUND THEN
                  -- In production, this would call the actual monitoring service API
                  -- Simulate API call with different results based on monitor type
                  CASE v_config.monitor_type
                      WHEN 'dark-web' THEN
                          -- Simulate dark web findings
                          IF random() < 0.2 THEN  -- 20% chance of finding
                              INSERT INTO monitoring_findings (
                                  config_id,
                                  detected_at,
                                  severity,
                                  finding_type,
                                  description,
                                  raw_data
                              )
                              VALUES (
                                  v_config.config_id,
                                  CURRENT_TIMESTAMP,
                                  CASE
                                      WHEN random() < 0.3 THEN 'critical'
                                      WHEN random() < 0.6 THEN 'high'
                                      ELSE 'medium'
                                  END,
                                  'credential-leak',
                                  'Vendor credentials detected on dark web',
                                  json_build_object(
                                      'leak_source', 'pastebin.com',
                                      'data_type', 'credentials',
                                      'confidence', round(random()::numeric, 2)
                                  )
                              );
                              v_finding_count := v_finding_count + 1;
                          END IF;

                      WHEN 'financial' THEN
                          -- Simulate financial alerts
                          IF random() < 0.15 THEN
                              INSERT INTO monitoring_findings (
                                  config_id,
                                  detected_at,
                                  severity,
                                  finding_type,
                                  description,
                                  raw_data
                              )
                              VALUES (
                                  v_config.config_id,
                                  CURRENT_TIMESTAMP,
                                  CASE
                                      WHEN random() < 0.2 THEN 'high'
                                      WHEN random() < 0.5 THEN 'medium'
                                      ELSE 'low'
                                  END,
                                  'financial-indicator',
                                  'Negative financial indicator detected',
                                  json_build_object(
                                      'indicator_type', CASE round(random()*3)
                                                        WHEN 0 THEN 'downgrade'
                                                        WHEN 1 THEN 'delisting'
                                                        ELSE 'bankruptcy' END,
                                      'source', 'financial-api'
                                  )
                              );
                              v_finding_count := v_finding_count + 1;
                          END IF;

                      -- Additional monitor types would be implemented similarly
                      ELSE
                          -- Generic monitoring result
                          IF random() < 0.1 THEN
                              INSERT INTO monitoring_findings (
                                  config_id,
                                  detected_at,
                                  severity,
                                  finding_type,
                                  description
                              )
                              VALUES (
                                  v_config.config_id,
                                  CURRENT_TIMESTAMP,
                                  'low',
                                  'generic-alert',
                                  'Activity detected on ' || v_config.monitor_type || ' monitor'
                              );
                              v_finding_count := v_finding_count + 1;
                          END IF;
                  END CASE;

                  -- Update configuration with next scan time
                  UPDATE monitoring_configurations
                  SET
                      last_scan = CURRENT_TIMESTAMP,
                      next_scan = CURRENT_TIMESTAMP + (v_config.frequency_minutes * INTERVAL '1 minute'),
                      updated_at = CURRENT_TIMESTAMP
                  WHERE
                      config_id = v_config.config_id;
              END IF;
          END LOOP;

          -- Update monitoring services status
          UPDATE monitoring_services
          SET
              last_successful_connection = CURRENT_TIMESTAMP,
              updated_at = CURRENT_TIMESTAMP
          WHERE
              status = 'active';

          RAISE NOTICE 'Monitoring scan completed. % findings detected.', v_finding_count;
          COMMIT;
      END;
      $$;

      CREATE OR REPLACE PROCEDURE tprm.triage_monitoring_findings()
      LANGUAGE plpgsql
      AS $$
      BEGIN
          -- Auto-close low severity findings older than 14 days without review
          UPDATE monitoring_findings
          SET
              is_false_positive = TRUE,
              review_notes = 'Auto-closed as stale low severity finding',
              reviewed_by = 'system'
          WHERE
              severity = 'low'
              AND detected_at < CURRENT_DATE - INTERVAL '14 days'
              AND reviewed_by IS NULL;

          -- Create risk items for critical findings
          INSERT INTO risk_items (
              source_type,
              source_id,
              title,
              description,
              severity,
              status,
              due_date
          )
          SELECT
              'monitoring',
              mf.finding_id,
              'Monitoring Alert: ' || mf.finding_type,
              mf.description,
              mf.severity,
              'open',
              CURRENT_DATE + CASE
                  WHEN mf.severity = 'critical' THEN INTERVAL '1 day'
                  WHEN mf.severity = 'high' THEN INTERVAL '3 days'
                  ELSE INTERVAL '7 days'
              END
          FROM
              monitoring_findings mf
          WHERE
              mf.severity IN ('high', 'critical')
              AND mf.is_false_positive = FALSE
              AND NOT EXISTS (
                  SELECT 1 FROM risk_items
                  WHERE source_type = 'monitoring'
                  AND source_id = mf.finding_id
              );

          -- Update vendor risk scores based on recent findings
          UPDATE third_parties
          SET
              monitoring_risk_score = (
                  SELECT COUNT(*) * CASE
                      WHEN mf.severity = 'critical' THEN 10
                      WHEN mf.severity = 'high' THEN 5
                      WHEN mf.severity = 'medium' THEN 2
                      ELSE 1
                  END
                  FROM monitoring_findings mf
                  JOIN monitoring_configurations mc ON mf.config_id = mc.config_id
                  WHERE mc.party_id = third_parties.party_id
                  AND mf.is_false_positive = FALSE
                  AND mf.detected_at >= CURRENT_DATE - INTERVAL '30 days'
              ),
              updated_at = CURRENT_TIMESTAMP
          WHERE
              party_id IN (
                  SELECT DISTINCT mc.party_id
                  FROM monitoring_findings mf
                  JOIN monitoring_configurations mc ON mf.config_id = mc.config_id
                  WHERE mf.detected_at >= CURRENT_DATE - INTERVAL '30 days'
              );

          COMMIT;
      END;
      $$;

---------------------------------------------------------------------------------------------------------
--business case: AI powered vendor risk scoring engine
-- traditional risk scoring models often lack the sophistication ot capture emerging risk patterns
-- AN AI-Driven scoring engine can dynamically adjust risk weights based on real-time data and predictive analytics
CREATE TABLE tprm.ai_risk_models (
    model_id SERIAL PRIMARY KEY,
    model_name VARCHAR(255) NOT NULL,
    model_version VARCHAR(50) NOT NULL,
    model_type VARCHAR(100) NOT NULL, -- 'financial', 'cyber', 'operational', 'composite'
    training_data_range DATERANGE NOT NULL,
    accuracy_score NUMERIC(5,4),
    precision_score NUMERIC(5,4),
    recall_score NUMERIC(5,4),
    features_used JSONB NOT NULL,
    deployment_date TIMESTAMP WITH TIME ZONE,
    is_active BOOLEAN DEFAULT FALSE,
    retrain_frequency INTERVAL DEFAULT '90 days',
    next_retrain_date TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.ai_vendor_scores (
    score_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    model_id INTEGER NOT NULL REFERENCES ai_risk_models(model_id),
    score_date TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    risk_score NUMERIC(10,6) NOT NULL,
    risk_percentile NUMERIC(5,2),
    confidence_interval_lower NUMERIC(10,6),
    confidence_interval_upper NUMERIC(10,6),
    key_factors JSONB NOT NULL, -- Top 3-5 factors contributing to score
    explanation TEXT,
    reviewed BOOLEAN DEFAULT FALSE,
    reviewed_by VARCHAR(255),
    review_notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.ai_model_audit_log (
    audit_id SERIAL PRIMARY KEY,
    model_id INTEGER NOT NULL REFERENCES ai_risk_models(model_id),
    audit_type VARCHAR(50) NOT NULL, -- 'deployment', 'retraining', 'validation'
    audit_date TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    previous_version VARCHAR(50),
    performance_metrics JSONB,
    findings TEXT,
    recommendations TEXT,
    conducted_by VARCHAR(255) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);



  -- core audit trail schema

  CREATE TABLE auditcore.audit_log (
    audit_id SERIAL PRIMARY KEY,
    table_name VARCHAR(100) NOT NULL,
    record_id INTEGER,
    operation VARCHAR(20) NOT NULL, -- 'INSERT', 'UPDATE', 'DELETE'
    old_values JSONB,
    new_values JSONB,
    changed_by VARCHAR(100) DEFAULT CURRENT_USER,
    changed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE auditcore.audit_config (
    config_id SERIAL PRIMARY KEY,
    table_name VARCHAR(100) NOT NULL,
    is_audited BOOLEAN DEFAULT TRUE,
    columns_to_audit TEXT, -- comma-separated list or NULL for all
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_table UNIQUE (table_name)
);

CREATE MATERIALIZED VIEW tprm.mv_ai_risk_trends AS
SELECT
    tp.party_id,
    tp.legal_name,
    arm.model_type,
    DATE(avs.score_date) AS score_date,
    avs.risk_score,
    avs.risk_percentile,
    avs.key_factors,
    LAG(avs.risk_score) OVER (PARTITION BY tp.party_id, arm.model_id ORDER BY avs.score_date) AS previous_score,
    avs.risk_score - LAG(avs.risk_score) OVER (PARTITION BY tp.party_id, arm.model_id ORDER BY avs.score_date) AS score_delta,
    CASE
        WHEN avs.risk_score - LAG(avs.risk_score) OVER (PARTITION BY tp.party_id, arm.model_id ORDER BY avs.score_date) > 20 THEN 'sharp_increase'
        WHEN avs.risk_score - LAG(avs.risk_score) OVER (PARTITION BY tp.party_id, arm.model_id ORDER BY avs.score_date) > 10 THEN 'moderate_increase'
        WHEN avs.risk_score - LAG(avs.risk_score) OVER (PARTITION BY tp.party_id, arm.model_id ORDER BY avs.score_date) < -20 THEN 'sharp_decrease'
        WHEN avs.risk_score - LAG(avs.risk_score) OVER (PARTITION BY tp.party_id, arm.model_id ORDER BY avs.score_date) < -10 THEN 'moderate_decrease'
        ELSE 'stable'
    END AS score_trend,
    (SELECT COUNT(*) FROM incidents i
     WHERE i.party_id = tp.party_id
     AND i.incident_date BETWEEN DATE(avs.score_date) - INTERVAL '90 days' AND DATE(avs.score_date) + INTERVAL '90 days'
     AND i.severity IN ('high','critical')) AS surrounding_incidents
FROM
    ai_vendor_scores avs
JOIN
    third_parties tp ON avs.party_id = tp.party_id
JOIN
    ai_risk_models arm ON avs.model_id = arm.model_id
WHERE
    arm.is_active = TRUE
    AND avs.score_date >= CURRENT_DATE - INTERVAL '12 months'
WITH DATA;


--
CREATE OR REPLACE PROCEDURE tprm.generate_ai_risk_scores(
    p_model_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_model RECORD;
    v_vendor RECORD;
    v_features JSONB;
    v_score NUMERIC(10,6);
    v_factors JSONB;
    v_explanation TEXT;
BEGIN
    -- Get model details
    SELECT * INTO v_model FROM ai_risk_models WHERE model_id = p_model_id;

    IF NOT FOUND THEN
        RAISE EXCEPTION 'AI risk model with ID % not found', p_model_id;
    END IF;

    -- For each active vendor, generate score (in practice would call ML service)
    FOR v_vendor IN SELECT party_id FROM third_parties WHERE is_active = TRUE LOOP
        -- Simulate feature collection (real implementation would gather actual data)
        v_features := jsonb_build_object(
            'financial_health', (SELECT financial_health_score FROM financial_indicators
                               WHERE party_id = v_vendor.party_id
                               ORDER BY reporting_date DESC LIMIT 1),
            'incident_count', (SELECT COUNT(*) FROM incidents
                             WHERE party_id = v_vendor.party_id
                             AND incident_date >= CURRENT_DATE - INTERVAL '12 months'),
            'compliance_gaps', (SELECT COUNT(*) FROM party_compliance
                               WHERE party_id = v_vendor.party_id
                               AND compliance_status = 'non-compliant'),
            'esg_score', (SELECT overall_score FROM vendor_esg_assessments
                         WHERE party_id = v_vendor.party_id
                         ORDER BY assessment_date DESC LIMIT 1)
        );

        -- Simulate scoring (replace with actual model call)
        v_score := random() * 100;
        v_factors := jsonb_build_array(
            jsonb_build_object('factor', 'financial_health', 'weight', random()),
            jsonb_build_object('factor', 'incident_history', 'weight', random()),
            jsonb_build_object('factor', 'compliance_gaps', 'weight', random())
        );
        v_explanation := 'Score reflects ' ||
            CASE
                WHEN v_score > 75 THEN 'elevated risk across multiple dimensions'
                WHEN v_score > 50 THEN 'moderate risk with specific concerns'
                ELSE 'acceptable risk profile'
            END;

        -- Store score
        INSERT INTO ai_vendor_scores (
            party_id,
            model_id,
            risk_score,
            key_factors,
            explanation,
            confidence_interval_lower,
            confidence_interval_upper
        )
        VALUES (
            v_vendor.party_id,
            p_model_id,
            v_score,
            v_factors,
            v_explanation,
            v_score * 0.9,
            v_score * 1.1
        );
    END LOOP;

    -- Calculate percentiles
    UPDATE ai_vendor_scores avs
    SET risk_percentile = (
        SELECT COUNT(*)::FLOAT /
               (SELECT COUNT(*) FROM ai_vendor_scores avs2
                WHERE avs2.model_id = avs.model_id
                AND avs2.score_date = avs.score_date) * 100
        FROM ai_vendor_scores avs2
        WHERE avs2.model_id = avs.model_id
        AND avs2.score_date = avs.score_date
        AND avs2.risk_score <= avs.risk_score
    )
    WHERE model_id = p_model_id
    AND score_date = CURRENT_TIMESTAMP;

    -- Refresh materialized view
    REFRESH MATERIALIZED VIEW mv_ai_risk_trends;

    COMMIT;
END;
$$;

CREATE OR REPLACE PROCEDURE tprm.retrain_ai_model(
    p_model_id INTEGER,
    p_training_data_range DATERANGE DEFAULT NULL
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_new_version VARCHAR(50);
    v_accuracy NUMERIC(5,4);
BEGIN
    -- Validate model exists
    PERFORM 1 FROM ai_risk_models WHERE model_id = p_model_id;
    IF NOT FOUND THEN
        RAISE EXCEPTION 'Model with ID % not found', p_model_id;
    END IF;

    -- Generate new version number
    SELECT
        model_version || '.' ||
        (COALESCE(SUBSTRING(model_version, '\.(\d+)$')::INT + 1,
        1) INTO v_new_version
    FROM ai_risk_models
    WHERE model_id = p_model_id;

    -- In practice, this would:
    -- 1. Extract training data
    -- 2. Call ML platform retraining API
    -- 3. Receive new model version and metrics
    -- Here we simulate with random metrics
    v_accuracy := 0.85 + random() * 0.1;

    -- Create new model version
    INSERT INTO ai_risk_models (
        model_name,
        model_version,
        model_type,
        training_data_range,
        accuracy_score,
        precision_score,
        recall_score,
        features_used,
        deployment_date,
        is_active,
        retrain_frequency,
        next_retrain_date
    )
    SELECT
        model_name,
        v_new_version,
        model_type,
        COALESCE(p_training_data_range, training_data_range),
        v_accuracy,
        v_accuracy * 0.9,
        v_accuracy * 0.95,
        features_used,
        CURRENT_TIMESTAMP,
        FALSE, -- Not active until validated
        retrain_frequency,
        CURRENT_TIMESTAMP + retrain_frequency
    FROM ai_risk_models
    WHERE model_id = p_model_id
    RETURNING model_id INTO p_model_id;

    -- Log audit record
    INSERT INTO ai_model_audit_log (
        model_id,
        audit_type,
        previous_version,
        performance_metrics,
        conducted_by
    )
    VALUES (
        p_model_id,
        'retraining',
        v_new_version,
        jsonb_build_object(
            'accuracy', v_accuracy,
            'precision', v_accuracy * 0.9,
            'recall', v_accuracy * 0.95
        ),
        CURRENT_USER
    );

    COMMIT;
END;
$$;
--------
CREATE TABLE tprm.control_test_templates (
    template_id SERIAL PRIMARY KEY,
    template_name VARCHAR(255) NOT NULL,
    description TEXT,
    applicable_regulations VARCHAR(255)[],
    test_frequency VARCHAR(50) NOT NULL, -- 'daily', 'weekly', 'monthly', 'quarterly'
    test_method VARCHAR(100) NOT NULL, -- 'api', 'questionnaire', 'file-upload'
    test_script TEXT, -- Could be SQL, Python, etc. depending on implementation
    expected_result_pattern TEXT,
    severity VARCHAR(20) NOT NULL, -- 'low', 'medium', 'high', 'critical'
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.vendor_control_tests (
    test_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    template_id INTEGER NOT NULL REFERENCES control_test_templates(template_id),
    test_date TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    test_status VARCHAR(50) NOT NULL, -- 'pending', 'in-progress', 'completed', 'failed'
    test_result TEXT,
    result_status VARCHAR(50), -- 'pass', 'fail', 'partial'
    evidence_reference VARCHAR(255),
    next_test_date TIMESTAMP WITH TIME ZONE,
    executed_by VARCHAR(255), -- System or user
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.control_test_exceptions (
    exception_id SERIAL PRIMARY KEY,
    test_id INTEGER NOT NULL REFERENCES vendor_control_tests(test_id),
    exception_reason TEXT NOT NULL,
    requested_by VARCHAR(255) NOT NULL,
    approved_by VARCHAR(255),
    approval_date TIMESTAMP WITH TIME ZONE,
    exception_end_date TIMESTAMP WITH TIME ZONE,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);


CREATE VIEW tprm.vw_control_test_coverage AS
SELECT
    tp.party_id,
    tp.legal_name,
    COUNT(DISTINCT ct.template_id) AS applicable_controls,
    COUNT(DISTINCT vct.template_id) AS tested_controls,
    ROUND(COUNT(DISTINCT vct.template_id) * 100.0 /
          NULLIF(COUNT(DISTINCT ct.template_id), 0), 2) AS coverage_percentage,
    COUNT(DISTINCT vct.template_id) FILTER (WHERE vct.result_status = 'pass') AS passed_controls,
    COUNT(DISTINCT vct.template_id) FILTER (WHERE vct.result_status = 'fail') AS failed_controls,
    MAX(vct.test_date) AS last_test_date
FROM
    third_parties tp
CROSS JOIN
    control_test_templates ct
LEFT JOIN
    vendor_control_tests vct ON tp.party_id = vct.party_id AND ct.template_id = vct.template_id
    AND vct.test_date >= CURRENT_DATE - INTERVAL '12 months'
GROUP BY
    tp.party_id, tp.legal_name;

CREATE VIEW tprm.vw_control_test_failures AS
SELECT
    vct.test_id,
    tp.party_id,
    tp.legal_name,
    ctt.template_name,
    ctt.severity,
    vct.test_date,
    vct.result_status,
    vct.test_result,
    (SELECT COUNT(*)
     FROM vendor_control_tests vct2
     WHERE vct2.party_id = vct.party_id
     AND vct2.template_id = vct.template_id
     AND vct2.result_status = 'fail'
     AND vct2.test_date >= CURRENT_DATE - INTERVAL '12 months') AS repeat_failures_12m,
    CASE
        WHEN EXISTS (SELECT 1 FROM control_test_exceptions cte
                    WHERE cte.test_id = vct.test_id AND cte.is_active = TRUE)
        THEN 'exemption-granted'
        ELSE 'needs-remediation'
    END AS remediation_status
FROM
    vendor_control_tests vct
JOIN
    third_parties tp ON vct.party_id = tp.party_id
JOIN
    control_test_templates ctt ON vct.template_id = ctt.template_id
WHERE
    vct.result_status = 'fail'
    AND vct.test_date >= CURRENT_DATE - INTERVAL '90 days';



    CREATE OR REPLACE PROCEDURE tprm.execute_control_tests()
    LANGUAGE plpgsql
    AS $$
    DECLARE
        v_test RECORD;
        v_result TEXT;
        v_status VARCHAR(50);
        v_evidence VARCHAR(255);
    BEGIN
        -- Process tests due for execution
        FOR v_test IN
            SELECT
                tp.party_id,
                ctt.template_id,
                ctt.test_method,
                ctt.test_script,
                ctt.expected_result_pattern
            FROM
                third_parties tp
            CROSS JOIN
                control_test_templates ctt
            WHERE
                -- Tests that are due (no test or past next_test_date)
                (NOT EXISTS (
                    SELECT 1 FROM vendor_control_tests vct
                    WHERE vct.party_id = tp.party_id
                    AND vct.template_id = ctt.template_id
                ) OR EXISTS (
                    SELECT 1 FROM vendor_control_tests vct
                    WHERE vct.party_id = tp.party_id
                    AND vct.template_id = ctt.template_id
                    AND (vct.next_test_date IS NULL OR vct.next_test_date <= CURRENT_TIMESTAMP)
                ))
                -- And no active exceptions
                AND NOT EXISTS (
                    SELECT 1 FROM vendor_control_tests vct
                    JOIN control_test_exceptions cte ON vct.test_id = cte.test_id
                    WHERE vct.party_id = tp.party_id
                    AND vct.template_id = ctt.template_id
                    AND cte.is_active = TRUE
                    AND (cte.exception_end_date IS NULL OR cte.exception_end_date >= CURRENT_DATE)
                )
        LOOP
            -- Execute test based on method (simplified example)
            CASE v_test.test_method
                WHEN 'api' THEN
                    -- Call vendor API and validate response
                    -- In practice, this would use dynamic SQL or external call
                    v_result := 'API Response: 200 OK';
                    v_status := CASE WHEN random() > 0.3 THEN 'pass' ELSE 'fail' END;
                    v_evidence := 'api-response-' || v_test.party_id || '-' || v_test.template_id || '.json';

                WHEN 'questionnaire' THEN
                    -- Send automated questionnaire
                    v_result := 'Questionnaire completed with ' ||
                               CASE WHEN random() > 0.4 THEN 'all' ELSE 'some' END || ' controls met';
                    v_status := CASE WHEN random() > 0.4 THEN 'pass' ELSE 'partial' END;
                    v_evidence := 'questionnaire-response-' || v_test.party_id || '-' || v_test.template_id || '.pdf';

                ELSE
                    -- Default test method
                    v_result := 'Test executed with simulated result';
                    v_status := CASE WHEN random() > 0.2 THEN 'pass' ELSE 'fail' END;
                    v_evidence := 'test-result-' || v_test.party_id || '-' || v_test.template_id || '.txt';
            END CASE;

            -- Record test result
            INSERT INTO vendor_control_tests (
                party_id,
                template_id,
                test_status,
                test_result,
                result_status,
                evidence_reference,
                next_test_date,
                executed_by
            )
            VALUES (
                v_test.party_id,
                v_test.template_id,
                'completed',
                v_result,
                v_status,
                v_evidence,
                CURRENT_TIMESTAMP + (
                    SELECT INTERVAL '1 ' ||
                    CASE test_frequency
                        WHEN 'daily' THEN 'day'
                        WHEN 'weekly' THEN 'week'
                        WHEN 'monthly' THEN 'month'
                        ELSE 'quarter'
                    END
                    FROM control_test_templates WHERE template_id = v_test.template_id
                ),
                'automated-test-engine'
            );
        END LOOP;

        -- Create remediation tasks for failed tests
        INSERT INTO remediation_tasks (
            source_type,
            source_id,
            party_id,
            title,
            description,
            severity,
            due_date,
            assigned_to
        )
        SELECT
            'control-test',
            vct.test_id,
            vct.party_id,
            'Control Test Failure: ' || ctt.template_name,
            'Test result: ' || vct.test_result || E'\nExpected: ' || ctt.expected_result_pattern,
            ctt.severity,
            CURRENT_DATE +
                CASE ctt.severity
                    WHEN 'critical' THEN INTERVAL '3 days'
                    WHEN 'high' THEN INTERVAL '7 days'
                    WHEN 'medium' THEN INTERVAL '14 days'
                    ELSE INTERVAL '30 days'
                END,
            (SELECT user_id FROM users WHERE role = 'compliance' ORDER BY random() LIMIT 1)
        FROM
            vendor_control_tests vct
        JOIN
            control_test_templates ctt ON vct.template_id = ctt.template_id
        WHERE
            vct.result_status IN ('fail', 'partial')
            AND vct.test_date >= CURRENT_DATE - INTERVAL '7 days'
            AND NOT EXISTS (
                SELECT 1 FROM remediation_tasks rt
                WHERE rt.source_type = 'control-test'
                AND rt.source_id = vct.test_id
            );

        COMMIT;
    END;
    $$;

    CREATE OR REPLACE PROCEDURE tprm.grant_control_test_exception(
        p_test_id INTEGER,
        p_reason TEXT,
        p_requested_by VARCHAR(255),
        p_end_date TIMESTAMP WITH TIME ZONE DEFAULT NULL
    )
    LANGUAGE plpgsql
    AS $$
    DECLARE
        v_test RECORD;
    BEGIN
        -- Get test details
        SELECT * INTO v_test FROM vendor_control_tests WHERE test_id = p_test_id;

        IF NOT FOUND THEN
            RAISE EXCEPTION 'Control test with ID % not found', p_test_id;
        END IF;

        -- Create exception
        INSERT INTO control_test_exceptions (
            test_id,
            exception_reason,
            requested_by,
            exception_end_date,
            is_active
        )
        VALUES (
            p_test_id,
            p_reason,
            p_requested_by,
            p_end_date,
            TRUE
        );

        -- Update next test date to after exception period
        IF p_end_date IS NOT NULL THEN
            UPDATE vendor_control_tests
            SET next_test_date = p_end_date + INTERVAL '1 day'
            WHERE test_id = p_test_id;
        END IF;

        -- Close any open remediation tasks
        UPDATE remediation_tasks
        SET status = 'closed',
            resolution = 'Exception granted',
            resolved_at = CURRENT_TIMESTAMP,
            resolved_by = p_requested_by
        WHERE source_type = 'control-test'
        AND source_id = p_test_id
        AND status NOT IN ('closed', 'rejected');

        COMMIT;
    END;
    $$;



----dynamic risk threshold adjustment system
-- business case: static risk threshold often fails to adapt to changing business environments. a dynamic system adjusts thresholds based on internal risk appetite and external factors
CREATE TABLE tprm.risk_appetite_settings (
    setting_id SERIAL PRIMARY KEY,
    risk_domain VARCHAR(100) NOT NULL, -- 'financial', 'cyber', 'compliance', 'operational'
    base_threshold NUMERIC(10,2) NOT NULL,
    min_threshold NUMERIC(10,2) NOT NULL,
    max_threshold NUMERIC(10,2) NOT NULL,
    adjustment_factors JSONB NOT NULL, -- Factors that influence threshold adjustments
    current_threshold NUMERIC(10,2) NOT NULL,
    last_adjusted TIMESTAMP WITH TIME ZONE,
    next_review TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.threshold_adjustment_log (
    adjustment_id SERIAL PRIMARY KEY,
    setting_id INTEGER NOT NULL REFERENCES risk_appetite_settings(setting_id),
    adjustment_date TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    previous_threshold NUMERIC(10,2) NOT NULL,
    new_threshold NUMERIC(10,2) NOT NULL,
    adjustment_reason TEXT NOT NULL,
    triggered_by VARCHAR(255) NOT NULL, -- 'system', 'user'
    external_factors JSONB, -- External conditions influencing adjustment
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.external_risk_indicators (
    indicator_id SERIAL PRIMARY KEY,
    indicator_name VARCHAR(255) NOT NULL,
    source VARCHAR(255) NOT NULL,
    current_value NUMERIC(15,2) NOT NULL,
    last_updated TIMESTAMP WITH TIME ZONE,
    weight NUMERIC(5,2) NOT NULL, -- Influence on threshold adjustments
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);


CREATE VIEW tprm.vw_risk_threshold_status AS
SELECT
    ras.setting_id,
    ras.risk_domain,
    ras.base_threshold,
    ras.current_threshold,
    ras.min_threshold,
    ras.max_threshold,
    CASE
        WHEN ras.current_threshold > ras.base_threshold THEN 'elevated'
        WHEN ras.current_threshold < ras.base_threshold THEN 'reduced'
        ELSE 'baseline'
    END AS threshold_status,
    ras.last_adjusted,
    ras.next_review,
    (SELECT COUNT(*)
     FROM threshold_adjustment_log tal
     WHERE tal.setting_id = ras.setting_id
     AND tal.adjustment_date >= CURRENT_DATE - INTERVAL '12 months') AS adjustments_12m,
    (SELECT STRING_AGG(eri.indicator_name || ': ' || eri.current_value, ', ')
     FROM jsonb_each_text(ras.adjustment_factors) j
     JOIN external_risk_indicators eri ON j.key = eri.indicator_name
     WHERE j.value::BOOLEAN = TRUE) AS active_factors
FROM
    risk_appetite_settings ras;

CREATE VIEW tprm.vw_threshold_breaches AS
SELECT
    tp.party_id,
    tp.legal_name,
    kd.kpi_name,
    km.measured_value,
    ras.current_threshold,
    CASE
        WHEN kd.target_direction = 'higher' AND km.measured_value > ras.current_threshold THEN 'breach'
        WHEN kd.target_direction = 'lower' AND km.measured_value < ras.current_threshold THEN 'breach'
        ELSE 'within-limits'
    END AS breach_status,
    km.measurement_date,
    (SELECT adjustment_reason
     FROM threshold_adjustment_log tal
     WHERE tal.setting_id = ras.setting_id
     ORDER BY tal.adjustment_date DESC LIMIT 1) AS last_adjustment_reason
FROM
    kpi_measurements km
JOIN
    kpi_definitions kd ON km.kpi_id = kd.kpi_id
JOIN
    third_parties tp ON km.party_id = tp.party_id
JOIN
    risk_appetite_settings ras ON kd.category = ras.risk_domain
WHERE
    km.measurement_date >= CURRENT_DATE - INTERVAL '7 days'
    AND (
        (kd.target_direction = 'higher' AND km.measured_value > ras.current_threshold)
        OR (kd.target_direction = 'lower' AND km.measured_value < ras.current_threshold)
    );

--- business case: smart contract analysis
-- traditional contract review processes are time-consuming and error-prone. An engine to automatically extract and analyze contract obligations, SLAs, risk clauses using NLP and machine learning
CREATE TABLE tprm.contract_ai_analysis (
    analysis_id SERIAL PRIMARY KEY,
    contract_id INTEGER NOT NULL REFERENCES contracts(contract_id),
    analysis_date TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    analysis_version VARCHAR(50) NOT NULL,
    key_terms JSONB NOT NULL, -- Extracted obligations, SLAs, penalties
    risk_clauses JSONB NOT NULL, -- Identified risk-related clauses
    obligation_matrix JSONB NOT NULL, -- Party-specific obligations
    sentiment_score NUMERIC(5,2), -- Overall contract sentiment (-1 to 1)
    red_flag_count INTEGER,
    summary TEXT,
    processed_document_path VARCHAR(512),
    is_validated BOOLEAN DEFAULT FALSE,
    validated_by VARCHAR(255),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.contract_obligations (
    obligation_id SERIAL PRIMARY KEY,
    analysis_id INTEGER NOT NULL REFERENCES contract_ai_analysis(analysis_id),
    obligation_type VARCHAR(100) NOT NULL, -- 'SLA', 'compliance', 'notification'
    obligated_party VARCHAR(50) NOT NULL, -- 'vendor', 'company', 'both'
    description TEXT NOT NULL,
    trigger_condition TEXT,
    timeframe TEXT,
    penalty_clause TEXT,
    criticality VARCHAR(20) NOT NULL, -- 'low', 'medium', 'high', 'critical'
    tracking_frequency VARCHAR(50), -- 'real-time', 'daily', 'weekly'
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.contract_compliance_events (
    event_id SERIAL PRIMARY KEY,
    obligation_id INTEGER NOT NULL REFERENCES contract_obligations(obligation_id),
    event_date TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    event_type VARCHAR(100) NOT NULL, -- 'fulfilled', 'missed', 'disputed'
    evidence_reference VARCHAR(512),
    notes TEXT,
    automated_validation BOOLEAN,
    validated_by VARCHAR(255),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);


CREATE MATERIALIZED VIEW tprm.mv_contract_risk_heatmap AS
SELECT
    c.contract_id,
    tp.party_id,
    tp.legal_name,
    c.contract_name,
    cai.analysis_date,
    cai.red_flag_count,
    cai.sentiment_score,
    COUNT(co.obligation_id) FILTER (WHERE co.criticality = 'critical') AS critical_obligations,
    COUNT(co.obligation_id) FILTER (WHERE co.criticality = 'high') AS high_obligations,
    COUNT(ce.event_id) FILTER (WHERE ce.event_type = 'missed') AS missed_obligations,
    COUNT(ce.event_id) FILTER (WHERE ce.event_type = 'fulfilled') AS fulfilled_obligations,
    CASE
        WHEN COUNT(ce.event_id) FILTER (WHERE ce.event_type = 'missed') > 3 THEN 'high-risk'
        WHEN COUNT(ce.event_id) FILTER (WHERE ce.event_type = 'missed') > 0 THEN 'medium-risk'
        ELSE 'low-risk'
    END AS compliance_risk_status,
    (SELECT STRING_AGG(co.obligation_type || ' (' || co.criticality || ')', ', ')
     FROM contract_obligations co
     WHERE co.analysis_id = cai.analysis_id
     AND co.criticality = 'critical') AS critical_obligation_types
FROM
    contract_ai_analysis cai
JOIN
    contracts c ON cai.contract_id = c.contract_id
JOIN
    third_parties tp ON c.party_id = tp.party_id
LEFT JOIN
    contract_obligations co ON cai.analysis_id = co.analysis_id
LEFT JOIN
    contract_compliance_events ce ON co.obligation_id = ce.obligation_id
WHERE
    cai.analysis_date >= CURRENT_DATE - INTERVAL '12 months'
GROUP BY
    c.contract_id, tp.party_id, tp.legal_name, c.contract_name, cai.analysis_id, cai.red_flag_count, cai.sentiment_score
WITH DATA;


CREATE OR REPLACE PROCEDURE analyze_contract_with_ai(
    p_contract_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_analysis_id INTEGER;
    v_document_path TEXT;
    v_ai_results JSONB;
BEGIN
    -- Get contract document path (implementation specific)
    SELECT document_location INTO v_document_path
    FROM contracts WHERE contract_id = p_contract_id;

    IF v_document_path IS NULL THEN
        RAISE EXCEPTION 'No document available for contract ID %', p_contract_id;
    END IF;

    -- In production, this would call an AI contract analysis service
    -- Simulate results with placeholder data
    v_ai_results := jsonb_build_object(
        'key_terms', jsonb_build_array(
            jsonb_build_object('term', 'SLA', 'value', '99.9% uptime', 'importance', 'high'),
            jsonb_build_object('term', 'Data Protection', 'value', 'GDPR compliance required', 'importance', 'critical')
        ),
        'risk_clauses', jsonb_build_array(
            jsonb_build_object('clause_type', 'Liability', 'text', 'Limited to 12 months fees', 'risk_level', 'medium'),
            jsonb_build_object('clause_type', 'Termination', 'text', '30 days notice required', 'risk_level', 'low')
        ),
        'obligation_matrix', jsonb_build_object(
            'vendor', jsonb_build_array('Monthly reporting', 'Incident notification within 2 hours'),
            'company', jsonb_build_array('Payment within 30 days', 'Access to necessary systems')
        ),
        'sentiment_score', 0.65,
        'red_flag_count', 2
    );

    -- Store AI analysis results
    INSERT INTO contract_ai_analysis (
        contract_id,
        analysis_version,
        key_terms,
        risk_clauses,
        obligation_matrix,
        sentiment_score,
        red_flag_count,
        summary,
        processed_document_path
    )
    VALUES (
        p_contract_id,
        'v2.1.5',
        v_ai_results->'key_terms',
        v_ai_results->'risk_clauses',
        v_ai_results->'obligation_matrix',
        (v_ai_results->>'sentiment_score')::NUMERIC,
        (v_ai_results->>'red_flag_count')::INTEGER,
        'AI analysis completed with ' || (v_ai_results->>'red_flag_count') || ' red flags identified',
        v_document_path || '.processed'
    )
    RETURNING analysis_id INTO v_analysis_id;

    -- Extract and store obligations
    INSERT INTO contract_obligations (
        analysis_id,
        obligation_type,
        obligated_party,
        description,
        trigger_condition,
        timeframe,
        penalty_clause,
        criticality
    )
    SELECT
        v_analysis_id,
        item->>'term',
        CASE
            WHEN item->>'term' IN ('Monthly reporting', 'Incident notification') THEN 'vendor'
            WHEN item->>'term' IN ('Payment terms', 'Access provision') THEN 'company'
            ELSE 'both'
        END,
        item->>'value',
        'Automatic based on contract terms',
        CASE
            WHEN item->>'term' = 'Incident notification' THEN '2 hours'
            WHEN item->>'term' = 'Monthly reporting' THEN '5 business days after month end'
            ELSE 'As specified in contract'
        END,
        CASE
            WHEN item->>'importance' = 'critical' THEN 'Contract termination possible'
            WHEN item->>'importance' = 'high' THEN 'Financial penalties apply'
            ELSE 'Remediation plan required'
        END,
        LOWER(item->>'importance')
    FROM jsonb_array_elements(v_ai_results->'key_terms') item
    WHERE item->>'importance' IN ('high', 'critical');

    -- Refresh materialized view
    REFRESH MATERIALIZED VIEW tprm.mv_contract_risk_heatmap;

    COMMIT;
END;
$$;

CREATE OR REPLACE PROCEDURE tprm.monitor_contract_obligations()
LANGUAGE plpgsql
AS $$
DECLARE
    v_obligation RECORD;
    v_compliance_status TEXT;
    v_evidence TEXT;
BEGIN
    -- Check each active obligation
    FOR v_obligation IN
        SELECT co.*, c.party_id, c.contract_name
        FROM contract_obligations co
        JOIN contract_ai_analysis cai ON co.analysis_id = cai.analysis_id
        JOIN contracts c ON cai.contract_id = c.contract_id
        WHERE co.is_active = TRUE
    LOOP
        -- In production, this would integrate with monitoring systems
        -- Simulate compliance check with random results
        IF random() < 0.85 THEN -- 85% compliance rate
            v_compliance_status := 'fulfilled';
            v_evidence := 'Automated verification ' || CURRENT_TIMESTAMP;
        ELSE
            v_compliance_status := 'missed';
            v_evidence := 'Deadline passed without fulfillment';
        END IF;

        -- Record compliance event
        INSERT INTO contract_compliance_events (
            obligation_id,
            event_type,
            evidence_reference,
            automated_validation
        )
        VALUES (
            v_obligation.obligation_id,
            v_compliance_status,
            v_evidence,
            TRUE
        );

        -- Create action items for missed obligations
        IF v_compliance_status = 'missed' THEN
            INSERT INTO risk_items (
                source_type,
                source_id,
                party_id,
                title,
                description,
                severity,
                due_date
            )
            VALUES (
                'contract-obligation',
                v_obligation.obligation_id,
                v_obligation.party_id,
                'Missed contract obligation: ' || v_obligation.obligation_type,
                'Contract: ' || v_obligation.contract_name || E'\n' ||
                'Obligation: ' || v_obligation.description || E'\n' ||
                'Penalty: ' || v_obligation.penalty_clause,
                v_obligation.criticality,
                CURRENT_DATE +
                    CASE v_obligation.criticality
                        WHEN 'critical' THEN INTERVAL '2 days'
                        WHEN 'high' THEN INTERVAL '5 days'
                        WHEN 'medium' THEN INTERVAL '10 days'
                        ELSE INTERVAL '20 days'
                    END
            );
        END IF;
    END LOOP;

    -- Refresh materialized view
    REFRESH MATERIALIZED VIEW tprm.mv_contract_risk_heatmap;

    COMMIT;
END;
$$;

--
CREATE OR REPLACE PROCEDURE tprm.adjust_risk_thresholds()
LANGUAGE plpgsql
AS $$
DECLARE
    v_setting RECORD;
    v_new_threshold NUMERIC(10,2);
    v_adjustment_factor NUMERIC(10,2) := 1.0;
    v_reason TEXT;
    v_external_factors JSONB := '{}';
BEGIN
    -- Process each risk appetite setting
    FOR v_setting IN SELECT * FROM risk_appetite_settings WHERE next_review <= CURRENT_TIMESTAMP LOOP
        -- Reset to base threshold
        v_new_threshold := v_setting.base_threshold;
        v_reason := 'Base threshold';

        -- Apply adjustment factors from external indicators
        FOR factor_key, factor_active IN SELECT * FROM jsonb_each_text(v_setting.adjustment_factors) LOOP
            IF factor_active = 'true' THEN
                DECLARE
                    v_indicator RECORD;
                BEGIN
                    SELECT * INTO v_indicator
                    FROM external_risk_indicators
                    WHERE indicator_name = factor_key
                    AND is_active = TRUE;

                    IF FOUND THEN
                        -- Simple linear adjustment for demonstration
                        -- Real implementation would use more sophisticated formulas
                        v_adjustment_factor := v_adjustment_factor *
                            (1 + (v_indicator.current_value * v_indicator.weight / 100));

                        -- Record influencing factor
                        v_external_factors := jsonb_set(v_external_factors, ARRAY[factor_key],
                                             to_jsonb(v_indicator.current_value));
                    END IF;
                END;
            END IF;
        END LOOP;

        -- Calculate new threshold with bounds checking
        v_new_threshold := LEAST(
            GREATEST(
                v_setting.base_threshold * v_adjustment_factor,
                v_setting.min_threshold
            ),
            v_setting.max_threshold
        );

        -- Determine adjustment reason
        IF v_new_threshold > v_setting.base_threshold THEN
            v_reason := 'Increased due to elevated external risks';
        ELSIF v_new_threshold < v_setting.base_threshold THEN
            v_reason := 'Decreased due to reduced external risks';
        END IF;

        -- Only adjust if change is significant (>5%)
        IF ABS(v_new_threshold - v_setting.current_threshold) > (v_setting.base_threshold * 0.05) THEN
            -- Record adjustment
            INSERT INTO threshold_adjustment_log (
                setting_id,
                previous_threshold,
                new_threshold,
                adjustment_reason,
                triggered_by,
                external_factors
            )
            VALUES (
                v_setting.setting_id,
                v_setting.current_threshold,
                v_new_threshold,
                v_reason,
                'system',
                v_external_factors
            );

            -- Update current threshold
            UPDATE risk_appetite_settings
            SET
                current_threshold = v_new_threshold,
                last_adjusted = CURRENT_TIMESTAMP,
                next_review = CURRENT_TIMESTAMP + INTERVAL '7 days',
                updated_at = CURRENT_TIMESTAMP
            WHERE
                setting_id = v_setting.setting_id;
        ELSE
            -- No significant change, just update review date
            UPDATE risk_appetite_settings
            SET
                next_review = CURRENT_TIMESTAMP + INTERVAL '7 days',
                updated_at = CURRENT_TIMESTAMP
            WHERE
                setting_id = v_setting.setting_id;
        END IF;
    END LOOP;

    COMMIT;
END;
$$;

CREATE OR REPLACE PROCEDURE tprm.update_external_indicators()
LANGUAGE plpgsql
AS $$
DECLARE
    v_indicator RECORD;
    v_new_value NUMERIC(15,2);
BEGIN
    -- Update each active external indicator
    FOR v_indicator IN SELECT * FROM external_risk_indicators WHERE is_active = TRUE LOOP
        -- In practice, this would call external APIs or data feeds
        -- Here we simulate with random walk pattern
        v_new_value := v_indicator.current_value * (0.95 + random() * 0.1);

        -- Apply some mean reversion
        IF v_indicator.indicator_name = 'Economic Stress Index' THEN
            v_new_value := GREATEST(0, v_new_value * 0.9 + 10 * 0.1);
        ELSIF v_indicator.indicator_name = 'Cyber Threat Level' THEN
            v_new_value := LEAST(100, v_new_value * 0.8 + 30 * 0.2);
        END IF;

        -- Update indicator
        UPDATE external_risk_indicators
        SET
            current_value = v_new_value,
            last_updated = CURRENT_TIMESTAMP,
            updated_at = CURRENT_TIMESTAMP
        WHERE
            indicator_id = v_indicator.indicator_id;
    END LOOP;

    COMMIT;
END;
$$;


-- analyze contract with AI
CREATE OR REPLACE PROCEDURE tprm.analyze_contract_with_ai(
    p_contract_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_analysis_id INTEGER;
    v_document_path TEXT;
    v_ai_results JSONB;
BEGIN
    -- Get contract document path (implementation specific)
    SELECT document_location INTO v_document_path
    FROM contracts WHERE contract_id = p_contract_id;

    IF v_document_path IS NULL THEN
        RAISE EXCEPTION 'No document available for contract ID %', p_contract_id;
    END IF;

    -- In production, this would call an AI contract analysis service
    -- Simulate results with placeholder data
    v_ai_results := jsonb_build_object(
        'key_terms', jsonb_build_array(
            jsonb_build_object('term', 'SLA', 'value', '99.9% uptime', 'importance', 'high'),
            jsonb_build_object('term', 'Data Protection', 'value', 'GDPR compliance required', 'importance', 'critical')
        ),
        'risk_clauses', jsonb_build_array(
            jsonb_build_object('clause_type', 'Liability', 'text', 'Limited to 12 months fees', 'risk_level', 'medium'),
            jsonb_build_object('clause_type', 'Termination', 'text', '30 days notice required', 'risk_level', 'low')
        ),
        'obligation_matrix', jsonb_build_object(
            'vendor', jsonb_build_array('Monthly reporting', 'Incident notification within 2 hours'),
            'company', jsonb_build_array('Payment within 30 days', 'Access to necessary systems')
        ),
        'sentiment_score', 0.65,
        'red_flag_count', 2
    );

    -- Store AI analysis results
    INSERT INTO tprm.contract_ai_analysis (
        contract_id,
        analysis_version,
        key_terms,
        risk_clauses,
        obligation_matrix,
        sentiment_score,
        red_flag_count,
        summary,
        processed_document_path
    )
    VALUES (
        p_contract_id,
        'v2.1.5',
        v_ai_results->'key_terms',
        v_ai_results->'risk_clauses',
        v_ai_results->'obligation_matrix',
        (v_ai_results->>'sentiment_score')::NUMERIC,
        (v_ai_results->>'red_flag_count')::INTEGER,
        'AI analysis completed with ' || (v_ai_results->>'red_flag_count') || ' red flags identified',
        v_document_path || '.processed'
    )
    RETURNING analysis_id INTO v_analysis_id;

    -- Extract and store obligations
    INSERT INTO tprm.contract_obligations (
        analysis_id,
        obligation_type,
        obligated_party,
        description,
        trigger_condition,
        timeframe,
        penalty_clause,
        criticality
    )
    SELECT
        v_analysis_id,
        item->>'term',
        CASE
            WHEN item->>'term' IN ('Monthly reporting', 'Incident notification') THEN 'vendor'
            WHEN item->>'term' IN ('Payment terms', 'Access provision') THEN 'company'
            ELSE 'both'
        END,
        item->>'value',
        'Automatic based on contract terms',
        CASE
            WHEN item->>'term' = 'Incident notification' THEN '2 hours'
            WHEN item->>'term' = 'Monthly reporting' THEN '5 business days after month end'
            ELSE 'As specified in contract'
        END,
        CASE
            WHEN item->>'importance' = 'critical' THEN 'Contract termination possible'
            WHEN item->>'importance' = 'high' THEN 'Financial penalties apply'
            ELSE 'Remediation plan required'
        END,
        LOWER(item->>'importance')
    FROM jsonb_array_elements(v_ai_results->'key_terms') item
    WHERE item->>'importance' IN ('high', 'critical');

    -- Refresh materialized view
    REFRESH MATERIALIZED VIEW mv_contract_risk_heatmap;

    COMMIT;
END;
$$;

CREATE OR REPLACE PROCEDURE tprm.monitor_contract_obligations()
LANGUAGE plpgsql
AS $$
DECLARE
    v_obligation RECORD;
    v_compliance_status TEXT;
    v_evidence TEXT;
BEGIN
    -- Check each active obligation
    FOR v_obligation IN
        SELECT co.*, c.party_id, c.contract_name
        FROM contract_obligations co
        JOIN contract_ai_analysis cai ON co.analysis_id = cai.analysis_id
        JOIN contracts c ON cai.contract_id = c.contract_id
        WHERE co.is_active = TRUE
    LOOP
        -- In production, this would integrate with monitoring systems
        -- Simulate compliance check with random results
        IF random() < 0.85 THEN -- 85% compliance rate
            v_compliance_status := 'fulfilled';
            v_evidence := 'Automated verification ' || CURRENT_TIMESTAMP;
        ELSE
            v_compliance_status := 'missed';
            v_evidence := 'Deadline passed without fulfillment';
        END IF;

        -- Record compliance event
        INSERT INTO contract_compliance_events (
            obligation_id,
            event_type,
            evidence_reference,
            automated_validation
        )
        VALUES (
            v_obligation.obligation_id,
            v_compliance_status,
            v_evidence,
            TRUE
        );

        -- Create action items for missed obligations
        IF v_compliance_status = 'missed' THEN
            INSERT INTO risk_items (
                source_type,
                source_id,
                party_id,
                title,
                description,
                severity,
                due_date
            )
            VALUES (
                'contract-obligation',
                v_obligation.obligation_id,
                v_obligation.party_id,
                'Missed contract obligation: ' || v_obligation.obligation_type,
                'Contract: ' || v_obligation.contract_name || E'\n' ||
                'Obligation: ' || v_obligation.description || E'\n' ||
                'Penalty: ' || v_obligation.penalty_clause,
                v_obligation.criticality,
                CURRENT_DATE +
                    CASE v_obligation.criticality
                        WHEN 'critical' THEN INTERVAL '2 days'
                        WHEN 'high' THEN INTERVAL '5 days'
                        WHEN 'medium' THEN INTERVAL '10 days'
                        ELSE INTERVAL '20 days'
                    END
            );
        END IF;
    END LOOP;

    -- Refresh materialized view
    REFRESH MATERIALIZED VIEW tprm.mv_contract_risk_heatmap;

    COMMIT;
END;
$$;

---
--business case: predictive vendor viability scoring . to proactively identify vendors at risk of financial distress or operational failure by combining financial metris, market signals and behavioral patterns
CREATE TABLE tprm.vendor_viability_models (
    model_id SERIAL PRIMARY KEY,
    model_name VARCHAR(255) NOT NULL,
    model_version VARCHAR(50) NOT NULL,
    model_scope VARCHAR(100) NOT NULL, -- 'financial', 'operational', 'composite'
    prediction_horizon VARCHAR(50) NOT NULL, -- '30d', '90d', '180d', '365d'
    accuracy_score NUMERIC(5,4),
    precision_score NUMERIC(5,4),
    recall_score NUMERIC(5,4),
    features_used JSONB NOT NULL,
    last_trained TIMESTAMP WITH TIME ZONE,
    next_retrain TIMESTAMP WITH TIME ZONE,
    is_active BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.viability_risk_scores (
    score_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    model_id INTEGER NOT NULL REFERENCES vendor_viability_models(model_id),
    score_date TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    risk_score NUMERIC(10,6) NOT NULL, -- 0 (low risk) to 100 (high risk)
    risk_category VARCHAR(50) NOT NULL, -- 'stable', 'monitor', 'elevated', 'critical'
    key_risk_factors JSONB NOT NULL,
    recommended_actions JSONB NOT NULL,
    reviewed BOOLEAN DEFAULT FALSE,
    reviewed_by VARCHAR(255),
    review_notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.viability_risk_events (
    event_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    event_date TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    event_type VARCHAR(100) NOT NULL, -- 'downgrade', 'upgrade', 'alert'
    previous_score NUMERIC(10,6),
    new_score NUMERIC(10,6),
    change_reason TEXT,
    action_taken TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE MATERIALIZED VIEW tprm.mv_vendor_viability_monitor AS
SELECT
    tp.party_id,
    tp.legal_name,
    tp.industry_sector,
    vvm.model_scope,
    vvm.prediction_horizon,
    vrs.risk_score,
    vrs.risk_category,
    vrs.score_date,
    vrs.key_risk_factors,
    (SELECT COUNT(*)
     FROM viability_risk_events vre
     WHERE vre.party_id = tp.party_id
     AND vre.event_date >= CURRENT_DATE - INTERVAL '12 months'
     AND vre.event_type = 'alert') AS alert_count_12m,
    (SELECT STRING_AGG(factor->>'factor', ', ')
     FROM jsonb_array_elements(vrs.key_risk_factors) factor
     WHERE (factor->>'weight')::NUMERIC > 0.7) AS major_risk_factors,
    (SELECT MAX(fi.reporting_date)
     FROM financial_indicators fi
     WHERE fi.party_id = tp.party_id) AS last_financial_report_date,
    CASE
        WHEN vrs.risk_category = 'critical' AND tp.is_active = TRUE THEN 'immediate-action'
        WHEN vrs.risk_category = 'elevated' THEN 'enhanced-monitoring'
        WHEN vrs.risk_score - LAG(vrs.risk_score) OVER (
            PARTITION BY tp.party_id, vvm.model_id
            ORDER BY vrs.score_date) > 15 THEN 'deteriorating'
        ELSE 'stable'
    END AS risk_trend
FROM
    viability_risk_scores vrs
JOIN
    third_parties tp ON vrs.party_id = tp.party_id
JOIN
    vendor_viability_models vvm ON vrs.model_id = vvm.model_id
WHERE
    vrs.score_date >= CURRENT_DATE - INTERVAL '6 months'
    AND vvm.is_active = TRUE
WITH DATA;


CREATE OR REPLACE PROCEDURE tprm.calculate_viability_scores()
LANGUAGE plpgsql
AS $$
DECLARE
    v_model RECORD;
    v_vendor RECORD;
    v_features JSONB;
    v_score NUMERIC(10,6);
    v_category VARCHAR(50);
    v_factors JSONB;
    v_actions JSONB;
BEGIN
    -- Process each active viability model
    FOR v_model IN SELECT * FROM vendor_viability_models WHERE is_active = TRUE LOOP
        -- For each vendor, calculate viability score
        FOR v_vendor IN SELECT party_id FROM third_parties WHERE is_active = TRUE LOOP
            -- Collect relevant features (simplified example)
            v_features := jsonb_build_object(
                'financial_health', (SELECT financial_health_score FROM financial_indicators
                                   WHERE party_id = v_vendor.party_id
                                   ORDER BY reporting_date DESC LIMIT 1),
                'payment_trend', (SELECT AVG(payment_days) FROM vendor_payments
                                WHERE party_id = v_vendor.party_id
                                AND payment_date >= CURRENT_DATE - INTERVAL '90 days'),
                'contract_risk', (SELECT AVG(overall_risk_score) FROM contract_analysis
                                WHERE contract_id IN (SELECT contract_id FROM contracts
                                                     WHERE party_id = v_vendor.party_id)
                                AND analysis_date >= CURRENT_DATE - INTERVAL '6 months'),
                'market_position', (SELECT COUNT(*) FROM market_intelligence
                                  WHERE company_id = v_vendor.party_id
                                  AND sentiment_score < -0.5
                                  AND report_date >= CURRENT_DATE - INTERVAL '30 days')
            );

            -- Simulate score calculation (real implementation would use ML model)
            v_score := random() * 100;
            v_category := CASE
                WHEN v_score < 20 THEN 'stable'
                WHEN v_score < 50 THEN 'monitor'
                WHEN v_score < 75 THEN 'elevated'
                ELSE 'critical'
            END;

            v_factors := jsonb_build_array(
                jsonb_build_object('factor', 'financial_health', 'weight', random()),
                jsonb_build_object('factor', 'contract_risk', 'weight', random()),
                jsonb_build_object('factor', 'market_position', 'weight', random())
            );

            v_actions := jsonb_build_array(
                CASE
                    WHEN v_score > 75 THEN 'Initiate contingency planning'
                    WHEN v_score > 50 THEN 'Request updated financials'
                    ELSE 'Continue standard monitoring'
                END,
                CASE
                    WHEN (v_features->>'market_position')::INTEGER > 3 THEN 'Assess competitive threats'
                    ELSE NULL
                END
            );

            -- Store viability score
            INSERT INTO tprm.viability_risk_scores (
                party_id,
                model_id,
                risk_score,
                risk_category,
                key_risk_factors,
                recommended_actions
            )
            VALUES (
                v_vendor.party_id,
                v_model.model_id,
                v_score,
                v_category,
                v_factors,
                v_actions
            );

            -- Check for significant changes
            DECLARE
                v_previous_score NUMERIC(10,6);
                v_previous_category VARCHAR(50);
            BEGIN
                SELECT risk_score, risk_category INTO v_previous_score, v_previous_category
                FROM viability_risk_scores
                WHERE party_id = v_vendor.party_id
                AND model_id = v_model.model_id
                ORDER BY score_date DESC
                LIMIT 1 OFFSET 1;

                IF FOUND AND (
                    (v_category != v_previous_category AND v_previous_category IS NOT NULL) OR
                    ABS(v_score - v_previous_score) > 20
                ) THEN
                    INSERT INTO tprm.viability_risk_events (
                        party_id,
                        event_type,
                        previous_score,
                        new_score,
                        change_reason
                    )
                    VALUES (
                        v_vendor.party_id,
                        CASE
                            WHEN v_score > v_previous_score + 20 THEN 'downgrade'
                            WHEN v_score < v_previous_score - 20 THEN 'upgrade'
                            ELSE 'alert'
                        END,
                        v_previous_score,
                        v_score,
                        CASE
                            WHEN (v_features->>'financial_health')::NUMERIC < 30 THEN 'Deteriorating financial health'
                            WHEN (v_features->>'market_position')::INTEGER > 3 THEN 'Negative market signals'
                            ELSE 'Multiple risk factors'
                        END
                    );
                END IF;
            END;
        END LOOP;
    END LOOP;

    -- Refresh materialized view
    REFRESH MATERIALIZED VIEW tprm.mv_vendor_viability_monitor;

    COMMIT;
END;
$$;

CREATE OR REPLACE PROCEDURE tprm.generate_viability_reports()
LANGUAGE plpgsql
AS $$
DECLARE
    v_report_id INTEGER;
    v_critical_count INTEGER;
    v_elevated_count INTEGER;
BEGIN
    -- Get counts of critical/elevated vendors
    SELECT COUNT(*) INTO v_critical_count
    FROM mv_vendor_viability_monitor
    WHERE risk_category = 'critical';

    SELECT COUNT(*) INTO v_elevated_count
    FROM mv_vendor_viability_monitor
    WHERE risk_category = 'elevated';

    -- Create report record
    INSERT INTO tprm_reports (
        report_type,
        generated_by,
        generation_date
    )
    VALUES (
        'viability-risk',
        CURRENT_USER,
        CURRENT_DATE
    )
    RETURNING report_id INTO v_report_id;

    -- Generate report content
    UPDATE tprm_reports
    SET
        report_content = (
            SELECT
                'Vendor Viability Risk Report - ' || TO_CHAR(CURRENT_DATE, 'YYYY-MM-DD') || E'\n\n' ||
                'Critical Risk Vendors: ' || v_critical_count || E'\n' ||
                'Elevated Risk Vendors: ' || v_elevated_count || E'\n\n' ||
                'Top Risk Factors:\n' ||
                (SELECT STRING_AGG(factor, E'\n')
                FROM (
                    SELECT jsonb_array_elements(key_risk_factors)->>'factor' AS factor
                    FROM viability_risk_scores
                    WHERE risk_category IN ('critical', 'elevated')
                    AND score_date >= CURRENT_DATE - INTERVAL '7 days'
                    GROUP BY 1
                    ORDER BY COUNT(*) DESC
                    LIMIT 5
                ) t
        ),
        status = 'completed',
        completion_date = CURRENT_TIMESTAMP
    WHERE
        report_id = v_report_id;

    COMMIT;
END;
$$;

-------------------------------------------------------------------------------------------------------------------------------------


CREATE OR REPLACE FUNCTION auditcore.log_audit_event()
RETURNS TRIGGER
LANGUAGE plpgsql
AS $$
DECLARE
    v_columns_to_audit TEXT;
    v_old_data JSONB := NULL;
    v_new_data JSONB := NULL;
    v_columns TEXT[];
    v_column TEXT;
    v_include_column BOOLEAN;
BEGIN
    -- Check if table is audited
    SELECT columns_to_audit INTO v_columns_to_audit
    FROM audit_config
    WHERE table_name = TG_TABLE_NAME;

    IF NOT FOUND THEN
        RETURN NULL; -- Table not configured for auditing
    END IF;

    -- Prepare column list
    IF v_columns_to_audit IS NULL THEN
        -- Audit all columns
        SELECT array_agg(column_name::TEXT) INTO v_columns
        FROM information_schema.columns
        WHERE table_name = TG_TABLE_NAME;
    ELSE
        -- Audit only specified columns
        v_columns := string_to_array(v_columns_to_audit, ',');
    END IF;

    -- Process OLD and NEW data
    IF TG_OP = 'UPDATE' OR TG_OP = 'DELETE' THEN
        v_old_data := '{}'::JSONB;
        FOREACH v_column IN ARRAY v_columns LOOP
            v_old_data := jsonb_set(v_old_data, ARRAY[v_column], to_jsonb(OLD.(v_column)));
        END LOOP;
    END IF;

    IF TG_OP = 'INSERT' OR TG_OP = 'UPDATE' THEN
        v_new_data := '{}'::JSONB;
        FOREACH v_column IN ARRAY v_columns LOOP
            v_new_data := jsonb_set(v_new_data, ARRAY[v_column], to_jsonb(NEW.(v_column)));
        END LOOP;
    END IF;

    -- Insert audit record
    INSERT INTO auditcore.audit_log (
        table_name,
        record_id,
        operation,
        old_values,
        new_values
    )
    VALUES (
        TG_TABLE_NAME,
        CASE
            WHEN TG_OP = 'DELETE' THEN OLD.id
            ELSE NEW.id
        END,
        TG_OP,
        v_old_data,
        v_new_data
    );

    RETURN NULL;
END;
$$;

-- Enable auditing for critical tables
INSERT INTO auditcore.audit_config (table_name, columns_to_audit) VALUES
('third_parties', NULL), -- Audit all columns
('contracts', 'contract_status,risk_level,end_date,auto_renewal'),
('party_assessments', 'overall_score,overall_risk_level,status'),
('assessment_issues', 'severity,status,due_date,resolution_notes');

-- Create triggers for audited tables
CREATE TRIGGER auditcore.audit_third_parties
AFTER INSERT OR UPDATE OR DELETE ON third_parties
FOR EACH ROW EXECUTE FUNCTION log_audit_event();

CREATE TRIGGER auditcore.audit_contracts
AFTER INSERT OR UPDATE OR DELETE ON contracts
FOR EACH ROW EXECUTE FUNCTION log_audit_event();

CREATE TRIGGER auditcore.audit_party_assessments
AFTER INSERT OR UPDATE OR DELETE ON party_assessments
FOR EACH ROW EXECUTE FUNCTION log_audit_event();

CREATE TRIGGER auditcore.audit_assessment_issues
AFTER INSERT OR UPDATE OR DELETE ON assessment_issues
FOR EACH ROW EXECUTE FUNCTION log_audit_event();


--- data rentention policy
CREATE TABLE data_retention_policies (
    policy_id SERIAL PRIMARY KEY,
    table_name VARCHAR(100) NOT NULL,
    retention_period INTERVAL NOT NULL, -- e.g., '1 year', '6 months'
    archive_strategy VARCHAR(50) NOT NULL, -- 'delete', 'archive', 'anonymize'
    archive_location VARCHAR(255),
    is_active BOOLEAN DEFAULT TRUE,
    last_run_at TIMESTAMP WITH TIME ZONE,
    next_run_at TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_table_policy UNIQUE (table_name)
);

CREATE TABLE archived_data (
    archive_id SERIAL PRIMARY KEY,
    policy_id INTEGER REFERENCES data_retention_policies(policy_id),
    table_name VARCHAR(100) NOT NULL,
    record_id INTEGER,
    archived_data JSONB NOT NULL,
    archived_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    archived_by VARCHAR(100) DEFAULT CURRENT_USER
);

-- Procedure to apply retention policies
CREATE OR REPLACE PROCEDURE apply_retention_policies()
LANGUAGE plpgsql
AS $$
DECLARE
    v_policy RECORD;
    v_sql TEXT;
    v_count INTEGER;
    v_archive_data JSONB;
    v_cutoff_date TIMESTAMP WITH TIME ZONE;
BEGIN
    FOR v_policy IN SELECT * FROM data_retention_policies WHERE is_active = TRUE LOOP
        v_cutoff_date := CURRENT_TIMESTAMP - v_policy.retention_period;

        IF v_policy.archive_strategy = 'archive' THEN
            -- Archive records before deleting
            v_sql := format('
                WITH records_to_archive AS (
                    SELECT * FROM %I WHERE created_at < %L
                )
                INSERT INTO archived_data (
                    policy_id,
                    table_name,
                    record_id,
                    archived_data
                )
                SELECT
                    %s,
                    %L,
                    id,
                    to_jsonb(records_to_archive)
                FROM
                    records_to_archive',
                v_policy.table_name, v_cutoff_date, v_policy.policy_id, v_policy.table_name);

            EXECUTE v_sql;
            GET DIAGNOSTICS v_count = ROW_COUNT;

            RAISE NOTICE 'Archived % records from %', v_count, v_policy.table_name;
        END IF;

        -- Delete records older than retention period
        v_sql := format('DELETE FROM %I WHERE created_at < %L', v_policy.table_name, v_cutoff_date);
        EXECUTE v_sql;
        GET DIAGNOSTICS v_count = ROW_COUNT;

        RAISE NOTICE 'Deleted % records from %', v_count, v_policy.table_name;

        -- Update policy last run time
        UPDATE data_retention_policies
        SET
            last_run_at = CURRENT_TIMESTAMP,
            next_run_at = CURRENT_TIMESTAMP + INTERVAL '1 month',
            updated_at = CURRENT_TIMESTAMP
        WHERE
            policy_id = v_policy.policy_id;
    END LOOP;

    COMMIT;
END;
$$;

----contract risk analytics
-- business case: analyze contract terms across the vendor portfolio to identify unfavorable terms, concentration risks and upcoming renewals requiring attention
CREATE TABLE tprm.contract_clauses (
    clause_id SERIAL PRIMARY KEY,
    clause_type VARCHAR(100) NOT NULL, -- 'liability', 'termination', 'data-protection'
    clause_name VARCHAR(255) NOT NULL,
    description TEXT,
    risk_level VARCHAR(20), -- 'low', 'medium', 'high', 'critical'
    is_standard BOOLEAN DEFAULT FALSE,
    recommended_language TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.ontract_analysis (
    analysis_id SERIAL PRIMARY KEY,
    contract_id INTEGER NOT NULL REFERENCES contracts(contract_id),
    analysis_date TIMESTAMP WITH TIME ZONE NOT NULL,
    analyzed_by VARCHAR(255) NOT NULL,
    analysis_method VARCHAR(100) NOT NULL, -- 'manual', 'ai', 'template'
    overall_risk_score NUMERIC(5,2),
    clauses_reviewed INTEGER,
    clauses_at_risk INTEGER,
    summary_findings TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.contract_clause_assessments (
    assessment_id SERIAL PRIMARY KEY,
    analysis_id INTEGER NOT NULL REFERENCES contract_analysis(analysis_id),
    clause_id INTEGER NOT NULL REFERENCES contract_clauses(clause_id),
    presence BOOLEAN NOT NULL,
    deviation_score NUMERIC(5,2) CHECK (deviation_score BETWEEN 0 AND 100),
    assessment_notes TEXT,
    recommended_action TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);


CREATE MATERIALIZED VIEW tprm.mv_contract_risk_analysis AS
SELECT
    c.contract_id,
    c.contract_name,
    tp.party_id,
    tp.legal_name,
    ca.analysis_date,
    ca.overall_risk_score,
    ca.clauses_reviewed,
    ca.clauses_at_risk,
    ROUND((ca.clauses_at_risk::NUMERIC / NULLIF(ca.clauses_reviewed, 0)) * 100, 2) AS pct_clauses_at_risk,
    (SELECT STRING_AGG(cc.clause_type || ': ' || cca.deviation_score, ', ' ORDER BY cca.deviation_score DESC)
     FROM contract_clause_assessments cca
     JOIN contract_clauses cc ON cca.clause_id = cc.clause_id
     WHERE cca.analysis_id = ca.analysis_id
     AND cca.deviation_score > 50) AS high_risk_clauses,
    (SELECT COUNT(*)
     FROM contract_clause_assessments cca
     JOIN contract_clauses cc ON cca.clause_id = cc.clause_id
     WHERE cca.analysis_id = ca.analysis_id
     AND cc.risk_level = 'critical'
     AND cca.deviation_score > 30) AS critical_deviations,
    c.end_date,
    CASE
        WHEN c.end_date IS NULL THEN 'perpetual'
        WHEN c.end_date < CURRENT_DATE THEN 'expired'
        WHEN c.end_date < CURRENT_DATE + INTERVAL '3 months' THEN 'imminent-renewal'
        WHEN c.end_date < CURRENT_DATE + INTERVAL '12 months' THEN 'upcoming-renewal'
        ELSE 'active'
    END AS renewal_status
FROM
    contract_analysis ca
JOIN
    contracts c ON ca.contract_id = c.contract_id
JOIN
    third_parties tp ON c.party_id = tp.party_id
WHERE
    ca.analysis_date = (SELECT MAX(analysis_date)
                       FROM contract_analysis
                       WHERE contract_id = ca.contract_id)
WITH DATA;


CREATE OR REPLACE PROCEDURE tprm.analyze_contract_terms(
    p_contract_id INTEGER,
    p_analyzed_by VARCHAR(255)
LANGUAGE plpgsql
AS $$
DECLARE
    v_analysis_id INTEGER;
    v_clause RECORD;
    v_presence BOOLEAN;
    v_deviation_score NUMERIC(5,2);
    v_clauses_reviewed INTEGER := 0;
    v_clauses_at_risk INTEGER := 0;
    v_total_risk_score NUMERIC(10,2) := 0;
BEGIN
    -- Create analysis record
    INSERT INTO contract_analysis (
        contract_id,
        analysis_date,
        analyzed_by,
        analysis_method,
        clauses_reviewed,
        clauses_at_risk
    )
    VALUES (
        p_contract_id,
        CURRENT_TIMESTAMP,
        p_analyzed_by,
        'ai', -- In practice, this might be determined by input parameters
        0, -- Will be updated
        0  -- Will be updated
    )
    RETURNING analysis_id INTO v_analysis_id;

    -- Analyze each standard clause
    FOR v_clause IN SELECT * FROM contract_clauses WHERE is_standard = TRUE LOOP
        -- In practice, this would call a contract analysis API or NLP service
        -- For demonstration, we use random values
        v_presence := random() > 0.3; -- 70% chance clause is present
        v_deviation_score := CASE WHEN v_presence THEN random() * 30 ELSE random() * 100 END;

        -- Record assessment
        INSERT INTO contract_clause_assessments (
            analysis_id,
            clause_id,
            presence,
            deviation_score,
            assessment_notes,
            recommended_action
        )
        VALUES (
            v_analysis_id,
            v_clause.clause_id,
            v_presence,
            v_deviation_score,
            CASE
                WHEN NOT v_presence AND v_clause.risk_level IN ('high','critical')
                THEN 'Critical clause missing from contract'
                WHEN v_deviation_score > 70
                THEN 'Significant deviation from recommended language'
                ELSE 'Within acceptable parameters'
            END,
            CASE
                WHEN NOT v_presence AND v_clause.risk_level IN ('high','critical')
                THEN 'Add clause using recommended language'
                WHEN v_deviation_score > 70
                THEN 'Negotiate amendment to align with standards'
                ELSE 'Monitor for future changes'
            END
        );

        v_clauses_reviewed := v_clauses_reviewed + 1;
        IF v_deviation_score > 50 OR (NOT v_presence AND v_clause.risk_level IN ('high','critical')) THEN
            v_clauses_at_risk := v_clauses_at_risk + 1;
            v_total_risk_score := v_total_risk_score +
                (v_deviation_score * CASE v_clause.risk_level
                    WHEN 'critical' THEN 1.5
                    WHEN 'high' THEN 1.2
                    WHEN 'medium' THEN 1.0
                    ELSE 0.8
                END);
        END IF;
    END LOOP;

    -- Calculate overall risk score (weighted average)
    DECLARE
        v_overall_risk_score NUMERIC(5,2);
    BEGIN
        IF v_clauses_at_risk > 0 THEN
            v_overall_risk_score := LEAST(100, v_total_risk_score / v_clauses_at_risk);
        ELSE
            v_overall_risk_score := 0;
        END IF;

        -- Update analysis record with results
        UPDATE contract_analysis
        SET
            overall_risk_score = v_overall_risk_score,
            clauses_reviewed = v_clauses_reviewed,
            clauses_at_risk = v_clauses_at_risk,
            summary_findings = CASE
                WHEN v_overall_risk_score > 75 THEN 'High risk contract requiring immediate review'
                WHEN v_overall_risk_score > 50 THEN 'Elevated risk contract needing attention'
                WHEN v_clauses_at_risk > 0 THEN 'Standard risk with some exceptions'
                ELSE 'Low risk contract'
            END
        WHERE
            analysis_id = v_analysis_id;
    END;

    -- Refresh materialized view
    REFRESH MATERIALIZED VIEW mv_contract_risk_analysis;

    COMMIT;
END;
$$;

CREATE OR REPLACE PROCEDURE tprm.flag_high_risk_contracts()
LANGUAGE plpgsql
AS $$
BEGIN
    -- Update contract risk flags based on analysis
    UPDATE contracts
    SET
        risk_level = CASE
            WHEN cra.overall_risk_score > 75 THEN 'critical'
            WHEN cra.overall_risk_score > 50 THEN 'high'
            WHEN cra.overall_risk_score > 25 THEN 'medium'
            ELSE 'low'
        END,
        requires_review = (cra.overall_risk_score > 50 OR cra.critical_deviations > 0),
        updated_at = CURRENT_TIMESTAMP
    FROM
        mv_contract_risk_analysis cra
    WHERE
        contracts.contract_id = cra.contract_id
        AND cra.analysis_date >= CURRENT_DATE - INTERVAL '6 months';

    -- Create review tasks for high risk contracts
    INSERT INTO contract_review_tasks (
        contract_id,
        priority,
        due_date,
        review_type,
        assigned_to,
        status
    )
    SELECT
        cra.contract_id,
        CASE
            WHEN cra.overall_risk_score > 75 THEN 'critical'
            WHEN cra.overall_risk_score > 50 THEN 'high'
            ELSE 'medium'
        END,
        CURRENT_DATE + CASE
            WHEN cra.overall_risk_score > 75 THEN INTERVAL '7 days'
            WHEN cra.overall_risk_score > 50 THEN INTERVAL '14 days'
            ELSE INTERVAL '30 days'
        END,
        'risk-review',
        (SELECT user_id FROM users WHERE department = 'legal' ORDER BY random() LIMIT 1),
        'pending'
    FROM
        mv_contract_risk_analysis cra
    WHERE
        cra.overall_risk_score > 50
        AND cra.analysis_date >= CURRENT_DATE - INTERVAL '6 months'
        AND NOT EXISTS (
            SELECT 1 FROM contract_review_tasks
            WHERE contract_id = cra.contract_id
            AND status NOT IN ('completed', 'rejected')
        );

    COMMIT;
END;
$$;

---vendor performance benchmarking
-- business case: compare vendor performance against industry peers to identify underperformers and set realistic improvement targets
CREATE TABLE tprm.performance_benchmarks (
    benchmark_id SERIAL PRIMARY KEY,
    metric_name VARCHAR(255) NOT NULL,
    industry_sector VARCHAR(100) NOT NULL,
    percentile_25 NUMERIC(15,2),
    percentile_50 NUMERIC(15,2),
    percentile_75 NUMERIC(15,2),
    source VARCHAR(255) NOT NULL,
    effective_date DATE NOT NULL,
    next_update_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE tprm.vendor_benchmark_results (
    result_id SERIAL PRIMARY KEY,
    party_id INTEGER NOT NULL REFERENCES third_parties(party_id),
    benchmark_id INTEGER NOT NULL REFERENCES performance_benchmarks(benchmark_id),
    measurement_period DATE NOT NULL,
    vendor_value NUMERIC(15,2) NOT NULL,
    percentile_rank NUMERIC(5,2),
    performance_rating VARCHAR(50), -- 'top-quartile', 'above-average', 'average', 'below-average', 'bottom-quartile'
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_vendor_benchmark UNIQUE (party_id, benchmark_id, measurement_period)
);


CREATE VIEW tprm.vw_vendor_benchmark_comparison AS
SELECT
    tp.party_id,
    tp.legal_name,
    tp.industry_sector,
    b.metric_name,
    vb.measurement_period,
    vb.vendor_value,
    b.percentile_25,
    b.percentile_50,
    b.percentile_75,
    vb.percentile_rank,
    vb.performance_rating,
    CASE
        WHEN vb.performance_rating = 'bottom-quartile' THEN 'Performance improvement plan required'
        WHEN vb.performance_rating = 'below-average' THEN 'Targeted improvement needed'
        WHEN vb.performance_rating = 'average' THEN 'Monitor for changes'
        WHEN vb.performance_rating = 'above-average' THEN 'Recognize strong performance'
        WHEN vb.performance_rating = 'top-quartile' THEN 'Consider as best practice example'
    END AS recommendation,
    b.source AS benchmark_source
FROM
    vendor_benchmark_results vb
JOIN
    third_parties tp ON vb.party_id = tp.party_id
JOIN
    performance_benchmarks b ON vb.benchmark_id = b.benchmark_id
WHERE
    vb.measurement_period >= DATE_TRUNC('year', CURRENT_DATE) - INTERVAL '2 years';

CREATE VIEW tprm.vw_industry_benchmark_gaps AS
SELECT
    b.benchmark_id,
    b.metric_name,
    b.industry_sector,
    COUNT(vb.result_id) AS vendors_benchmarked,
    AVG(vb.vendor_value) AS avg_vendor_performance,
    b.percentile_50 AS industry_median,
    AVG(vb.vendor_value) - b.percentile_50 AS performance_gap,
    COUNT(vb.result_id) FILTER (WHERE vb.performance_rating IN ('bottom-quartile', 'below-average')) AS underperforming_vendors,
    COUNT(vb.result_id) FILTER (WHERE vb.performance_rating IN ('top-quartile', 'above-average')) AS outperforming_vendors
FROM
    performance_benchmarks b
LEFT JOIN
    vendor_benchmark_results vb ON b.benchmark_id = vb.benchmark_id
    AND vb.measurement_period >= DATE_TRUNC('year', CURRENT_DATE) - INTERVAL '1 year'
GROUP BY
    b.benchmark_id, b.metric_name, b.industry_sector, b.percentile_50;


CREATE OR REPLACE PROCEDURE tprm.calculate_vendor_benchmarks(
        p_metric_name VARCHAR(255),
        p_industry_sector VARCHAR(100)
    LANGUAGE plpgsql
    AS $$
    DECLARE
        v_benchmark_id INTEGER;
        v_kpi_id INTEGER;
        v_period DATE := DATE_TRUNC('quarter', CURRENT_DATE);
    BEGIN
        -- Get benchmark definition
        SELECT benchmark_id INTO v_benchmark_id
        FROM performance_benchmarks
        WHERE metric_name = p_metric_name
        AND industry_sector = p_industry_sector
        AND effective_date <= CURRENT_DATE
        ORDER BY effective_date DESC
        LIMIT 1;

        IF NOT FOUND THEN
            RAISE EXCEPTION 'No benchmark found for metric % and industry %', p_metric_name, p_industry_sector;
        END IF;

        -- Get corresponding KPI ID (assuming naming alignment)
        SELECT kpi_id INTO v_kpi_id
        FROM kpi_definitions
        WHERE kpi_name ILIKE '%' || p_metric_name || '%'
        LIMIT 1;

        -- Calculate benchmark positions for all vendors in this industry
        INSERT INTO tprm.vendor_benchmark_results (
            party_id,
            benchmark_id,
            measurement_period,
            vendor_value,
            percentile_rank,
            performance_rating
        )
        SELECT
            km.party_id,
            v_benchmark_id,
            v_period,
            km.measured_value,
            -- Calculate percentile rank (simplified approach)
            (SELECT COUNT(*)::FLOAT / (SELECT COUNT(*) FROM kpi_measurements km2
                                      WHERE km2.kpi_id = km.kpi_id
                                      AND km2.measurement_date BETWEEN v_period - INTERVAL '3 months' AND v_period)
             FROM kpi_measurements km2
             WHERE km2.kpi_id = km.kpi_id
             AND km2.measurement_date BETWEEN v_period - INTERVAL '3 months' AND v_period
             AND km2.measured_value <= km.measured_value) * 100,
            CASE
                WHEN km.measured_value >= (SELECT percentile_75 FROM performance_benchmarks WHERE benchmark_id = v_benchmark_id)
                    THEN 'top-quartile'
                WHEN km.measured_value >= (SELECT percentile_50 FROM performance_benchmarks WHERE benchmark_id = v_benchmark_id)
                    THEN 'above-average'
                WHEN km.measured_value >= (SELECT percentile_25 FROM performance_benchmarks WHERE benchmark_id = v_benchmark_id)
                    THEN 'average'
                ELSE 'below-average'
            END
        FROM
            kpi_measurements km
        JOIN
            third_parties tp ON km.party_id = tp.party_id
        WHERE
            km.kpi_id = v_kpi_id
            AND km.measurement_date BETWEEN v_period - INTERVAL '3 months' AND v_period
            AND tp.industry_sector = p_industry_sector
        ON CONFLICT (party_id, benchmark_id, measurement_period)
        DO UPDATE SET
            vendor_value = EXCLUDED.vendor_value,
            percentile_rank = EXCLUDED.percentile_rank,
            performance_rating = EXCLUDED.performance_rating;

        -- Update vendor performance flags based on benchmarks
        UPDATE third_parties
        SET
            performance_benchmark_status = CASE
                WHEN EXISTS (SELECT 1 FROM vendor_benchmark_results vbr
                             JOIN performance_benchmarks pb ON vbr.benchmark_id = pb.benchmark_id
                             WHERE vbr.party_id = third_parties.party_id
                             AND pb.metric_name = p_metric_name
                             AND vbr.performance_rating IN ('top-quartile', 'above-average'))
                THEN 'outperforming'
                WHEN EXISTS (SELECT 1 FROM vendor_benchmark_results vbr
                             JOIN performance_benchmarks pb ON vbr.benchmark_id = pb.benchmark_id
                             WHERE vbr.party_id = third_parties.party_id
                             AND pb.metric_name = p_metric_name
                             AND vbr.performance_rating IN ('bottom-quartile', 'below-average'))
                THEN 'underperforming'
                ELSE 'meeting-expectations'
            END,
            updated_at = CURRENT_TIMESTAMP
        WHERE
            industry_sector = p_industry_sector;

        COMMIT;
    END;
    $$;

    CREATE OR REPLACE PROCEDURE tprm.generate_benchmark_report(
        p_industry_sector VARCHAR(100)
    )
    LANGUAGE plpgsql
    AS $$
    DECLARE
        v_report_id INTEGER;
    BEGIN
        -- Create report record
        INSERT INTO tprm_reports (
            report_type,
            report_parameters,
            generated_by,
            generation_date
        )
        VALUES (
            'benchmark',
            json_build_object('industry_sector', p_industry_sector),
            CURRENT_USER,
            CURRENT_DATE
        )
        RETURNING report_id INTO v_report_id;

        -- Generate report content
        UPDATE tprm_reports
        SET
            report_content = (
                SELECT
                    'Vendor Performance Benchmark Report - ' || p_industry_sector || ' - ' || TO_CHAR(CURRENT_DATE, 'YYYY-MM-DD') || E'\n\n' ||
                    'Benchmarks Analyzed: ' || COUNT(DISTINCT vb.benchmark_id) || E'\n' ||
                    'Vendors Included: ' || COUNT(DISTINCT vb.party_id) || E'\n\n' ||
                    'Performance Summary by Metric:\n' ||
                    (SELECT STRING_AGG(
                        b.metric_name || ':\n' ||
                        '  Industry Median: ' || b.percentile_50 || E'\n' ||
                        '  Our Average: ' || ROUND(AVG(vb.vendor_value), 2) || E'\n' ||
                        '  Gap: ' || ROUND(AVG(vb.vendor_value) - b.percentile_50, 2) || E'\n' ||
                        '  Top Performers: ' || COUNT(DISTINCT vb.party_id) FILTER (WHERE vb.performance_rating IN ('top-quartile', 'above-average')) || E'\n' ||
                        '  Underperformers: ' || COUNT(DISTINCT vb.party_id) FILTER (WHERE vb.performance_rating IN ('bottom-quartile', 'below-average')),
                        E'\n\n'
                     FROM vendor_benchmark_results vb
                     JOIN performance_benchmarks b ON vb.benchmark_id = b.benchmark_id
                     WHERE b.industry_sector = p_industry_sector
                     GROUP BY b.metric_name, b.percentile_50)
                FROM
                    vendor_benchmark_results vb
                JOIN
                    performance_benchmarks b ON vb.benchmark_id = b.benchmark_id
                WHERE
                    b.industry_sector = p_industry_sector
                    AND vb.measurement_period >= DATE_TRUNC('year', CURRENT_DATE)
            ),
            status = 'completed',
            completion_date = CURRENT_TIMESTAMP
        WHERE
            report_id = v_report_id;

        COMMIT;
    END;
    $$;



-- Procedure to initialize retention policies
CREATE OR REPLACE PROCEDURE initialize_retention_policies()
LANGUAGE plpgsql
AS $$
BEGIN
    -- Set up retention policies for key tables
    INSERT INTO data_retention_policies (
        table_name,
        retention_period,
        archive_strategy,
        archive_location,
        next_run_at
    ) VALUES
    ('incidents', INTERVAL '5 years', 'archive', 'S3://tprm-archive/incidents', CURRENT_DATE + INTERVAL '1 month'),
    ('kpi_measurements', INTERVAL '3 years', 'archive', 'S3://tprm-archive/kpi_measurements', CURRENT_DATE + INTERVAL '1 month'),
    ('krm_measurements', INTERVAL '3 years', 'archive', 'S3://tprm-archive/krm_measurements', CURRENT_DATE + INTERVAL '1 month'),
    ('audit_log', INTERVAL '7 years', 'archive', 'S3://tprm-archive/audit_log', CURRENT_DATE + INTERVAL '1 month'),
    ('job_executions', INTERVAL '1 year', 'delete', NULL, CURRENT_DATE + INTERVAL '1 month');

    COMMIT;
END;
$$;


-- ========================================================================
-- SECTION 4: Regulatory Compliance Management
-- ========================================================================
-- Module 2: Regulatory Compliance Management
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-- Jurisdictions
CREATE TABLE rcm.jurisdictions (
    jurisdiction_id SERIAL PRIMARY KEY,
    jurisdiction_name VARCHAR(255) NOT NULL,
    country_code VARCHAR(3) NOT NULL,
    region VARCHAR(100),
    description TEXT,
    is_active BOOLEAN DEFAULT TRUE
);

-- Regulatory Authorities
CREATE TABLE rcm.regulatory_authorities (
    authority_id SERIAL PRIMARY KEY,
    authority_name VARCHAR(255) NOT NULL,
    jurisdiction_id INTEGER REFERENCES rcm.jurisdictions(jurisdiction_id),
    website VARCHAR(255),
    contact_info TEXT,
    is_active BOOLEAN DEFAULT TRUE
);

-- Regulations
CREATE TABLE rcm.regulations (
    regulation_id SERIAL PRIMARY KEY,
    regulation_name VARCHAR(255) NOT NULL,
    regulation_code VARCHAR(100) NOT NULL,
    authority_id INTEGER REFERENCES rcm.regulatory_authorities(authority_id),
    description TEXT,
    effective_date DATE NOT NULL,
    last_updated_date DATE,
    review_frequency_months INTEGER,
    is_active BOOLEAN DEFAULT TRUE,
    UNIQUE(regulation_code, authority_id)
);

-- Regulation Versions
CREATE TABLE rcm.regulation_versions (
    version_id SERIAL PRIMARY KEY,
    regulation_id INTEGER NOT NULL REFERENCES rcm.regulations(regulation_id),
    version_number VARCHAR(50) NOT NULL,
    effective_date DATE NOT NULL,
    end_date DATE,
    change_description TEXT,
    document_id INTEGER REFERENCES core.documents(doc_id),
    UNIQUE(regulation_id, version_number)
);

-- Regulatory Requirements
CREATE TABLE rcm.regulatory_requirements (
    requirement_id SERIAL PRIMARY KEY,
    regulation_id INTEGER NOT NULL REFERENCES rcm.regulations(regulation_id),
    requirement_code VARCHAR(100) NOT NULL,
    description TEXT NOT NULL,
    category VARCHAR(100),
    risk_level VARCHAR(50),
    compliance_deadline DATE,
    is_critical BOOLEAN DEFAULT FALSE,
    UNIQUE(regulation_id, requirement_code)
);

-- Regulatory Obligations
CREATE TABLE rcm.regulatory_obligations (
    obligation_id SERIAL PRIMARY KEY,
    requirement_id INTEGER NOT NULL REFERENCES rcm.regulatory_requirements(requirement_id),
    org_id INTEGER NOT NULL REFERENCES core.organizations(org_id),
    applicable_date DATE NOT NULL,
    review_frequency_months INTEGER,
    last_review_date DATE,
    next_review_date DATE,
    is_active BOOLEAN DEFAULT TRUE,
    UNIQUE(requirement_id, org_id)
);


-- Control Categories
CREATE TABLE rcm.control_categories (
    category_id SERIAL PRIMARY KEY,
    category_name VARCHAR(255) NOT NULL,
    description TEXT,
    parent_category_id INTEGER REFERENCES rcm.control_categories(category_id),
    is_active BOOLEAN DEFAULT TRUE
);

-- Compliance Controls
CREATE TABLE rcm.compliance_controls (
    control_id SERIAL PRIMARY KEY,
    control_name VARCHAR(255) NOT NULL,
    control_code VARCHAR(100) NOT NULL,
    category_id INTEGER REFERENCES rcm.control_categories(category_id),
    description TEXT,
    control_type VARCHAR(100),
    frequency VARCHAR(50),
    owner_org_id INTEGER REFERENCES core.organizations(org_id),
    owner_user_id INTEGER REFERENCES core.users(user_id),
    is_active BOOLEAN DEFAULT TRUE,
    UNIQUE(control_code)
);

-- Control-Requirement Mapping
CREATE TABLE rcm.control_requirement_mapping (
    mapping_id SERIAL PRIMARY KEY,
    control_id INTEGER NOT NULL REFERENCES rcm.compliance_controls(control_id),
    requirement_id INTEGER NOT NULL REFERENCES rcm.regulatory_requirements(requirement_id),
    mapping_strength VARCHAR(50), -- e.g., "Full", "Partial"
    mapping_date DATE NOT NULL,
    mapped_by INTEGER REFERENCES core.users(user_id),
    notes TEXT,
    UNIQUE(control_id, requirement_id)
);

-- Control Procedures
CREATE TABLE rcm.control_procedures (
    procedure_id SERIAL PRIMARY KEY,
    control_id INTEGER NOT NULL REFERENCES rcm.compliance_controls(control_id),
    procedure_name VARCHAR(255) NOT NULL,
    description TEXT,
    steps TEXT,
    document_id INTEGER REFERENCES core.documents(doc_id),
    version VARCHAR(50) NOT NULL,
    effective_date DATE NOT NULL,
    is_active BOOLEAN DEFAULT TRUE
);


-- Assessment Templates
CREATE TABLE rcm.assessment_templates (
    template_id SERIAL PRIMARY KEY,
    template_name VARCHAR(255) NOT NULL,
    description TEXT,
    frequency VARCHAR(50),
    applicable_to VARCHAR(100), -- e.g., "All", "Department", "Regulation"
    document_id INTEGER REFERENCES core.documents(doc_id),
    is_active BOOLEAN DEFAULT TRUE
);

-- Assessments
CREATE TABLE rcm.assessments (
    assessment_id SERIAL PRIMARY KEY,
    template_id INTEGER REFERENCES rcm.assessment_templates(template_id),
    assessment_name VARCHAR(255) NOT NULL,
    assessment_type VARCHAR(100) NOT NULL,
    start_date DATE NOT NULL,
    end_date DATE,
    status VARCHAR(50) NOT NULL, -- e.g., "Planned", "In Progress", "Completed"
    lead_assessor_id INTEGER REFERENCES core.users(user_id),
    org_id INTEGER REFERENCES core.organizations(org_id),
    regulation_id INTEGER REFERENCES rcm.regulations(regulation_id),
    notes TEXT
);

-- Assessment Items (questions/tests)
CREATE TABLE rcm.assessment_items (
    item_id SERIAL PRIMARY KEY,
    assessment_id INTEGER NOT NULL REFERENCES rcm.assessments(assessment_id),
    control_id INTEGER REFERENCES rcm.compliance_controls(control_id),
    requirement_id INTEGER REFERENCES rcm.regulatory_requirements(requirement_id),
    item_text TEXT NOT NULL,
    item_type VARCHAR(50) NOT NULL, -- e.g., "Question", "Test", "Observation"
    weight INTEGER DEFAULT 1,
    sequence_number INTEGER NOT NULL
);

-- Assessment Results
CREATE TABLE rcm.assessment_results (
    result_id SERIAL PRIMARY KEY,
    item_id INTEGER NOT NULL REFERENCES rcm.assessment_items(item_id),
    assessment_id INTEGER NOT NULL REFERENCES rcm.assessments(assessment_id),
    result_status VARCHAR(50) NOT NULL, -- e.g., "Pass", "Fail", "Partial"
    result_score INTEGER,
    comments TEXT,
    evidence_doc_id INTEGER REFERENCES core.documents(doc_id),
    completed_by INTEGER REFERENCES core.users(user_id),
    completed_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    reviewed_by INTEGER REFERENCES core.users(user_id),
    reviewed_date TIMESTAMP
);

-- Gap Analysis
CREATE TABLE rcm.gap_analysis (
    gap_id SERIAL PRIMARY KEY,
    assessment_id INTEGER NOT NULL REFERENCES rcm.assessments(assessment_id),
    requirement_id INTEGER NOT NULL REFERENCES rcm.regulatory_requirements(requirement_id),
    gap_description TEXT NOT NULL,
    gap_severity VARCHAR(50) NOT NULL, -- e.g., "High", "Medium", "Low"
    root_cause TEXT,
    identified_by INTEGER REFERENCES core.users(user_id),
    identified_date DATE NOT NULL,
    target_remediation_date DATE
);


-- Non-Compliance Incidents
CREATE TABLE rcm.non_compliance_incidents (
    incident_id SERIAL PRIMARY KEY,
    incident_code VARCHAR(100) NOT NULL UNIQUE,
    incident_name VARCHAR(255) NOT NULL,
    description TEXT NOT NULL,
    incident_date DATE NOT NULL,
    detected_date DATE NOT NULL,
    reported_by INTEGER REFERENCES core.users(user_id),
    reported_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    regulation_id INTEGER REFERENCES rcm.regulations(regulation_id),
    requirement_id INTEGER REFERENCES rcm.regulatory_requirements(requirement_id),
    severity VARCHAR(50) NOT NULL,
    status VARCHAR(50) NOT NULL, -- e.g., "Open", "Investigating", "Remediating", "Closed"
    root_cause TEXT,
    impact_assessment TEXT,
    is_breach BOOLEAN DEFAULT FALSE
);

-- Remediation Plans
CREATE TABLE rcm.remediation_plans (
    plan_id SERIAL PRIMARY KEY,
    plan_name VARCHAR(255) NOT NULL,
    description TEXT,
    priority VARCHAR(50) NOT NULL,
    target_completion_date DATE NOT NULL,
    actual_completion_date DATE,
    status VARCHAR(50) NOT NULL,
    owner_id INTEGER REFERENCES core.users(user_id),
    created_by INTEGER REFERENCES core.users(user_id),
    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Remediation Actions
CREATE TABLE rcm.remediation_actions (
    action_id SERIAL PRIMARY KEY,
    plan_id INTEGER NOT NULL REFERENCES rcm.remediation_plans(plan_id),
    incident_id INTEGER REFERENCES rcm.non_compliance_incidents(incident_id),
    gap_id INTEGER REFERENCES rcm.gap_analysis(gap_id),
    assessment_id INTEGER REFERENCES rcm.assessments(assessment_id),
    action_description TEXT NOT NULL,
    action_type VARCHAR(100),
    responsible_party_id INTEGER REFERENCES core.users(user_id),
    start_date DATE,
    end_date DATE,
    status VARCHAR(50) NOT NULL,
    completion_percentage INTEGER DEFAULT 0,
    notes TEXT
);

-- Remediation Dependencies
CREATE TABLE rcm.remediation_dependencies (
    dependency_id SERIAL PRIMARY KEY,
    action_id INTEGER NOT NULL REFERENCES rcm.remediation_actions(action_id),
    depends_on_action_id INTEGER NOT NULL REFERENCES rcm.remediation_actions(action_id),
    dependency_type VARCHAR(100) NOT NULL,
    description TEXT,
    UNIQUE(action_id, depends_on_action_id)
);

-- Remediation Evidence
CREATE TABLE rcm.remediation_evidence (
    evidence_id SERIAL PRIMARY KEY,
    action_id INTEGER NOT NULL REFERENCES rcm.remediation_actions(action_id),
    document_id INTEGER NOT NULL REFERENCES core.documents(doc_id),
    evidence_type VARCHAR(100),
    upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    uploaded_by INTEGER REFERENCES core.users(user_id),
    description TEXT
);

-- Third-Party Vendors
CREATE TABLE rcm.vendors (
    vendor_id SERIAL PRIMARY KEY,
    vendor_name VARCHAR(255) NOT NULL,
    vendor_code VARCHAR(100) NOT NULL UNIQUE,
    vendor_type VARCHAR(100),
    industry VARCHAR(100),
    registration_number VARCHAR(100),
    registration_country VARCHAR(100),
    is_active BOOLEAN DEFAULT TRUE,
    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Vendor Compliance Assessments
CREATE TABLE rcm.vendor_assessments (
    assessment_id SERIAL PRIMARY KEY,
    vendor_id INTEGER NOT NULL REFERENCES rcm.vendors(vendor_id),
    assessment_type VARCHAR(100) NOT NULL,
    assessment_date DATE NOT NULL,
    conducted_by INTEGER REFERENCES core.users(user_id),
    overall_score INTEGER,
    overall_status VARCHAR(50),
    next_assessment_date DATE,
    notes TEXT
);

-- Vendor Assessment Items
CREATE TABLE rcm.vendor_assessment_items (
    item_id SERIAL PRIMARY KEY,
    assessment_id INTEGER NOT NULL REFERENCES rcm.vendor_assessments(assessment_id),
    requirement_id INTEGER REFERENCES rcm.regulatory_requirements(requirement_id),
    item_text TEXT NOT NULL,
    item_weight INTEGER DEFAULT 1,
    compliance_status VARCHAR(50),
    comments TEXT,
    evidence_doc_id INTEGER REFERENCES core.documents(doc_id)
);

-- Vendor Contracts
CREATE TABLE rcm.vendor_contracts (
    contract_id SERIAL PRIMARY KEY,
    vendor_id INTEGER NOT NULL REFERENCES rcm.vendors(vendor_id),
    contract_reference VARCHAR(100) NOT NULL,
    start_date DATE NOT NULL,
    end_date DATE,
    contract_value NUMERIC(15,2),
    contract_manager_id INTEGER REFERENCES core.users(user_id),
    document_id INTEGER REFERENCES core.documents(doc_id),
    is_active BOOLEAN DEFAULT TRUE,
    auto_renew BOOLEAN DEFAULT FALSE,
    renewal_reminder_days INTEGER DEFAULT 30
);

-- Vendor Compliance Clauses
CREATE TABLE rcm.vendor_compliance_clauses (
    clause_id SERIAL PRIMARY KEY,
    contract_id INTEGER NOT NULL REFERENCES rcm.vendor_contracts(contract_id),
    clause_title VARCHAR(255) NOT NULL,
    clause_text TEXT NOT NULL,
    regulation_id INTEGER REFERENCES rcm.regulations(regulation_id),
    requirement_id INTEGER REFERENCES rcm.regulatory_requirements(requirement_id),
    monitoring_frequency VARCHAR(50),
    last_verified_date DATE,
    next_verification_date DATE,
    compliance_status VARCHAR(50)
);

-- KPI/KRM Definitions
CREATE TABLE rcm.kpi_krm_definitions (
    definition_id SERIAL PRIMARY KEY,
    kpi_krm_code VARCHAR(100) NOT NULL UNIQUE,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    category VARCHAR(100),
    type VARCHAR(50) NOT NULL, -- "KPI" or "KRM"
    calculation_method TEXT,
    target_value NUMERIC(15,2),
    target_operator VARCHAR(10), -- ">", "<", "=", etc.
    measurement_unit VARCHAR(50),
    reporting_frequency VARCHAR(50),
    owner_org_id INTEGER REFERENCES core.organizations(org_id),
    is_active BOOLEAN DEFAULT TRUE
);

-- KPI/KRM Measurements
CREATE TABLE rcm.kpi_krm_measurements (
    measurement_id SERIAL PRIMARY KEY,
    definition_id INTEGER NOT NULL REFERENCES rcm.kpi_krm_definitions(definition_id),
    org_id INTEGER REFERENCES core.organizations(org_id),
    measurement_period_start DATE NOT NULL,
    measurement_period_end DATE NOT NULL,
    measured_value NUMERIC(15,2) NOT NULL,
    target_value NUMERIC(15,2),
    status VARCHAR(50), -- "On Target", "Below Target", "Above Target"
    notes TEXT,
    measured_by INTEGER REFERENCES core.users(user_id),
    measurement_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(definition_id, org_id, measurement_period_start, measurement_period_end)
);

-- KPI/KRM Thresholds
CREATE TABLE rcm.kpi_krm_thresholds (
    threshold_id SERIAL PRIMARY KEY,
    definition_id INTEGER NOT NULL REFERENCES rcm.kpi_krm_definitions(definition_id),
    threshold_name VARCHAR(100) NOT NULL,
    threshold_value NUMERIC(15,2) NOT NULL,
    threshold_operator VARCHAR(10) NOT NULL, -- ">", "<", "=", etc.
    severity VARCHAR(50) NOT NULL, -- "Critical", "High", "Medium", "Low"
    notification_template TEXT,
    UNIQUE(definition_id, threshold_name)
);

-- View for Compliance Coverage
CREATE OR REPLACE VIEW rcm.vw_compliance_coverage AS
SELECT
    o.org_id,
    o.org_name,
    COUNT(DISTINCT r.regulation_id) AS total_applicable_regulations,
    COUNT(DISTINCT CASE WHEN crm.mapping_id IS NOT NULL THEN r.regulation_id END) AS mapped_regulations,
    ROUND(COUNT(DISTINCT CASE WHEN crm.mapping_id IS NOT NULL THEN r.regulation_id END) * 100.0 /
        NULLIF(COUNT(DISTINCT r.regulation_id), 0), 2) AS coverage_percentage
FROM
    core.organizations o
JOIN
    rcm.regulatory_obligations ro ON o.org_id = ro.org_id
JOIN
    rcm.regulatory_requirements rr ON ro.requirement_id = rr.requirement_id
JOIN
    rcm.regulations r ON rr.regulation_id = r.regulation_id
LEFT JOIN
    rcm.control_requirement_mapping crm ON rr.requirement_id = crm.requirement_id
WHERE
    ro.is_active = TRUE
GROUP BY
    o.org_id, o.org_name;

-- View for Regulatory Change Adaptation Time
CREATE OR REPLACE VIEW rcm.vw_regulatory_change_adaptation AS
WITH regulation_changes AS (
    SELECT
        rv.regulation_id,
        r.regulation_name,
        rv.version_number,
        rv.effective_date AS change_date,
        LEAD(rv.effective_date) OVER (PARTITION BY rv.regulation_id ORDER BY rv.effective_date) AS next_change_date
    FROM
        rcm.regulation_versions rv
    JOIN
        rcm.regulations r ON rv.regulation_id = r.regulation_id
)
SELECT
    rc.regulation_id,
    rc.regulation_name,
    rc.version_number,
    rc.change_date,
    MIN(crm.mapping_date) AS first_mapping_date,
    MIN(crm.mapping_date) - rc.change_date AS days_to_adapt,
    COUNT(DISTINCT crm.control_id) AS controls_mapped
FROM
    regulation_changes rc
JOIN
    rcm.regulatory_requirements rr ON rc.regulation_id = rr.regulation_id
JOIN
    rcm.control_requirement_mapping crm ON rr.requirement_id = crm.requirement_id
WHERE
    crm.mapping_date >= rc.change_date
    AND (rc.next_change_date IS NULL OR crm.mapping_date < rc.next_change_date)
GROUP BY
    rc.regulation_id, rc.regulation_name, rc.version_number, rc.change_date;

-- View for Non-Compliance Incidents
CREATE OR REPLACE VIEW rcm.vw_non_compliance_incidents AS
SELECT
    EXTRACT(YEAR FROM nci.incident_date) AS year,
    EXTRACT(QUARTER FROM nci.incident_date) AS quarter,
    o.org_id,
    o.org_name,
    r.regulation_id,
    r.regulation_name,
    COUNT(nci.incident_id) AS incident_count,
    SUM(CASE WHEN nci.severity = 'High' THEN 1 ELSE 0 END) AS high_severity_count,
    SUM(CASE WHEN nci.severity = 'Medium' THEN 1 ELSE 0 END) AS medium_severity_count,
    SUM(CASE WHEN nci.severity = 'Low' THEN 1 ELSE 0 END) AS low_severity_count,
    SUM(CASE WHEN nci.is_breach THEN 1 ELSE 0 END) AS breach_count
FROM
    rcm.non_compliance_incidents nci
LEFT JOIN
    rcm.regulations r ON nci.regulation_id = r.regulation_id
LEFT JOIN
    core.organizations o ON nci.org_id = o.org_id
GROUP BY
    EXTRACT(YEAR FROM nci.incident_date),
    EXTRACT(QUARTER FROM nci.incident_date),
    o.org_id, o.org_name,
    r.regulation_id, r.regulation_name;

-- View for Audit Finding Reduction
CREATE OR REPLACE VIEW rcm.vw_audit_finding_reduction AS
WITH audit_findings AS (
    SELECT
        a.assessment_id,
        a.assessment_name,
        a.org_id,
        o.org_name,
        EXTRACT(YEAR FROM a.end_date) AS year,
        COUNT(ar.result_id) AS total_findings,
        COUNT(CASE WHEN ar.result_status = 'Fail' THEN 1 END) AS failed_findings,
        COUNT(CASE WHEN ar.result_status = 'Fail' AND ai.requirement_id IS NOT NULL THEN 1 END) AS regulatory_failures
    FROM
        rcm.assessments a
    JOIN
        core.organizations o ON a.org_id = o.org_id
    JOIN
        rcm.assessment_items ai ON a.assessment_id = ai.assessment_id
    JOIN
        rcm.assessment_results ar ON ai.item_id = ar.item_id
    WHERE
        a.assessment_type = 'Audit'
    GROUP BY
        a.assessment_id, a.assessment_name, a.org_id, o.org_name, EXTRACT(YEAR FROM a.end_date)
)
SELECT
    org_id,
    org_name,
    year,
    total_findings,
    failed_findings,
    regulatory_failures,
    LAG(regulatory_failures) OVER (PARTITION BY org_id ORDER BY year) AS prev_year_failures,
    CASE
        WHEN LAG(regulatory_failures) OVER (PARTITION BY org_id ORDER BY year) IS NULL THEN NULL
        ELSE ROUND((LAG(regulatory_failures) OVER (PARTITION BY org_id ORDER BY year) - regulatory_failures) * 100.0 /
             LAG(regulatory_failures) OVER (PARTITION BY org_id ORDER BY year), 2)
    END AS reduction_percentage
FROM
    audit_findings
ORDER BY
    org_id, year;

-- View for Control Effectiveness
CREATE OR REPLACE VIEW rcm.vw_control_effectiveness AS
SELECT
    c.control_id,
    c.control_name,
    c.control_code,
    cc.category_name,
    o.org_name AS owner_org,
    u.username AS owner_user,
    COUNT(ar.result_id) AS total_tests,
    COUNT(CASE WHEN ar.result_status = 'Pass' THEN 1 END) AS passed_tests,
    COUNT(CASE WHEN ar.result_status = 'Fail' THEN 1 END) AS failed_tests,
    COUNT(CASE WHEN ar.result_status = 'Partial' THEN 1 END) AS partial_tests,
    ROUND(COUNT(CASE WHEN ar.result_status = 'Pass' THEN 1 END) * 100.0 /
        NULLIF(COUNT(ar.result_id), 0), 2) AS effectiveness_percentage
FROM
    rcm.compliance_controls c
LEFT JOIN
    rcm.control_categories cc ON c.category_id = cc.category_id
LEFT JOIN
    core.organizations o ON c.owner_org_id = o.org_id
LEFT JOIN
    core.users u ON c.owner_user_id = u.user_id
LEFT JOIN
    rcm.assessment_items ai ON c.control_id = ai.control_id
LEFT JOIN
    rcm.assessment_results ar ON ai.item_id = ar.item_id
GROUP BY
    c.control_id, c.control_name, c.control_code, cc.category_name, o.org_name, u.username;

-- View for Vendor Compliance Status
CREATE OR REPLACE VIEW rcm.vw_vendor_compliance_status AS
SELECT
    v.vendor_id,
    v.vendor_name,
    v.vendor_code,
    COUNT(DISTINCT va.assessment_id) AS total_assessments,
    MAX(va.assessment_date) AS last_assessment_date,
    MAX(va.overall_score) AS last_assessment_score,
    MAX(va.overall_status) AS last_assessment_status,
    COUNT(DISTINCT vc.contract_id) AS active_contracts,
    COUNT(DISTINCT vcc.clause_id) AS compliance_clauses,
    COUNT(DISTINCT CASE WHEN vcc.compliance_status = 'Compliant' THEN vcc.clause_id END) AS compliant_clauses,
    COUNT(DISTINCT CASE WHEN vcc.compliance_status = 'Non-Compliant' THEN vcc.clause_id END) AS non_compliant_clauses,
    ROUND(COUNT(DISTINCT CASE WHEN vcc.compliance_status = 'Compliant' THEN vcc.clause_id END) * 100.0 /
        NULLIF(COUNT(DISTINCT vcc.clause_id), 0), 2) AS compliance_percentage
FROM
    rcm.vendors v
LEFT JOIN
    rcm.vendor_assessments va ON v.vendor_id = va.vendor_id
LEFT JOIN
    rcm.vendor_contracts vc ON v.vendor_id = vc.vendor_id AND vc.is_active = TRUE
LEFT JOIN
    rcm.vendor_compliance_clauses vcc ON vc.contract_id = vcc.contract_id
GROUP BY
    v.vendor_id, v.vendor_name, v.vendor_code;
-----------------------------------------------------------------------------------------------------------------------
-- Materialized view for regulatory compliance dashboard
CREATE MATERIALIZED VIEW rcm.mv_regulatory_compliance_dashboard AS
SELECT
    o.org_id,
    o.org_name,
    COUNT(DISTINCT ro.obligation_id) AS total_obligations,
    COUNT(DISTINCT ro.requirement_id) AS unique_requirements,
    COUNT(DISTINCT ro.regulation_id) AS unique_regulations,
    COUNT(DISTINCT c.control_id) AS total_controls,
    COUNT(DISTINCT crm.mapping_id) AS mapped_controls,
    ROUND(COUNT(DISTINCT crm.mapping_id) * 100.0 / NULLIF(COUNT(DISTINCT ro.requirement_id), 0), 2) AS coverage_percentage,
    COUNT(DISTINCT a.assessment_id) AS total_assessments,
    COUNT(DISTINCT nci.incident_id) AS total_incidents,
    COUNT(DISTINCT rp.plan_id) AS active_remediation_plans
FROM
    core.organizations o
LEFT JOIN
    rcm.regulatory_obligations ro ON o.org_id = ro.org_id AND ro.is_active = TRUE
LEFT JOIN
    rcm.control_requirement_mapping crm ON ro.requirement_id = crm.requirement_id
LEFT JOIN
    rcm.compliance_controls c ON crm.control_id = c.control_id AND c.is_active = TRUE
LEFT JOIN
    rcm.assessments a ON o.org_id = a.org_id
LEFT JOIN
    rcm.non_compliance_incidents nci ON o.org_id = nci.org_id
LEFT JOIN
    rcm.remediation_plans rp ON o.org_id = rp.org_id AND rp.status NOT IN ('Completed', 'Closed')
GROUP BY
    o.org_id, o.org_name;

-- Materialized view for KPI/KRM tracking
CREATE MATERIALIZED VIEW rcm.mv_kpi_krm_tracking AS
SELECT
    kd.definition_id,
    kd.kpi_krm_code,
    kd.name AS kpi_krm_name,
    kd.type,
    kd.category,
    o.org_id,
    o.org_name,
    km.measurement_period_start,
    km.measurement_period_end,
    km.measured_value,
    kd.target_value,
    kd.target_operator,
    CASE
        WHEN kd.target_operator = '>' AND km.measured_value > kd.target_value THEN 'On Target'
        WHEN kd.target_operator = '<' AND km.measured_value < kd.target_value THEN 'On Target'
        WHEN kd.target_operator = '=' AND km.measured_value = kd.target_value THEN 'On Target'
        ELSE 'Below Target'
    END AS status,
    kt.threshold_name,
    kt.threshold_value,
    kt.threshold_operator,
    kt.severity
FROM
    rcm.kpi_krm_definitions kd
JOIN
    rcm.kpi_krm_measurements km ON kd.definition_id = km.definition_id
LEFT JOIN
    core.organizations o ON km.org_id = o.org_id
LEFT JOIN
    rcm.kpi_krm_thresholds kt ON kd.definition_id = kt.definition_id
    AND (
        (kt.threshold_operator = '>' AND km.measured_value > kt.threshold_value) OR
        (kt.threshold_operator = '<' AND km.measured_value < kt.threshold_value) OR
        (kt.threshold_operator = '=' AND km.measured_value = kt.threshold_value)
    )
WHERE
    km.measurement_period_end >= (CURRENT_DATE - INTERVAL '12 months');

-- Materialized view for vendor risk dashboard
CREATE MATERIALIZED VIEW rcm.mv_vendor_risk_dashboard AS
SELECT
    v.vendor_id,
    v.vendor_name,
    v.vendor_code,
    v.vendor_type,
    v.industry,
    COUNT(DISTINCT vc.contract_id) AS active_contracts,
    MAX(vc.end_date) AS latest_contract_end_date,
    COUNT(DISTINCT va.assessment_id) AS total_assessments,
    MAX(va.assessment_date) AS last_assessment_date,
    MAX(va.overall_score) AS last_score,
    MAX(va.overall_status) AS last_status,
    COUNT(DISTINCT vcc.clause_id) AS total_clauses,
    COUNT(DISTINCT CASE WHEN vcc.compliance_status = 'Non-Compliant' THEN vcc.clause_id END) AS non_compliant_clauses,
    COUNT(DISTINCT nci.incident_id) AS compliance_incidents,
    COUNT(DISTINCT rp.plan_id) AS active_remediation_plans
FROM
    rcm.vendors v
LEFT JOIN
    rcm.vendor_contracts vc ON v.vendor_id = vc.vendor_id AND vc.is_active = TRUE
LEFT JOIN
    rcm.vendor_assessments va ON v.vendor_id = va.vendor_id
LEFT JOIN
    rcm.vendor_compliance_clauses vcc ON vc.contract_id = vcc.contract_id
LEFT JOIN
    rcm.non_compliance_incidents nci ON v.vendor_id = nci.vendor_id
LEFT JOIN
    rcm.remediation_plans rp ON v.vendor_id = rp.vendor_id AND rp.status NOT IN ('Completed', 'Closed')
GROUP BY
    v.vendor_id, v.vendor_name, v.vendor_code, v.vendor_type, v.industry;


-- regulatory change impact assessment
-- Regulatory Change Impact Assessment
CREATE TABLE rcm.regulatory_change_impact (
    impact_id SERIAL PRIMARY KEY,
    regulation_version_id INTEGER NOT NULL REFERENCES rcm.regulation_versions(version_id),
    impact_area VARCHAR(100) NOT NULL, -- 'Process', 'Technology', 'People', 'Data'
    impact_level VARCHAR(50) NOT NULL, -- 'High', 'Medium', 'Low'
    impact_description TEXT NOT NULL,
    assessed_by INTEGER REFERENCES core.users(user_id),
    assessment_date DATE NOT NULL,
    estimated_effort_hours INTEGER,
    estimated_cost NUMERIC(15,2)
);

-- Regulatory Change Implementation Tasks
CREATE TABLE rcm.regulatory_change_tasks (
    task_id SERIAL PRIMARY KEY,
    impact_id INTEGER NOT NULL REFERENCES rcm.regulatory_change_impact(impact_id),
    task_name VARCHAR(255) NOT NULL,
    task_description TEXT,
    assigned_to INTEGER REFERENCES core.users(user_id),
    due_date DATE NOT NULL,
    completion_date DATE,
    status VARCHAR(50) NOT NULL, -- 'Not Started', 'In Progress', 'Completed'
    priority VARCHAR(50) NOT NULL -- 'Critical', 'High', 'Medium', 'Low'
);


--advanced compliance training
-- Compliance Training Programs
CREATE TABLE rcm.compliance_training_programs (
    program_id SERIAL PRIMARY KEY,
    program_name VARCHAR(255) NOT NULL,
    description TEXT,
    target_audience VARCHAR(100),
    frequency_months INTEGER,
    duration_minutes INTEGER,
    is_mandatory BOOLEAN DEFAULT TRUE,
    regulation_id INTEGER REFERENCES rcm.regulations(regulation_id)
);

-- Compliance Training Sessions
CREATE TABLE rcm.compliance_training_sessions (
    session_id SERIAL PRIMARY KEY,
    program_id INTEGER NOT NULL REFERENCES rcm.compliance_training_programs(program_id),
    session_name VARCHAR(255) NOT NULL,
    session_date TIMESTAMP NOT NULL,
    conducted_by INTEGER REFERENCES core.users(user_id),
    delivery_method VARCHAR(100), -- 'In-Person', 'Virtual', 'E-Learning'
    document_id INTEGER REFERENCES core.documents(doc_id)
);

-- Employee Training Records
CREATE TABLE rcm.employee_training_records (
    record_id SERIAL PRIMARY KEY,
    session_id INTEGER REFERENCES rcm.compliance_training_sessions(session_id),
    user_id INTEGER NOT NULL REFERENCES core.users(user_id),
    completion_date TIMESTAMP,
    score NUMERIC(5,2),
    status VARCHAR(50) NOT NULL, -- 'Completed', 'Pending', 'Failed'
    next_due_date DATE
);

--advanced risk assessment
-- Risk Assessment Scenarios
CREATE TABLE rcm.risk_scenarios (
    scenario_id SERIAL PRIMARY KEY,
    scenario_name VARCHAR(255) NOT NULL,
    description TEXT NOT NULL,
    risk_category VARCHAR(100) NOT NULL,
    likelihood VARCHAR(50) NOT NULL, -- 'Rare', 'Unlikely', 'Possible', 'Likely', 'Certain'
    impact VARCHAR(50) NOT NULL, -- 'Negligible', 'Minor', 'Moderate', 'Major', 'Catastrophic'
    risk_score INTEGER GENERATED ALWAYS AS (
        CASE
            WHEN likelihood = 'Rare' AND impact = 'Negligible' THEN 1
            WHEN likelihood = 'Rare' AND impact = 'Minor' THEN 2
            -- Add all other combinations...
            WHEN likelihood = 'Certain' AND impact = 'Catastrophic' THEN 25
            ELSE 0
        END
    ) STORED,
    regulation_id INTEGER REFERENCES rcm.regulations(regulation_id)
);

-- Risk Treatment Plans
CREATE TABLE rcm.risk_treatment_plans (
    plan_id SERIAL PRIMARY KEY,
    scenario_id INTEGER NOT NULL REFERENCES rcm.risk_scenarios(scenario_id),
    treatment_option VARCHAR(50) NOT NULL, -- 'Avoid', 'Reduce', 'Transfer', 'Accept'
    treatment_description TEXT NOT NULL,
    owner_id INTEGER REFERENCES core.users(user_id),
    target_date DATE,
    status VARCHAR(50) NOT NULL, -- 'Pending', 'In Progress', 'Completed'
    residual_risk_score INTEGER
);

---policy documents
-- Policy Documents
CREATE TABLE rcm.policies (
    policy_id SERIAL PRIMARY KEY,
    policy_code VARCHAR(100) NOT NULL UNIQUE,
    policy_name VARCHAR(255) NOT NULL,
    policy_version VARCHAR(50) NOT NULL,
    effective_date DATE NOT NULL,
    review_date DATE,
    approval_date DATE,
    approved_by INTEGER REFERENCES core.users(user_id),
    document_id INTEGER NOT NULL REFERENCES core.documents(doc_id),
    status VARCHAR(50) NOT NULL, -- 'Draft', 'Approved', 'Archived'
    regulation_id INTEGER REFERENCES rcm.regulations(regulation_id)
);

-- Policy Acknowledgment
CREATE TABLE rcm.policy_acknowledgments (
    acknowledgment_id SERIAL PRIMARY KEY,
    policy_id INTEGER NOT NULL REFERENCES rcm.policies(policy_id),
    user_id INTEGER NOT NULL REFERENCES core.users(user_id),
    acknowledgment_date TIMESTAMP NOT NULL,
    acknowledgment_method VARCHAR(50), -- 'Digital', 'Physical'
    UNIQUE(policy_id, user_id)
);

-- View for Regulatory Change Implementation Status
CREATE OR REPLACE VIEW rcm.vw_regulatory_change_status AS
SELECT
    r.regulation_id,
    r.regulation_name,
    rv.version_number,
    rv.effective_date,
    COUNT(DISTINCT rci.impact_id) AS total_impacts,
    COUNT(DISTINCT CASE WHEN rct.status = 'Completed' THEN rct.task_id END) AS completed_tasks,
    COUNT(DISTINCT rct.task_id) AS total_tasks,
    ROUND(COUNT(DISTINCT CASE WHEN rct.status = 'Completed' THEN rct.task_id END) * 100.0 /
          NULLIF(COUNT(DISTINCT rct.task_id), 0), 2) AS completion_percentage,
    MIN(rct.due_date) AS earliest_due_date,
    MAX(rct.due_date) AS latest_due_date
FROM
    rcm.regulations r
JOIN
    rcm.regulation_versions rv ON r.regulation_id = rv.regulation_id
LEFT JOIN
    rcm.regulatory_change_impact rci ON rv.version_id = rci.regulation_version_id
LEFT JOIN
    rcm.regulatory_change_tasks rct ON rci.impact_id = rct.impact_id
GROUP BY
    r.regulation_id, r.regulation_name, rv.version_number, rv.effective_date;

-- View for Training Compliance
CREATE OR REPLACE VIEW rcm.vw_training_compliance AS
SELECT
    o.org_id,
    o.org_name,
    ctp.program_id,
    ctp.program_name,
    COUNT(DISTINCT u.user_id) AS total_employees,
    COUNT(DISTINCT CASE WHEN etr.status = 'Completed' AND
          (etr.next_due_date IS NULL OR etr.next_due_date > CURRENT_DATE) THEN u.user_id END) AS compliant_employees,
    COUNT(DISTINCT CASE WHEN etr.status IS NULL OR
          (etr.next_due_date IS NOT NULL AND etr.next_due_date <= CURRENT_DATE) THEN u.user_id END) AS non_compliant_employees,
    ROUND(COUNT(DISTINCT CASE WHEN etr.status = 'Completed' AND
          (etr.next_due_date IS NULL OR etr.next_due_date > CURRENT_DATE) THEN u.user_id END) * 100.0 /
          NULLIF(COUNT(DISTINCT u.user_id), 0), 2) AS compliance_percentage
FROM
    core.organizations o
JOIN
    core.user_org_mapping uom ON o.org_id = uom.org_id
JOIN
    core.users u ON uom.user_id = u.user_id
JOIN
    rcm.compliance_training_programs ctp ON 1=1
LEFT JOIN
    rcm.employee_training_records etr ON u.user_id = etr.user_id AND ctp.program_id = etr.program_id
WHERE
    ctp.is_mandatory = TRUE
    AND u.is_active = TRUE
GROUP BY
    o.org_id, o.org_name, ctp.program_id, ctp.program_name;

-- View for Risk Heatmap
CREATE OR REPLACE VIEW rcm.vw_risk_heatmap AS
SELECT
    rs.scenario_id,
    rs.scenario_name,
    rs.risk_category,
    rs.likelihood,
    rs.impact,
    rs.risk_score,
    r.regulation_id,
    r.regulation_name,
    CASE
        WHEN rs.risk_score >= 20 THEN 'Extreme'
        WHEN rs.risk_score >= 15 THEN 'High'
        WHEN rs.risk_score >= 10 THEN 'Medium'
        WHEN rs.risk_score >= 5 THEN 'Low'
        ELSE 'Very Low'
    END AS risk_level,
    rtp.treatment_option,
    rtp.status AS treatment_status
FROM
    rcm.risk_scenarios rs
LEFT JOIN
    rcm.regulations r ON rs.regulation_id = r.regulation_id
LEFT JOIN
    rcm.risk_treatment_plans rtp ON rs.scenario_id = rtp.scenario_id;

-- View for Policy Compliance
CREATE OR REPLACE VIEW rcm.vw_policy_compliance AS
SELECT
    p.policy_id,
    p.policy_code,
    p.policy_name,
    p.policy_version,
    o.org_id,
    o.org_name,
    COUNT(DISTINCT u.user_id) AS total_employees,
    COUNT(DISTINCT pa.user_id) AS acknowledged_employees,
    ROUND(COUNT(DISTINCT pa.user_id) * 100.0 / NULLIF(COUNT(DISTINCT u.user_id), 0), 2) AS acknowledgment_percentage,
    p.effective_date,
    p.review_date
FROM
    rcm.policies p
CROSS JOIN
    core.organizations o
JOIN
    core.user_org_mapping uom ON o.org_id = uom.org_id
JOIN
    core.users u ON uom.user_id = u.user_id
LEFT JOIN
    rcm.policy_acknowledgments pa ON p.policy_id = pa.policy_id AND u.user_id = pa.user_id
WHERE
    p.status = 'Approved'
    AND u.is_active = TRUE
GROUP BY
    p.policy_id, p.policy_code, p.policy_name, p.policy_version, o.org_id, o.org_name, p.effective_date, p.review_date;


--stored procedures
-- Procedure to assess regulatory change impact
CREATE OR REPLACE PROCEDURE rcm.sp_assess_regulatory_change(
    p_version_id INTEGER,
    p_impact_areas JSONB,
    p_assessed_by INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_impact RECORD;
BEGIN
    -- Validate version exists
    IF NOT EXISTS (SELECT 1 FROM rcm.regulation_versions WHERE version_id = p_version_id) THEN
        RAISE EXCEPTION 'Regulation version with ID % does not exist', p_version_id;
    END IF;

    -- Process impact areas
    FOR v_impact IN SELECT * FROM jsonb_to_recordset(p_impact_areas) AS x(
        impact_area VARCHAR(100),
        impact_level VARCHAR(50),
        impact_description TEXT,
        estimated_effort_hours INTEGER,
        estimated_cost NUMERIC(15,2)
    )
    LOOP
        INSERT INTO rcm.regulatory_change_impact (
            regulation_version_id,
            impact_area,
            impact_level,
            impact_description,
            assessed_by,
            assessment_date,
            estimated_effort_hours,
            estimated_cost
        ) VALUES (
            p_version_id,
            v_impact.impact_area,
            v_impact.impact_level,
            v_impact.impact_description,
            p_assessed_by,
            CURRENT_DATE,
            v_impact.estimated_effort_hours,
            v_impact.estimated_cost
        );
    END LOOP;

    COMMIT;
END;
$$;

-- Procedure to assign regulatory change tasks
CREATE OR REPLACE PROCEDURE rcm.sp_assign_regulatory_change_tasks(
    p_impact_id INTEGER,
    p_tasks JSONB
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_task RECORD;
BEGIN
    -- Validate impact exists
    IF NOT EXISTS (SELECT 1 FROM rcm.regulatory_change_impact WHERE impact_id = p_impact_id) THEN
        RAISE EXCEPTION 'Impact with ID % does not exist', p_impact_id;
    END IF;

    -- Process tasks
    FOR v_task IN SELECT * FROM jsonb_to_recordset(p_tasks) AS x(
        task_name VARCHAR(255),
        task_description TEXT,
        assigned_to INTEGER,
        due_date DATE,
        priority VARCHAR(50))
    LOOP
        INSERT INTO rcm.regulatory_change_tasks (
            impact_id,
            task_name,
            task_description,
            assigned_to,
            due_date,
            status,
            priority
        ) VALUES (
            p_impact_id,
            v_task.task_name,
            v_task.task_description,
            v_task.assigned_to,
            v_task.due_date,
            'Not Started',
            v_task.priority
        );
    END LOOP;

    COMMIT;
END;
$$;

-- Procedure to record training completion
CREATE OR REPLACE PROCEDURE rcm.sp_record_training_completion(
    p_session_id INTEGER,
    p_user_ids INTEGER[],
    p_scores JSONB DEFAULT NULL,
    p_recorded_by INTEGER DEFAULT NULL
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_user_id INTEGER;
    v_idx INTEGER;
    v_score NUMERIC(5,2);
    v_status VARCHAR(50);
BEGIN
    -- Validate session exists
    IF NOT EXISTS (SELECT 1 FROM rcm.compliance_training_sessions WHERE session_id = p_session_id) THEN
        RAISE EXCEPTION 'Training session with ID % does not exist', p_session_id;
    END IF;

    -- Get program ID and frequency for due date calculation
    DECLARE
        v_program_id INTEGER;
        v_frequency_months INTEGER;
    BEGIN
        SELECT ctp.program_id, ctp.frequency_months
        INTO v_program_id, v_frequency_months
        FROM rcm.compliance_training_sessions cts
        JOIN rcm.compliance_training_programs ctp ON cts.program_id = ctp.program_id
        WHERE cts.session_id = p_session_id;

        IF v_program_id IS NULL THEN
            RAISE EXCEPTION 'Could not find training program for session %', p_session_id;
        END IF;
    END;

    -- Process each user
    FOREACH v_user_id IN ARRAY p_user_ids
    LOOP
        -- Get score if provided
        v_idx := array_position(p_user_ids, v_user_id);
        IF p_scores IS NOT NULL AND jsonb_array_length(p_scores) >= v_idx THEN
            v_score := (p_scores->(v_idx-1))::NUMERIC;
            v_status := CASE WHEN v_score >= 70 THEN 'Completed' ELSE 'Failed' END;
        ELSE
            v_score := NULL;
            v_status := 'Completed';
        END IF;

        -- Insert or update training record
        INSERT INTO rcm.employee_training_records (
            session_id,
            user_id,
            completion_date,
            score,
            status,
            next_due_date
        ) VALUES (
            p_session_id,
            v_user_id,
            CURRENT_TIMESTAMP,
            v_score,
            v_status,
            CASE WHEN v_frequency_months IS NOT NULL THEN (CURRENT_DATE + (v_frequency_months * INTERVAL '1 month')) ELSE NULL END
        ) ON CONFLICT (session_id, user_id)
        DO UPDATE SET
            completion_date = EXCLUDED.completion_date,
            score = EXCLUDED.score,
            status = EXCLUDED.status,
            next_due_date = EXCLUDED.next_due_date;
    END LOOP;

    COMMIT;
END;
$$;

-- Procedure to create risk treatment plan
CREATE OR REPLACE PROCEDURE rcm.sp_create_risk_treatment_plan(
    p_scenario_id INTEGER,
    p_treatment_option VARCHAR(50),
    p_treatment_description TEXT,
    p_owner_id INTEGER,
    p_target_date DATE,
    p_recorded_by INTEGER DEFAULT NULL
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_residual_risk_score INTEGER;
BEGIN
    -- Validate scenario exists
    IF NOT EXISTS (SELECT 1 FROM rcm.risk_scenarios WHERE scenario_id = p_scenario_id) THEN
        RAISE EXCEPTION 'Risk scenario with ID % does not exist', p_scenario_id;
    END IF;

    -- Calculate residual risk based on treatment option
    v_residual_risk_score := CASE
        WHEN p_treatment_option = 'Avoid' THEN 0
        WHEN p_treatment_option = 'Reduce' THEN
            (SELECT risk_score FROM rcm.risk_scenarios WHERE scenario_id = p_scenario_id) * 0.5
        WHEN p_treatment_option = 'Transfer' THEN
            (SELECT risk_score FROM rcm.risk_scenarios WHERE scenario_id = p_scenario_id) * 0.8
        ELSE (SELECT risk_score FROM rcm.risk_scenarios WHERE scenario_id = p_scenario_id)
    END;

    -- Insert treatment plan
    INSERT INTO rcm.risk_treatment_plans (
        scenario_id,
        treatment_option,
        treatment_description,
        owner_id,
        target_date,
        status,
        residual_risk_score
    ) VALUES (
        p_scenario_id,
        p_treatment_option,
        p_treatment_description,
        p_owner_id,
        p_target_date,
        'Pending',
        v_residual_risk_score
    );

    COMMIT;
END;
$$;

-- Procedure to approve and distribute policy
CREATE OR REPLACE PROCEDURE rcm.sp_approve_policy(
    p_policy_id INTEGER,
    p_approver_id INTEGER,
    p_effective_date DATE,
    p_review_date DATE,
    p_notify_user_ids INTEGER[] DEFAULT NULL
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_user_id INTEGER;
BEGIN
    -- Update policy status
    UPDATE rcm.policies
    SET
        status = 'Approved',
        approved_by = p_approver_id,
        approval_date = CURRENT_DATE,
        effective_date = p_effective_date,
        review_date = p_review_date
    WHERE
        policy_id = p_policy_id;

    -- Notify users if provided
    IF p_notify_user_ids IS NOT NULL THEN
        FOREACH v_user_id IN ARRAY p_notify_user_ids
        LOOP
            -- In a real implementation, this would trigger notifications
            -- For now, we'll just log to the console
            RAISE NOTICE 'Notifying user % about new policy %', v_user_id, p_policy_id;
        END LOOP;
    END IF;

    COMMIT;
END;
$$;

---audit trail trigger s
-- Create audit log table
CREATE TABLE rcm.audit_log (
    log_id SERIAL PRIMARY KEY,
    table_name VARCHAR(100) NOT NULL,
    record_id INTEGER NOT NULL,
    operation VARCHAR(20) NOT NULL, -- 'INSERT', 'UPDATE', 'DELETE'
    old_values JSONB,
    new_values JSONB,
    changed_by INTEGER REFERENCES core.users(user_id),
    change_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Generic audit trigger function
CREATE OR REPLACE FUNCTION rcm.fn_audit_trigger()
RETURNS TRIGGER
LANGUAGE plpgsql
AS $$
BEGIN
    IF TG_OP = 'INSERT' THEN
        INSERT INTO rcm.audit_log (
            table_name,
            record_id,
            operation,
            new_values,
            changed_by
        ) VALUES (
            TG_TABLE_NAME,
            NEW.id,
            TG_OP,
            to_jsonb(NEW),
            NULL -- Replace with current user ID from session in real implementation
        );
    ELSIF TG_OP = 'UPDATE' THEN
        INSERT INTO rcm.audit_log (
            table_name,
            record_id,
            operation,
            old_values,
            new_values,
            changed_by
        ) VALUES (
            TG_TABLE_NAME,
            NEW.id,
            TG_OP,
            to_jsonb(OLD),
            to_jsonb(NEW),
            NULL -- Replace with current user ID from session in real implementation
        );
    ELSIF TG_OP = 'DELETE' THEN
        INSERT INTO rcm.audit_log (
            table_name,
            record_id,
            operation,
            old_values,
            changed_by
        ) VALUES (
            TG_TABLE_NAME,
            OLD.id,
            TG_OP,
            to_jsonb(OLD),
            NULL -- Replace with current user ID from session in real implementation
        );
    END IF;
    RETURN NULL;
END;
$$;

-- Example of adding audit trigger to regulations table
CREATE TRIGGER trg_regulations_audit
AFTER INSERT OR UPDATE OR DELETE ON rcm.regulations
FOR EACH ROW EXECUTE FUNCTION rcm.fn_audit_trigger();


-- Data Retention Policies
CREATE TABLE rcm.data_retention_policies (
    retention_id SERIAL PRIMARY KEY,
    policy_name VARCHAR(255) NOT NULL,
    description TEXT,
    data_category VARCHAR(100) NOT NULL,
    retention_period_years INTEGER NOT NULL,
    regulation_id INTEGER REFERENCES rcm.regulations(regulation_id),
    is_active BOOLEAN DEFAULT TRUE
);

-- Data Inventory
CREATE TABLE rcm.data_inventory (
    inventory_id SERIAL PRIMARY KEY,
    data_name VARCHAR(255) NOT NULL,
    data_description TEXT,
    data_category VARCHAR(100) NOT NULL,
    owner_org_id INTEGER REFERENCES core.organizations(org_id),
    owner_user_id INTEGER REFERENCES core.users(user_id),
    retention_policy_id INTEGER REFERENCES rcm.data_retention_policies(retention_id),
    created_date DATE NOT NULL,
    scheduled_deletion_date DATE,
    actual_deletion_date DATE,
    status VARCHAR(50) NOT NULL -- 'Active', 'Archived', 'Deleted'
);

-- View for Data Retention Compliance
CREATE OR REPLACE VIEW rcm.vw_data_retention_compliance AS
SELECT
    drp.retention_id,
    drp.policy_name,
    drp.data_category,
    drp.retention_period_years,
    r.regulation_id,
    r.regulation_name,
    COUNT(di.inventory_id) AS total_datasets,
    COUNT(CASE WHEN di.status = 'Deleted' AND di.actual_deletion_date <= di.scheduled_deletion_date THEN 1 END) AS properly_deleted,
    COUNT(CASE WHEN di.status != 'Deleted' AND di.scheduled_deletion_date < CURRENT_DATE THEN 1 END) AS overdue_for_deletion,
    ROUND(COUNT(CASE WHEN di.status = 'Deleted' AND di.actual_deletion_date <= di.scheduled_deletion_date THEN 1 END) * 100.0 /
        NULLIF(COUNT(CASE WHEN di.status = 'Deleted' THEN 1 END), 0), 2) AS compliance_percentage
FROM
    rcm.data_retention_policies drp
LEFT JOIN
    rcm.regulations r ON drp.regulation_id = r.regulation_id
LEFT JOIN
    rcm.data_inventory di ON drp.retention_id = di.retention_policy_id
WHERE
    drp.is_active = TRUE
GROUP BY
    drp.retention_id, drp.policy_name, drp.data_category, drp.retention_period_years, r.regulation_id, r.regulation_name;



    -- Regulatory Compliance Maturity Assessment
    CREATE OR REPLACE VIEW rcm.vw_compliance_maturity AS
    WITH maturity_components AS (
        SELECT
            o.org_id,
            o.org_name,
            -- Policy maturity
            COUNT(DISTINCT p.policy_id) AS total_policies,
            COUNT(DISTINCT CASE WHEN p.review_date >= CURRENT_DATE - INTERVAL '1 year' THEN p.policy_id END) AS reviewed_policies,
            -- Training maturity
            COUNT(DISTINCT ctp.program_id) AS total_training_programs,
            AVG(CASE WHEN etr.status = 'Completed' THEN 1 ELSE 0 END) AS training_completion_rate,
            -- Control maturity
            COUNT(DISTINCT c.control_id) AS total_controls,
            AVG(CASE WHEN ar.result_status = 'Pass' THEN 1
                     WHEN ar.result_status = 'Partial' THEN 0.5
                     ELSE 0 END) AS control_effectiveness,
            -- Incident maturity
            COUNT(DISTINCT nci.incident_id) AS total_incidents,
            COUNT(DISTINCT CASE WHEN nci.status = 'Closed' AND
                  EXISTS (SELECT 1 FROM rcm.remediation_actions ra
                          WHERE ra.incident_id = nci.incident_id AND ra.status = 'Completed') THEN nci.incident_id END) AS properly_remediated_incidents
        FROM
            core.organizations o
        LEFT JOIN
            rcm.policies p ON 1=1
        LEFT JOIN
            rcm.compliance_training_programs ctp ON 1=1
        LEFT JOIN
            rcm.employee_training_records etr ON ctp.program_id = etr.program_id
        LEFT JOIN
            rcm.compliance_controls c ON 1=1
        LEFT JOIN
            rcm.assessment_results ar ON c.control_id = ar.control_id
        LEFT JOIN
            rcm.non_compliance_incidents nci ON o.org_id = nci.org_id
        GROUP BY
            o.org_id, o.org_name
    )
    SELECT
        org_id,
        org_name,
        total_policies,
        reviewed_policies,
        ROUND(reviewed_policies * 100.0 / NULLIF(total_policies, 0), 2) AS policy_maturity,
        total_training_programs,
        ROUND(training_completion_rate * 100, 2) AS training_maturity,
        total_controls,
        ROUND(control_effectiveness * 100, 2) AS control_maturity,
        total_incidents,
        ROUND(properly_remediated_incidents * 100.0 / NULLIF(total_incidents, 0), 2) AS incident_maturity,
        ROUND(
            (COALESCE(reviewed_policies * 100.0 / NULLIF(total_policies, 0), 0) +
             COALESCE(training_completion_rate * 100, 0) +
             COALESCE(control_effectiveness * 100, 0) +
             COALESCE(properly_remediated_incidents * 100.0 / NULLIF(total_incidents, 0), 0)) / 4, 2
        ) AS overall_maturity_score,
        CASE
            WHEN (COALESCE(reviewed_policies * 100.0 / NULLIF(total_policies, 0), 0) +
                  COALESCE(training_completion_rate * 100, 0) +
                  COALESCE(control_effectiveness * 100, 0) +
                  COALESCE(properly_remediated_incidents * 100.0 / NULLIF(total_incidents, 0), 0)) / 4 >= 80 THEN 'Advanced'
            WHEN (COALESCE(reviewed_policies * 100.0 / NULLIF(total_policies, 0), 0) +
                  COALESCE(training_completion_rate * 100, 0) +
                  COALESCE(control_effectiveness * 100, 0) +
                  COALESCE(properly_remediated_incidents * 100.0 / NULLIF(total_incidents, 0), 0)) / 4 >= 50 THEN 'Intermediate'
            ELSE 'Basic'
        END AS maturity_level
    FROM
        maturity_components;



        -- Compliance Monitoring Rules
        CREATE TABLE rcm.compliance_monitoring_rules (
            rule_id SERIAL PRIMARY KEY,
            rule_name VARCHAR(255) NOT NULL,
            description TEXT,
            rule_condition TEXT NOT NULL, -- SQL condition that indicates non-compliance
            rule_query TEXT NOT NULL, -- SQL query to identify non-compliant items
            severity VARCHAR(50) NOT NULL, -- 'Critical', 'High', 'Medium', 'Low'
            notification_template TEXT,
            is_active BOOLEAN DEFAULT TRUE
        );

        -- Compliance Monitoring Results
        CREATE TABLE rcm.compliance_monitoring_results (
            result_id SERIAL PRIMARY KEY,
            rule_id INTEGER NOT NULL REFERENCES rcm.compliance_monitoring_rules(rule_id),
            execution_time TIMESTAMP NOT NULL,
            non_compliant_count INTEGER NOT NULL,
            details JSONB -- Detailed results of the monitoring check
        );

        -- Procedure to run compliance monitoring
        CREATE OR REPLACE PROCEDURE rcm.sp_run_compliance_monitoring(
            p_rule_ids INTEGER[] DEFAULT NULL,
            p_executed_by INTEGER DEFAULT NULL
        )
        LANGUAGE plpgsql
        AS $$
        DECLARE
            v_rule RECORD;
            v_result JSONB;
            v_count INTEGER;
        BEGIN
            -- Process each rule
            FOR v_rule IN SELECT * FROM rcm.compliance_monitoring_rules
                         WHERE is_active = TRUE
                         AND (p_rule_ids IS NULL OR rule_id = ANY(p_rule_ids))
            LOOP
                -- Execute the rule query
                EXECUTE 'SELECT COUNT(*), jsonb_agg(row_to_json(t)) FROM (' || v_rule.rule_query || ') t'
                INTO v_count, v_result;

                -- Store the results
                INSERT INTO rcm.compliance_monitoring_results (
                    rule_id,
                    execution_time,
                    non_compliant_count,
                    details
                ) VALUES (
                    v_rule.rule_id,
                    CURRENT_TIMESTAMP,
                    v_count,
                    v_result
                );

                -- In a real implementation, send notifications for critical findings
                IF v_count > 0 AND v_rule.severity IN ('Critical', 'High') THEN
                    RAISE NOTICE 'Compliance issue detected: % (count: %, severity: %)',
                                 v_rule.rule_name, v_count, v_rule.severity;
                END IF;
            END LOOP;

            COMMIT;
        END;
        $$;


  -----
  -- Regulatory Change Analysis (AI-enhanced)
CREATE TABLE rcm.regulatory_change_analysis (
    analysis_id SERIAL PRIMARY KEY,
    regulation_version_id INTEGER NOT NULL REFERENCES rcm.regulation_versions(version_id),
    summary TEXT NOT NULL,
    key_changes JSONB NOT NULL, -- Structured changes detected by AI
    affected_business_units JSONB, -- AI-predicted impact areas
    similarity_score NUMERIC(5,2), -- Similarity to previous versions (0-100)
    complexity_score NUMERIC(5,2), -- Implementation complexity (0-100)
    risk_implications JSONB, -- AI-generated risk analysis
    generated_at TIMESTAMP NOT NULL,
    generated_by VARCHAR(100) NOT NULL, -- 'AI' or 'Manual'
    reviewed_by INTEGER REFERENCES core.users(user_id),
    reviewed_at TIMESTAMP,
    review_status VARCHAR(50) DEFAULT 'Pending' -- 'Pending', 'Approved', 'Rejected'
);

-- AI Model Versions for Compliance
CREATE TABLE rcm.ai_models (
    model_id SERIAL PRIMARY KEY,
    model_name VARCHAR(255) NOT NULL,
    model_version VARCHAR(100) NOT NULL,
    model_type VARCHAR(100) NOT NULL, -- 'Change Analysis', 'Risk Prediction', etc.
    deployment_date TIMESTAMP NOT NULL,
    performance_metrics JSONB,
    is_active BOOLEAN DEFAULT TRUE,
    UNIQUE(model_name, model_version)
);

-- Regulatory Text Embeddings (for semantic search)
CREATE TABLE rcm.regulatory_embeddings (
    embedding_id SERIAL PRIMARY KEY,
    regulation_id INTEGER REFERENCES rcm.regulations(regulation_id),
    requirement_id INTEGER REFERENCES rcm.regulatory_requirements(requirement_id),
    embedding_vector VECTOR(1536) NOT NULL, -- Using pgvector extension
    text_chunk TEXT NOT NULL,
    chunk_hash VARCHAR(64) NOT NULL,
    UNIQUE(requirement_id, chunk_hash)
);

-- Semantic Search Index (Virtual Table)
CREATE MATERIALIZED VIEW rcm.mv_regulatory_semantic_search AS
SELECT
    re.embedding_id,
    re.regulation_id,
    re.requirement_id,
    r.regulation_name,
    rr.requirement_code,
    rr.description AS requirement_description,
    re.text_chunk,
    re.embedding_vector
FROM
    rcm.regulatory_embeddings re
JOIN
    rcm.regulations r ON re.regulation_id = r.regulation_id
LEFT JOIN
    rcm.regulatory_requirements rr ON re.requirement_id = rr.requirement_id;


-----
-- Compliance Workflow Templates
CREATE TABLE rcm.workflow_templates (
    template_id SERIAL PRIMARY KEY,
    template_name VARCHAR(255) NOT NULL,
    description TEXT,
    workflow_definition JSONB NOT NULL, -- BPMN or custom JSON definition
    applicable_regulations INTEGER[],
    version VARCHAR(50) NOT NULL,
    created_by INTEGER REFERENCES core.users(user_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    is_active BOOLEAN DEFAULT TRUE
);

-- Active Compliance Workflows
CREATE TABLE rcm.compliance_workflows (
    workflow_id SERIAL PRIMARY KEY,
    template_id INTEGER NOT NULL REFERENCES rcm.workflow_templates(template_id),
    workflow_name VARCHAR(255) NOT NULL,
    current_state VARCHAR(100) NOT NULL,
    initiation_reason TEXT,
    initiated_by INTEGER REFERENCES core.users(user_id),
    initiated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP,
    status VARCHAR(50) NOT NULL, -- 'Draft', 'Active', 'Completed', 'Cancelled'
    context_data JSONB -- Additional workflow-specific data
);

-- Workflow Execution Log
CREATE TABLE rcm.workflow_execution_log (
    log_id SERIAL PRIMARY KEY,
    workflow_id INTEGER NOT NULL REFERENCES rcm.compliance_workflows(workflow_id),
    step_name VARCHAR(255) NOT NULL,
    step_state VARCHAR(100) NOT NULL,
    action_taken VARCHAR(255),
    performed_by INTEGER REFERENCES core.users(user_id),
    performed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    comments TEXT,
    decision_points JSONB -- Key decisions made at this step
);

-- Workflow Escalation Rules
CREATE TABLE rcm.workflow_escalation_rules (
    rule_id SERIAL PRIMARY KEY,
    template_id INTEGER NOT NULL REFERENCES rcm.workflow_templates(template_id),
    trigger_condition VARCHAR(255) NOT NULL,
    escalation_path JSONB NOT NULL, -- Who to escalate to under what conditions
    timeout_duration INTERVAL,
    severity VARCHAR(50) NOT NULL,
    is_active BOOLEAN DEFAULT TRUE
);

-- Compliance Risk Predictions
CREATE TABLE rcm.compliance_risk_predictions (
    prediction_id SERIAL PRIMARY KEY,
    org_id INTEGER REFERENCES core.organizations(org_id),
    regulation_id INTEGER REFERENCES rcm.regulations(regulation_id),
    prediction_date DATE NOT NULL,
    time_horizon VARCHAR(50) NOT NULL, -- '30-day', '90-day', '1-year'
    risk_score NUMERIC(5,2) NOT NULL,
    confidence_score NUMERIC(5,2) NOT NULL,
    key_risk_factors JSONB NOT NULL,
    model_id INTEGER REFERENCES rcm.ai_models(model_id),
    reviewed_status VARCHAR(50) DEFAULT 'Unreviewed'
);

-- Control Failure Predictions
CREATE TABLE rcm.control_failure_predictions (
    prediction_id SERIAL PRIMARY KEY,
    control_id INTEGER REFERENCES rcm.compliance_controls(control_id),
    prediction_date DATE NOT NULL,
    failure_probability NUMERIC(5,2) NOT NULL, -- 0-100%
    expected_impact VARCHAR(50) NOT NULL, -- 'Low', 'Medium', 'High'
    contributing_factors JSONB NOT NULL,
    recommended_actions JSONB,
    model_id INTEGER REFERENCES rcm.ai_models(model_id)
);

-- Regulatory Change Impact Predictions
CREATE TABLE rcm.change_impact_predictions (
    prediction_id SERIAL PRIMARY KEY,
    regulation_id INTEGER REFERENCES rcm.regulations(regulation_id),
    prediction_date DATE NOT NULL,
    predicted_effort_hours NUMERIC(10,2),
    predicted_cost NUMERIC(15,2),
    impacted_departments JSONB,
    readiness_score NUMERIC(5,2), -- 0-100
    model_id INTEGER REFERENCES rcm.ai_models(model_id)
);

-- Blockchain Network Configuration
CREATE TABLE rcm.blockchain_networks (
    network_id SERIAL PRIMARY KEY,
    network_name VARCHAR(255) NOT NULL,
    network_type VARCHAR(100) NOT NULL, -- 'Hyperledger', 'Ethereum', 'Corda'
    endpoint_url VARCHAR(255) NOT NULL,
    chain_id VARCHAR(100),
    is_testnet BOOLEAN DEFAULT FALSE,
    is_active BOOLEAN DEFAULT TRUE
);

-- Compliance Evidence Blockchain Transactions
CREATE TABLE rcm.blockchain_evidence (
    evidence_id SERIAL PRIMARY KEY,
    local_evidence_id INTEGER REFERENCES rcm.remediation_evidence(evidence_id),
    network_id INTEGER REFERENCES rcm.blockchain_networks(nwork_id),
    transaction_hash VARCHAR(66) NOT NULL,
    block_number INTEGER,
    timestamp TIMESTAMP,
    smart_contract_address VARCHAR(42),
    metadata JSONB,
    verification_status VARCHAR(50) DEFAULT 'Pending'
);

-- Smart Contract Templates
CREATE TABLE rcm.smart_contract_templates (
    template_id SERIAL PRIMARY KEY,
    template_name VARCHAR(255) NOT NULL,
    description TEXT,
    contract_code TEXT NOT NULL,
    version VARCHAR(50) NOT NULL,
    applicable_for VARCHAR(255) NOT NULL, -- 'Evidence', 'Policy', 'Consent'
    is_active BOOLEAN DEFAULT TRUE
);


-- AI-Powered Regulatory Change Impact View
CREATE OR REPLACE VIEW rcm.vw_ai_regulatory_change_impact AS
SELECT
    r.regulation_id,
    r.regulation_name,
    rv.version_number,
    rca.summary,
    rca.key_changes,
    rca.affected_business_units->>'departments' AS impacted_departments,
    rca.complexity_score,
    rca.similarity_score,
    rca.risk_implications,
    COUNT(rt.task_id) AS total_tasks,
    COUNT(CASE WHEN rt.status = 'Completed' THEN rt.task_id END) AS completed_tasks,
    cip.predicted_effort_hours,
    cip.predicted_cost,
    cip.readiness_score
FROM
    rcm.regulations r
JOIN
    rcm.regulation_versions rv ON r.regulation_id = rv.regulation_id
JOIN
    rcm.regulatory_change_analysis rca ON rv.version_id = rca.regulation_version_id
LEFT JOIN
    rcm.regulatory_change_impact rci ON rv.version_id = rci.regulation_version_id
LEFT JOIN
    rcm.regulatory_change_tasks rt ON rci.impact_id = rt.impact_id
LEFT JOIN
    rcm.change_impact_predictions cip ON r.regulation_id = cip.regulation_id
GROUP BY
    r.regulation_id, r.regulation_name, rv.version_number,
    rca.summary, rca.key_changes, rca.affected_business_units,
    rca.complexity_score, rca.similarity_score, rca.risk_implications,
    cip.predicted_effort_hours, cip.predicted_cost, cip.readiness_score;

-- Predictive Risk Dashboard View
CREATE OR REPLACE VIEW rcm.vw_predictive_risk_dashboard AS
WITH org_risks AS (
    SELECT
        org_id,
        COUNT(prediction_id) FILTER (WHERE risk_score > 70) AS high_risk_predictions,
        COUNT(prediction_id) FILTER (WHERE risk_score > 50 AND risk_score <= 70) AS medium_risk_predictions,
        AVG(risk_score) AS avg_risk_score
    FROM
        rcm.compliance_risk_predictions
    WHERE
        prediction_date >= CURRENT_DATE - INTERVAL '90 days'
    GROUP BY
        org_id
),
control_risks AS (
    SELECT
        c.owner_org_id AS org_id,
        COUNT(cfp.prediction_id) FILTER (WHERE cfp.failure_probability > 30) AS high_failure_controls,
        AVG(cfp.failure_probability) AS avg_failure_probability
    FROM
        rcm.control_failure_predictions cfp
    JOIN
        rcm.compliance_controls c ON cfp.control_id = c.control_id
    WHERE
        cfp.prediction_date >= CURRENT_DATE - INTERVAL '90 days'
    GROUP BY
        c.owner_org_id
)
SELECT
    o.org_id,
    o.org_name,
    COALESCE(orr.high_risk_predictions, 0) AS high_risk_predictions,
    COALESCE(orr.medium_risk_predictions, 0) AS medium_risk_predictions,
    COALESCE(orr.avg_risk_score, 0) AS avg_risk_score,
    COALESCE(cr.high_failure_controls, 0) AS high_failure_controls,
    COALESCE(cr.avg_failure_probability, 0) AS avg_failure_probability,
    RANK() OVER (ORDER BY COALESCE(orr.avg_risk_score, 0) DESC) AS risk_rank
FROM
    core.organizations o
LEFT JOIN
    org_risks orr ON o.org_id = orr.org_id
LEFT JOIN
    control_risks cr ON o.org_id = cr.org_id
ORDER BY
    risk_rank;


    -- Procedure for AI-Powered Regulatory Change Analysis
    CREATE OR REPLACE PROCEDURE rcm.sp_analyze_regulatory_change(
        p_version_id INTEGER,
        p_model_id INTEGER DEFAULT NULL,
        OUT p_analysis_id INTEGER
    )
    LANGUAGE plpgsql
    AS $$
    DECLARE
        v_model_version VARCHAR(100);
        v_summary TEXT;
        v_key_changes JSONB;
        v_impact_areas JSONB;
    BEGIN
        -- Get the latest model if not specified
        IF p_model_id IS NULL THEN
            SELECT model_id INTO p_model_id
            FROM rcm.ai_models
            WHERE model_type = 'Change Analysis'
            AND is_active = TRUE
            ORDER BY deployment_date DESC
            LIMIT 1;
        END IF;

        -- Get model version
        SELECT model_version INTO v_model_version
        FROM rcm.ai_models
        WHERE model_id = p_model_id;

        -- In a real implementation, this would call an AI service
        -- For demonstration, we'll simulate results
        SELECT
            'This regulation update introduces new data privacy requirements for cross-border transfers and expands individual rights.',
            '[{"change_type": "New Requirement", "description": "Data Protection Impact Assessments required for cross-border transfers"},
              {"change_type": "Modified Requirement", "description": "Expanded right to data portability"}]',
            '{"departments": ["IT", "Legal", "Data Privacy"], "processes": ["Data Transfer", "DSAR Handling"]}'
        INTO
            v_summary,
            v_key_changes,
            v_impact_areas;

        -- Store the analysis
        INSERT INTO rcm.regulatory_change_analysis (
            regulation_version_id,
            summary,
            key_changes,
            affected_business_units,
            similarity_score,
            complexity_score,
            risk_implications,
            generated_at,
            generated_by,
            model_id
        ) VALUES (
            p_version_id,
            v_summary,
            v_key_changes,
            v_impact_areas,
            65.5, -- similarity
            72.3, -- complexity
            '{"high_risk_areas": ["Data Transfer"], "recommendations": ["Update DPIA process", "Train staff on new portability requirements"]}',
            CURRENT_TIMESTAMP,
            'AI Model v' || v_model_version,
            p_model_id
        ) RETURNING analysis_id INTO p_analysis_id;

        COMMIT;
    END;
    $$;

    -- Procedure for Predictive Risk Scoring
    CREATE OR REPLACE PROCEDURE rcm.sp_predict_compliance_risks(
        p_time_horizon VARCHAR(50),
        p_model_id INTEGER DEFAULT NULL,
        p_org_ids INTEGER[] DEFAULT NULL
    )
    LANGUAGE plpgsql
    AS $$
    DECLARE
        v_model_version VARCHAR(100);
        v_org_id INTEGER;
        v_regulation_id INTEGER;
        v_risk_score NUMERIC(5,2);
        v_factors JSONB;
    BEGIN
        -- Get the latest model if not specified
        IF p_model_id IS NULL THEN
            SELECT model_id INTO p_model_id
            FROM rcm.ai_models
            WHERE model_type = 'Risk Prediction'
            AND is_active = TRUE
            ORDER BY deployment_date DESC
            LIMIT 1;
        END IF;

        -- Get model version
        SELECT model_version INTO v_model_version
        FROM rcm.ai_models
        WHERE model_id = p_model_id;

        -- Process each organization and regulation
        FOR v_org_id IN SELECT org_id FROM core.organizations
                       WHERE p_org_ids IS NULL OR org_id = ANY(p_org_ids))
        LOOP
            FOR v_regulation_id IN SELECT regulation_id FROM rcm.regulatory_obligations
                                  WHERE org_id = v_org_id AND is_active = TRUE)
            LOOP
                -- In a real implementation, this would call an AI service
                -- For demonstration, we'll simulate results
                SELECT
                    ROUND(30 + RANDOM() * 50, 1), -- Random score between 30-80
                    '[{"factor": "Control Effectiveness", "value": "65%", "weight": 0.4},
                      {"factor": "Recent Incidents", "value": "2 in last quarter", "weight": 0.3},
                      {"factor": "Training Completion", "value": "78%", "weight": 0.2},
                      {"factor": "Regulatory Change", "value": "Recent update", "weight": 0.1}]'
                INTO
                    v_risk_score,
                    v_factors;

                -- Store the prediction
                INSERT INTO rcm.compliance_risk_predictions (
                    org_id,
                    regulation_id,
                    prediction_date,
                    time_horizon,
                    risk_score,
                    confidence_score,
                    key_risk_factors,
                    model_id
                ) VALUES (
                    v_org_id,
                    v_regulation_id,
                    CURRENT_DATE,
                    p_time_horizon,
                    v_risk_score,
                    ROUND(80 + RANDOM() * 15, 1), -- Confidence 80-95%
                    v_factors,
                    p_model_id
                );
            END LOOP;
        END LOOP;

        COMMIT;
    END;
    $$;

    -- Procedure for Blockchain Evidence Submission
    CREATE OR REPLACE PROCEDURE rcm.sp_submit_evidence_to_blockchain(
        p_evidence_id INTEGER,
        p_network_id INTEGER,
        OUT p_transaction_hash VARCHAR(66)
    )
    LANGUAGE plpgsql
    AS $$
    DECLARE
        v_evidence RECORD;
        v_network RECORD;
        v_smart_contract VARCHAR(42);
    BEGIN
        -- Get evidence details
        SELECT
            re.evidence_id, re.action_id, re.document_id,
            d.doc_name, d.file_path, d.upload_date
        INTO v_evidence
        FROM
            rcm.remediation_evidence re
        JOIN
            core.documents d ON re.document_id = d.doc_id
        WHERE
            re.evidence_id = p_evidence_id;

        -- Get network details
        SELECT
            network_name, endpoint_url, chain_id
        INTO v_network
        FROM
            rcm.blockchain_networks
        WHERE
            network_id = p_network_id;

        -- In a real implementation, this would interact with a blockchain node
        -- For demonstration, we'll simulate a transaction hash
        p_transaction_hash := '0x' || encode(gen_random_bytes(32), 'hex');

        -- Get a smart contract address (simulated)
        v_smart_contract := '0x' || encode(gen_random_bytes(20), 'hex');

        -- Record the blockchain transaction
        INSERT INTO rcm.blockchain_evidence (
            local_evidence_id,
            network_id,
            transaction_hash,
            block_number,
            timestamp,
            smart_contract_address,
            metadata
        ) VALUES (
            p_evidence_id,
            p_network_id,
            p_transaction_hash,
            (1000000 + RANDOM() * 100000)::INTEGER, -- Simulated block number
            CURRENT_TIMESTAMP,
            v_smart_contract,
            jsonb_build_object(
                'network', v_network.network_name,
                'evidence', jsonb_build_object(
                    'name', v_evidence.doc_name,
                    'upload_date', v_evidence.upload_date
                )
            )
        );

        COMMIT;
    END;
    $$;


    -- Regulatory Document NLP Analysis
  CREATE TABLE rcm.regulatory_nlp_analysis (
      analysis_id SERIAL PRIMARY KEY,
      document_id INTEGER NOT NULL REFERENCES core.documents(doc_id),
      analysis_type VARCHAR(100) NOT NULL, -- 'Entity Extraction', 'Sentiment', 'Topic Modeling'
      entities JSONB, -- Extracted entities (people, organizations, dates)
      topics JSONB, -- Detected topics with weights
      sentiment_score NUMERIC(5,2), -- -1 to 1
      summary TEXT,
      processed_at TIMESTAMP NOT NULL,
      model_id INTEGER REFERENCES rcm.ai_models(model_id)
  );

  -- Procedure for NLP Document Analysis
  CREATE OR REPLACE PROCEDURE rcm.sp_analyze_regulatory_document(
      p_document_id INTEGER,
      p_analysis_types VARCHAR(100)[] DEFAULT ARRAY['Entity Extraction', 'Topic Modeling'],
      p_model_id INTEGER DEFAULT NULL
  )
  LANGUAGE plpgsql
  AS $$
  DECLARE
      v_model_version VARCHAR(100);
      v_entities JSONB;
      v_topics JSONB;
      v_sentiment NUMERIC(5,2);
      v_summary TEXT;
  BEGIN
      -- Get the latest model if not specified
      IF p_model_id IS NULL THEN
          SELECT model_id INTO p_model_id
          FROM rcm.ai_models
          WHERE model_type = 'NLP Analysis'
          AND is_active = TRUE
          ORDER BY deployment_date DESC
          LIMIT 1;
      END IF;

      -- Get model version
      SELECT model_version INTO v_model_version
      FROM rcm.ai_models
      WHERE model_id = p_model_id;

      -- In a real implementation, this would call an NLP service
      -- For demonstration, we'll simulate results
      SELECT
          '[{"type": "Requirement", "text": "data portability", "count": 5},
            {"type": "Deadline", "text": "within 30 days", "count": 3},
            {"type": "Penalty", "text": "up to 4% of revenue", "count": 2}]',
          '[{"topic": "Data Subject Rights", "weight": 0.85},
            {"topic": "Compliance Deadlines", "weight": 0.72},
            {"topic": "Enforcement", "weight": 0.65}]',
          0.65, -- sentiment
          'This document primarily focuses on data subject rights with specific requirements around data portability and strict compliance deadlines.'
      INTO
          v_entities,
          v_topics,
          v_sentiment,
          v_summary;

      -- Store the analysis for each requested type
      IF 'Entity Extraction' = ANY(p_analysis_types) THEN
          INSERT INTO rcm.regulatory_nlp_analysis (
              document_id,
              analysis_type,
              entities,
              processed_at,
              model_id
          ) VALUES (
              p_document_id,
              'Entity Extraction',
              v_entities,
              CURRENT_TIMESTAMP,
              p_model_id
          );
      END IF;

      IF 'Topic Modeling' = ANY(p_analysis_types) THEN
          INSERT INTO rcm.regulatory_nlp_analysis (
              document_id,
              analysis_type,
              topics,
              processed_at,
              model_id
          ) VALUES (
              p_document_id,
              'Topic Modeling',
              v_topics,
              CURRENT_TIMESTAMP,
              p_model_id
          );
      END IF;

      IF 'Sentiment Analysis' = ANY(p_analysis_types) THEN
          INSERT INTO rcm.regulatory_nlp_analysis (
              document_id,
              analysis_type,
              sentiment_score,
              processed_at,
              model_id
          ) VALUES (
              p_document_id,
              'Sentiment Analysis',
              v_sentiment,
              CURRENT_TIMESTAMP,
              p_model_id
          );
      END IF;

      IF 'Summarization' = ANY(p_analysis_types) THEN
          INSERT INTO rcm.regulatory_nlp_analysis (
              document_id,
              analysis_type,
              summary,
              processed_at,
              model_id
          ) VALUES (
              p_document_id,
              'Summarization',
              v_summary,
              CURRENT_TIMESTAMP,
              p_model_id
          );
      END IF;

      COMMIT;
  END;
  $$;

  -- Real-Time Compliance Events
CREATE TABLE rcm.realtime_compliance_events (
    event_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    event_type VARCHAR(100) NOT NULL, -- 'Control Failure', 'Policy Violation', 'Threshold Breach'
    event_source VARCHAR(100) NOT NULL, -- 'System', 'User', 'Integration'
    severity VARCHAR(50) NOT NULL, -- 'Critical', 'High', 'Medium', 'Low'
    event_data JSONB NOT NULL,
    detected_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    processed_at TIMESTAMP,
    status VARCHAR(50) DEFAULT 'New' -- 'New', 'Processing', 'Resolved', 'Escalated'
);

-- Event Processing Rules
CREATE TABLE rcm.event_processing_rules (
    rule_id SERIAL PRIMARY KEY,
    rule_name VARCHAR(255) NOT NULL,
    event_type VARCHAR(100) NOT NULL,
    match_conditions JSONB NOT NULL, -- Conditions to match events
    actions JSONB NOT NULL, -- Actions to take (notify, create ticket, etc.)
    priority INTEGER NOT NULL,
    is_active BOOLEAN DEFAULT TRUE
);

-- Procedure for Processing Real-Time Events
CREATE OR REPLACE PROCEDURE rcm.sp_process_compliance_events()
LANGUAGE plpgsql
AS $$
DECLARE
    v_event RECORD;
    v_rule RECORD;
    v_action RECORD;
    v_processed_count INTEGER := 0;
BEGIN
    -- Process new events
    FOR v_event IN SELECT * FROM rcm.realtime_compliance_events
                  WHERE status = 'New'
                  ORDER BY detected_at
                  LIMIT 100
    LOOP
        -- Find matching rules ordered by priority
        FOR v_rule IN SELECT * FROM rcm.event_processing_rules
                     WHERE event_type = v_event.event_type
                     AND is_active = TRUE
                     ORDER BY priority DESC
        LOOP
            -- In a real implementation, evaluate match_conditions against event_data
            -- For demo, assume first matching rule is applied

            -- Process actions
            FOR v_action IN SELECT * FROM jsonb_array_elements(v_rule.actions)
            LOOP
                -- Execute action based on type
                IF v_action->>'type' = 'create_ticket' THEN
                    -- In real implementation, create a ticket in ticketing system
                    RAISE NOTICE 'Creating ticket for event %: %',
                                 v_event.event_id, v_action->>'description';
                ELSIF v_action->>'type' = 'send_notification' THEN
                    -- In real implementation, send notification
                    RAISE NOTICE 'Sending notification to %: %',
                                 v_action->>'recipient', v_action->>'message';
                ELSIF v_action->>'type' = 'trigger_workflow' THEN
                    -- In real implementation, start a workflow
                    RAISE NOTICE 'Triggering workflow % for event %',
                                 v_action->>'workflow_type', v_event.event_id;
                END IF;
            END LOOP;

            -- Update event status
            UPDATE rcm.realtime_compliance_events
            SET
                status = 'Processing',
                processed_at = CURRENT_TIMESTAMP
            WHERE
                event_id = v_event.event_id;

            v_processed_count := v_processed_count + 1;

            -- Exit rule loop after first match (highest priority)
            EXIT;
        END LOOP;
    END LOOP;

    RAISE NOTICE 'Processed % compliance events', v_processed_count;
END;
$$;

---------------------
----------------------
--Module : Policy Management
----------------------
CREATE SCHEMA policy_management;


CREATE TABLE policy_management.policies (
    policy_id SERIAL PRIMARY KEY,
    policy_name VARCHAR(255) NOT NULL,
    policy_description TEXT,
    policy_category VARCHAR(100),
    policy_owner_id INT REFERENCES core.employees(employee_id),
    effective_date DATE NOT NULL,
    review_frequency_months INT DEFAULT 12,
    next_review_date DATE,
    status VARCHAR(50) CHECK (status IN ('Draft', 'Active', 'Under Review', 'Archived')),
    version VARCHAR(20) NOT NULL,
    previous_version_id INT REFERENCES policy_management.policies(policy_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by INT REFERENCES core.employees(employee_id),
    updated_by INT REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE policy_management.policies IS 'Master table for all organizational policies';
COMMENT ON COLUMN policy_management.policies.review_frequency_months IS 'Frequency in months for policy review';


CREATE TABLE policy_management.policy_documents (
    document_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    file_name VARCHAR(255) NOT NULL,
    file_path VARCHAR(512) NOT NULL,
    file_type VARCHAR(50),
    file_size BIGINT,
    is_current BOOLEAN DEFAULT TRUE,
    uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    uploaded_by INT REFERENCES core.employees(employee_id),
    checksum VARCHAR(64) NOT NULL
);

COMMENT ON TABLE policy_management.policy_documents IS 'Stores the actual policy documents with version tracking';

CREATE TABLE policy_management.policy_reviews (
    review_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    review_start_date DATE NOT NULL,
    review_end_date DATE,
    status VARCHAR(50) CHECK (status IN ('Pending', 'In Progress', 'Completed', 'Overdue')),
    initiated_by INT REFERENCES core.employees(employee_id),
    completed_by INT REFERENCES core.employees(employee_id),
    notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_reviews IS 'Tracks the review cycles for each policy';


CREATE TABLE policy_management.policy_approvals (
    approval_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    review_id INT REFERENCES policy_management.policy_reviews(review_id),
    approver_id INT NOT NULL REFERENCES core.employees(employee_id),
    approval_date TIMESTAMP,
    status VARCHAR(50) CHECK (status IN ('Pending', 'Approved', 'Rejected', 'Deferred')),
    comments TEXT,
    required BOOLEAN DEFAULT TRUE,
    approval_order INT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_approvals IS 'Tracks approval workflow for policy changes';


CREATE TABLE policy_management.policy_attestations (
    attestation_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    employee_id INT NOT NULL REFERENCES core.employees(employee_id),
    attestation_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status VARCHAR(50) CHECK (status IN ('Pending', 'Completed', 'Overdue')),
    ip_address VARCHAR(45),
    device_info VARCHAR(255),
    acknowledged_version VARCHAR(20) NOT NULL,
    next_attestation_date DATE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_attestations IS 'Tracks employee acknowledgments and attestations of policies';


CREATE TABLE policy_management.policy_risk_mapping (
    mapping_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    risk_id INT NOT NULL REFERENCES core.risks(risk_id),
    mitigation_effectiveness VARCHAR(50) CHECK (mitigation_effectiveness IN ('High', 'Medium', 'Low')),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by INT REFERENCES core.employees(employee_id),
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_by INT REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE policy_management.policy_risk_mapping IS 'Maps policies to the risks they mitigate';


CREATE TABLE policy_management.policy_control_mapping (
    mapping_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    control_id INT NOT NULL REFERENCES core.controls(control_id),
    relationship_type VARCHAR(50) CHECK (relationship_type IN ('Direct', 'Indirect', 'Supporting')),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by INT REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE policy_management.policy_control_mapping IS 'Maps policies to related controls';


CREATE TABLE policy_management.policy_regulatory_mapping (
    mapping_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    regulation_id INT NOT NULL REFERENCES core.regulations(regulation_id),
    compliance_status VARCHAR(50) CHECK (compliance_status IN ('Compliant', 'Partially Compliant', 'Non-Compliant', 'Not Applicable')),
    evidence TEXT,
    last_assessed_date DATE,
    next_assessment_date DATE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by INT REFERENCES core.employees(employee_id),
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_by INT REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE policy_management.policy_regulatory_mapping IS 'Maps policies to regulatory requirements';


CREATE TABLE policy_management.policy_training (
    training_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    training_name VARCHAR(255) NOT NULL,
    training_description TEXT,
    training_type VARCHAR(50) CHECK (training_type IN ('Online', 'Classroom', 'Webinar', 'Self-Study')),
    frequency_months INT DEFAULT 12,
    duration_minutes INT,
    is_mandatory BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by INT REFERENCES core.employees(employee_id),
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_by INT REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE policy_management.policy_training IS 'Defines training requirements for policies';


CREATE TABLE policy_management.policy_training_records (
    record_id SERIAL PRIMARY KEY,
    training_id INT NOT NULL REFERENCES policy_management.policy_training(training_id),
    employee_id INT NOT NULL REFERENCES core.employees(employee_id),
    completion_date TIMESTAMP,
    status VARCHAR(50) CHECK (status IN ('Not Started', 'In Progress', 'Completed', 'Failed')),
    score DECIMAL(5,2),
    expiration_date DATE,
    training_hours DECIMAL(5,2),
    trainer_id INT REFERENCES core.employees(employee_id),
    training_method VARCHAR(50),
    notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_training_records IS 'Tracks employee training completion for policies';


CREATE TABLE policy_management.policy_exceptions (
    exception_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    requester_id INT NOT NULL REFERENCES core.employees(employee_id),
    request_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    exception_start_date DATE NOT NULL,
    exception_end_date DATE,
    status VARCHAR(50) CHECK (status IN ('Pending', 'Approved', 'Rejected', 'Expired')),
    justification TEXT NOT NULL,
    risk_assessment TEXT,
    compensating_controls TEXT,
    approver_id INT REFERENCES core.employees(employee_id),
    approval_date TIMESTAMP,
    review_cycle_months INT DEFAULT 6,
    next_review_date DATE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_exceptions IS 'Tracks exceptions granted to policies';


CREATE TABLE policy_management.policy_violations (
    violation_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    employee_id INT REFERENCES core.employees(employee_id),
    violation_date TIMESTAMP,
    detection_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    detection_method VARCHAR(100),
    severity VARCHAR(50) CHECK (severity IN ('Low', 'Medium', 'High', 'Critical')),
    description TEXT NOT NULL,
    corrective_actions TEXT,
    status VARCHAR(50) CHECK (status IN ('Open', 'Investigating', 'Resolved', 'Closed')),
    resolution_date TIMESTAMP,
    reported_by INT REFERENCES core.employees(employee_id),
    assigned_to INT REFERENCES core.employees(employee_id),
    is_repeat_violation BOOLEAN DEFAULT FALSE,
    previous_violation_id INT REFERENCES policy_management.policy_violations(violation_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_violations IS 'Tracks violations of organizational policies';


CREATE TABLE policy_management.policy_feedback (
    feedback_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    employee_id INT NOT NULL REFERENCES core.employees(employee_id),
    feedback_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    feedback_type VARCHAR(50) CHECK (feedback_type IN ('Suggestion', 'Issue', 'Clarification', 'Compliment')),
    feedback_text TEXT NOT NULL,
    status VARCHAR(50) CHECK (status IN ('Submitted', 'Under Review', 'Implemented', 'Rejected', 'Duplicate')),
    response TEXT,
    responded_by INT REFERENCES core.employees(employee_id),
    response_date TIMESTAMP,
    is_anonymous BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_feedback IS 'Collects feedback on policies from employees';


CREATE TABLE policy_management.policy_kpis (
    kpi_id SERIAL PRIMARY KEY,
    kpi_name VARCHAR(255) NOT NULL,
    kpi_description TEXT,
    measurement_frequency VARCHAR(50) CHECK (measurement_frequency IN ('Daily', 'Weekly', 'Monthly', 'Quarterly', 'Annually')),
    target_value DECIMAL(10,2),
    min_threshold DECIMAL(10,2),
    max_threshold DECIMAL(10,2),
    unit_of_measure VARCHAR(50),
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by INT REFERENCES core.employees(employee_id),
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_by INT REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE policy_management.policy_kpis IS 'Master table for Policy Management KPIs';


CREATE TABLE policy_management.policy_kpi_measurements (
    measurement_id SERIAL PRIMARY KEY,
    kpi_id INT NOT NULL REFERENCES policy_management.policy_kpis(kpi_id),
    policy_id INT REFERENCES policy_management.policies(policy_id),
    measurement_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    measured_value DECIMAL(10,2) NOT NULL,
    data_source VARCHAR(255),
    comments TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by INT REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE policy_management.policy_kpi_measurements IS 'Stores measurements for Policy Management KPIs';

CREATE TABLE policy_management.policy_krms (
    krm_id SERIAL PRIMARY KEY,
    krm_name VARCHAR(255) NOT NULL,
    krm_description TEXT,
    measurement_frequency VARCHAR(50) CHECK (measurement_frequency IN ('Daily', 'Weekly', 'Monthly', 'Quarterly', 'Annually')),
    risk_threshold DECIMAL(10,2),
    unit_of_measure VARCHAR(50),
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by INT REFERENCES core.employees(employee_id),
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_by INT REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE policy_management.policy_krms IS 'Master table for Policy Management Key Risk Metrics';

CREATE TABLE policy_management.policy_krm_measurements (
    measurement_id SERIAL PRIMARY KEY,
    krm_id INT NOT NULL REFERENCES policy_management.policy_krms(krm_id),
    policy_id INT REFERENCES policy_management.policies(policy_id),
    measurement_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    measured_value DECIMAL(10,2) NOT NULL,
    data_source VARCHAR(255),
    comments TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by INT REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE policy_management.policy_krm_measurements IS 'Stores measurements for Policy Management KRMs';


CREATE TABLE policy_management.policy_change_log (
    change_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    change_type VARCHAR(50) NOT NULL CHECK (change_type IN ('Create', 'Update', 'Status Change', 'Review', 'Approval', 'Attestation')),
    change_description TEXT NOT NULL,
    changed_by INT NOT NULL REFERENCES core.employees(employee_id),
    changed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    old_value TEXT,
    new_value TEXT,
    change_reason TEXT
);

COMMENT ON TABLE policy_management.policy_change_log IS 'Audit log for all changes to policies and related records';


CREATE OR REPLACE VIEW policy_management.vw_policy_compliance_status AS
SELECT
    p.policy_id,
    p.policy_name,
    p.status AS policy_status,
    p.version,
    p.effective_date,
    p.next_review_date,
    COUNT(DISTINCT pr.review_id) AS review_count,
    MAX(pr.review_end_date) AS last_review_date,
    COUNT(DISTINCT a.attestation_id) AS attestation_count,
    COUNT(DISTINCT CASE WHEN a.status = 'Completed' THEN a.attestation_id END) AS completed_attestations,
    COUNT(DISTINCT t.training_id) AS training_programs,
    COUNT(DISTINCT tr.record_id) AS training_records,
    COUNT(DISTINCT CASE WHEN tr.status = 'Completed' THEN tr.record_id END) AS completed_trainings,
    COUNT(DISTINCT v.violation_id) AS violation_count,
    COUNT(DISTINCT e.exception_id) AS exception_count,
    COUNT(DISTINCT r.regulation_id) AS regulation_count,
    COUNT(DISTINCT CASE WHEN prm.compliance_status = 'Compliant' THEN r.regulation_id END) AS compliant_regulations
FROM
    policy_management.policies p
LEFT JOIN
    policy_management.policy_reviews pr ON p.policy_id = pr.policy_id
LEFT JOIN
    policy_management.policy_attestations a ON p.policy_id = a.policy_id
LEFT JOIN
    policy_management.policy_training t ON p.policy_id = t.policy_id
LEFT JOIN
    policy_management.policy_training_records tr ON t.training_id = tr.training_id
LEFT JOIN
    policy_management.policy_violations v ON p.policy_id = v.policy_id
LEFT JOIN
    policy_management.policy_exceptions e ON p.policy_id = e.policy_id
LEFT JOIN
    policy_management.policy_regulatory_mapping prm ON p.policy_id = prm.policy_id
LEFT JOIN
    core.regulations r ON prm.regulation_id = r.regulation_id
GROUP BY
    p.policy_id, p.policy_name, p.status, p.version, p.effective_date, p.next_review_date;

COMMENT ON VIEW policy_management.vw_policy_compliance_status IS 'Provides a comprehensive view of policy compliance status including reviews, attestations, training, violations, and regulatory compliance';


CREATE OR REPLACE VIEW policy_management.vw_policy_review_dashboard AS
SELECT
    p.policy_id,
    p.policy_name,
    p.policy_category,
    p.status AS policy_status,
    p.version,
    p.effective_date,
    p.next_review_date,
    p.review_frequency_months,
    e.employee_name AS policy_owner,
    pr.review_id,
    pr.review_start_date,
    pr.review_end_date,
    pr.status AS review_status,
    CASE
        WHEN pr.status = 'Completed' THEN 0
        WHEN pr.status = 'In Progress' THEN 1
        WHEN pr.status = 'Pending' AND p.next_review_date <= CURRENT_DATE THEN 2
        WHEN pr.status = 'Pending' AND p.next_review_date > CURRENT_DATE THEN 3
        ELSE 4
    END AS review_priority,
    COUNT(pa.approval_id) AS total_approvals,
    COUNT(CASE WHEN pa.status = 'Approved' THEN pa.approval_id END) AS approved_count,
    COUNT(CASE WHEN pa.status = 'Pending' THEN pa.approval_id END) AS pending_approvals,
    COUNT(f.feedback_id) AS feedback_count,
    COUNT(CASE WHEN f.status = 'Implemented' THEN f.feedback_id END) AS implemented_feedback
FROM
    policy_management.policies p
JOIN
    core.employees e ON p.policy_owner_id = e.employee_id
LEFT JOIN
    policy_management.policy_reviews pr ON p.policy_id = pr.policy_id AND
        (pr.status != 'Completed' OR pr.review_end_date >= CURRENT_DATE - INTERVAL '6 months')
LEFT JOIN
    policy_management.policy_approvals pa ON pr.review_id = pa.review_id
LEFT JOIN
    policy_management.policy_feedback f ON p.policy_id = f.policy_id AND
        f.feedback_date >= CURRENT_DATE - INTERVAL '12 months'
GROUP BY
    p.policy_id, p.policy_name, p.policy_category, p.status, p.version,
    p.effective_date, p.next_review_date, p.review_frequency_months,
    e.employee_name, pr.review_id, pr.review_start_date, pr.review_end_date, pr.status;

COMMENT ON VIEW policy_management.vw_policy_review_dashboard IS 'Provides data for the policy review dashboard including review status, approvals, and feedback';



CREATE OR REPLACE VIEW policy_management.vw_policy_attestation_compliance AS
SELECT
    p.policy_id,
    p.policy_name,
    d.department_name,
    COUNT(DISTINCT e.employee_id) AS total_employees,
    COUNT(DISTINCT a.employee_id) AS attested_employees,
    COUNT(DISTINCT a.employee_id) * 100.0 / NULLIF(COUNT(DISTINCT e.employee_id), 0) AS attestation_percentage,
    COUNT(DISTINCT tr.employee_id) AS trained_employees,
    COUNT(DISTINCT tr.employee_id) * 100.0 / NULLIF(COUNT(DISTINCT e.employee_id), 0) AS training_percentage,
    COUNT(DISTINCT v.employee_id) AS violating_employees,
    COUNT(DISTINCT v.employee_id) * 100.0 / NULLIF(COUNT(DISTINCT e.employee_id), 0) AS violation_percentage
FROM
    policy_management.policies p
CROSS JOIN
    core.employees e
JOIN
    core.departments d ON e.department_id = d.department_id
LEFT JOIN
    policy_management.policy_attestations a ON p.policy_id = a.policy_id AND e.employee_id = a.employee_id AND a.status = 'Completed'
LEFT JOIN
    policy_management.policy_training t ON p.policy_id = t.policy_id
LEFT JOIN
    policy_management.policy_training_records tr ON t.training_id = tr.training_id AND e.employee_id = tr.employee_id AND tr.status = 'Completed'
LEFT JOIN
    policy_management.policy_violations v ON p.policy_id = v.policy_id AND e.employee_id = v.employee_id
WHERE
    p.status = 'Active'
GROUP BY
    p.policy_id, p.policy_name, d.department_id, d.department_name;

COMMENT ON VIEW policy_management.vw_policy_attestation_compliance IS 'Shows policy attestation and training compliance by department';



CREATE OR REPLACE VIEW policy_management.vw_policy_risk_exposure AS
SELECT
    p.policy_id,
    p.policy_name,
    COUNT(DISTINCT prm.risk_id) AS mapped_risks,
    COUNT(DISTINCT v.violation_id) AS violation_count,
    COUNT(DISTINCT e.exception_id) AS exception_count,
    MAX(r.risk_score) AS max_risk_score,
    AVG(r.risk_score) AS avg_risk_score,
    STRING_AGG(DISTINCT r.risk_name, ', ') AS risk_names,
    STRING_AGG(DISTINCT cat.category_name, ', ') AS risk_categories
FROM
    policy_management.policies p
LEFT JOIN
    policy_management.policy_risk_mapping prm ON p.policy_id = prm.policy_id
LEFT JOIN
    core.risks r ON prm.risk_id = r.risk_id
LEFT JOIN
    core.risk_categories cat ON r.category_id = cat.category_id
LEFT JOIN
    policy_management.policy_violations v ON p.policy_id = v.policy_id
LEFT JOIN
    policy_management.policy_exceptions e ON p.policy_id = e.policy_id
WHERE
    p.status = 'Active'
GROUP BY
    p.policy_id, p.policy_name;

COMMENT ON VIEW policy_management.vw_policy_risk_exposure IS 'Shows risk exposure for each policy including mapped risks and violations';



CREATE OR REPLACE VIEW policy_management.vw_policy_kpi_performance AS
SELECT
    p.policy_id,
    p.policy_name,
    k.kpi_id,
    k.kpi_name,
    k.target_value,
    k.min_threshold,
    k.max_threshold,
    m.measurement_date,
    m.measured_value,
    CASE
        WHEN k.target_value IS NOT NULL THEN
            CASE
                WHEN m.measured_value >= k.target_value THEN 'Met'
                ELSE 'Not Met'
            END
        WHEN k.min_threshold IS NOT NULL AND k.max_threshold IS NOT NULL THEN
            CASE
                WHEN m.measured_value BETWEEN k.min_threshold AND k.max_threshold THEN 'Within Range'
                WHEN m.measured_value < k.min_threshold THEN 'Below Minimum'
                ELSE 'Above Maximum'
            END
        ELSE 'No Target'
    END AS performance_status,
    m.data_source,
    m.comments
FROM
    policy_management.policies p
JOIN
    policy_management.policy_kpi_measurements m ON p.policy_id = m.policy_id
JOIN
    policy_management.policy_kpis k ON m.kpi_id = k.kpi_id
WHERE
    m.measurement_date = (SELECT MAX(measurement_date)
                          FROM policy_management.policy_kpi_measurements
                          WHERE kpi_id = m.kpi_id AND policy_id = m.policy_id);

COMMENT ON VIEW policy_management.vw_policy_kpi_performance IS 'Shows the latest KPI measurements for each policy with performance status';


CREATE MATERIALIZED VIEW policy_management.mv_policy_compliance_summary AS
SELECT
    p.policy_id,
    p.policy_name,
    p.status AS policy_status,
    p.version,
    p.effective_date,
    p.next_review_date,
    e.employee_name AS policy_owner,
    d.department_name AS owner_department,
    COUNT(DISTINCT pr.review_id) AS total_reviews,
    MAX(pr.review_end_date) AS last_review_date,
    COUNT(DISTINCT a.attestation_id) AS total_attestations,
    COUNT(DISTINCT CASE WHEN a.status = 'Completed' THEN a.attestation_id END) AS completed_attestations,
    COUNT(DISTINCT v.violation_id) AS total_violations,
    COUNT(DISTINCT e.exception_id) AS total_exceptions,
    COUNT(DISTINCT r.regulation_id) AS total_regulations,
    COUNT(DISTINCT CASE WHEN prm.compliance_status = 'Compliant' THEN r.regulation_id END) AS compliant_regulations,
    CURRENT_TIMESTAMP AS refresh_time
FROM
    policy_management.policies p
LEFT JOIN
    core.employees e ON p.policy_owner_id = e.employee_id
LEFT JOIN
    core.departments d ON e.department_id = d.department_id
LEFT JOIN
    policy_management.policy_reviews pr ON p.policy_id = pr.policy_id
LEFT JOIN
    policy_management.policy_attestations a ON p.policy_id = a.policy_id
LEFT JOIN
    policy_management.policy_violations v ON p.policy_id = v.policy_id
LEFT JOIN
    policy_management.policy_exceptions e ON p.policy_id = e.policy_id
LEFT JOIN
    policy_management.policy_regulatory_mapping prm ON p.policy_id = prm.policy_id
LEFT JOIN
    core.regulations r ON prm.regulation_id = r.regulation_id
GROUP BY
    p.policy_id, p.policy_name, p.status, p.version, p.effective_date,
    p.next_review_date, e.employee_name, d.department_name;

COMMENT ON MATERIALIZED VIEW policy_management.mv_policy_compliance_summary IS 'Materialized view providing a summary of policy compliance metrics for reporting purposes';


CREATE MATERIALIZED VIEW policy_management.mv_policy_review_status AS
SELECT
    p.policy_id,
    p.policy_name,
    p.status AS policy_status,
    p.version,
    p.effective_date,
    p.next_review_date,
    p.review_frequency_months,
    CASE
        WHEN p.next_review_date IS NULL THEN 'Not Scheduled'
        WHEN p.next_review_date <= CURRENT_DATE THEN 'Overdue'
        WHEN p.next_review_date <= CURRENT_DATE + INTERVAL '1 month' THEN 'Due Soon'
        ELSE 'On Track'
    END AS review_status,
    e.employee_name AS policy_owner,
    d.department_name AS owner_department,
    COUNT(DISTINCT pr.review_id) AS total_reviews,
    MAX(pr.review_end_date) AS last_review_date,
    CURRENT_TIMESTAMP AS refresh_time
FROM
    policy_management.policies p
LEFT JOIN
    core.employees e ON p.policy_owner_id = e.employee_id
LEFT JOIN
    core.departments d ON e.department_id = d.department_id
LEFT JOIN
    policy_management.policy_reviews pr ON p.policy_id = pr.policy_id
WHERE
    p.status IN ('Active', 'Under Review')
GROUP BY
    p.policy_id, p.policy_name, p.status, p.version, p.effective_date,
    p.next_review_date, p.review_frequency_months, e.employee_name, d.department_name;

COMMENT ON MATERIALIZED VIEW policy_management.mv_policy_review_status IS 'Materialized view showing the review status of all active policies';


CREATE MATERIALIZED VIEW policy_management.mv_policy_attestation_by_dept AS
SELECT
    p.policy_id,
    p.policy_name,
    d.department_id,
    d.department_name,
    COUNT(DISTINCT e.employee_id) AS total_employees,
    COUNT(DISTINCT a.employee_id) AS attested_employees,
    COUNT(DISTINCT a.employee_id) * 100.0 / NULLIF(COUNT(DISTINCT e.employee_id), 0) AS attestation_percentage,
    CURRENT_TIMESTAMP AS refresh_time
FROM
    policy_management.policies p
CROSS JOIN
    core.employees e
JOIN
    core.departments d ON e.department_id = d.department_id
LEFT JOIN
    policy_management.policy_attestations a ON p.policy_id = a.policy_id AND e.employee_id = a.employee_id AND a.status = 'Completed'
WHERE
    p.status = 'Active'
GROUP BY
    p.policy_id, p.policy_name, d.department_id, d.department_name;

COMMENT ON MATERIALIZED VIEW policy_management.mv_policy_attestation_by_dept IS 'Materialized view showing policy attestation compliance by department for reporting';


CREATE OR REPLACE PROCEDURE policy_management.sp_create_policy_version(
    IN p_policy_id INT,
    IN p_new_version VARCHAR(20),
    IN p_effective_date DATE,
    IN p_review_frequency INT,
    IN p_updated_by INT,
    IN p_change_description TEXT,
    OUT p_new_version_id INT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_old_policy RECORD;
BEGIN
    -- Get current policy details
    SELECT * INTO v_old_policy
    FROM policy_management.policies
    WHERE policy_id = p_policy_id;

    IF NOT FOUND THEN
        RAISE EXCEPTION 'Policy with ID % not found', p_policy_id;
    END IF;

    -- Archive current version
    UPDATE policy_management.policies
    SET status = 'Archived',
        updated_at = CURRENT_TIMESTAMP,
        updated_by = p_updated_by
    WHERE policy_id = p_policy_id;

    -- Create new version
    INSERT INTO policy_management.policies (
        policy_name,
        policy_description,
        policy_category,
        policy_owner_id,
        effective_date,
        review_frequency_months,
        next_review_date,
        status,
        version,
        previous_version_id,
        created_by,
        updated_by
    ) VALUES (
        v_old_policy.policy_name,
        v_old_policy.policy_description,
        v_old_policy.policy_category,
        v_old_policy.policy_owner_id,
        p_effective_date,
        COALESCE(p_review_frequency, v_old_policy.review_frequency_months),
        p_effective_date + (COALESCE(p_review_frequency, v_old_policy.review_frequency_months) * INTERVAL '1 month'),
        'Active',
        p_new_version,
        p_policy_id,
        p_updated_by,
        p_updated_by
    ) RETURNING policy_id INTO p_new_version_id;

    -- Log the version change
    INSERT INTO policy_management.policy_change_log (
        policy_id,
        change_type,
        change_description,
        changed_by,
        old_value,
        new_value,
        change_reason
    ) VALUES (
        p_policy_id,
        'Version Change',
        p_change_description,
        p_updated_by,
        v_old_policy.version,
        p_new_version,
        'New version created'
    );

    -- Copy regulatory mappings to new version
    INSERT INTO policy_management.policy_regulatory_mapping (
        policy_id,
        regulation_id,
        compliance_status,
        evidence,
        last_assessed_date,
        next_assessment_date,
        created_by,
        updated_by
    )
    SELECT
        p_new_version_id,
        regulation_id,
        compliance_status,
        evidence,
        last_assessed_date,
        next_assessment_date,
        p_updated_by,
        p_updated_by
    FROM
        policy_management.policy_regulatory_mapping
    WHERE
        policy_id = p_policy_id;

    -- Copy risk mappings to new version
    INSERT INTO policy_management.policy_risk_mapping (
        policy_id,
        risk_id,
        mitigation_effectiveness,
        created_by,
        updated_by
    )
    SELECT
        p_new_version_id,
        risk_id,
        mitigation_effectiveness,
        p_updated_by,
        p_updated_by
    FROM
        policy_management.policy_risk_mapping
    WHERE
        policy_id = p_policy_id;

    -- Copy control mappings to new version
    INSERT INTO policy_management.policy_control_mapping (
        policy_id,
        control_id,
        relationship_type,
        created_by
    )
    SELECT
        p_new_version_id,
        control_id,
        relationship_type,
        p_updated_by
    FROM
        policy_management.policy_control_mapping
    WHERE
        policy_id = p_policy_id;

    -- Copy training requirements to new version
    INSERT INTO policy_management.policy_training (
        policy_id,
        training_name,
        training_description,
        training_type,
        frequency_months,
        duration_minutes,
        is_mandatory,
        created_by,
        updated_by
    )
    SELECT
        p_new_version_id,
        training_name,
        training_description,
        training_type,
        frequency_months,
        duration_minutes,
        is_mandatory,
        p_updated_by,
        p_updated_by
    FROM
        policy_management.policy_training
    WHERE
        policy_id = p_policy_id;
END;
$$;

COMMENT ON PROCEDURE policy_management.sp_create_policy_version IS 'Creates a new version of an existing policy, archiving the old version and copying relevant mappings and training requirements';


CREATE OR REPLACE PROCEDURE policy_management.sp_initiate_policy_review(
    IN p_policy_id INT,
    IN p_initiated_by INT,
    IN p_review_start_date DATE DEFAULT NULL,
    OUT p_review_id INT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_policy RECORD;
    v_approvers RECORD;
    v_approval_order INT := 1;
BEGIN
    -- Get policy details
    SELECT * INTO v_policy
    FROM policy_management.policies
    WHERE policy_id = p_policy_id;

    IF NOT FOUND THEN
        RAISE EXCEPTION 'Policy with ID % not found', p_policy_id;
    END IF;

    -- Create review record
    INSERT INTO policy_management.policy_reviews (
        policy_id,
        review_start_date,
        status,
        initiated_by
    ) VALUES (
        p_policy_id,
        COALESCE(p_review_start_date, CURRENT_DATE),
        'Pending',
        p_initiated_by
    ) RETURNING review_id INTO p_review_id;

    -- Update policy next review date if this is a scheduled review
    IF v_policy.next_review_date <= CURRENT_DATE THEN
        UPDATE policy_management.policies
        SET next_review_date = CURRENT_DATE + (v_policy.review_frequency_months * INTERVAL '1 month'),
            updated_at = CURRENT_TIMESTAMP,
            updated_by = p_initiated_by
        WHERE policy_id = p_policy_id;
    END IF;

    -- Change policy status to Under Review
    UPDATE policy_management.policies
    SET status = 'Under Review',
        updated_at = CURRENT_TIMESTAMP,
        updated_by = p_initiated_by
    WHERE policy_id = p_policy_id;

    -- Log the review initiation
    INSERT INTO policy_management.policy_change_log (
        policy_id,
        change_type,
        change_description,
        changed_by,
        new_value
    ) VALUES (
        p_policy_id,
        'Review',
        'Policy review initiated',
        p_initiated_by,
        'Review ID: ' || p_review_id
    );

    -- Get approval workflow (simplified example - in practice this would be more complex)
    -- Here we assume policy owner is first approver, then department head, then compliance officer
    -- First approver - policy owner
    INSERT INTO policy_management.policy_approvals (
        policy_id,
        review_id,
        approver_id,
        status,
        approval_order,
        required
    ) VALUES (
        p_policy_id,
        p_review_id,
        v_policy.policy_owner_id,
        'Pending',
        v_approval_order,
        TRUE
    );

    v_approval_order := v_approval_order + 1;

    -- Second approver - department head
    SELECT e.employee_id INTO v_approvers
    FROM core.employees e
    JOIN core.departments d ON e.department_id = d.department_id
    WHERE e.employee_id = v_policy.policy_owner_id;

    IF FOUND AND v_approvers.employee_id IS NOT NULL THEN
        INSERT INTO policy_management.policy_approvals (
            policy_id,
            review_id,
            approver_id,
            status,
            approval_order,
            required
        ) VALUES (
            p_policy_id,
            p_review_id,
            v_approvers.employee_id,
            'Pending',
            v_approval_order,
            TRUE
        );

        v_approval_order := v_approval_order + 1;
    END IF;

    -- Third approver - compliance officer (first employee in compliance department)
    SELECT e.employee_id INTO v_approvers
    FROM core.employees e
    JOIN core.departments d ON e.department_id = d.department_id
    WHERE d.department_name LIKE '%Compliance%'
    LIMIT 1;

    IF FOUND AND v_approvers.employee_id IS NOT NULL THEN
        INSERT INTO policy_management.policy_approvals (
            policy_id,
            review_id,
            approver_id,
            status,
            approval_order,
            required
        ) VALUES (
            p_policy_id,
            p_review_id,
            v_approvers.employee_id,
            'Pending',
            v_approval_order,
            TRUE
        );
    END IF;
END;
$$;

COMMENT ON PROCEDURE policy_management.sp_initiate_policy_review IS 'Initiates a policy review process, sets up approval workflow, and updates policy status';



CREATE OR REPLACE PROCEDURE policy_management.sp_complete_policy_attestation(
    IN p_policy_id INT,
    IN p_employee_id INT,
    IN p_ip_address VARCHAR(45) DEFAULT NULL,
    IN p_device_info VARCHAR(255) DEFAULT NULL,
    OUT p_attestation_id INT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_policy RECORD;
    v_training_required BOOLEAN := FALSE;
    v_training_complete BOOLEAN := TRUE;
    v_training RECORD;
BEGIN
    -- Get policy details
    SELECT * INTO v_policy
    FROM policy_management.policies
    WHERE policy_id = p_policy_id AND status = 'Active';

    IF NOT FOUND THEN
        RAISE EXCEPTION 'Active policy with ID % not found', p_policy_id;
    END IF;

    -- Check if training is required and completed
    FOR v_training IN
        SELECT * FROM policy_management.policy_training
        WHERE policy_id = p_policy_id AND is_mandatory = TRUE
    LOOP
        v_training_required := TRUE;

        PERFORM 1 FROM policy_management.policy_training_records
        WHERE training_id = v_training.training_id
          AND employee_id = p_employee_id
          AND status = 'Completed'
          AND (expiration_date IS NULL OR expiration_date > CURRENT_DATE);

        IF NOT FOUND THEN
            v_training_complete := FALSE;
            EXIT;
        END IF;
    END LOOP;

    IF v_training_required AND NOT v_training_complete THEN
        RAISE EXCEPTION 'Required training not completed for policy ID %', p_policy_id;
    END IF;

    -- Check for existing attestation
    SELECT attestation_id INTO p_attestation_id
    FROM policy_management.policy_attestations
    WHERE policy_id = p_policy_id AND employee_id = p_employee_id
    ORDER BY attestation_date DESC
    LIMIT 1;

    IF FOUND THEN
        -- Update existing attestation
        UPDATE policy_management.policy_attestations
        SET attestation_date = CURRENT_TIMESTAMP,
            status = 'Completed',
            ip_address = p_ip_address,
            device_info = p_device_info,
            acknowledged_version = v_policy.version
        WHERE attestation_id = p_attestation_id;
    ELSE
        -- Create new attestation
        INSERT INTO policy_management.policy_attestations (
            policy_id,
            employee_id,
            status,
            ip_address,
            device_info,
            acknowledged_version,
            next_attestation_date
        ) VALUES (
            p_policy_id,
            p_employee_id,
            'Completed',
            p_ip_address,
            p_device_info,
            v_policy.version,
            CURRENT_DATE + INTERVAL '1 year' -- Default to annual attestation
        ) RETURNING attestation_id INTO p_attestation_id;
    END IF;

    -- Log the attestation
    INSERT INTO policy_management.policy_change_log (
        policy_id,
        change_type,
        change_description,
        changed_by,
        new_value
    ) VALUES (
        p_policy_id,
        'Attestation',
        'Policy attestation completed',
        p_employee_id,
        'Employee ID: ' || p_employee_id
    );
END;
$$;

COMMENT ON PROCEDURE policy_management.sp_complete_policy_attestation IS 'Records a policy attestation by an employee, verifying training requirements are met';


CREATE OR REPLACE PROCEDURE policy_management.sp_calculate_policy_kpis(
    IN p_kpi_id INT DEFAULT NULL,
    IN p_policy_id INT DEFAULT NULL,
    IN p_measurement_date DATE DEFAULT NULL
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_kpi RECORD;
    v_measurement_date DATE := COALESCE(p_measurement_date, CURRENT_DATE);
    v_measured_value DECIMAL(10,2);
    v_exists BOOLEAN;
BEGIN
    -- Loop through KPIs (either specific one or all)
    FOR v_kpi IN
        SELECT * FROM policy_management.policy_kpis
        WHERE (p_kpi_id IS NULL OR kpi_id = p_kpi_id)
          AND is_active = TRUE
    LOOP
        -- Check if measurement already exists for this date
        SELECT EXISTS(
            SELECT 1 FROM policy_management.policy_kpi_measurements
            WHERE kpi_id = v_kpi.kpi_id
              AND (p_policy_id IS NULL OR policy_id = p_policy_id)
              AND measurement_date = v_measurement_date
        ) INTO v_exists;

        IF v_exists THEN
            CONTINUE; -- Skip if already measured for this date
        END IF;

        -- Calculate based on KPI type (simplified examples)
        IF v_kpi.kpi_name = 'Policy Review Cycle Time' THEN
            -- Average time to complete policy reviews
            SELECT AVG(EXTRACT(DAY FROM (pr.review_end_date - pr.review_start_date)))
            INTO v_measured_value
            FROM policy_management.policy_reviews pr
            WHERE pr.status = 'Completed'
              AND (p_policy_id IS NULL OR pr.policy_id = p_policy_id)
              AND pr.review_end_date >= v_measurement_date - INTERVAL '3 months';

            -- Insert measurement
            INSERT INTO policy_management.policy_kpi_measurements (
                kpi_id,
                policy_id,
                measurement_date,
                measured_value,
                data_source
            ) VALUES (
                v_kpi.kpi_id,
                p_policy_id,
                v_measurement_date,
                v_measured_value,
                'policy_reviews table'
            );

        ELSIF v_kpi.kpi_name = 'Policy Attestation Rate' THEN
            -- Percentage of employees who have attested to required policies
            WITH policy_employees AS (
                SELECT DISTINCT e.employee_id
                FROM core.employees e
                JOIN core.employee_department ed ON e.employee_id = ed.employee_id
                JOIN core.department_policy dp ON ed.department_id = dp.department_id
                WHERE dp.policy_id = p_policy_id
                  AND e.is_active = TRUE
            ),
            attested_employees AS (
                SELECT DISTINCT a.employee_id
                FROM policy_management.policy_attestations a
                WHERE a.policy_id = p_policy_id
                  AND a.status = 'Completed'
                  AND a.acknowledged_version = (
                      SELECT version FROM policy_management.policies
                      WHERE policy_id = p_policy_id
                  )
            )
            SELECT
                COUNT(DISTINCT ae.employee_id) * 100.0 /
                NULLIF(COUNT(DISTINCT pe.employee_id), 0)
            INTO v_measured_value
            FROM policy_employees pe
            LEFT JOIN attested_employees ae ON pe.employee_id = ae.employee_id;

            -- Insert measurement
            INSERT INTO policy_management.policy_kpi_measurements (
                kpi_id,
                policy_id,
                measurement_date,
                measured_value,
                data_source
            ) VALUES (
                v_kpi.kpi_id,
                p_policy_id,
                v_measurement_date,
                v_measured_value,
                'policy_attestations table'
            );

        ELSIF v_kpi.kpi_name = 'Policy Version Control Accuracy' THEN
            -- Check if all policies have correct version history
            SELECT
                COUNT(*) * 100.0 / NULLIF((
                    SELECT COUNT(*) FROM policy_management.policies
                    WHERE (p_policy_id IS NULL OR policy_id = p_policy_id)
                ), 0)
            INTO v_measured_value
            FROM policy_management.policies p
            WHERE (p_policy_id IS NULL OR p.policy_id = p_policy_id)
              AND (
                  -- Either no previous version (first version)
                  (p.previous_version_id IS NULL AND NOT EXISTS (
                      SELECT 1 FROM policy_management.policies
                      WHERE previous_version_id = p.policy_id
                  ))
                  -- Or has previous version and next version points back correctly
                  OR (p.previous_version_id IS NOT NULL AND EXISTS (
                      SELECT 1 FROM policy_management.policies
                      WHERE policy_id = p.previous_version_id
                        AND (
                            previous_version_id IS NULL
                            OR policy_id IN (
                                SELECT previous_version_id
                                FROM policy_management.policies
                                WHERE policy_id = p.policy_id
                            )
                        )
                  ))
              );

            -- Insert measurement
            INSERT INTO policy_management.policy_kpi_measurements (
                kpi_id,
                policy_id,
                measurement_date,
                measured_value,
                data_source
            ) VALUES (
                v_kpi.kpi_id,
                p_policy_id,
                v_measurement_date,
                v_measured_value,
                'policies table'
            );

        -- Add additional KPI calculations as needed
        ELSE
            RAISE NOTICE 'No calculation defined for KPI ID % (%)', v_kpi.kpi_id, v_kpi.kpi_name;
        END IF;
    END LOOP;
END;
$$;

COMMENT ON PROCEDURE policy_management.sp_calculate_policy_kpis IS 'Calculates and stores measurements for policy KPIs based on current data';



CREATE OR REPLACE PROCEDURE policy_management.sp_refresh_materialized_views()
LANGUAGE plpgsql
AS $$
BEGIN
    RAISE NOTICE 'Refreshing policy_management.mv_policy_compliance_summary...';
    REFRESH MATERIALIZED VIEW policy_management.mv_policy_compliance_summary;

    RAISE NOTICE 'Refreshing policy_management.mv_policy_review_status...';
    REFRESH MATERIALIZED VIEW policy_management.mv_policy_review_status;

    RAISE NOTICE 'Refreshing policy_management.mv_policy_attestation_by_dept...';
    REFRESH MATERIALIZED VIEW policy_management.mv_policy_attestation_by_dept;

    RAISE NOTICE 'All materialized views refreshed successfully.';
END;
$$;

COMMENT ON PROCEDURE policy_management.sp_refresh_materialized_views IS 'Refreshes all materialized views in the policy_management schema';


CREATE OR REPLACE PROCEDURE policy_management.sp_refresh_materialized_views()
LANGUAGE plpgsql
AS $$
BEGIN
    RAISE NOTICE 'Refreshing policy_management.mv_policy_compliance_summary...';
    REFRESH MATERIALIZED VIEW policy_management.mv_policy_compliance_summary;

    RAISE NOTICE 'Refreshing policy_management.mv_policy_review_status...';
    REFRESH MATERIALIZED VIEW policy_management.mv_policy_review_status;

    RAISE NOTICE 'Refreshing policy_management.mv_policy_attestation_by_dept...';
    REFRESH MATERIALIZED VIEW policy_management.mv_policy_attestation_by_dept;

    RAISE NOTICE 'All materialized views refreshed successfully.';
END;
$$;

COMMENT ON PROCEDURE policy_management.sp_refresh_materialized_views IS 'Refreshes all materialized views in the policy_management schema';


--policy stakeholders
CREATE TABLE policy_management.policy_stakeholders (
    stakeholder_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    employee_id INT NOT NULL REFERENCES core.employees(employee_id),
    stakeholder_type VARCHAR(50) NOT NULL CHECK (stakeholder_type IN ('Reviewer', 'Approver', 'Consultant', 'Subject Matter Expert', 'Implementer')),
    is_primary BOOLEAN DEFAULT FALSE,
    notification_preference VARCHAR(50) CHECK (notification_preference IN ('Email', 'System Alert', 'Both', 'None')),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by INT REFERENCES core.employees(employee_id),
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_by INT REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE policy_management.policy_stakeholders IS 'Identifies all stakeholders involved with each policy beyond just the owner';


CREATE TABLE policy_management.policy_communication_log (
    communication_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    communication_type VARCHAR(50) NOT NULL CHECK (communication_type IN ('Launch', 'Update', 'Reminder', 'Training', 'Violation', 'Exception')),
    communication_channel VARCHAR(50) NOT NULL CHECK (communication_channel IN ('Email', 'Intranet', 'Newsletter', 'Meeting', 'Training Session', 'Alert')),
    subject VARCHAR(255) NOT NULL,
    message TEXT,
    sent_by INT NOT NULL REFERENCES core.employees(employee_id),
    sent_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    target_audience VARCHAR(50) CHECK (target_audience IN ('All Employees', 'Department', 'Role', 'Specific Group', 'Individuals')),
    audience_count INT,
    open_rate DECIMAL(5,2),
    click_rate DECIMAL(5,2),
    acknowledgement_rate DECIMAL(5,2)
);

COMMENT ON TABLE policy_management.policy_communication_log IS 'Tracks all communications related to policy dissemination and awareness';


CREATE TABLE policy_management.policy_effectiveness_metrics (
    metric_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    metric_date DATE NOT NULL,
    metric_type VARCHAR(50) NOT NULL CHECK (metric_type IN ('Compliance', 'Risk Reduction', 'Efficiency', 'Cost Savings', 'Employee Feedback')),
    metric_name VARCHAR(100) NOT NULL,
    metric_value DECIMAL(10,2) NOT NULL,
    target_value DECIMAL(10,2),
    benchmark_value DECIMAL(10,2),
    data_source VARCHAR(255),
    comments TEXT,
    calculated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    calculated_by INT REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE policy_management.policy_effectiveness_metrics IS 'Stores measurements of policy effectiveness beyond basic KPIs';



CREATE TABLE policy_management.policy_exception_workflow (
    workflow_id SERIAL PRIMARY KEY,
    exception_id INT NOT NULL REFERENCES policy_management.policy_exceptions(exception_id),
    step_number INT NOT NULL,
    action_type VARCHAR(50) NOT NULL CHECK (action_type IN ('Submission', 'Review', 'Approval', 'Rejection', 'Escalation', 'Implementation')),
    action_by INT REFERENCES core.employees(employee_id),
    action_date TIMESTAMP,
    comments TEXT,
    status VARCHAR(50) CHECK (status IN ('Pending', 'Completed', 'Overdue', 'Cancelled')),
    required BOOLEAN DEFAULT TRUE,
    sla_days INT,
    completed_at TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_exception_workflow IS 'Tracks the workflow steps for policy exception requests';

CREATE TABLE policy_management.policy_integration_points (
    integration_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    system_name VARCHAR(100) NOT NULL,
    integration_type VARCHAR(50) NOT NULL CHECK (integration_type IN ('ERP', 'HRIS', 'GRC', 'Learning Management', 'Document Management', 'Other')),
    integration_point VARCHAR(255) NOT NULL,
    is_automated BOOLEAN DEFAULT FALSE,
    last_sync_date TIMESTAMP,
    sync_status VARCHAR(50) CHECK (sync_status IN ('Success', 'Failed', 'Pending', 'Not Configured')),
    error_message TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_integration_points IS 'Tracks integration points between policies and other business systems';

CREATE TABLE policy_management.policy_audit_trail (
    audit_id SERIAL PRIMARY KEY,
    policy_id INT REFERENCES policy_management.policies(policy_id),
    audit_type VARCHAR(50) NOT NULL CHECK (audit_type IN ('Creation', 'Modification', 'Deletion', 'Access', 'Approval', 'Attestation', 'Violation')),
    table_name VARCHAR(100),
    record_id INT,
    field_name VARCHAR(100),
    old_value TEXT,
    new_value TEXT,
    changed_by INT NOT NULL REFERENCES core.employees(employee_id),
    changed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    ip_address VARCHAR(45),
    user_agent VARCHAR(255),
    additional_context TEXT
);

COMMENT ON TABLE policy_management.policy_audit_trail IS 'Comprehensive audit trail for all policy-related activities';


CREATE OR REPLACE VIEW policy_management.vw_policy_exception_dashboard AS
SELECT
    e.exception_id,
    p.policy_id,
    p.policy_name,
    e.request_date,
    e.exception_start_date,
    e.exception_end_date,
    e.status,
    req.employee_name AS requester,
    req.department_name AS requester_department,
    app.employee_name AS approver,
    e.justification,
    e.risk_assessment,
    e.compensating_controls,
    wf.current_step,
    wf.steps_completed,
    wf.total_steps,
    wf.next_action,
    wf.sla_status,
    DATEDIFF(day, CURRENT_DATE, e.exception_end_date) AS days_remaining
FROM
    policy_management.policy_exceptions e
JOIN
    policy_management.policies p ON e.policy_id = p.policy_id
JOIN
    (SELECT employee_id, employee_name, department_name
     FROM core.employees e JOIN core.departments d ON e.department_id = d.department_id) req
    ON e.requester_id = req.employee_id
LEFT JOIN
    (SELECT employee_id, employee_name
     FROM core.employees) app
    ON e.approver_id = app.employee_id
JOIN
    (SELECT
        exception_id,
        MAX(step_number) FILTER (WHERE status = 'Completed') AS steps_completed,
        COUNT(*) AS total_steps,
        MIN(step_number) FILTER (WHERE status = 'Pending') AS current_step,
        STRING_AGG(action_type, ', ' ORDER BY step_number) AS workflow_steps,
        CASE
            WHEN MIN(completed_at) FILTER (WHERE status = 'Pending' AND sla_days IS NOT NULL) < CURRENT_DATE - INTERVAL '1 day' * sla_days THEN 'Breached'
            WHEN MIN(completed_at) FILTER (WHERE status = 'Pending' AND sla_days IS NOT NULL) < CURRENT_DATE - INTERVAL '1 day' * (sla_days-1) THEN 'Due Soon'
            ELSE 'On Track'
        END AS sla_status,
        MIN(action_type) FILTER (WHERE status = 'Pending') AS next_action
     FROM
        policy_management.policy_exception_workflow
     GROUP BY
        exception_id) wf
    ON e.exception_id = wf.exception_id;

COMMENT ON VIEW policy_management.vw_policy_exception_dashboard IS 'Provides a comprehensive view of policy exceptions with workflow status and SLA tracking';



CREATE OR REPLACE VIEW policy_management.vw_policy_communication_effectiveness AS
SELECT
    p.policy_id,
    p.policy_name,
    p.version,
    COUNT(DISTINCT cl.communication_id) AS total_communications,
    COUNT(DISTINCT CASE WHEN cl.communication_type = 'Launch' THEN cl.communication_id END) AS launch_communications,
    COUNT(DISTINCT CASE WHEN cl.communication_type = 'Update' THEN cl.communication_id END) AS update_communications,
    COUNT(DISTINCT CASE WHEN cl.communication_type = 'Reminder' THEN cl.communication_id END) AS reminder_communications,
    AVG(cl.open_rate) AS avg_open_rate,
    AVG(cl.click_rate) AS avg_click_rate,
    AVG(cl.acknowledgement_rate) AS avg_acknowledgement_rate,
    COUNT(DISTINCT a.attestation_id) AS total_attestations,
    COUNT(DISTINCT a.attestation_id) * 100.0 / NULLIF((
        SELECT COUNT(DISTINCT e.employee_id)
        FROM core.employees e
        JOIN core.employee_department ed ON e.employee_id = ed.employee_id
        JOIN core.department_policy dp ON ed.department_id = dp.department_id
        WHERE dp.policy_id = p.policy_id AND e.is_active = TRUE
    ), 0) AS attestation_rate,
    COUNT(DISTINCT v.violation_id) AS total_violations
FROM
    policy_management.policies p
LEFT JOIN
    policy_management.policy_communication_log cl ON p.policy_id = cl.policy_id
LEFT JOIN
    policy_management.policy_attestations a ON p.policy_id = a.policy_id AND a.status = 'Completed'
LEFT JOIN
    policy_management.policy_violations v ON p.policy_id = v.policy_id
GROUP BY
    p.policy_id, p.policy_name, p.version;

COMMENT ON VIEW policy_management.vw_policy_communication_effectiveness IS 'Analyzes the effectiveness of policy communications by correlating with attestation and violation rates';


CREATE OR REPLACE VIEW policy_management.vw_policy_risk_heatmap AS
SELECT
    p.policy_id,
    p.policy_name,
    p.status AS policy_status,
    r.risk_id,
    r.risk_name,
    r.risk_score,
    rc.category_name AS risk_category,
    prm.mitigation_effectiveness,
    COUNT(DISTINCT v.violation_id) AS violation_count,
    COUNT(DISTINCT e.exception_id) AS exception_count,
    CASE
        WHEN COUNT(DISTINCT v.violation_id) > 5 OR r.risk_score >= 4.5 THEN 'Critical'
        WHEN COUNT(DISTINCT v.violation_id) > 2 OR r.risk_score >= 3.5 THEN 'High'
        WHEN COUNT(DISTINCT v.violation_id) > 0 OR r.risk_score >= 2.5 THEN 'Medium'
        ELSE 'Low'
    END AS heatmap_rating,
    MAX(v.violation_date) AS last_violation_date,
    MAX(e.request_date) AS last_exception_date
FROM
    policy_management.policies p
JOIN
    policy_management.policy_risk_mapping prm ON p.policy_id = prm.policy_id
JOIN
    core.risks r ON prm.risk_id = r.risk_id
LEFT JOIN
    core.risk_categories rc ON r.category_id = rc.category_id
LEFT JOIN
    policy_management.policy_violations v ON p.policy_id = v.policy_id AND r.risk_id = v.risk_id
LEFT JOIN
    policy_management.policy_exceptions e ON p.policy_id = e.policy_id AND r.risk_id = e.risk_id
GROUP BY
    p.policy_id, p.policy_name, p.status, r.risk_id, r.risk_name, r.risk_score,
    rc.category_name, prm.mitigation_effectiveness;

COMMENT ON VIEW policy_management.vw_policy_risk_heatmap IS 'Provides a risk heatmap view showing policy-risk relationships with violation and exception data';


CREATE MATERIALIZED VIEW policy_management.mv_policy_exception_trends AS
SELECT
    p.policy_id,
    p.policy_name,
    DATE_TRUNC('month', e.request_date) AS month,
    COUNT(DISTINCT e.exception_id) AS exception_count,
    COUNT(DISTINCT CASE WHEN e.status = 'Approved' THEN e.exception_id END) AS approved_count,
    COUNT(DISTINCT CASE WHEN e.status = 'Rejected' THEN e.exception_id END) AS rejected_count,
    COUNT(DISTINCT CASE WHEN e.status = 'Pending' THEN e.exception_id END) AS pending_count,
    AVG(EXTRACT(DAY FROM (e.approval_date - e.request_date))) AS avg_approval_days,
    COUNT(DISTINCT e.requester_id) AS unique_requesters,
    COUNT(DISTINCT d.department_id) AS departments_affected,
    CURRENT_TIMESTAMP AS refresh_time
FROM
    policy_management.policy_exceptions e
JOIN
    policy_management.policies p ON e.policy_id = p.policy_id
JOIN
    core.employees emp ON e.requester_id = emp.employee_id
JOIN
    core.departments d ON emp.department_id = d.department_id
GROUP BY
    p.policy_id, p.policy_name, DATE_TRUNC('month', e.request_date);

COMMENT ON MATERIALIZED VIEW policy_management.mv_policy_exception_trends IS 'Aggregates policy exception data by month for trend analysis and reporting';



CREATE MATERIALIZED VIEW policy_management.mv_policy_training_compliance AS
SELECT
    p.policy_id,
    p.policy_name,
    d.department_id,
    d.department_name,
    t.training_id,
    t.training_name,
    COUNT(DISTINCT e.employee_id) AS total_employees,
    COUNT(DISTINCT tr.employee_id) AS trained_employees,
    COUNT(DISTINCT tr.employee_id) * 100.0 / NULLIF(COUNT(DISTINCT e.employee_id), 0) AS completion_rate,
    COUNT(DISTINCT CASE WHEN tr.expiration_date < CURRENT_DATE THEN tr.employee_id END) AS expired_trainings,
    COUNT(DISTINCT CASE WHEN tr.expiration_date BETWEEN CURRENT_DATE AND CURRENT_DATE + INTERVAL '30 days' THEN tr.employee_id END) AS expiring_soon,
    MIN(tr.expiration_date) AS next_expiration,
    CURRENT_TIMESTAMP AS refresh_time
FROM
    policy_management.policies p
JOIN
    policy_management.policy_training t ON p.policy_id = t.policy_id
JOIN
    core.department_policy dp ON p.policy_id = dp.policy_id
JOIN
    core.departments d ON dp.department_id = d.department_id
JOIN
    core.employees e ON e.department_id = d.department_id AND e.is_active = TRUE
LEFT JOIN
    policy_management.policy_training_records tr ON t.training_id = tr.training_id AND e.employee_id = tr.employee_id AND tr.status = 'Completed'
WHERE
    t.is_mandatory = TRUE
GROUP BY
    p.policy_id, p.policy_name, d.department_id, d.department_name, t.training_id, t.training_name;

COMMENT ON MATERIALIZED VIEW policy_management.mv_policy_training_compliance IS 'Tracks training compliance by policy and department with expiration status';


CREATE MATERIALIZED VIEW policy_management.mv_policy_effectiveness_scorecard AS
SELECT
    p.policy_id,
    p.policy_name,
    p.version,
    p.status,
    p.effective_date,
    p.next_review_date,
    e.employee_name AS policy_owner,
    d.department_name AS owner_department,
    -- Compliance metrics
    COUNT(DISTINCT a.attestation_id) AS attestation_count,
    COUNT(DISTINCT a.attestation_id) * 100.0 / NULLIF((
        SELECT COUNT(DISTINCT e.employee_id)
        FROM core.employees e
        JOIN core.employee_department ed ON e.employee_id = ed.employee_id
        JOIN core.department_policy dp ON ed.department_id = dp.department_id
        WHERE dp.policy_id = p.policy_id AND e.is_active = TRUE
    ), 0) AS attestation_rate,
    -- Training metrics
    COUNT(DISTINCT t.training_id) AS training_programs,
    COUNT(DISTINCT tr.record_id) AS training_completions,
    -- Risk metrics
    COUNT(DISTINCT prm.risk_id) AS mapped_risks,
    AVG(r.risk_score) AS avg_risk_score,
    COUNT(DISTINCT v.violation_id) AS violation_count,
    -- Exception metrics
    COUNT(DISTINCT e.exception_id) AS exception_count,
    -- Effectiveness metrics
    AVG(em.metric_value) FILTER (WHERE em.metric_type = 'Compliance') AS avg_compliance_score,
    AVG(em.metric_value) FILTER (WHERE em.metric_type = 'Risk Reduction') AS avg_risk_reduction,
    AVG(em.metric_value) FILTER (WHERE em.metric_type = 'Efficiency') AS avg_efficiency_gain,
    -- Composite effectiveness score
    (
        (COUNT(DISTINCT a.attestation_id) * 100.0 / NULLIF((
            SELECT COUNT(DISTINCT e.employee_id)
            FROM core.employees e
            JOIN core.employee_department ed ON e.employee_id = ed.employee_id
            JOIN core.department_policy dp ON ed.department_id = dp.department_id
            WHERE dp.policy_id = p.policy_id AND e.is_active = TRUE
        ), 0) * 0.3) +
        (AVG(em.metric_value) FILTER (WHERE em.metric_type = 'Risk Reduction') * 0.4) +
        (100 - (COUNT(DISTINCT v.violation_id) * 5)) * 0.2) +
        (100 - (COUNT(DISTINCT e.exception_id) * 2)) * 0.1)
    ) AS effectiveness_score,
    CURRENT_TIMESTAMP AS refresh_time
FROM
    policy_management.policies p
LEFT JOIN
    core.employees e ON p.policy_owner_id = e.employee_id
LEFT JOIN
    core.departments d ON e.department_id = d.department_id
LEFT JOIN
    policy_management.policy_attestations a ON p.policy_id = a.policy_id AND a.status = 'Completed'
LEFT JOIN
    policy_management.policy_training t ON p.policy_id = t.policy_id
LEFT JOIN
    policy_management.policy_training_records tr ON t.training_id = tr.training_id AND tr.status = 'Completed'
LEFT JOIN
    policy_management.policy_risk_mapping prm ON p.policy_id = prm.policy_id
LEFT JOIN
    core.risks r ON prm.risk_id = r.risk_id
LEFT JOIN
    policy_management.policy_violations v ON p.policy_id = v.policy_id
LEFT JOIN
    policy_management.policy_exceptions ex ON p.policy_id = ex.policy_id
LEFT JOIN
    policy_management.policy_effectiveness_metrics em ON p.policy_id = em.policy_id
GROUP BY
    p.policy_id, p.policy_name, p.version, p.status, p.effective_date,
    p.next_review_date, e.employee_name, d.department_name;

COMMENT ON MATERIALIZED VIEW policy_management.mv_policy_effectiveness_scorecard IS 'Comprehensive scorecard evaluating policy effectiveness across multiple dimensions';



CREATE OR REPLACE PROCEDURE policy_management.sp_analyze_exception_patterns(
    IN p_policy_id INT DEFAULT NULL,
    IN p_department_id INT DEFAULT NULL,
    IN p_timeframe_months INT DEFAULT 12,
    OUT p_total_exceptions INT,
    OUT p_approval_rate DECIMAL(5,2),
    OUT p_avg_approval_days DECIMAL(5,2),
    OUT p_top_requester_id INT,
    OUT p_top_requester_name VARCHAR(255),
    OUT p_common_justification TEXT
)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Total exceptions
    SELECT COUNT(*) INTO p_total_exceptions
    FROM policy_management.policy_exceptions e
    WHERE (p_policy_id IS NULL OR e.policy_id = p_policy_id)
      AND (p_department_id IS NULL OR e.requester_id IN (
          SELECT employee_id FROM core.employees WHERE department_id = p_department_id
      ))
      AND e.request_date >= CURRENT_DATE - (p_timeframe_months * INTERVAL '1 month');

    -- Approval rate
    SELECT
        COUNT(*) FILTER (WHERE status = 'Approved') * 100.0 / NULLIF(COUNT(*), 0)
    INTO p_approval_rate
    FROM policy_management.policy_exceptions e
    WHERE (p_policy_id IS NULL OR e.policy_id = p_policy_id)
      AND (p_department_id IS NULL OR e.requester_id IN (
          SELECT employee_id FROM core.employees WHERE department_id = p_department_id
      ))
      AND e.request_date >= CURRENT_DATE - (p_timeframe_months * INTERVAL '1 month');

    -- Average approval days
    SELECT
        AVG(EXTRACT(DAY FROM (approval_date - request_date)))
    INTO p_avg_approval_days
    FROM policy_management.policy_exceptions e
    WHERE (p_policy_id IS NULL OR e.policy_id = p_policy_id)
      AND (p_department_id IS NULL OR e.requester_id IN (
          SELECT employee_id FROM core.employees WHERE department_id = p_department_id
      ))
      AND e.status = 'Approved'
      AND e.request_date >= CURRENT_DATE - (p_timeframe_months * INTERVAL '1 month');

    -- Top requester
    SELECT
        e.requester_id,
        emp.employee_name
    INTO
        p_top_requester_id,
        p_top_requester_name
    FROM
        policy_management.policy_exceptions e
    JOIN
        core.employees emp ON e.requester_id = emp.employee_id
    WHERE
        (p_policy_id IS NULL OR e.policy_id = p_policy_id)
        AND (p_department_id IS NULL OR e.requester_id IN (
            SELECT employee_id FROM core.employees WHERE department_id = p_department_id
        ))
        AND e.request_date >= CURRENT_DATE - (p_timeframe_months * INTERVAL '1 month')
    GROUP BY
        e.requester_id, emp.employee_name
    ORDER BY
        COUNT(*) DESC
    LIMIT 1;

    -- Common justification (simplified example)
    WITH justification_words AS (
        SELECT
            regexp_split_to_table(LOWER(justification), '\s+') AS word
        FROM
            policy_management.policy_exceptions
        WHERE
            (p_policy_id IS NULL OR policy_id = p_policy_id)
            AND (p_department_id IS NULL OR requester_id IN (
                SELECT employee_id FROM core.employees WHERE department_id = p_department_id
            ))
            AND request_date >= CURRENT_DATE - (p_timeframe_months * INTERVAL '1 month')
            AND justification IS NOT NULL
    ),
    word_counts AS (
        SELECT
            word,
            COUNT(*) AS frequency
        FROM
            justification_words
        WHERE
            word NOT IN ('the', 'and', 'for', 'with', 'this', 'that', 'are', 'not')
            AND length(word) > 3
        GROUP BY
            word
        ORDER BY
            frequency DESC
        LIMIT 5
    )
    SELECT
        string_agg(word, ', ')
    INTO
        p_common_justification
    FROM
        word_counts;
END;
$$;

COMMENT ON PROCEDURE policy_management.sp_analyze_exception_patterns IS 'Analyzes patterns in policy exceptions to identify trends and potential policy issues';




CREATE OR REPLACE PROCEDURE policy_management.sp_generate_policy_effectiveness_report(
    IN p_policy_id INT,
    IN p_start_date DATE DEFAULT NULL,
    IN p_end_date DATE DEFAULT NULL,
    OUT p_report_json JSON
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_policy RECORD;
    v_metrics JSON[];
    v_attestation_data JSON;
    v_violation_data JSON;
    v_exception_data JSON;
    v_training_data JSON;
    v_communication_data JSON;
    v_risk_data JSON;
BEGIN
    -- Get policy basic info
    SELECT
        policy_id,
        policy_name,
        version,
        status,
        effective_date,
        next_review_date,
        (SELECT employee_name FROM core.employees WHERE employee_id = policy_owner_id) AS policy_owner
    INTO
        v_policy
    FROM
        policy_management.policies
    WHERE
        policy_id = p_policy_id;

    -- Get effectiveness metrics
    SELECT
        array_agg(json_build_object(
            'metric_type', metric_type,
            'metric_name', metric_name,
            'metric_value', metric_value,
            'target_value', target_value,
            'benchmark_value', benchmark_value,
            'measurement_date', metric_date
        ))
    INTO
        v_metrics
    FROM
        policy_management.policy_effectiveness_metrics
    WHERE
        policy_id = p_policy_id
        AND (p_start_date IS NULL OR metric_date >= p_start_date)
        AND (p_end_date IS NULL OR metric_date <= p_end_date);

    -- Get attestation data
    SELECT
        json_build_object(
            'total_attestations', COUNT(*),
            'attestation_rate', COUNT(*) * 100.0 / NULLIF((
                SELECT COUNT(DISTINCT e.employee_id)
                FROM core.employees e
                JOIN core.employee_department ed ON e.employee_id = ed.employee_id
                JOIN core.department_policy dp ON ed.department_id = dp.department_id
                WHERE dp.policy_id = p_policy_id AND e.is_active = TRUE
            ), 0),
            'department_breakdown', (
                SELECT json_object_agg(d.department_name, subq.attestation_count)
                FROM (
                    SELECT
                        d.department_name,
                        COUNT(a.attestation_id) AS attestation_count
                    FROM
                        core.departments d
                    JOIN
                        core.employees e ON d.department_id = e.department_id
                    LEFT JOIN
                        policy_management.policy_attestations a ON e.employee_id = a.employee_id
                            AND a.policy_id = p_policy_id
                            AND a.status = 'Completed'
                            AND (p_start_date IS NULL OR a.attestation_date >= p_start_date)
                            AND (p_end_date IS NULL OR a.attestation_date <= p_end_date)
                    WHERE
                        e.is_active = TRUE
                        AND EXISTS (
                            SELECT 1 FROM core.department_policy dp
                            WHERE dp.department_id = d.department_id AND dp.policy_id = p_policy_id
                        )
                    GROUP BY
                        d.department_name
                ) subq
            )
        )
    INTO
        v_attestation_data
    FROM
        policy_management.policy_attestations
    WHERE
        policy_id = p_policy_id
        AND status = 'Completed'
        AND (p_start_date IS NULL OR attestation_date >= p_start_date)
        AND (p_end_date IS NULL OR attestation_date <= p_end_date);

    -- Get violation data
    SELECT
        json_build_object(
            'total_violations', COUNT(*),
            'by_severity', (
                SELECT json_object_agg(severity, count)
                FROM (
                    SELECT
                        severity,
                        COUNT(*) AS count
                    FROM
                        policy_management.policy_violations
                    WHERE
                        policy_id = p_policy_id
                        AND (p_start_date IS NULL OR violation_date >= p_start_date)
                        AND (p_end_date IS NULL OR violation_date <= p_end_date)
                    GROUP BY
                        severity
                ) subq
            ),
            'repeat_offenders', (
                SELECT json_agg(json_build_object(
                    'employee_id', employee_id,
                    'employee_name', employee_name,
                    'violation_count', violation_count
                ))
                FROM (
                    SELECT
                        v.employee_id,
                        e.employee_name,
                        COUNT(*) AS violation_count
                    FROM
                        policy_management.policy_violations v
                    JOIN
                        core.employees e ON v.employee_id = e.employee_id
                    WHERE
                        v.policy_id = p_policy_id
                        AND (p_start_date IS NULL OR v.violation_date >= p_start_date)
                        AND (p_end_date IS NULL OR v.violation_date <= p_end_date)
                    GROUP BY
                        v.employee_id, e.employee_name
                    HAVING
                        COUNT(*) > 1
                    ORDER BY
                        COUNT(*) DESC
                    LIMIT 5
                ) subq
            )
        )
    INTO
        v_violation_data
    FROM
        policy_management.policy_violations
    WHERE
        policy_id = p_policy_id
        AND (p_start_date IS NULL OR violation_date >= p_start_date)
        AND (p_end_date IS NULL OR violation_date <= p_end_date);

    -- Get exception data
    SELECT
        json_build_object(
            'total_exceptions', COUNT(*),
            'approval_rate', COUNT(*) FILTER (WHERE status = 'Approved') * 100.0 / NULLIF(COUNT(*), 0),
            'avg_approval_days', AVG(EXTRACT(DAY FROM (approval_date - request_date)))),
            'common_justifications', (
                SELECT json_agg(justification)
                FROM (
                    SELECT
                        justification
                    FROM
                        policy_management.policy_exceptions
                    WHERE
                        policy_id = p_policy_id
                        AND (p_start_date IS NULL OR request_date >= p_start_date)
                        AND (p_end_date IS NULL OR request_date <= p_end_date)
                        AND justification IS NOT NULL
                    GROUP BY
                        justification
                    ORDER BY
                        COUNT(*) DESC
                    LIMIT 3
                ) subq
            )
        )
    INTO
        v_exception_data
    FROM
        policy_management.policy_exceptions
    WHERE
        policy_id = p_policy_id
        AND (p_start_date IS NULL OR request_date >= p_start_date)
        AND (p_end_date IS NULL OR request_date <= p_end_date);

    -- Get training data
    SELECT
        json_build_object(
            'training_programs', COUNT(DISTINCT t.training_id),
            'completion_rate', (
                SELECT COUNT(DISTINCT tr.employee_id) * 100.0 / NULLIF((
                    SELECT COUNT(DISTINCT e.employee_id)
                    FROM core.employees e
                    JOIN core.employee_department ed ON e.employee_id = ed.employee_id
                    JOIN core.department_policy dp ON ed.department_id = dp.department_id
                    WHERE dp.policy_id = p_policy_id AND e.is_active = TRUE
                ), 0)
            ),
            'expiring_soon', COUNT(DISTINCT tr.record_id) FILTER (
                WHERE tr.expiration_date BETWEEN CURRENT_DATE AND CURRENT_DATE + INTERVAL '30 days'
            )
        )
    INTO
        v_training_data
    FROM
        policy_management.policy_training t
    LEFT JOIN
        policy_management.policy_training_records tr ON t.training_id = tr.training_id
            AND tr.status = 'Completed'
            AND (p_start_date IS NULL OR tr.completion_date >= p_start_date)
            AND (p_end_date IS NULL OR tr.completion_date <= p_end_date)
    WHERE
        t.policy_id = p_policy_id;

    -- Get communication data
    SELECT
        json_build_object(
            'total_communications', COUNT(*),
            'avg_open_rate', AVG(open_rate),
            'avg_click_rate', AVG(click_rate),
            'channels', (
                SELECT json_object_agg(communication_channel, count)
                FROM (
                    SELECT
                        communication_channel,
                        COUNT(*) AS count
                    FROM
                        policy_management.policy_communication_log
                    WHERE
                        policy_id = p_policy_id
                        AND (p_start_date IS NULL OR sent_at >= p_start_date)
                        AND (p_end_date IS NULL OR sent_at <= p_end_date)
                    GROUP BY
                        communication_channel
                ) subq
            )
        )
    INTO
        v_communication_data
    FROM
        policy_management.policy_communication_log
    WHERE
        policy_id = p_policy_id
        AND (p_start_date IS NULL OR sent_at >= p_start_date)
        AND (p_end_date IS NULL OR sent_at <= p_end_date);

    -- Get risk data
    SELECT
        json_build_object(
            'mapped_risks', COUNT(DISTINCT prm.risk_id),
            'avg_risk_score', AVG(r.risk_score),
            'highest_risk', (
                SELECT json_build_object(
                    'risk_id', r.risk_id,
                    'risk_name', r.risk_name,
                    'risk_score', r.risk_score
                )
                FROM
                    policy_management.policy_risk_mapping prm
                JOIN
                    core.risks r ON prm.risk_id = r.risk_id
                WHERE
                    prm.policy_id = p_policy_id
                ORDER BY
                    r.risk_score DESC
                LIMIT 1
            )
        )
    INTO
        v_risk_data
    FROM
        policy_management.policy_risk_mapping prm
    JOIN
        core.risks r ON prm.risk_id = r.risk_id
    WHERE
        prm.policy_id = p_policy_id;

    -- Compile final report
    p_report_json := json_build_object(
        'policy_info', json_build_object(
            'policy_id', v_policy.policy_id,
            'policy_name', v_policy.policy_name,
            'version', v_policy.version,
            'status', v_policy.status,
            'effective_date', v_policy.effective_date,
            'next_review_date', v_policy.next_review_date,
            'policy_owner', v_policy.policy_owner
        ),
        'time_period', json_build_object(
            'start_date', p_start_date,
            'end_date', p_end_date
        ),
        'effectiveness_metrics', v_metrics,
        'attestation_data', v_attestation_data,
        'violation_data', v_violation_data,
        'exception_data', v_exception_data,
        'training_data', v_training_data,
        'communication_data', v_communication_data,
        'risk_data', v_risk_data,
        'report_generated_at', CURRENT_TIMESTAMP
    );
END;
$$;

COMMENT ON PROCEDURE policy_management.sp_generate_policy_effectiveness_report IS 'Generates a comprehensive JSON report on policy effectiveness across multiple dimensions';




--procedure to automate policy review workflow
CREATE OR REPLACE PROCEDURE policy_management.sp_automate_policy_review_workflow()
LANGUAGE plpgsql
AS $$
DECLARE
    v_policy RECORD;
    v_review_id INT;
    v_overdue_count INT := 0;
    v_due_soon_count INT := 0;
    v_initiated_count INT := 0;
BEGIN
    -- Process overdue reviews
    FOR v_policy IN
        SELECT p.*
        FROM policy_management.policies p
        WHERE p.status = 'Active'
          AND p.next_review_date <= CURRENT_DATE
          AND NOT EXISTS (
              SELECT 1 FROM policy_management.policy_reviews pr
              WHERE pr.policy_id = p.policy_id
                AND pr.status IN ('Pending', 'In Progress')
          )
    LOOP
        CALL policy_management.sp_initiate_policy_review(
            p_policy_id := v_policy.policy_id,
            p_initiated_by := 1, -- System user
            p_review_start_date := CURRENT_DATE,
            p_review_id := v_review_id
        );

        -- Log the initiation
        INSERT INTO policy_management.policy_communication_log (
            policy_id,
            communication_type,
            communication_channel,
            subject,
            message,
            sent_by,
            target_audience,
            audience_count
        ) VALUES (
            v_policy.policy_id,
            'Reminder',
            'Email',
            'Policy Review Initiated: ' || v_policy.policy_name,
            'An overdue review has been automatically initiated for policy: ' || v_policy.policy_name ||
            ' (Version ' || v_policy.version || '). Please complete your review tasks promptly.',
            1, -- System user
            'All Employees',
            (SELECT COUNT(*) FROM policy_management.policy_stakeholders WHERE policy_id = v_policy.policy_id)
        );

        v_overdue_count := v_overdue_count + 1;
        v_initiated_count := v_initiated_count + 1;
    END LOOP;

    -- Process due soon reviews (next 7 days)
    FOR v_policy IN
        SELECT p.*
        FROM policy_management.policies p
        WHERE p.status = 'Active'
          AND p.next_review_date BETWEEN CURRENT_DATE + INTERVAL '1 day' AND CURRENT_DATE + INTERVAL '7 days'
          AND NOT EXISTS (
              SELECT 1 FROM policy_management.policy_reviews pr
              WHERE pr.policy_id = p.policy_id
                AND pr.status IN ('Pending', 'In Progress')
          )
    LOOP
        -- Notify stakeholders but don't initiate yet
        INSERT INTO policy_management.policy_communication_log (
            policy_id,
            communication_type,
            communication_channel,
            subject,
            message,
            sent_by,
            target_audience,
            audience_count
        ) VALUES (
            v_policy.policy_id,
            'Reminder',
            'Email',
            'Upcoming Policy Review: ' || v_policy.policy_name,
            'A review is due soon for policy: ' || v_policy.policy_name ||
            ' (Version ' || v_policy.version || '). The review is scheduled to begin on ' ||
            v_policy.next_review_date || '. Please prepare any feedback or updates.',
            1, -- System user
            'All Employees',
            (SELECT COUNT(*) FROM policy_management.policy_stakeholders WHERE policy_id = v_policy.policy_id)
        );

        v_due_soon_count := v_due_soon_count + 1;
    END LOOP;

    -- Check for stuck reviews (no progress in 14 days)
    FOR v_policy IN
        SELECT p.policy_id, p.policy_name, pr.review_id
        FROM policy_management.policies p
        JOIN policy_management.policy_reviews pr ON p.policy_id = pr.policy_id
        WHERE pr.status IN ('Pending', 'In Progress')
          AND pr.review_start_date <= CURRENT_DATE - INTERVAL '14 days'
          AND NOT EXISTS (
              SELECT 1 FROM policy_management.policy_approvals pa
              WHERE pa.review_id = pr.review_id
                AND pa.status = 'Approved'
                AND pa.approval_date >= CURRENT_DATE - INTERVAL '7 days'
          )
    LOOP
        -- Escalate to department head
        INSERT INTO policy_management.policy_communication_log (
            policy_id,
            communication_type,
            communication_channel,
            subject,
            message,
            sent_by,
            target_audience,
            audience_count
        ) VALUES (
            v_policy.policy_id,
            'Escalation',
            'Email',
            'URGENT: Stalled Policy Review - ' || v_policy.policy_name,
            'The review for policy ' || v_policy.policy_name || ' (Review ID: ' || v_policy.review_id ||
            ') has shown no progress in the last 14 days. Please take immediate action to move this review forward.',
            1, -- System user
            'Department',
            1 -- Assuming department head is a single person
        );

        -- Log the escalation
        INSERT INTO policy_management.policy_change_log (
            policy_id,
            change_type,
            change_description,
            changed_by
        ) VALUES (
            v_policy.policy_id,
            'Escalation',
            'Review stalled for 14 days - escalated to department head',
            1 -- System user
        );
    END LOOP;

    -- Log the job results
    INSERT INTO policy_management.policy_audit_trail (
        audit_type,
        table_name,
        changed_by,
        additional_context
    ) VALUES (
        'System Job',
        'N/A',
        1,
        'Policy Review Workflow Automation completed. ' ||
        v_initiated_count || ' reviews initiated (' || v_overdue_count || ' overdue), ' ||
        v_due_soon_count || ' upcoming reviews notified.'
    );
END;
$$;

COMMENT ON PROCEDURE policy_management.sp_automate_policy_review_workflow IS 'Automates the policy review workflow by initiating overdue reviews, notifying about upcoming reviews, and escalating stalled reviews';




---policy AI Assistant Integration table
CREATE TABLE policy_management.policy_ai_assistant (
    interaction_id SERIAL PRIMARY KEY,
    policy_id INT REFERENCES policy_management.policies(policy_id),
    employee_id INT REFERENCES core.employees(employee_id),
    interaction_type VARCHAR(50) CHECK (interaction_type IN ('Question', 'Clarification', 'Guidance', 'Feedback')),
    user_query TEXT NOT NULL,
    ai_response TEXT,
    response_sources TEXT, -- JSON array of policy sections or documents referenced
    confidence_score DECIMAL(3,2),
    feedback_rating INT CHECK (feedback_rating BETWEEN 1 AND 5),
    feedback_comment TEXT,
    interaction_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    processing_time_ms INT
);

COMMENT ON TABLE policy_management.policy_ai_assistant IS 'Tracks interactions with AI assistant for policy questions and guidance';


CREATE TABLE policy_management.policy_translations (
    translation_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    language_code VARCHAR(10) NOT NULL, -- ISO 639-1 code
    translated_title VARCHAR(255) NOT NULL,
    translated_document_path VARCHAR(512) NOT NULL,
    translation_status VARCHAR(50) CHECK (translation_status IN ('Requested', 'In Progress', 'Completed', 'Reviewed', 'Approved')),
    translator_id INT REFERENCES core.employees(employee_id),
    reviewer_id INT REFERENCES core.employees(employee_id),
    requested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP,
    reviewed_at TIMESTAMP,
    approved_at TIMESTAMP,
    is_primary_translation BOOLEAN DEFAULT FALSE,
    checksum VARCHAR(64)
);

COMMENT ON TABLE policy_management.policy_translations IS 'Manages translations of policies into different languages';


CREATE TABLE policy_management.policy_access_controls (
    access_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    access_type VARCHAR(50) NOT NULL CHECK (access_type IN ('View', 'Edit', 'Approve', 'Attest')),
    grantee_type VARCHAR(50) NOT NULL CHECK (grantee_type IN ('Role', 'Department', 'Individual', 'Group')),
    grantee_id INT NOT NULL, -- Could be role_id, department_id, or employee_id depending on grantee_type
    granted_by INT NOT NULL REFERENCES core.employees(employee_id),
    granted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP,
    is_active BOOLEAN DEFAULT TRUE,
    justification TEXT
);

COMMENT ON TABLE policy_management.policy_access_controls IS 'Implements fine-grained access control for policies beyond basic ownership';


---query: global policy management
-- Get all translations for a policy with status
SELECT
    p.policy_name,
    t.language_code,
    t.translated_title,
    t.translation_status,
    t.completed_at
FROM
    policy_management.policies p
JOIN
    policy_management.policy_translations t ON p.policy_id = t.policy_id
WHERE
    p.policy_id = 123
ORDER BY
    t.language_code;


--policy exception management
-- Analyze exception patterns for a department
CALL policy_management.sp_analyze_exception_patterns(
    p_policy_id := NULL,
    p_department_id := 5,
    p_timeframe_months := 6,
    p_total_exceptions := NULL,
    p_approval_rate := NULL,
    p_avg_approval_days := NULL,
    p_top_requester_id := NULL,
    p_top_requester_name := NULL,
    p_common_justification := NULL
);


-- policy effectiveness measurement
-- Generate effectiveness report for a policy
CALL policy_management.sp_generate_policy_effectiveness_report(
    p_policy_id := 123,
    p_start_date := '2023-01-01',
    p_end_date := '2023-12-31',
    p_report_json := NULL
);


--policy related Documents
CREATE TABLE policy_management.policy_related_documents (
    related_doc_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    document_type VARCHAR(100) NOT NULL CHECK (document_type IN ('Guideline', 'Template', 'Form', 'Procedure', 'Standard', 'Reference')),
    document_name VARCHAR(255) NOT NULL,
    document_description TEXT,
    file_path VARCHAR(512) NOT NULL,
    file_type VARCHAR(50),
    file_size BIGINT,
    is_active BOOLEAN DEFAULT TRUE,
    effective_date DATE,
    review_frequency_months INT,
    next_review_date DATE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by INT REFERENCES core.employees(employee_id),
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_by INT REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE policy_management.policy_related_documents IS 'Stores documents related to policies such as guidelines, templates, and forms';

CREATE TABLE policy_management.policy_implementation_checklist (
    checklist_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    item_name VARCHAR(255) NOT NULL,
    item_description TEXT,
    is_mandatory BOOLEAN DEFAULT TRUE,
    responsible_party_type VARCHAR(50) CHECK (responsible_party_type IN ('Role', 'Department', 'Individual', 'Position')),
    responsible_party_id INT, -- Could be role_id, department_id, or employee_id
    due_date DATE,
    completion_status VARCHAR(50) CHECK (completion_status IN ('Not Started', 'In Progress', 'Completed', 'Deferred')),
    completed_at TIMESTAMP,
    completed_by INT REFERENCES core.employees(employee_id),
    verification_required BOOLEAN DEFAULT FALSE,
    verified_at TIMESTAMP,
    verified_by INT REFERENCES core.employees(employee_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by INT REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE policy_management.policy_implementation_checklist IS 'Tracks implementation tasks required for policy adoption and compliance';


CREATE TABLE policy_management.policy_feedback_analysis (
    analysis_id SERIAL PRIMARY KEY,
    feedback_id INT REFERENCES policy_management.policy_feedback(feedback_id),
    sentiment_score DECIMAL(3,2) CHECK (sentiment_score BETWEEN -1 AND 1),
    sentiment_label VARCHAR(50) CHECK (sentiment_label IN ('Very Negative', 'Negative', 'Neutral', 'Positive', 'Very Positive')),
    key_topics TEXT[], -- Array of key topics identified in feedback
    urgency_level VARCHAR(50) CHECK (urgency_level IN ('Low', 'Medium', 'High', 'Critical')),
    action_required BOOLEAN DEFAULT FALSE,
    action_taken BOOLEAN DEFAULT FALSE,
    action_description TEXT,
    analyzed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    analyzed_by INT REFERENCES core.employees(employee_id),
    analysis_method VARCHAR(100),
    confidence_score DECIMAL(3,2)
);

COMMENT ON TABLE policy_management.policy_feedback_analysis IS 'Stores analyzed feedback data including sentiment and key topics';

CREATE TABLE policy_management.policy_training_effectiveness (
    effectiveness_id SERIAL PRIMARY KEY,
    training_id INT NOT NULL REFERENCES policy_management.policy_training(training_id),
    evaluation_date DATE NOT NULL,
    evaluation_method VARCHAR(100) NOT NULL,
    knowledge_gain_score DECIMAL(5,2) CHECK (knowledge_gain_score BETWEEN 0 AND 100),
    behavior_change_score DECIMAL(5,2) CHECK (behavior_change_score BETWEEN 0 AND 100),
    compliance_improvement_score DECIMAL(5,2) CHECK (compliance_improvement_score BETWEEN 0 AND 100),
    participant_satisfaction_score DECIMAL(5,2) CHECK (participant_satisfaction_score BETWEEN 0 AND 100),
    total_participants INT,
    evaluation_notes TEXT,
    evaluated_by INT REFERENCES core.employees(employee_id),
    next_evaluation_date DATE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_training_effectiveness IS 'Tracks the effectiveness of policy-related training programs';


CREATE TABLE policy_management.policy_version_comparison (
    comparison_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    from_version VARCHAR(20) NOT NULL,
    to_version VARCHAR(20) NOT NULL,
    comparison_type VARCHAR(50) NOT NULL CHECK (comparison_type IN ('Auto', 'Manual', 'Hybrid')),
    change_summary TEXT NOT NULL,
    significant_changes BOOLEAN DEFAULT FALSE,
    regulatory_impact_analysis TEXT,
    training_required BOOLEAN DEFAULT FALSE,
    communication_required BOOLEAN DEFAULT FALSE,
    compared_by INT REFERENCES core.employees(employee_id),
    compared_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    approval_status VARCHAR(50) CHECK (approval_status IN ('Pending', 'Approved', 'Rejected')),
    approved_by INT REFERENCES core.employees(employee_id),
    approved_at TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_version_comparison IS 'Stores comparisons between policy versions to track changes and impacts';


CREATE OR REPLACE VIEW policy_management.vw_policy_implementation_status AS
SELECT
    p.policy_id,
    p.policy_name,
    p.status AS policy_status,
    p.effective_date,
    COUNT(DISTINCT c.checklist_id) AS total_tasks,
    COUNT(DISTINCT CASE WHEN c.completion_status = 'Completed' THEN c.checklist_id END) AS completed_tasks,
    COUNT(DISTINCT CASE WHEN c.completion_status IN ('Not Started', 'In Progress')
                    AND (c.due_date IS NULL OR c.due_date <= CURRENT_DATE)
                    THEN c.checklist_id END) AS overdue_tasks,
    COUNT(DISTINCT CASE WHEN c.completion_status IN ('Not Started', 'In Progress')
                    AND c.due_date > CURRENT_DATE
                    THEN c.checklist_id END) AS pending_tasks,
    COUNT(DISTINCT CASE WHEN c.verification_required = TRUE AND c.verified_at IS NULL
                    AND c.completion_status = 'Completed'
                    THEN c.checklist_id END) AS unverified_tasks,
    MIN(c.due_date) AS next_due_date,
    CASE
        WHEN COUNT(DISTINCT c.checklist_id) = 0 THEN 'No Tasks'
        WHEN COUNT(DISTINCT c.checklist_id) = COUNT(DISTINCT CASE WHEN c.completion_status = 'Completed' THEN c.checklist_id END)
        THEN 'Fully Implemented'
        WHEN COUNT(DISTINCT CASE WHEN c.completion_status = 'Completed' THEN c.checklist_id END) * 100.0 /
             NULLIF(COUNT(DISTINCT c.checklist_id), 0) >= 80 THEN 'Mostly Implemented'
        WHEN COUNT(DISTINCT CASE WHEN c.completion_status = 'Completed' THEN c.checklist_id END) * 100.0 /
             NULLIF(COUNT(DISTINCT c.checklist_id), 0) >= 50 THEN 'Partially Implemented'
        ELSE 'Minimally Implemented'
    END AS implementation_status
FROM
    policy_management.policies p
LEFT JOIN
    policy_management.policy_implementation_checklist c ON p.policy_id = c.policy_id
GROUP BY
    p.policy_id, p.policy_name, p.status, p.effective_date;

COMMENT ON VIEW policy_management.vw_policy_implementation_status IS 'Provides an overview of policy implementation progress through checklist completion';


CREATE OR REPLACE VIEW policy_management.vw_policy_feedback_sentiment AS
SELECT
    p.policy_id,
    p.policy_name,
    p.version,
    COUNT(DISTINCT f.feedback_id) AS total_feedback,
    COUNT(DISTINCT a.analysis_id) AS analyzed_feedback,
    AVG(a.sentiment_score) AS avg_sentiment_score,
    MODE() WITHIN GROUP (ORDER BY a.sentiment_label) AS most_common_sentiment,
    COUNT(DISTINCT CASE WHEN a.sentiment_label IN ('Positive', 'Very Positive') THEN f.feedback_id END) AS positive_feedback,
    COUNT(DISTINCT CASE WHEN a.sentiment_label IN ('Negative', 'Very Negative') THEN f.feedback_id END) AS negative_feedback,
    COUNT(DISTINCT CASE WHEN a.urgency_level = 'High' THEN f.feedback_id END) AS high_urgency_feedback,
    COUNT(DISTINCT CASE WHEN a.urgency_level = 'Critical' THEN f.feedback_id END) AS critical_feedback,
    COUNT(DISTINCT CASE WHEN a.action_required = TRUE THEN f.feedback_id END) AS action_required_feedback,
    COUNT(DISTINCT CASE WHEN a.action_taken = TRUE THEN f.feedback_id END) AS action_taken_feedback,
    ARRAY(
        SELECT DISTINCT unnest(a.key_topics)
        FROM policy_management.policy_feedback_analysis a
        JOIN policy_management.policy_feedback f ON a.feedback_id = f.feedback_id
        WHERE f.policy_id = p.policy_id
    ) AS all_key_topics
FROM
    policy_management.policies p
LEFT JOIN
    policy_management.policy_feedback f ON p.policy_id = f.policy_id
LEFT JOIN
    policy_management.policy_feedback_analysis a ON f.feedback_id = a.feedback_id
GROUP BY
    p.policy_id, p.policy_name, p.version;

COMMENT ON VIEW policy_management.vw_policy_feedback_sentiment IS 'Analyzes feedback sentiment and urgency levels for policies';


CREATE OR REPLACE VIEW policy_management.vw_policy_training_effectiveness AS
SELECT
    p.policy_id,
    p.policy_name,
    t.training_id,
    t.training_name,
    t.training_type,
    COUNT(DISTINCT tr.record_id) AS total_completions,
    COUNT(DISTINCT e.effectiveness_id) AS evaluation_count,
    AVG(e.knowledge_gain_score) AS avg_knowledge_gain,
    AVG(e.behavior_change_score) AS avg_behavior_change,
    AVG(e.compliance_improvement_score) AS avg_compliance_improvement,
    AVG(e.participant_satisfaction_score) AS avg_participant_satisfaction,
    MAX(e.evaluation_date) AS last_evaluation_date,
    MIN(e.next_evaluation_date) AS next_evaluation_due,
    CASE
        WHEN COUNT(DISTINCT e.effectiveness_id) = 0 THEN 'Not Evaluated'
        WHEN AVG(e.knowledge_gain_score) >= 80 AND AVG(e.behavior_change_score) >= 70 THEN 'Highly Effective'
        WHEN AVG(e.knowledge_gain_score) >= 70 AND AVG(e.behavior_change_score) >= 60 THEN 'Effective'
        WHEN AVG(e.knowledge_gain_score) >= 60 AND AVG(e.behavior_change_score) >= 50 THEN 'Moderately Effective'
        ELSE 'Needs Improvement'
    END AS effectiveness_rating
FROM
    policy_management.policies p
JOIN
    policy_management.policy_training t ON p.policy_id = t.policy_id
LEFT JOIN
    policy_management.policy_training_records tr ON t.training_id = tr.training_id AND tr.status = 'Completed'
LEFT JOIN
    policy_management.policy_training_effectiveness e ON t.training_id = e.training_id
GROUP BY
    p.policy_id, p.policy_name, t.training_id, t.training_name, t.training_type;

COMMENT ON VIEW policy_management.vw_policy_training_effectiveness IS 'Evaluates the effectiveness of policy-related training programs';


CREATE MATERIALIZED VIEW policy_management.mv_policy_document_relationships AS
SELECT
    p.policy_id,
    p.policy_name,
    p.version AS policy_version,
    COUNT(DISTINCT d.document_id) AS policy_document_count,
    COUNT(DISTINCT rd.related_doc_id) AS related_document_count,
    COUNT(DISTINCT t.training_id) AS training_program_count,
    COUNT(DISTINCT CASE WHEN rd.document_type = 'Procedure' THEN rd.related_doc_id END) AS procedure_count,
    COUNT(DISTINCT CASE WHEN rd.document_type = 'Template' THEN rd.related_doc_id END) AS template_count,
    COUNT(DISTINCT CASE WHEN rd.document_type = 'Form' THEN rd.related_doc_id END) AS form_count,
    STRING_AGG(DISTINCT rd.document_type, ', ') AS related_document_types,
    CURRENT_TIMESTAMP AS refresh_time
FROM
    policy_management.policies p
LEFT JOIN
    policy_management.policy_documents d ON p.policy_id = d.policy_id AND d.is_current = TRUE
LEFT JOIN
    policy_management.policy_related_documents rd ON p.policy_id = rd.policy_id AND rd.is_active = TRUE
LEFT JOIN
    policy_management.policy_training t ON p.policy_id = t.policy_id
GROUP BY
    p.policy_id, p.policy_name, p.version;

COMMENT ON MATERIALIZED VIEW policy_management.mv_policy_document_relationships IS 'Aggregates relationships between policies and their related documents for quick reference';



CREATE MATERIALIZED VIEW policy_management.mv_policy_version_change_impact AS
SELECT
    p.policy_id,
    p.policy_name,
    vc.from_version,
    vc.to_version,
    vc.significant_changes,
    vc.regulatory_impact_analysis IS NOT NULL AS has_regulatory_impact,
    vc.training_required,
    vc.communication_required,
    COUNT(DISTINCT a.attestation_id) FILTER (WHERE a.acknowledged_version = vc.from_version) AS attestations_on_old_version,
    COUNT(DISTINCT tr.record_id) FILTER (WHERE t.created_at < vc.compared_at) AS training_on_old_version,
    COUNT(DISTINCT v.violation_id) FILTER (WHERE v.violation_date BETWEEN
        (SELECT effective_date FROM policy_management.policies WHERE policy_id = p.policy_id AND version = vc.from_version) AND
        COALESCE((SELECT effective_date FROM policy_management.policies WHERE policy_id = p.policy_id AND version = vc.to_version), CURRENT_DATE)
    ) AS violations_during_version,
    vc.compared_at,
    vc.approval_status,
    CURRENT_TIMESTAMP AS refresh_time
FROM
    policy_management.policies p
JOIN
    policy_management.policy_version_comparison vc ON p.policy_id = vc.policy_id
LEFT JOIN
    policy_management.policy_attestations a ON p.policy_id = a.policy_id
LEFT JOIN
    policy_management.policy_training t ON p.policy_id = t.policy_id
LEFT JOIN
    policy_management.policy_training_records tr ON t.training_id = tr.training_id
LEFT JOIN
    policy_management.policy_violations v ON p.policy_id = v.policy_id
GROUP BY
    p.policy_id, p.policy_name, vc.from_version, vc.to_version, vc.significant_changes,
    vc.regulatory_impact_analysis, vc.training_required, vc.communication_required,
    vc.compared_at, vc.approval_status;

COMMENT ON MATERIALIZED VIEW policy_management.mv_policy_version_change_impact IS 'Analyzes the impact of policy version changes including attestations, training, and violations';



CREATE MATERIALIZED VIEW policy_management.mv_policy_compliance_risk_exposure AS
SELECT
    p.policy_id,
    p.policy_name,
    p.status AS policy_status,
    COUNT(DISTINCT prm.risk_id) AS mapped_risks,
    COUNT(DISTINCT v.violation_id) AS total_violations,
    COUNT(DISTINCT e.exception_id) AS total_exceptions,
    COUNT(DISTINCT r.regulation_id) AS total_regulations,
    COUNT(DISTINCT CASE WHEN prm.compliance_status = 'Non-Compliant' THEN r.regulation_id END) AS non_compliant_regulations,
    COUNT(DISTINCT c.checklist_id) AS implementation_tasks,
    COUNT(DISTINCT CASE WHEN c.completion_status != 'Completed' THEN c.checklist_id END) AS incomplete_tasks,
    COUNT(DISTINCT a.attestation_id) AS total_attestations,
    COUNT(DISTINCT a.attestation_id) * 100.0 / NULLIF((
        SELECT COUNT(DISTINCT e.employee_id)
        FROM core.employees e
        JOIN core.employee_department ed ON e.employee_id = ed.employee_id
        JOIN core.department_policy dp ON ed.department_id = dp.department_id
        WHERE dp.policy_id = p.policy_id AND e.is_active = TRUE
    ), 0) AS attestation_rate,
    CASE
        WHEN COUNT(DISTINCT prm.risk_id) = 0 THEN 0
        ELSE (
            (COUNT(DISTINCT v.violation_id) * 0.4) +
            (COUNT(DISTINCT e.exception_id) * 0.3) +
            (COUNT(DISTINCT CASE WHEN prm.compliance_status = 'Non-Compliant' THEN r.regulation_id END) * 0.2) +
            (COUNT(DISTINCT CASE WHEN c.completion_status != 'Completed' THEN c.checklist_id END) * 0.1)
        )
    END AS risk_exposure_score,
    CURRENT_TIMESTAMP AS refresh_time
FROM
    policy_management.policies p
LEFT JOIN
    policy_management.policy_risk_mapping prm ON p.policy_id = prm.policy_id
LEFT JOIN
    policy_management.policy_violations v ON p.policy_id = v.policy_id
LEFT JOIN
    policy_management.policy_exceptions e ON p.policy_id = e.policy_id
LEFT JOIN
    policy_management.policy_regulatory_mapping prm ON p.policy_id = prm.policy_id
LEFT JOIN
    core.regulations r ON prm.regulation_id = r.regulation_id
LEFT JOIN
    policy_management.policy_implementation_checklist c ON p.policy_id = c.policy_id
LEFT JOIN
    policy_management.policy_attestations a ON p.policy_id = a.policy_id AND a.status = 'Completed'
GROUP BY
    p.policy_id, p.policy_name, p.status;

COMMENT ON MATERIALIZED VIEW policy_management.mv_policy_compliance_risk_exposure IS 'Calculates a comprehensive risk exposure score for each policy based on violations, exceptions, compliance status, and implementation';



CREATE OR REPLACE PROCEDURE policy_management.sp_analyze_policy_feedback(
    IN p_policy_id INT DEFAULT NULL,
    IN p_timeframe_days INT DEFAULT 90,
    IN p_min_confidence DECIMAL(3,2) DEFAULT 0.7,
    OUT p_total_feedback INT,
    OUT p_avg_sentiment DECIMAL(3,2),
    OUT p_positive_feedback INT,
    OUT p_negative_feedback INT,
    OUT p_common_topics TEXT[],
    OUT p_high_urgency_count INT
)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Total feedback
    SELECT COUNT(*) INTO p_total_feedback
    FROM policy_management.policy_feedback f
    WHERE (p_policy_id IS NULL OR f.policy_id = p_policy_id)
      AND f.feedback_date >= CURRENT_DATE - (p_timeframe_days * INTERVAL '1 day');

    -- Average sentiment
    SELECT AVG(a.sentiment_score) INTO p_avg_sentiment
    FROM policy_management.policy_feedback f
    JOIN policy_management.policy_feedback_analysis a ON f.feedback_id = a.feedback_id
    WHERE (p_policy_id IS NULL OR f.policy_id = p_policy_id)
      AND f.feedback_date >= CURRENT_DATE - (p_timeframe_days * INTERVAL '1 day')
      AND a.confidence_score >= p_min_confidence;

    -- Positive feedback count
    SELECT COUNT(*) INTO p_positive_feedback
    FROM policy_management.policy_feedback f
    JOIN policy_management.policy_feedback_analysis a ON f.feedback_id = a.feedback_id
    WHERE (p_policy_id IS NULL OR f.policy_id = p_policy_id)
      AND f.feedback_date >= CURRENT_DATE - (p_timeframe_days * INTERVAL '1 day')
      AND a.sentiment_label IN ('Positive', 'Very Positive')
      AND a.confidence_score >= p_min_confidence;

    -- Negative feedback count
    SELECT COUNT(*) INTO p_negative_feedback
    FROM policy_management.policy_feedback f
    JOIN policy_management.policy_feedback_analysis a ON f.feedback_id = a.feedback_id
    WHERE (p_policy_id IS NULL OR f.policy_id = p_policy_id)
      AND f.feedback_date >= CURRENT_DATE - (p_timeframe_days * INTERVAL '1 day')
      AND a.sentiment_label IN ('Negative', 'Very Negative')
      AND a.confidence_score >= p_min_confidence;

    -- Common topics (simplified example)
    WITH topic_counts AS (
        SELECT
            unnest(a.key_topics) AS topic,
            COUNT(*) AS frequency
        FROM
            policy_management.policy_feedback f
        JOIN
            policy_management.policy_feedback_analysis a ON f.feedback_id = a.feedback_id
        WHERE
            (p_policy_id IS NULL OR f.policy_id = p_policy_id)
            AND f.feedback_date >= CURRENT_DATE - (p_timeframe_days * INTERVAL '1 day')
            AND a.confidence_score >= p_min_confidence
            AND a.key_topics IS NOT NULL
        GROUP BY
            unnest(a.key_topics)
        ORDER BY
            COUNT(*) DESC
        LIMIT 5
    )
    SELECT ARRAY_AGG(topic) INTO p_common_topics
    FROM topic_counts;

    -- High urgency count
    SELECT COUNT(*) INTO p_high_urgency_count
    FROM policy_management.policy_feedback f
    JOIN policy_management.policy_feedback_analysis a ON f.feedback_id = a.feedback_id
    WHERE (p_policy_id IS NULL OR f.policy_id = p_policy_id)
      AND f.feedback_date >= CURRENT_DATE - (p_timeframe_days * INTERVAL '1 day')
      AND a.urgency_level IN ('High', 'Critical')
      AND a.confidence_score >= p_min_confidence;
END;
$$;

COMMENT ON PROCEDURE policy_management.sp_analyze_policy_feedback IS 'Analyzes policy feedback data including sentiment, topics, and urgency levels';




CREATE OR REPLACE PROCEDURE policy_management.sp_generate_policy_implementation_report(
    IN p_policy_id INT,
    OUT p_report_json JSON
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_policy RECORD;
    v_checklist_items JSON[];
    v_departments JSON[];
    v_overdue_tasks INT;
    v_completion_rate DECIMAL(5,2);
BEGIN
    -- Get policy basic info
    SELECT
        policy_id,
        policy_name,
        version,
        status,
        effective_date,
        next_review_date,
        (SELECT employee_name FROM core.employees WHERE employee_id = policy_owner_id) AS policy_owner
    INTO
        v_policy
    FROM
        policy_management.policies
    WHERE
        policy_id = p_policy_id;

    -- Get checklist items
    SELECT
        array_agg(json_build_object(
            'item_name', item_name,
            'description', item_description,
            'responsible_party', CASE
                WHEN responsible_party_type = 'Role' THEN (SELECT role_name FROM core.roles WHERE role_id = responsible_party_id)
                WHEN responsible_party_type = 'Department' THEN (SELECT department_name FROM core.departments WHERE department_id = responsible_party_id)
                WHEN responsible_party_type = 'Individual' THEN (SELECT employee_name FROM core.employees WHERE employee_id = responsible_party_id)
                ELSE 'Not Specified'
            END,
            'due_date', due_date,
            'status', completion_status,
            'completed_at', completed_at,
            'verified', CASE WHEN verification_required THEN COALESCE(verified_at IS NOT NULL, FALSE) ELSE NULL END
        ))
    INTO
        v_checklist_items
    FROM
        policy_management.policy_implementation_checklist
    WHERE
        policy_id = p_policy_id
    ORDER BY
        due_date NULLS FIRST, item_name;

    -- Get department implementation status
    SELECT
        array_agg(json_build_object(
            'department_id', d.department_id,
            'department_name', d.department_name,
            'total_tasks', COUNT(DISTINCT c.checklist_id),
            'completed_tasks', COUNT(DISTINCT CASE WHEN c.completion_status = 'Completed' THEN c.checklist_id END),
            'completion_rate', COUNT(DISTINCT CASE WHEN c.completion_status = 'Completed' THEN c.checklist_id END) * 100.0 /
                              NULLIF(COUNT(DISTINCT c.checklist_id), 0),
            'overdue_tasks', COUNT(DISTINCT CASE WHEN c.completion_status IN ('Not Started', 'In Progress') AND
                                              (c.due_date IS NULL OR c.due_date <= CURRENT_DATE)
                                          THEN c.checklist_id END)
        ))
    INTO
        v_departments
    FROM
        core.departments d
    LEFT JOIN
        policy_management.policy_implementation_checklist c ON
            (c.responsible_party_type = 'Department' AND c.responsible_party_id = d.department_id) OR
            (c.responsible_party_type = 'Individual' AND c.responsible_party_id IN (
                SELECT employee_id FROM core.employees WHERE department_id = d.department_id
            ))
    WHERE
        c.policy_id = p_policy_id
    GROUP BY
        d.department_id, d.department_name;

    -- Get overdue tasks count
    SELECT
        COUNT(DISTINCT checklist_id)
    INTO
        v_overdue_tasks
    FROM
        policy_management.policy_implementation_checklist
    WHERE
        policy_id = p_policy_id
        AND completion_status IN ('Not Started', 'In Progress')
        AND (due_date IS NULL OR due_date <= CURRENT_DATE);

    -- Calculate overall completion rate
    SELECT
        COUNT(DISTINCT CASE WHEN completion_status = 'Completed' THEN checklist_id END) * 100.0 /
        NULLIF(COUNT(DISTINCT checklist_id), 0)
    INTO
        v_completion_rate
    FROM
        policy_management.policy_implementation_checklist
    WHERE
        policy_id = p_policy_id;

    -- Compile final report
    p_report_json := json_build_object(
        'policy_info', json_build_object(
            'policy_id', v_policy.policy_id,
            'policy_name', v_policy.policy_name,
            'version', v_policy.version,
            'status', v_policy.status,
            'effective_date', v_policy.effective_date,
            'next_review_date', v_policy.next_review_date,
            'policy_owner', v_policy.policy_owner
        ),
        'implementation_summary', json_build_object(
            'total_tasks', (SELECT COUNT(*) FROM json_array_elements(v_checklist_items)),
            'completed_tasks', (SELECT COUNT(*) FROM json_array_elements(v_checklist_items)
                              WHERE (value->>'status')::text = 'Completed'),
            'completion_rate', v_completion_rate,
            'overdue_tasks', v_overdue_tasks
        ),
        'checklist_items', v_checklist_items,
        'department_status', v_departments,
        'report_generated_at', CURRENT_TIMESTAMP
    );
END;
$$;

COMMENT ON PROCEDURE policy_management.sp_generate_policy_implementation_report IS 'Generates a detailed JSON report on policy implementation status including checklist items and department progress';



CREATE OR REPLACE PROCEDURE policy_management.sp_sync_policy_training_requirements()
LANGUAGE plpgsql
AS $$
DECLARE
    v_policy RECORD;
    v_training RECORD;
    v_employee_count INT;
    v_new_training_records INT := 0;
    v_expired_records INT := 0;
    v_updated_records INT := 0;
BEGIN
    -- Process each policy with mandatory training
    FOR v_policy IN
        SELECT DISTINCT p.policy_id, p.policy_name
        FROM policy_management.policies p
        JOIN policy_management.policy_training t ON p.policy_id = t.policy_id
        WHERE p.status = 'Active'
          AND t.is_mandatory = TRUE
    LOOP
        -- Process each training for the policy
        FOR v_training IN
            SELECT t.training_id, t.training_name, t.frequency_months
            FROM policy_management.policy_training t
            WHERE t.policy_id = v_policy.policy_id
              AND t.is_mandatory = TRUE
        LOOP
            -- Get count of employees who should have this training
            SELECT COUNT(*) INTO v_employee_count
            FROM core.employees e
            JOIN core.employee_department ed ON e.employee_id = ed.employee_id
            JOIN core.department_policy dp ON ed.department_id = dp.department_id
            WHERE dp.policy_id = v_policy.policy_id
              AND e.is_active = TRUE;

            -- Create missing training records
            WITH required_employees AS (
                SELECT e.employee_id
                FROM core.employees e
                JOIN core.employee_department ed ON e.employee_id = ed.employee_id
                JOIN core.department_policy dp ON ed.department_id = dp.department_id
                WHERE dp.policy_id = v_policy.policy_id
                  AND e.is_active = TRUE
                  AND NOT EXISTS (
                      SELECT 1 FROM policy_management.policy_training_records tr
                      WHERE tr.training_id = v_training.training_id
                        AND tr.employee_id = e.employee_id
                  )
            )
            INSERT INTO policy_management.policy_training_records (
                training_id,
                employee_id,
                status,
                expiration_date
            )
            SELECT
                v_training.training_id,
                employee_id,
                'Not Started',
                CASE
                    WHEN v_training.frequency_months IS NOT NULL THEN
                        CURRENT_DATE + (v_training.frequency_months * INTERVAL '1 month')
                    ELSE NULL
                END
            FROM
                required_employees;

            GET DIAGNOSTICS v_new_training_records = ROW_COUNT;

            -- Update expired training records
            UPDATE policy_management.policy_training_records tr
            SET
                status = 'Expired',
                updated_at = CURRENT_TIMESTAMP
            WHERE
                tr.training_id = v_training.training_id
                AND tr.status = 'Completed'
                AND tr.expiration_date < CURRENT_DATE;

            GET DIAGNOSTICS v_expired_records = ROW_COUNT;

            -- Update soon-to-expire training records
            UPDATE policy_management.policy_training_records tr
            SET
                status = 'Expiring Soon',
                updated_at = CURRENT_TIMESTAMP
            WHERE
                tr.training_id = v_training.training_id
                AND tr.status = 'Completed'
                AND tr.expiration_date BETWEEN CURRENT_DATE AND CURRENT_DATE + INTERVAL '30 days';

            GET DIAGNOSTICS v_updated_records = ROW_COUNT + v_updated_records;

            -- Log the sync activity
            INSERT INTO policy_management.policy_audit_trail (
                policy_id,
                audit_type,
                table_name,
                changed_by,
                additional_context
            ) VALUES (
                v_policy.policy_id,
                'System Job',
                'policy_training_records',
                1, -- System user
                'Training requirement sync for training ' || v_training.training_name ||
                ': ' || v_new_training_records || ' new records, ' ||
                v_expired_records || ' expired records, ' ||
                v_updated_records || ' updated records'
            );
        END LOOP;
    END LOOP;

    -- Log the overall job completion
    INSERT INTO policy_management.policy_audit_trail (
        audit_type,
        table_name,
        changed_by,
        additional_context
    ) VALUES (
        'System Job',
        'N/A',
        1,
        'Policy Training Requirement Synchronization completed successfully'
    );
END;
$$;

COMMENT ON PROCEDURE policy_management.sp_sync_policy_training_requirements IS 'Synchronizes training requirements with current employees and department assignments, creating new records and updating statuses as needed';





CREATE TABLE policy_management.policy_impact_assessment (
    assessment_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    assessment_date DATE NOT NULL,
    assessment_type VARCHAR(50) NOT NULL CHECK (assessment_type IN ('Initial', 'Periodic', 'Ad-Hoc', 'Post-Implementation')),
    business_impact_analysis TEXT,
    operational_impact_analysis TEXT,
    financial_impact_analysis TEXT,
    technology_impact_analysis TEXT,
    risk_impact_analysis TEXT,
    compliance_impact_analysis TEXT,
    overall_impact_score DECIMAL(3,1) CHECK (overall_impact_score BETWEEN 1 AND 5),
    assessed_by INT NOT NULL REFERENCES core.employees(employee_id),
    reviewed_by INT REFERENCES core.employees(employee_id),
    next_assessment_date DATE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_impact_assessment IS 'Stores comprehensive impact assessments for policies across multiple dimensions';



CREATE TABLE policy_management.policy_exception_analytics (
    analytics_id SERIAL PRIMARY KEY,
    exception_id INT NOT NULL REFERENCES policy_management.policy_exceptions(exception_id),
    risk_score DECIMAL(3,1) CHECK (risk_score BETWEEN 1 AND 5),
    compliance_exposure_score DECIMAL(3,1) CHECK (compliance_exposure_score BETWEEN 1 AND 5),
    operational_impact_score DECIMAL(3,1) CHECK (operational_impact_score BETWEEN 1 AND 5),
    financial_impact_score DECIMAL(3,1) CHECK (financial_impact_score BETWEEN 1 AND 5),
    exception_pattern_match VARCHAR(100), -- Matches known exception patterns
    similar_exceptions INT, -- Count of similar exceptions
    exception_trend VARCHAR(50) CHECK (exception_trend IN ('Increasing', 'Stable', 'Decreasing')),
    analyzed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    analyzed_by INT REFERENCES core.employees(employee_id),
    recommendations TEXT
);

COMMENT ON TABLE policy_management.policy_exception_analytics IS 'Provides advanced analytics and scoring for policy exceptions';




CREATE TABLE policy_management.policy_automation_rules (
    rule_id SERIAL PRIMARY KEY,
    rule_name VARCHAR(100) NOT NULL,
    rule_description TEXT,
    policy_id INT REFERENCES policy_management.policies(policy_id),
    trigger_condition TEXT NOT NULL, -- SQL-like condition
    action_type VARCHAR(50) NOT NULL CHECK (action_type IN ('Notification', 'Workflow', 'Approval', 'Remediation', 'Escalation')),
    action_parameters JSONB, -- Parameters for the action
    is_active BOOLEAN DEFAULT TRUE,
    priority INT DEFAULT 5 CHECK (priority BETWEEN 1 AND 10),
    last_triggered_at TIMESTAMP,
    trigger_count INT DEFAULT 0,
    created_by INT NOT NULL REFERENCES core.employees(employee_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_automation_rules IS 'Defines automation rules for policy-related workflows and triggers';



CREATE TABLE policy_management.policy_benchmarking (
    benchmark_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    benchmark_source VARCHAR(100) NOT NULL CHECK (benchmark_source IN ('Industry Standard', 'Regulatory Body', 'Peer Organization', 'Best Practice')),
    benchmark_name VARCHAR(255) NOT NULL,
    benchmark_version VARCHAR(50),
    benchmark_date DATE,
    compliance_status VARCHAR(50) CHECK (compliance_status IN ('Compliant', 'Partial', 'Non-Compliant', 'Not Applicable')),
    gap_analysis TEXT,
    maturity_level VARCHAR(50) CHECK (maturity_level IN ('Initial', 'Developing', 'Defined', 'Managed', 'Optimized')),
    target_maturity_level VARCHAR(50),
    improvement_plan TEXT,
    benchmarked_by INT REFERENCES core.employees(employee_id),
    benchmarked_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    next_benchmark_date DATE
);

COMMENT ON TABLE policy_management.policy_benchmarking IS 'Tracks benchmarking of policies against external standards and best practices';



CREATE TABLE policy_management.policy_ai_recommendations (
    recommendation_id SERIAL PRIMARY KEY,
    policy_id INT REFERENCES policy_management.policies(policy_id),
    recommendation_type VARCHAR(50) NOT NULL CHECK (recommendation_type IN ('Content', 'Structure', 'Compliance', 'Risk', 'Effectiveness')),
    recommendation_text TEXT NOT NULL,
    confidence_score DECIMAL(3,2) CHECK (confidence_score BETWEEN 0 AND 1),
    impact_score DECIMAL(3,2) CHECK (impact_score BETWEEN 0 AND 1),
    implementation_effort VARCHAR(50) CHECK (implementation_effort IN ('Low', 'Medium', 'High')),
    related_risk_id INT REFERENCES core.risks(risk_id),
    related_control_id INT REFERENCES core.controls(control_id),
    related_regulation_id INT REFERENCES core.regulations(regulation_id),
    status VARCHAR(50) CHECK (status IN ('Pending', 'Approved', 'Rejected', 'Implemented')),
    reviewed_by INT REFERENCES core.employees(employee_id),
    reviewed_at TIMESTAMP,
    implementation_date TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_ai_recommendations IS 'Stores AI-generated recommendations for policy improvement and optimization';



CREATE OR REPLACE VIEW policy_management.vw_policy_impact_dashboard AS
SELECT
    p.policy_id,
    p.policy_name,
    p.status AS policy_status,
    p.version,
    p.effective_date,
    p.next_review_date,
    ia.overall_impact_score,
    ia.business_impact_analysis,
    ia.operational_impact_analysis,
    ia.financial_impact_analysis,
    ia.technology_impact_analysis,
    ia.risk_impact_analysis,
    ia.compliance_impact_analysis,
    ia.assessment_date,
    ia.next_assessment_date,
    e.employee_name AS assessed_by,
    (SELECT STRING_AGG(r.risk_name, ', ')
     FROM policy_management.policy_risk_mapping prm
     JOIN core.risks r ON prm.risk_id = r.risk_id
     WHERE prm.policy_id = p.policy_id) AS mapped_risks,
    (SELECT COUNT(*) FROM policy_management.policy_violations WHERE policy_id = p.policy_id) AS violation_count,
    (SELECT COUNT(*) FROM policy_management.policy_exceptions WHERE policy_id = p.policy_id AND status = 'Approved') AS exception_count
FROM
    policy_management.policies p
LEFT JOIN
    policy_management.policy_impact_assessment ia ON p.policy_id = ia.policy_id
    AND ia.assessment_date = (SELECT MAX(assessment_date)
                             FROM policy_management.policy_impact_assessment
                             WHERE policy_id = p.policy_id)
LEFT JOIN
    core.employees e ON ia.assessed_by = e.employee_id;

COMMENT ON VIEW policy_management.vw_policy_impact_dashboard IS 'Provides a comprehensive view of policy impacts across business, operational, financial, technology, risk, and compliance dimensions';



CREATE OR REPLACE VIEW policy_management.vw_policy_exception_risk AS
SELECT
    e.exception_id,
    p.policy_id,
    p.policy_name,
    e.request_date,
    e.exception_start_date,
    e.exception_end_date,
    e.status,
    req.employee_name AS requester,
    req.department_name AS requester_department,
    a.risk_score,
    a.compliance_exposure_score,
    a.operational_impact_score,
    a.financial_impact_score,
    a.exception_pattern_match,
    a.similar_exceptions,
    a.exception_trend,
    a.recommendations,
    CASE
        WHEN a.risk_score >= 4.0 OR a.compliance_exposure_score >= 4.0 THEN 'Critical'
        WHEN a.risk_score >= 3.0 OR a.compliance_exposure_score >= 3.0 THEN 'High'
        WHEN a.risk_score >= 2.0 OR a.compliance_exposure_score >= 2.0 THEN 'Medium'
        ELSE 'Low'
    END AS exception_risk_level,
    DATEDIFF(day, CURRENT_DATE, e.exception_end_date) AS days_remaining
FROM
    policy_management.policy_exceptions e
JOIN
    policy_management.policies p ON e.policy_id = p.policy_id
JOIN
    (SELECT employee_id, employee_name, department_name
     FROM core.employees e JOIN core.departments d ON e.department_id = d.department_id) req
    ON e.requester_id = req.employee_id
LEFT JOIN
    policy_management.policy_exception_analytics a ON e.exception_id = a.exception_id
WHERE
    e.status = 'Approved'
    AND (e.exception_end_date IS NULL OR e.exception_end_date >= CURRENT_DATE);

COMMENT ON VIEW policy_management.vw_policy_exception_risk IS 'Provides a risk-focused view of active policy exceptions with analytics and scoring';



CREATE OR REPLACE VIEW policy_management.vw_policy_automation_rules AS
SELECT
    r.rule_id,
    r.rule_name,
    r.rule_description,
    p.policy_id,
    p.policy_name,
    r.trigger_condition,
    r.action_type,
    r.action_parameters,
    r.is_active,
    r.priority,
    r.last_triggered_at,
    r.trigger_count,
    e.employee_name AS created_by,
    r.created_at,
    (SELECT COUNT(*) FROM policy_management.policy_audit_trail
     WHERE additional_context LIKE '%Rule ID: ' || r.rule_id || '%') AS execution_count,
    (SELECT MAX(changed_at) FROM policy_management.policy_audit_trail
     WHERE additional_context LIKE '%Rule ID: ' || r.rule_id || '%') AS last_execution
FROM
    policy_management.policy_automation_rules r
LEFT JOIN
    policy_management.policies p ON r.policy_id = p.policy_id
JOIN
    core.employees e ON r.created_by = e.employee_id;

COMMENT ON VIEW policy_management.vw_policy_automation_rules IS 'Shows all policy automation rules with execution statistics and status';


CREATE OR REPLACE VIEW policy_management.vw_policy_benchmarking_comparison AS
SELECT
    p.policy_id,
    p.policy_name,
    p.version AS policy_version,
    p.status AS policy_status,
    b.benchmark_source,
    b.benchmark_name,
    b.benchmark_version,
    b.compliance_status,
    b.maturity_level,
    b.target_maturity_level,
    CASE
        WHEN b.maturity_level = 'Initial' AND b.target_maturity_level = 'Optimized' THEN 'Significant Gap'
        WHEN b.maturity_level = 'Developing' AND b.target_maturity_level IN ('Managed', 'Optimized') THEN 'Moderate Gap'
        WHEN b.maturity_level = 'Defined' AND b.target_maturity_level IN ('Optimized') THEN 'Small Gap'
        WHEN b.maturity_level = b.target_maturity_level THEN 'At Target'
        ELSE 'Minor Gap'
    END AS maturity_gap,
    e.employee_name AS benchmarked_by,
    b.benchmarked_at,
    b.next_benchmark_date,
    (SELECT COUNT(*) FROM policy_management.policy_ai_recommendations
     WHERE policy_id = p.policy_id AND status = 'Pending') AS pending_recommendations
FROM
    policy_management.policies p
JOIN
    policy_management.policy_benchmarking b ON p.policy_id = b.policy_id
LEFT JOIN
    core.employees e ON b.benchmarked_by = e.employee_id
WHERE
    b.benchmarked_at = (SELECT MAX(benchmarked_at)
                        FROM policy_management.policy_benchmarking
                        WHERE policy_id = p.policy_id);

COMMENT ON VIEW policy_management.vw_policy_benchmarking_comparison IS 'Compares policy maturity against benchmarked standards and identifies gaps';



CREATE MATERIALIZED VIEW policy_management.mv_policy_exception_trends_analysis AS
SELECT
    p.policy_id,
    p.policy_name,
    DATE_TRUNC('month', e.request_date) AS month,
    COUNT(DISTINCT e.exception_id) AS total_exceptions,
    AVG(a.risk_score) AS avg_risk_score,
    AVG(a.compliance_exposure_score) AS avg_compliance_exposure,
    AVG(a.operational_impact_score) AS avg_operational_impact,
    AVG(a.financial_impact_score) AS avg_financial_impact,
    MODE() WITHIN GROUP (ORDER BY a.exception_pattern_match) AS most_common_pattern,
    MODE() WITHIN GROUP (ORDER BY a.exception_trend) AS most_common_trend,
    COUNT(DISTINCT e.requester_id) AS unique_requesters,
    COUNT(DISTINCT d.department_id) AS departments_affected,
    (SELECT COUNT(DISTINCT e2.exception_id)
     FROM policy_management.policy_exceptions e2
     WHERE e2.policy_id = p.policy_id
       AND DATE_TRUNC('month', e2.request_date) = DATE_TRUNC('month', e.request_date - INTERVAL '1 year')
    ) AS year_ago_comparison,
    CURRENT_TIMESTAMP AS refresh_time
FROM
    policy_management.policies p
JOIN
    policy_management.policy_exceptions e ON p.policy_id = e.policy_id
LEFT JOIN
    policy_management.policy_exception_analytics a ON e.exception_id = a.exception_id
LEFT JOIN
    core.employees emp ON e.requester_id = emp.employee_id
LEFT JOIN
    core.departments d ON emp.department_id = d.department_id
GROUP BY
    p.policy_id, p.policy_name, DATE_TRUNC('month', e.request_date);

COMMENT ON MATERIALIZED VIEW policy_management.mv_policy_exception_trends_analysis IS 'Provides advanced trend analysis of policy exceptions with risk and impact scoring';


CREATE MATERIALIZED VIEW policy_management.mv_policy_automation_effectiveness AS
SELECT
    r.rule_id,
    r.rule_name,
    p.policy_id,
    p.policy_name,
    r.action_type,
    COUNT(DISTINCT a.audit_id) AS execution_count,
    COUNT(DISTINCT CASE WHEN a.additional_context LIKE '%Success%' THEN a.audit_id END) AS success_count,
    COUNT(DISTINCT CASE WHEN a.additional_context LIKE '%Failed%' THEN a.audit_id END) AS failure_count,
    AVG(CASE WHEN a.additional_context LIKE '%Processing Time:%' THEN
        CAST(SUBSTRING(a.additional_context FROM '%Processing Time: ([0-9]+)%') AS INT
    END) AS avg_processing_time_ms,
    MAX(a.changed_at) AS last_execution,
    (SELECT COUNT(DISTINCT e.exception_id)
     FROM policy_management.policy_exceptions e
     JOIN policy_management.policy_automation_rules r2 ON e.policy_id = r2.policy_id
     WHERE r2.rule_id = r.rule_id
       AND e.request_date >= r.last_triggered_at - INTERVAL '30 days'
    ) AS exceptions_since_last_trigger,
    CURRENT_TIMESTAMP AS refresh_time
FROM
    policy_management.policy_automation_rules r
LEFT JOIN
    policy_management.policies p ON r.policy_id = p.policy_id
LEFT JOIN
    policy_management.policy_audit_trail a ON a.additional_context LIKE '%Rule ID: ' || r.rule_id || '%'
GROUP BY
    r.rule_id, r.rule_name, p.policy_id, p.policy_name, r.action_type;

COMMENT ON MATERIALIZED VIEW policy_management.mv_policy_automation_effectiveness IS 'Tracks the effectiveness of policy automation rules with execution metrics';



CREATE MATERIALIZED VIEW policy_management.mv_policy_maturity_assessment AS
SELECT
    p.policy_id,
    p.policy_name,
    p.version,
    p.status,
    p.effective_date,
    p.next_review_date,
    -- Compliance maturity
    (SELECT AVG(
        CASE
            WHEN b.compliance_status = 'Compliant' THEN 5
            WHEN b.compliance_status = 'Partial' THEN 3
            WHEN b.compliance_status = 'Non-Compliant' THEN 1
            ELSE 0
        END)
     FROM policy_management.policy_benchmarking b
     WHERE b.policy_id = p.policy_id
    ) AS compliance_maturity_score,
    -- Implementation maturity
    (SELECT
        CASE
            WHEN COUNT(*) = 0 THEN 0
            WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') = COUNT(*) THEN 5
            WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') >= COUNT(*) * 0.8 THEN 4
            WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') >= COUNT(*) * 0.6 THEN 3
            WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') >= COUNT(*) * 0.4 THEN 2
            ELSE 1
        END
     FROM policy_management.policy_implementation_checklist c
     WHERE c.policy_id = p.policy_id
    ) AS implementation_maturity_score,
    -- Training maturity
    (SELECT
        CASE
            WHEN COUNT(*) = 0 THEN 0
            WHEN AVG(e.knowledge_gain_score) >= 85 AND AVG(e.behavior_change_score) >= 75 THEN 5
            WHEN AVG(e.knowledge_gain_score) >= 75 AND AVG(e.behavior_change_score) >= 65 THEN 4
            WHEN AVG(e.knowledge_gain_score) >= 65 AND AVG(e.behavior_change_score) >= 55 THEN 3
            WHEN AVG(e.knowledge_gain_score) >= 55 AND AVG(e.behavior_change_score) >= 45 THEN 2
            ELSE 1
        END
     FROM policy_management.policy_training t
     LEFT JOIN policy_management.policy_training_effectiveness e ON t.training_id = e.training_id
     WHERE t.policy_id = p.policy_id
    ) AS training_maturity_score,
    -- Exception maturity
    (SELECT
        CASE
            WHEN COUNT(*) = 0 THEN 5
            WHEN AVG(a.risk_score) <= 2.0 AND COUNT(*) <= 3 THEN 4
            WHEN AVG(a.risk_score) <= 3.0 AND COUNT(*) <= 5 THEN 3
            WHEN AVG(a.risk_score) <= 4.0 AND COUNT(*) <= 10 THEN 2
            ELSE 1
        END
     FROM policy_management.policy_exceptions e
     LEFT JOIN policy_management.policy_exception_analytics a ON e.exception_id = a.exception_id
     WHERE e.policy_id = p.policy_id
       AND e.status = 'Approved'
       AND (e.exception_end_date IS NULL OR e.exception_end_date >= CURRENT_DATE)
    ) AS exception_maturity_score,
    -- Overall maturity
    CASE
        WHEN (SELECT COUNT(*) FROM policy_management.policy_benchmarking WHERE policy_id = p.policy_id) = 0 THEN 'Not Assessed'
        WHEN ((SELECT AVG(
                CASE
                    WHEN b.compliance_status = 'Compliant' THEN 5
                    WHEN b.compliance_status = 'Partial' THEN 3
                    WHEN b.compliance_status = 'Non-Compliant' THEN 1
                    ELSE 0
                END)
             FROM policy_management.policy_benchmarking b
             WHERE b.policy_id = p.policy_id) +
             (SELECT
                CASE
                    WHEN COUNT(*) = 0 THEN 0
                    WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') = COUNT(*) THEN 5
                    WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') >= COUNT(*) * 0.8 THEN 4
                    WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') >= COUNT(*) * 0.6 THEN 3
                    WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') >= COUNT(*) * 0.4 THEN 2
                    ELSE 1
                END
             FROM policy_management.policy_implementation_checklist c
             WHERE c.policy_id = p.policy_id) +
             (SELECT
                CASE
                    WHEN COUNT(*) = 0 THEN 0
                    WHEN AVG(e.knowledge_gain_score) >= 85 AND AVG(e.behavior_change_score) >= 75 THEN 5
                    WHEN AVG(e.knowledge_gain_score) >= 75 AND AVG(e.behavior_change_score) >= 65 THEN 4
                    WHEN AVG(e.knowledge_gain_score) >= 65 AND AVG(e.behavior_change_score) >= 55 THEN 3
                    WHEN AVG(e.knowledge_gain_score) >= 55 AND AVG(e.behavior_change_score) >= 45 THEN 2
                    ELSE 1
                END
             FROM policy_management.policy_training t
             LEFT JOIN policy_management.policy_training_effectiveness e ON t.training_id = e.training_id
             WHERE t.policy_id = p.policy_id) +
             (SELECT
                CASE
                    WHEN COUNT(*) = 0 THEN 5
                    WHEN AVG(a.risk_score) <= 2.0 AND COUNT(*) <= 3 THEN 4
                    WHEN AVG(a.risk_score) <= 3.0 AND COUNT(*) <= 5 THEN 3
                    WHEN AVG(a.risk_score) <= 4.0 AND COUNT(*) <= 10 THEN 2
                    ELSE 1
                END
             FROM policy_management.policy_exceptions e
             LEFT JOIN policy_management.policy_exception_analytics a ON e.exception_id = a.exception_id
             WHERE e.policy_id = p.policy_id
               AND e.status = 'Approved'
               AND (e.exception_end_date IS NULL OR e.exception_end_date >= CURRENT_DATE))
            ) / 4 >= 4.5 THEN 'Optimized'
        WHEN ((SELECT AVG(
                CASE
                    WHEN b.compliance_status = 'Compliant' THEN 5
                    WHEN b.compliance_status = 'Partial' THEN 3
                    WHEN b.compliance_status = 'Non-Compliant' THEN 1
                    ELSE 0
                END)
             FROM policy_management.policy_benchmarking b
             WHERE b.policy_id = p.policy_id) +
             (SELECT
                CASE
                    WHEN COUNT(*) = 0 THEN 0
                    WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') = COUNT(*) THEN 5
                    WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') >= COUNT(*) * 0.8 THEN 4
                    WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') >= COUNT(*) * 0.6 THEN 3
                    WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') >= COUNT(*) * 0.4 THEN 2
                    ELSE 1
                END
             FROM policy_management.policy_implementation_checklist c
             WHERE c.policy_id = p.policy_id) +
             (SELECT
                CASE
                    WHEN COUNT(*) = 0 THEN 0
                    WHEN AVG(e.knowledge_gain_score) >= 85 AND AVG(e.behavior_change_score) >= 75 THEN 5
                    WHEN AVG(e.knowledge_gain_score) >= 75 AND AVG(e.behavior_change_score) >= 65 THEN 4
                    WHEN AVG(e.knowledge_gain_score) >= 65 AND AVG(e.behavior_change_score) >= 55 THEN 3
                    WHEN AVG(e.knowledge_gain_score) >= 55 AND AVG(e.behavior_change_score) >= 45 THEN 2
                    ELSE 1
                END
             FROM policy_management.policy_training t
             LEFT JOIN policy_management.policy_training_effectiveness e ON t.training_id = e.training_id
             WHERE t.policy_id = p.policy_id) +
             (SELECT
                CASE
                    WHEN COUNT(*) = 0 THEN 5
                    WHEN AVG(a.risk_score) <= 2.0 AND COUNT(*) <= 3 THEN 4
                    WHEN AVG(a.risk_score) <= 3.0 AND COUNT(*) <= 5 THEN 3
                    WHEN AVG(a.risk_score) <= 4.0 AND COUNT(*) <= 10 THEN 2
                    ELSE 1
                END
             FROM policy_management.policy_exceptions e
             LEFT JOIN policy_management.policy_exception_analytics a ON e.exception_id = a.exception_id
             WHERE e.policy_id = p.policy_id
               AND e.status = 'Approved'
               AND (e.exception_end_date IS NULL OR e.exception_end_date >= CURRENT_DATE))
            ) / 4 >= 3.5 THEN 'Managed'
        WHEN ((SELECT AVG(
                CASE
                    WHEN b.compliance_status = 'Compliant' THEN 5
                    WHEN b.compliance_status = 'Partial' THEN 3
                    WHEN b.compliance_status = 'Non-Compliant' THEN 1
                    ELSE 0
                END)
             FROM policy_management.policy_benchmarking b
             WHERE b.policy_id = p.policy_id) +
             (SELECT
                CASE
                    WHEN COUNT(*) = 0 THEN 0
                    WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') = COUNT(*) THEN 5
                    WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') >= COUNT(*) * 0.8 THEN 4
                    WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') >= COUNT(*) * 0.6 THEN 3
                    WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') >= COUNT(*) * 0.4 THEN 2
                    ELSE 1
                END
             FROM policy_management.policy_implementation_checklist c
             WHERE c.policy_id = p.policy_id) +
             (SELECT
                CASE
                    WHEN COUNT(*) = 0 THEN 0
                    WHEN AVG(e.knowledge_gain_score) >= 85 AND AVG(e.behavior_change_score) >= 75 THEN 5
                    WHEN AVG(e.knowledge_gain_score) >= 75 AND AVG(e.behavior_change_score) >= 65 THEN 4
                    WHEN AVG(e.knowledge_gain_score) >= 65 AND AVG(e.behavior_change_score) >= 55 THEN 3
                    WHEN AVG(e.knowledge_gain_score) >= 55 AND AVG(e.behavior_change_score) >= 45 THEN 2
                    ELSE 1
                END
             FROM policy_management.policy_training t
             LEFT JOIN policy_management.policy_training_effectiveness e ON t.training_id = e.training_id
             WHERE t.policy_id = p.policy_id) +
             (SELECT
                CASE
                    WHEN COUNT(*) = 0 THEN 5
                    WHEN AVG(a.risk_score) <= 2.0 AND COUNT(*) <= 3 THEN 4
                    WHEN AVG(a.risk_score) <= 3.0 AND COUNT(*) <= 5 THEN 3
                    WHEN AVG(a.risk_score) <= 4.0 AND COUNT(*) <= 10 THEN 2
                    ELSE 1
                END
             FROM policy_management.policy_exceptions e
             LEFT JOIN policy_management.policy_exception_analytics a ON e.exception_id = a.exception_id
             WHERE e.policy_id = p.policy_id
               AND e.status = 'Approved'
               AND (e.exception_end_date IS NULL OR e.exception_end_date >= CURRENT_DATE))
            ) / 4 >= 2.5 THEN 'Defined'
        WHEN ((SELECT AVG(
                CASE
                    WHEN b.compliance_status = 'Compliant' THEN 5
                    WHEN b.compliance_status = 'Partial' THEN 3
                    WHEN b.compliance_status = 'Non-Compliant' THEN 1
                    ELSE 0
                END)
             FROM policy_management.policy_benchmarking b
             WHERE b.policy_id = p.policy_id) +
             (SELECT
                CASE
                    WHEN COUNT(*) = 0 THEN 0
                    WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') = COUNT(*) THEN 5
                    WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') >= COUNT(*) * 0.8 THEN 4
                    WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') >= COUNT(*) * 0.6 THEN 3
                    WHEN COUNT(*) FILTER (WHERE completion_status = 'Completed') >= COUNT(*) * 0.4 THEN 2
                    ELSE 1
                END
             FROM policy_management.policy_implementation_checklist c
             WHERE c.policy_id = p.policy_id) +
             (SELECT
                CASE
                    WHEN COUNT(*) = 0 THEN 0
                    WHEN AVG(e.knowledge_gain_score) >= 85 AND AVG(e.behavior_change_score) >= 75 THEN 5
                    WHEN AVG(e.knowledge_gain_score) >= 75 AND AVG(e.behavior_change_score) >= 65 THEN 4
                    WHEN AVG(e.knowledge_gain_score) >= 65 AND AVG(e.behavior_change_score) >= 55 THEN 3
                    WHEN AVG(e.knowledge_gain_score) >= 55 AND AVG(e.behavior_change_score) >= 45 THEN 2
                    ELSE 1
                END
             FROM policy_management.policy_training t
             LEFT JOIN policy_management.policy_training_effectiveness e ON t.training_id = e.training_id
             WHERE t.policy_id = p.policy_id) +
             (SELECT
                CASE
                    WHEN COUNT(*) = 0 THEN 5
                    WHEN AVG(a.risk_score) <= 2.0 AND COUNT(*) <= 3 THEN 4
                    WHEN AVG(a.risk_score) <= 3.0 AND COUNT(*) <= 5 THEN 3
                    WHEN AVG(a.risk_score) <= 4.0 AND COUNT(*) <= 10 THEN 2
                    ELSE 1
                END
             FROM policy_management.policy_exceptions e
             LEFT JOIN policy_management.policy_exception_analytics a ON e.exception_id = a.exception_id
             WHERE e.policy_id = p.policy_id
               AND e.status = 'Approved'
               AND (e.exception_end_date IS NULL OR e.exception_end_date >= CURRENT_DATE))
            ) / 4 >= 1.5 THEN 'Developing'
        ELSE 'Initial'
    END AS overall_maturity_level,
    CURRENT_TIMESTAMP AS refresh_time
FROM
    policy_management.policies p
WHERE
    p.status IN ('Active', 'Under Review');

COMMENT ON MATERIALIZED VIEW policy_management.mv_policy_maturity_assessment IS 'Provides a comprehensive maturity assessment for policies across multiple dimensions';


CREATE TABLE policy_management.policy_change_impact_log (
    impact_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    change_version VARCHAR(20) NOT NULL,
    impact_type VARCHAR(50) NOT NULL CHECK (impact_type IN ('Process', 'System', 'Role', 'Compliance', 'Risk')),
    impacted_area VARCHAR(100) NOT NULL,
    impact_description TEXT NOT NULL,
    severity VARCHAR(20) CHECK (severity IN ('Low', 'Medium', 'High', 'Critical')),
    mitigation_plan TEXT,
    responsible_party_id INT REFERENCES core.employees(employee_id),
    target_completion_date DATE,
    status VARCHAR(50) CHECK (status IN ('Pending', 'In Progress', 'Completed', 'Deferred')),
    completed_at TIMESTAMP,
    verified_by INT REFERENCES core.employees(employee_id),
    verified_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by INT REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE policy_management.policy_change_impact_log IS 'Tracks downstream impacts of policy changes across the organization';


CREATE TABLE policy_management.policy_access_request (
    request_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    requester_id INT NOT NULL REFERENCES core.employees(employee_id),
    request_type VARCHAR(50) NOT NULL CHECK (request_type IN ('View', 'Edit', 'Approve', 'Exception')),
    justification TEXT NOT NULL,
    requested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status VARCHAR(50) CHECK (status IN ('Pending', 'Approved', 'Rejected', 'Revoked')),
    reviewed_by INT REFERENCES core.employees(employee_id),
    reviewed_at TIMESTAMP,
    review_comments TEXT,
    access_duration_days INT,
    access_expires_at TIMESTAMP,
    ip_restriction VARCHAR(100),
    device_restriction VARCHAR(100),
    additional_parameters JSONB
);

COMMENT ON TABLE policy_management.policy_access_request IS 'Manages requests for access to policies beyond standard permissions';


CREATE TABLE policy_management.policy_knowledge_graph (
    relationship_id SERIAL PRIMARY KEY,
    source_policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    target_policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    relationship_type VARCHAR(50) NOT NULL CHECK (relationship_type IN ('References', 'Conflicts', 'Depends', 'Extends', 'Replaces')),
    relationship_strength DECIMAL(3,2) CHECK (relationship_strength BETWEEN 0 AND 1),
    automated_detection BOOLEAN DEFAULT FALSE,
    last_validated_at TIMESTAMP,
    validated_by INT REFERENCES core.employees(employee_id),
    notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_policy_knowledge_graph_source ON policy_management.policy_knowledge_graph(source_policy_id);
CREATE INDEX idx_policy_knowledge_graph_target ON policy_management.policy_knowledge_graph(target_policy_id);

COMMENT ON TABLE policy_management.policy_knowledge_graph IS 'Stores relationships between policies to create a network graph of policy dependencies';



CREATE TABLE policy_management.policy_scenario_testing (
    scenario_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    scenario_name VARCHAR(100) NOT NULL,
    scenario_description TEXT,
    test_type VARCHAR(50) NOT NULL CHECK (test_type IN ('Compliance', 'Risk', 'Exception', 'Edge Case')),
    test_parameters JSONB NOT NULL,
    expected_result TEXT NOT NULL,
    actual_result TEXT,
    test_status VARCHAR(50) CHECK (test_status IN ('Pending', 'Passed', 'Failed', 'Inconclusive')),
    tested_by INT REFERENCES core.employees(employee_id),
    tested_at TIMESTAMP,
    test_duration_seconds INT,
    environment VARCHAR(50) CHECK (environment IN ('Production', 'Staging', 'Development', 'Sandbox')),
    notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by INT REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE policy_management.policy_scenario_testing IS 'Stores test scenarios for policy validation before implementation';


CREATE TABLE policy_management.policy_metadata_registry (
    metadata_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    metadata_type VARCHAR(50) NOT NULL CHECK (metadata_type IN ('Classification', 'Jurisdiction', 'Data Category', 'Retention', 'Ownership')),
    metadata_key VARCHAR(100) NOT NULL,
    metadata_value TEXT NOT NULL,
    source VARCHAR(100) CHECK (source IN ('Manual', 'Auto-Detected', 'System', 'Import')),
    confidence_score DECIMAL(3,2) CHECK (confidence_score BETWEEN 0 AND 1),
    valid_from TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    valid_to TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by INT REFERENCES core.employees(employee_id)
);

CREATE INDEX idx_policy_metadata_policy ON policy_management.policy_metadata_registry(policy_id);
CREATE INDEX idx_policy_metadata_type ON policy_management.policy_metadata_registry(metadata_type);

COMMENT ON TABLE policy_management.policy_metadata_registry IS 'Stores rich metadata about policies for advanced classification and discovery';



CREATE TABLE policy_management.policy_optimization_history (
    optimization_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    optimization_type VARCHAR(50) NOT NULL CHECK (optimization_type IN ('Language', 'Structure', 'Compliance', 'Efficiency')),
    original_value TEXT,
    optimized_value TEXT,
    optimization_method VARCHAR(100),
    improvement_metrics JSONB,
    applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    applied_by INT REFERENCES core.employees(employee_id),
    review_status VARCHAR(50) CHECK (review_status IN ('Pending', 'Approved', 'Rejected')),
    reviewed_by INT REFERENCES core.employees(employee_id),
    reviewed_at TIMESTAMP,
    notes TEXT
);

COMMENT ON TABLE policy_management.policy_optimization_history IS 'Tracks optimizations made to policy content and structure over time';



CREATE TABLE policy_management.policy_delegation_log (
    delegation_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    delegator_id INT NOT NULL REFERENCES core.employees(employee_id),
    delegatee_id INT NOT NULL REFERENCES core.employees(employee_id),
    delegation_type VARCHAR(50) NOT NULL CHECK (delegation_type IN ('Approval', 'Review', 'Ownership', 'Implementation')),
    start_date TIMESTAMP NOT NULL,
    end_date TIMESTAMP,
    status VARCHAR(50) CHECK (status IN ('Active', 'Expired', 'Revoked', 'Completed')),
    permissions_granted JSONB NOT NULL,
    justification TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by INT REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE policy_management.policy_delegation_log IS 'Temporary delegations of policy responsibilities between employees';


CREATE OR REPLACE VIEW policy_management.vw_policy_impact_analysis AS
SELECT
    p.policy_id,
    p.policy_name,
    p.version,
    COUNT(DISTINCT cil.impact_id) AS total_impacts,
    COUNT(DISTINCT CASE WHEN cil.severity = 'Critical' THEN cil.impact_id END) AS critical_impacts,
    COUNT(DISTINCT CASE WHEN cil.severity = 'High' THEN cil.impact_id END) AS high_impacts,
    COUNT(DISTINCT CASE WHEN cil.impact_type = 'Process' THEN cil.impact_id END) AS process_impacts,
    COUNT(DISTINCT CASE WHEN cil.impact_type = 'System' THEN cil.impact_id END) AS system_impacts,
    COUNT(DISTINCT CASE WHEN cil.status = 'Pending' THEN cil.impact_id END) AS pending_impacts,
    COUNT(DISTINCT CASE WHEN cil.status = 'Completed' THEN cil.impact_id END) AS completed_impacts,
    MIN(CASE WHEN cil.status = 'Pending' THEN cil.target_completion_date END) AS next_due_date,
    STRING_AGG(DISTINCT cil.impacted_area, ', ') AS impacted_areas
FROM
    policy_management.policies p
LEFT JOIN
    policy_management.policy_change_impact_log cil ON p.policy_id = cil.policy_id
GROUP BY
    p.policy_id, p.policy_name, p.version;

COMMENT ON VIEW policy_management.vw_policy_impact_analysis IS 'Provides an overview of policy change impacts across the organization';



CREATE OR REPLACE VIEW policy_management.vw_policy_knowledge_graph AS
SELECT
    p1.policy_id AS source_policy_id,
    p1.policy_name AS source_policy_name,
    p2.policy_id AS target_policy_id,
    p2.policy_name AS target_policy_name,
    kg.relationship_type,
    kg.relationship_strength,
    kg.last_validated_at,
    COUNT(DISTINCT v.violation_id) AS shared_violations,
    COUNT(DISTINCT e.exception_id) AS shared_exceptions
FROM
    policy_management.policy_knowledge_graph kg
JOIN
    policy_management.policies p1 ON kg.source_policy_id = p1.policy_id
JOIN
    policy_management.policies p2 ON kg.target_policy_id = p2.policy_id
LEFT JOIN
    policy_management.policy_violations v ON
        (v.policy_id = p1.policy_id AND EXISTS (
            SELECT 1 FROM policy_management.policy_violations
            WHERE policy_id = p2.policy_id AND employee_id = v.employee_id
        )) OR
        (v.policy_id = p2.policy_id AND EXISTS (
            SELECT 1 FROM policy_management.policy_violations
            WHERE policy_id = p1.policy_id AND employee_id = v.employee_id
        ))
LEFT JOIN
    policy_management.policy_exceptions e ON
        (e.policy_id = p1.policy_id AND EXISTS (
            SELECT 1 FROM policy_management.policy_exceptions
            WHERE policy_id = p2.policy_id AND requester_id = e.requester_id
        )) OR
        (e.policy_id = p2.policy_id AND EXISTS (
            SELECT 1 FROM policy_management.policy_exceptions
            WHERE policy_id = p1.policy_id AND requester_id = e.requester_id
        ))
GROUP BY
    p1.policy_id, p1.policy_name, p2.policy_id, p2.policy_name,
    kg.relationship_type, kg.relationship_strength, kg.last_validated_at;

COMMENT ON VIEW policy_management.vw_policy_knowledge_graph IS 'Shows relationships between policies with shared violations and exceptions';


CREATE OR REPLACE VIEW policy_management.vw_policy_scenario_coverage AS
SELECT
    p.policy_id,
    p.policy_name,
    p.version,
    COUNT(DISTINCT st.scenario_id) AS total_scenarios,
    COUNT(DISTINCT CASE WHEN st.test_status = 'Passed' THEN st.scenario_id END) AS passed_scenarios,
    COUNT(DISTINCT CASE WHEN st.test_status = 'Failed' THEN st.scenario_id END) AS failed_scenarios,
    COUNT(DISTINCT CASE WHEN st.test_type = 'Compliance' THEN st.scenario_id END) AS compliance_scenarios,
    COUNT(DISTINCT CASE WHEN st.test_type = 'Risk' THEN st.scenario_id END) AS risk_scenarios,
    COUNT(DISTINCT CASE WHEN st.test_status IS NULL THEN st.scenario_id END) AS untested_scenarios,
    MAX(st.tested_at) AS last_tested_date,
    COUNT(DISTINCT st.tested_by) AS testers_count
FROM
    policy_management.policies p
LEFT JOIN
    policy_management.policy_scenario_testing st ON p.policy_id = st.policy_id
GROUP BY
    p.policy_id, p.policy_name, p.version;

COMMENT ON VIEW policy_management.vw_policy_scenario_coverage IS 'Shows testing coverage and results for policy validation scenarios';



CREATE MATERIALIZED VIEW policy_management.mv_policy_metadata_summary AS
SELECT
    p.policy_id,
    p.policy_name,
    p.version,
    p.status,
    (SELECT STRING_AGG(DISTINCT pmr.metadata_value, ', ')
     FROM policy_management.policy_metadata_registry pmr
     WHERE pmr.policy_id = p.policy_id AND pmr.metadata_type = 'Classification'
       AND (pmr.valid_to IS NULL OR pmr.valid_to > CURRENT_TIMESTAMP)
    ) AS classifications,
    (SELECT STRING_AGG(DISTINCT pmr.metadata_value, ', ')
     FROM policy_management.policy_metadata_registry pmr
     WHERE pmr.policy_id = p.policy_id AND pmr.metadata_type = 'Jurisdiction'
       AND (pmr.valid_to IS NULL OR pmr.valid_to > CURRENT_TIMESTAMP)
    ) AS jurisdictions,
    (SELECT STRING_AGG(DISTINCT pmr.metadata_key || ':' || pmr.metadata_value, '| ')
     FROM policy_management.policy_metadata_registry pmr
     WHERE pmr.policy_id = p.policy_id AND pmr.metadata_type = 'Data Category'
       AND (pmr.valid_to IS NULL OR pmr.valid_to > CURRENT_TIMESTAMP)
    ) AS data_categories,
    (SELECT MAX(pmr.metadata_value)
     FROM policy_management.policy_metadata_registry pmr
     WHERE pmr.policy_id = p.policy_id AND pmr.metadata_type = 'Retention'
       AND (pmr.valid_to IS NULL OR pmr.valid_to > CURRENT_TIMESTAMP)
    ) AS retention_period,
    COUNT(DISTINCT pmr.metadata_id) AS total_metadata_items,
    CURRENT_TIMESTAMP AS refresh_time
FROM
    policy_management.policies p
LEFT JOIN
    policy_management.policy_metadata_registry pmr ON p.policy_id = pmr.policy_id
    AND (pmr.valid_to IS NULL OR pmr.valid_to > CURRENT_TIMESTAMP)
GROUP BY
    p.policy_id, p.policy_name, p.version, p.status;

COMMENT ON MATERIALIZED VIEW policy_management.mv_policy_metadata_summary IS 'Aggregates policy metadata for quick searching and filtering';


CREATE MATERIALIZED VIEW policy_management.mv_policy_optimization_tracking AS
SELECT
    p.policy_id,
    p.policy_name,
    p.version,
    COUNT(DISTINCT poh.optimization_id) AS total_optimizations,
    COUNT(DISTINCT CASE WHEN poh.optimization_type = 'Language' THEN poh.optimization_id END) AS language_optimizations,
    COUNT(DISTINCT CASE WHEN poh.optimization_type = 'Structure' THEN poh.optimization_id END) AS structure_optimizations,
    COUNT(DISTINCT CASE WHEN poh.review_status = 'Approved' THEN poh.optimization_id END) AS approved_optimizations,
    MAX(poh.applied_at) AS last_optimized_date,
    (SELECT STRING_AGG(DISTINCT e.employee_name, ', ')
     FROM policy_management.policy_optimization_history poh
     JOIN core.employees e ON poh.applied_by = e.employee_id
     WHERE poh.policy_id = p.policy_id
    ) AS optimizers,
    AVG((poh.improvement_metrics->>'readability_score_change')::DECIMAL) AS avg_readability_improvement,
    AVG((poh.improvement_metrics->>'compliance_score_change')::DECIMAL) AS avg_compliance_improvement,
    CURRENT_TIMESTAMP AS refresh_time
FROM
    policy_management.policies p
LEFT JOIN
    policy_management.policy_optimization_history poh ON p.policy_id = poh.policy_id
GROUP BY
    p.policy_id, p.policy_name, p.version;

COMMENT ON MATERIALIZED VIEW policy_management.mv_policy_optimization_tracking IS 'Tracks optimization improvements made to policies over time';




CREATE MATERIALIZED VIEW policy_management.mv_policy_delegation_status AS
SELECT
    p.policy_id,
    p.policy_name,
    p.version,
    COUNT(DISTINCT dl.delegation_id) AS active_delegations,
    COUNT(DISTINCT CASE WHEN dl.delegation_type = 'Approval' THEN dl.delegation_id END) AS approval_delegations,
    COUNT(DISTINCT CASE WHEN dl.delegation_type = 'Ownership' THEN dl.delegation_id END) AS ownership_delegations,
    COUNT(DISTINCT dl.delegator_id) AS unique_delegators,
    COUNT(DISTINCT dl.delegatee_id) AS unique_delegatees,
    MIN(dl.end_date) AS next_expiration,
    (SELECT STRING_AGG(DISTINCT e.employee_name, ', ')
     FROM policy_management.policy_delegation_log dl
     JOIN core.employees e ON dl.delegatee_id = e.employee_id
     WHERE dl.policy_id = p.policy_id
       AND dl.status = 'Active'
    ) AS current_delegatees,
    CURRENT_TIMESTAMP AS refresh_time
FROM
    policy_management.policies p
LEFT JOIN
    policy_management.policy_delegation_log dl ON p.policy_id = dl.policy_id
    AND dl.status = 'Active'
    AND (dl.end_date IS NULL OR dl.end_date > CURRENT_TIMESTAMP)
GROUP BY
    p.policy_id, p.policy_name, p.version;

COMMENT ON MATERIALIZED VIEW policy_management.mv_policy_delegation_status IS 'Shows current delegation status for policy responsibilities';



CREATE OR REPLACE PROCEDURE policy_management.sp_generate_policy_impact_assessment(
    IN p_policy_id INT,
    IN p_assessment_type VARCHAR(50),
    IN p_assessed_by INT,
    OUT p_assessment_id INT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_policy RECORD;
    v_impact_areas TEXT[];
    v_impact_types TEXT[] := ARRAY['Process', 'System', 'Role', 'Compliance', 'Risk'];
    v_area TEXT;
    v_type TEXT;
BEGIN
    -- Get policy details
    SELECT * INTO v_policy FROM policy_management.policies WHERE policy_id = p_policy_id;

    -- Generate impact assessment record
    INSERT INTO policy_management.policy_impact_assessment (
        policy_id,
        assessment_date,
        assessment_type,
        assessed_by
    ) VALUES (
        p_policy_id,
        CURRENT_DATE,
        p_assessment_type,
        p_assessed_by
    ) RETURNING assessment_id INTO p_assessment_id;

    -- Identify potential impact areas (simplified example)
    v_impact_areas := ARRAY[
        'HR Department',
        'IT Systems',
        'Compliance Reporting',
        'Risk Management',
        'Financial Controls'
    ];

    -- Generate impact log entries for each area and type
    FOREACH v_area IN ARRAY v_impact_areas LOOP
        FOREACH v_type IN ARRAY v_impact_types LOOP
            INSERT INTO policy_management.policy_change_impact_log (
                policy_id,
                change_version,
                impact_type,
                impacted_area,
                impact_description,
                severity,
                created_by
            ) VALUES (
                p_policy_id,
                v_policy.version,
                v_type,
                v_area,
                'Potential ' || v_type || ' impact on ' || v_area || ' from policy ' || v_policy.policy_name,
                CASE
                    WHEN v_type IN ('Compliance', 'Risk') THEN 'High'
                    ELSE 'Medium'
                END,
                p_assessed_by
            );
        END LOOP;
    END LOOP;

    -- Update next assessment date on policy
    UPDATE policy_management.policies
    SET next_review_date = CURRENT_DATE + INTERVAL '6 months'
    WHERE policy_id = p_policy_id;

    -- Log the assessment generation
    INSERT INTO policy_management.policy_audit_trail (
        policy_id,
        audit_type,
        table_name,
        changed_by,
        additional_context
    ) VALUES (
        p_policy_id,
        'Impact Assessment',
        'policy_impact_assessment',
        p_assessed_by,
        'Generated initial impact assessment with ' || array_length(v_impact_areas, 1) * array_length(v_impact_types, 1) || ' potential impacts'
    );
END;
$$;

COMMENT ON PROCEDURE policy_management.sp_generate_policy_impact_assessment IS 'Automates the generation of initial policy impact assessments with potential impact areas';


CREATE OR REPLACE PROCEDURE policy_management.sp_update_policy_knowledge_graph(
    IN p_policy_id INT DEFAULT NULL,
    IN p_validate BOOLEAN DEFAULT TRUE,
    OUT p_relationships_found INT,
    OUT p_conflicts_identified INT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_policy RECORD;
    v_other_policy RECORD;
    v_relationship_strength DECIMAL(3,2);
    v_common_keywords INT;
    v_shared_violations INT;
    v_shared_exceptions INT;
    v_existing_relationship BOOLEAN;
BEGIN
    p_relationships_found := 0;
    p_conflicts_identified := 0;

    -- Process specific policy or all policies
    FOR v_policy IN
        SELECT * FROM policy_management.policies
        WHERE (p_policy_id IS NULL OR policy_id = p_policy_id)
          AND status = 'Active'
    LOOP
        -- Compare with other policies
        FOR v_other_policy IN
            SELECT * FROM policy_management.policies
            WHERE policy_id != v_policy.policy_id
              AND status = 'Active'
        LOOP
            -- Calculate relationship strength factors (simplified example)
            -- 1. Common keywords in policy name and description
            SELECT COUNT(*) INTO v_common_keywords
            FROM (
                SELECT unnest(regexp_split_to_array(LOWER(v_policy.policy_name), '\s+') AS word
                INTERSECT
                SELECT unnest(regexp_split_to_array(LOWER(v_other_policy.policy_name), '\s+')
            ) AS common_words;

            -- 2. Shared violations
            SELECT COUNT(*) INTO v_shared_violations
            FROM (
                SELECT employee_id FROM policy_management.policy_violations WHERE policy_id = v_policy.policy_id
                INTERSECT
                SELECT employee_id FROM policy_management.policy_violations WHERE policy_id = v_other_policy.policy_id
            ) AS shared_violators;

            -- 3. Shared exceptions
            SELECT COUNT(*) INTO v_shared_exceptions
            FROM (
                SELECT requester_id FROM policy_management.policy_exceptions WHERE policy_id = v_policy.policy_id
                INTERSECT
                SELECT requester_id FROM policy_management.policy_exceptions WHERE policy_id = v_other_policy.policy_id
            ) AS shared_requesters;

            -- Calculate overall relationship strength
            v_relationship_strength := LEAST(0.3 * v_common_keywords + 0.4 * v_shared_violations + 0.3 * v_shared_exceptions, 1.0);

            -- Determine relationship type
            IF v_relationship_strength >= 0.7 THEN
                -- Check if relationship already exists
                SELECT EXISTS(
                    SELECT 1 FROM policy_management.policy_knowledge_graph
                    WHERE source_policy_id = v_policy.policy_id
                      AND target_policy_id = v_other_policy.policy_id
                ) INTO v_existing_relationship;

                IF NOT v_existing_relationship AND v_relationship_strength > 0 THEN
                    -- Insert new relationship
                    INSERT INTO policy_management.policy_knowledge_graph (
                        source_policy_id,
                        target_policy_id,
                        relationship_type,
                        relationship_strength,
                        automated_detection,
                        last_validated_at
                    ) VALUES (
                        v_policy.policy_id,
                        v_other_policy.policy_id,
                        CASE
                            WHEN v_shared_violations > 3 THEN 'Conflicts'
                            WHEN v_common_keywords > 5 THEN 'References'
                            ELSE 'Depends'
                        END,
                        v_relationship_strength,
                        TRUE,
                        CASE WHEN p_validate THEN CURRENT_TIMESTAMP ELSE NULL END
                    );

                    p_relationships_found := p_relationships_found + 1;

                    -- Count conflicts
                    IF v_shared_violations > 3 THEN
                        p_conflicts_identified := p_conflicts_identified + 1;
                    END IF;
                END IF;
            END IF;
        END LOOP;
    END LOOP;

    -- Log the update
    INSERT INTO policy_management.policy_audit_trail (
        audit_type,
        table_name,
        changed_by,
        additional_context
    ) VALUES (
        'System Job',
        'policy_knowledge_graph',
        1, -- System user
        'Updated policy knowledge graph: ' || p_relationships_found || ' relationships found, ' ||
        p_conflicts_identified || ' conflicts identified'
    );
END;
$$;

COMMENT ON PROCEDURE policy_management.sp_update_policy_knowledge_graph IS 'Automatically updates policy relationships in the knowledge graph based on common keywords, shared violations, and exceptions';


CREATE TABLE policy_management.policy_compliance_evidence (
    evidence_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    regulation_id INT REFERENCES core.regulations(regulation_id),
    evidence_type VARCHAR(50) NOT NULL CHECK (evidence_type IN ('Document', 'Screenshot', 'Log', 'Test Result', 'Audit Report')),
    evidence_name VARCHAR(255) NOT NULL,
    evidence_description TEXT,
    capture_date TIMESTAMP NOT NULL,
    capture_method VARCHAR(100),
    captured_by INT REFERENCES core.employees(employee_id),
    file_path VARCHAR(512) NOT NULL,
    file_hash VARCHAR(64) NOT NULL,
    is_valid BOOLEAN DEFAULT TRUE,
    validity_end_date TIMESTAMP,
    reviewed_by INT REFERENCES core.employees(employee_id),
    reviewed_at TIMESTAMP,
    review_comments TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_compliance_evidence IS 'Stores evidence demonstrating compliance with policies and regulations';


CREATE TABLE policy_management.policy_adoption_metrics (
    metric_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    measurement_date DATE NOT NULL,
    adoption_rate DECIMAL(5,2) CHECK (adoption_rate BETWEEN 0 AND 100),
    awareness_score DECIMAL(5,2) CHECK (awareness_score BETWEEN 0 AND 100),
    understanding_score DECIMAL(5,2) CHECK (understanding_score BETWEEN 0 AND 100),
    compliance_score DECIMAL(5,2) CHECK (compliance_score BETWEEN 0 AND 100),
    department_id INT REFERENCES core.departments(department_id),
    measurement_method VARCHAR(100) NOT NULL,
    sample_size INT,
    confidence_level DECIMAL(3,2) CHECK (confidence_level BETWEEN 0 AND 1),
    notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by INT REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE policy_management.policy_adoption_metrics IS 'Tracks organizational adoption and understanding of policies over time';


CREATE TABLE policy_management.policy_exception_pattern (
    pattern_id SERIAL PRIMARY KEY,
    pattern_name VARCHAR(100) NOT NULL,
    pattern_description TEXT,
    policy_id INT REFERENCES policy_management.policies(policy_id),
    common_justification_keywords TEXT[],
    typical_duration_days INT,
    frequent_requester_roles TEXT[],
    risk_profile VARCHAR(50) CHECK (risk_profile IN ('Low', 'Medium', 'High', 'Critical')),
    mitigation_strategy TEXT,
    detection_algorithm TEXT,
    is_active BOOLEAN DEFAULT TRUE,
    last_detected_at TIMESTAMP,
    detection_count INT DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_by INT REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE policy_management.policy_exception_pattern IS 'Identifies and tracks common patterns in policy exceptions for proactive management';


CREATE TABLE policy_management.policy_content_analytics (
    analysis_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    analysis_date TIMESTAMP NOT NULL,
    readability_score DECIMAL(5,2),
    complexity_score DECIMAL(5,2),
    ambiguity_index DECIMAL(5,2),
    key_term_count JSONB, -- {"term": "count"} pairs
    sentence_length_stats JSONB, -- {avg, max, min}
    paragraph_length_stats JSONB,
    document_structure_score DECIMAL(5,2),
    compliance_keyword_coverage DECIMAL(5,2),
    analysis_method VARCHAR(100) NOT NULL,
    recommendations TEXT,
    analyzed_by INT REFERENCES core.employees(employee_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_content_analytics IS 'Stores detailed analysis of policy document content quality and characteristics';


CREATE TABLE policy_management.policy_integration_audit (
    audit_id SERIAL PRIMARY KEY,
    integration_id INT REFERENCES policy_management.policy_integration_points(integration_id),
    policy_id INT REFERENCES policy_management.policies(policy_id),
    system_name VARCHAR(100) NOT NULL,
    audit_type VARCHAR(50) NOT NULL CHECK (audit_type IN ('Sync', 'Validation', 'Error', 'Manual Review')),
    status VARCHAR(50) NOT NULL CHECK (status IN ('Success', 'Partial', 'Failed', 'Pending')),
    records_processed INT,
    records_failed INT,
    error_details TEXT,
    start_time TIMESTAMP NOT NULL,
    end_time TIMESTAMP,
    duration_seconds INT,
    initiated_by VARCHAR(100), -- System or user
    additional_context JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_integration_audit IS 'Tracks all integration activities between policy system and other enterprise systems';

CREATE TABLE policy_management.policy_stakeholder_sentiment (
    sentiment_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    stakeholder_id INT NOT NULL REFERENCES core.employees(employee_id),
    sentiment_date DATE NOT NULL,
    sentiment_score DECIMAL(3,2) CHECK (sentiment_score BETWEEN -1 AND 1),
    sentiment_category VARCHAR(50) CHECK (sentiment_category IN ('Strongly Negative', 'Negative', 'Neutral', 'Positive', 'Strongly Positive')),
    feedback_channel VARCHAR(50) CHECK (feedback_channel IN ('Survey', 'Interview', 'Email', 'Meeting', 'System')),
    key_themes TEXT[],
    concerns_raised TEXT,
    suggestions TEXT,
    is_anonymous BOOLEAN DEFAULT FALSE,
    collected_by INT REFERENCES core.employees(employee_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_stakeholder_sentiment IS 'Captures stakeholder sentiment and feedback about specific policies over time';


CREATE OR REPLACE VIEW policy_management.vw_policy_compliance_evidence AS
SELECT
    p.policy_id,
    p.policy_name,
    r.regulation_name,
    r.regulation_code,
    COUNT(e.evidence_id) AS total_evidence,
    COUNT(e.evidence_id) FILTER (WHERE e.is_valid = TRUE) AS valid_evidence,
    COUNT(e.evidence_id) FILTER (WHERE e.reviewed_at IS NOT NULL) AS reviewed_evidence,
    MIN(e.capture_date) AS earliest_evidence,
    MAX(e.capture_date) AS latest_evidence,
    COUNT(DISTINCT e.captured_by) AS evidence_providers,
    (SELECT STRING_AGG(DISTINCT e2.evidence_type, ', ')
     FROM policy_management.policy_compliance_evidence e2
     WHERE e2.policy_id = p.policy_id
    ) AS evidence_types
FROM
    policy_management.policies p
LEFT JOIN
    policy_management.policy_compliance_evidence e ON p.policy_id = e.policy_id
LEFT JOIN
    core.regulations r ON e.regulation_id = r.regulation_id
GROUP BY
    p.policy_id, p.policy_name, r.regulation_name, r.regulation_code;

COMMENT ON VIEW policy_management.vw_policy_compliance_evidence IS 'Provides an overview of compliance evidence collected for each policy';


CREATE OR REPLACE VIEW policy_management.vw_policy_adoption_dashboard AS
SELECT
    p.policy_id,
    p.policy_name,
    p.version,
    d.department_name,
    am.measurement_date,
    am.adoption_rate,
    am.awareness_score,
    am.understanding_score,
    am.compliance_score,
    am.sample_size,
    am.confidence_level,
    LAG(am.adoption_rate) OVER (PARTITION BY p.policy_id, d.department_id ORDER BY am.measurement_date) AS previous_adoption_rate,
    LAG(am.awareness_score) OVER (PARTITION BY p.policy_id, d.department_id ORDER BY am.measurement_date) AS previous_awareness_score,
    (SELECT STRING_AGG(DISTINCT am2.measurement_method, ', ')
     FROM policy_management.policy_adoption_metrics am2
     WHERE am2.policy_id = p.policy_id
       AND (am2.department_id = d.department_id OR (am2.department_id IS NULL AND d.department_id IS NULL))
    ) AS measurement_methods,
    (SELECT COUNT(DISTINCT am3.measurement_date)
     FROM policy_management.policy_adoption_metrics am3
     WHERE am3.policy_id = p.policy_id
       AND (am3.department_id = d.department_id OR (am3.department_id IS NULL AND d.department_id IS NULL))
    ) AS measurement_count
FROM
    policy_management.policies p
JOIN
    policy_management.policy_adoption_metrics am ON p.policy_id = am.policy_id
LEFT JOIN
    core.departments d ON am.department_id = d.department_id
WHERE
    am.measurement_date = (SELECT MAX(measurement_date)
                          FROM policy_management.policy_adoption_metrics
                          WHERE policy_id = am.policy_id
                            AND (department_id = am.department_id
                                 OR (am.department_id IS NULL AND department_id IS NULL)));

COMMENT ON VIEW policy_management.vw_policy_adoption_dashboard IS 'Shows current adoption metrics for policies across departments with trend comparison';


CREATE OR REPLACE VIEW policy_management.vw_policy_exception_patterns AS
SELECT
    pp.pattern_id,
    pp.pattern_name,
    p.policy_id,
    p.policy_name,
    pp.risk_profile,
    pp.detection_count,
    pp.last_detected_at,
    COUNT(e.exception_id) AS matched_exceptions,
    AVG(EXTRACT(DAY FROM (e.approval_date - e.request_date))) AS avg_approval_days,
    (SELECT STRING_AGG(DISTINCT r.role_name, ', ')
     FROM core.employees emp
     JOIN core.roles r ON emp.role_id = r.role_id
     JOIN policy_management.policy_exceptions e2 ON emp.employee_id = e2.requester_id
     WHERE e2.justification ~* ANY(pp.common_justification_keywords)
    ) AS common_requester_roles,
    pp.mitigation_strategy
FROM
    policy_management.policy_exception_pattern pp
LEFT JOIN
    policy_management.policies p ON pp.policy_id = p.policy_id
LEFT JOIN
    policy_management.policy_exceptions e ON e.policy_id = pp.policy_id
    AND e.justification ~* ANY(pp.common_justification_keywords)
GROUP BY
    pp.pattern_id, pp.pattern_name, p.policy_id, p.policy_name,
    pp.risk_profile, pp.detection_count, pp.last_detected_at,
    pp.mitigation_strategy;

COMMENT ON VIEW policy_management.vw_policy_exception_patterns IS 'Identifies common patterns in policy exceptions with risk profiles and mitigation strategies';



CREATE MATERIALIZED VIEW policy_management.mv_policy_content_quality AS
SELECT
    p.policy_id,
    p.policy_name,
    p.version,
    ca.readability_score,
    ca.complexity_score,
    ca.ambiguity_index,
    (SELECT AVG(ca2.readability_score)
     FROM policy_management.policy_content_analytics ca2
     JOIN policy_management.policies p2 ON ca2.policy_id = p2.policy_id
     WHERE p2.policy_category = p.policy_category
    ) AS category_readability_avg,
    (SELECT AVG(ca2.readability_score)
     FROM policy_management.policy_content_analytics ca2
    ) AS overall_readability_avg,
    ca.key_term_count,
    ca.sentence_length_stats,
    ca.paragraph_length_stats,
    ca.document_structure_score,
    ca.compliance_keyword_coverage,
    ca.analysis_date,
    CURRENT_TIMESTAMP AS refresh_time
FROM
    policy_management.policies p
JOIN
    policy_management.policy_content_analytics ca ON p.policy_id = ca.policy_id
WHERE
    ca.analysis_date = (SELECT MAX(analysis_date)
                       FROM policy_management.policy_content_analytics
                       WHERE policy_id = p.policy_id);

COMMENT ON MATERIALIZED VIEW policy_management.mv_policy_content_quality IS 'Provides a dashboard view of policy content quality metrics with comparative benchmarks';

CREATE MATERIALIZED VIEW policy_management.mv_policy_integration_health AS
SELECT
    ip.integration_id,
    ip.policy_id,
    p.policy_name,
    ip.system_name,
    ip.integration_type,
    ip.is_automated,
    ip.last_sync_date,
    ip.sync_status,
    COUNT(a.audit_id) AS total_syncs,
    COUNT(a.audit_id) FILTER (WHERE a.status = 'Success') AS success_syncs,
    COUNT(a.audit_id) FILTER (WHERE a.status = 'Failed') AS failed_syncs,
    AVG(a.duration_seconds) FILTER (WHERE a.status = 'Success') AS avg_sync_duration,
    MAX(a.end_time) AS last_sync_time,
    (SELECT STRING_AGG(DISTINCT a2.error_details, '| ')
     FROM policy_management.policy_integration_audit a2
     WHERE a2.integration_id = ip.integration_id
       AND a2.status = 'Failed'
       AND a2.audit_type = 'Sync'
    ) AS recent_errors,
    CURRENT_TIMESTAMP AS refresh_time
FROM
    policy_management.policy_integration_points ip
JOIN
    policy_management.policies p ON ip.policy_id = p.policy_id
LEFT JOIN
    policy_management.policy_integration_audit a ON ip.integration_id = a.integration_id
    AND a.audit_type = 'Sync'
    AND a.end_time >= CURRENT_TIMESTAMP - INTERVAL '30 days'
GROUP BY
    ip.integration_id, ip.policy_id, p.policy_name, ip.system_name,
    ip.integration_type, ip.is_automated, ip.last_sync_date, ip.sync_status;

COMMENT ON MATERIALIZED VIEW policy_management.mv_policy_integration_health IS 'Tracks the health and performance of policy integrations with other systems';




CREATE MATERIALIZED VIEW policy_management.mv_policy_stakeholder_sentiment_trend AS
SELECT
    p.policy_id,
    p.policy_name,
    DATE_TRUNC('month', ss.sentiment_date) AS month,
    AVG(ss.sentiment_score) AS avg_sentiment_score,
    COUNT(ss.sentiment_id) AS feedback_count,
    COUNT(DISTINCT ss.stakeholder_id) AS unique_stakeholders,
    MODE() WITHIN GROUP (ORDER BY ss.sentiment_category) AS most_common_sentiment,
    (SELECT STRING_AGG(DISTINCT ss2.feedback_channel, ', ')
     FROM policy_management.policy_stakeholder_sentiment ss2
     WHERE ss2.policy_id = p.policy_id
       AND DATE_TRUNC('month', ss2.sentiment_date) = DATE_TRUNC('month', ss.sentiment_date)
    ) AS feedback_channels,
    (SELECT ARRAY_AGG(DISTINCT unnest(ss3.key_themes))
     FROM policy_management.policy_stakeholder_sentiment ss3
     WHERE ss3.policy_id = p.policy_id
       AND DATE_TRUNC('month', ss3.sentiment_date) = DATE_TRUNC('month', ss.sentiment_date)
    ) AS key_themes,
    CURRENT_TIMESTAMP AS refresh_time
FROM
    policy_management.policies p
JOIN
    policy_management.policy_stakeholder_sentiment ss ON p.policy_id = ss.policy_id
GROUP BY
    p.policy_id, p.policy_name, DATE_TRUNC('month', ss.sentiment_date);

COMMENT ON MATERIALIZED VIEW policy_management.mv_policy_stakeholder_sentiment_trend IS 'Tracks trends in stakeholder sentiment about policies over time';


CREATE OR REPLACE PROCEDURE policy_management.sp_analyze_policy_content(
    IN p_policy_id INT,
    IN p_analyzed_by INT DEFAULT NULL,
    OUT p_analysis_id INT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_policy RECORD;
    v_document_text TEXT;
    v_readability_score DECIMAL(5,2);
    v_complexity_score DECIMAL(5,2);
    v_key_terms JSONB;
BEGIN
    -- Get policy details
    SELECT * INTO v_policy FROM policy_management.policies WHERE policy_id = p_policy_id;

    -- Get document text (simplified example - would use full text extraction in practice)
    SELECT pg_read_file(d.file_path) INTO v_document_text
    FROM policy_management.policy_documents d
    WHERE d.policy_id = p_policy_id AND d.is_current = TRUE
    LIMIT 1;

    -- Calculate readability score (simplified example)
    -- In practice would use proper readability algorithm
    v_readability_score := 100 - (LENGTH(v_document_text) / 1000.0);
    v_readability_score := GREATEST(LEAST(v_readability_score, 100), 0);

    -- Calculate complexity score (simplified example)
    v_complexity_score := (array_length(regexp_split_to_array(v_document_text, E'\\s+'), 1) / 100.0);
    v_complexity_score := GREATEST(LEAST(v_complexity_score, 100), 0);

    -- Extract key terms (simplified example)
    WITH term_counts AS (
        SELECT lower(unnest(regexp_split_to_array(v_document_text, E'\\W+'))) AS term,
               COUNT(*) AS count
        WHERE length(unnest(regexp_split_to_array(v_document_text, E'\\W+'))) > 4
        GROUP BY lower(unnest(regexp_split_to_array(v_document_text, E'\\W+')))
        ORDER BY COUNT(*) DESC
        LIMIT 10
    )
    SELECT jsonb_object_agg(term, count) INTO v_key_terms
    FROM term_counts;

    -- Store analysis results
    INSERT INTO policy_management.policy_content_analytics (
        policy_id,
        analysis_date,
        readability_score,
        complexity_score,
        ambiguity_index,
        key_term_count,
        sentence_length_stats,
        paragraph_length_stats,
        document_structure_score,
        compliance_keyword_coverage,
        analysis_method,
        analyzed_by
    ) VALUES (
        p_policy_id,
        CURRENT_TIMESTAMP,
        v_readability_score,
        v_complexity_score,
        0, -- Placeholder for actual ambiguity calculation
        v_key_terms,
        '{"avg": 20, "max": 50, "min": 5}', -- Placeholder for actual stats
        '{"avg": 5, "max": 10, "min": 1}', -- Placeholder for actual stats
        75, -- Placeholder for actual structure score
        80, -- Placeholder for actual coverage score
        'Automated Content Analysis v1.0',
        p_analyzed_by
    ) RETURNING analysis_id INTO p_analysis_id;

    -- Log the analysis
    INSERT INTO policy_management.policy_audit_trail (
        policy_id,
        audit_type,
        table_name,
        changed_by,
        additional_context
    ) VALUES (
        p_policy_id,
        'Content Analysis',
        'policy_content_analytics',
        COALESCE(p_analyzed_by, 1),
        'Automated content analysis performed with readability score: ' || v_readability_score
    );
END;
$$;

COMMENT ON PROCEDURE policy_management.sp_analyze_policy_content IS 'Performs automated analysis of policy document content quality and characteristics';




CREATE OR REPLACE PROCEDURE policy_management.sp_detect_exception_patterns(
    IN p_policy_id INT DEFAULT NULL,
    IN p_min_occurrences INT DEFAULT 3,
    OUT p_patterns_found INT,
    OUT p_exceptions_matched INT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_pattern RECORD;
    v_exception RECORD;
    v_justification_keywords TEXT[];
    v_common_roles TEXT[];
    v_avg_duration INT;
    v_pattern_exists BOOLEAN;
BEGIN
    p_patterns_found := 0;
    p_exceptions_matched := 0;

    -- Find common justification phrases
    FOR v_pattern IN
        WITH justification_words AS (
            SELECT regexp_matches(LOWER(justification), E'\\w{5,}', 'g') AS word
            FROM policy_management.policy_exceptions
            WHERE (p_policy_id IS NULL OR policy_id = p_policy_id)
              AND status = 'Approved'
        ),
        word_pairs AS (
            SELECT w1.word || ' ' || w2.word AS phrase
            FROM justification_words w1
            JOIN justification_words w2 ON w1.word < w2.word
        )
        SELECT phrase, COUNT(*) AS frequency
        FROM word_pairs
        GROUP BY phrase
        HAVING COUNT(*) >= p_min_occurrences
        ORDER BY COUNT(*) DESC
        LIMIT 10
    LOOP
        -- Check if pattern already exists
        SELECT EXISTS(
            SELECT 1 FROM policy_management.policy_exception_pattern
            WHERE (p_policy_id IS NULL OR policy_id = p_policy_id)
              AND v_pattern.phrase = ANY(common_justification_keywords)
        ) INTO v_pattern_exists;

        IF NOT v_pattern_exists THEN
            -- Get additional pattern characteristics
            SELECT ARRAY_AGG(DISTINCT r.role_name)
            INTO v_common_roles
            FROM policy_management.policy_exceptions e
            JOIN core.employees emp ON e.requester_id = emp.employee_id
            JOIN core.roles r ON emp.role_id = r.role_id
            WHERE (p_policy_id IS NULL OR e.policy_id = p_policy_id)
              AND e.status = 'Approved'
              AND LOWER(e.justification) LIKE '%' || v_pattern.phrase || '%';

            SELECT AVG(EXTRACT(DAY FROM (approval_date - request_date)))
            INTO v_avg_duration
            FROM policy_management.policy_exceptions
            WHERE (p_policy_id IS NULL OR policy_id = p_policy_id)
              AND status = 'Approved'
              AND LOWER(justification) LIKE '%' || v_pattern.phrase || '%';

            -- Create new pattern record
            INSERT INTO policy_management.policy_exception_pattern (
                pattern_name,
                pattern_description,
                policy_id,
                common_justification_keywords,
                typical_duration_days,
                frequent_requester_roles,
                risk_profile,
                mitigation_strategy,
                created_by
            ) VALUES (
                'Pattern: ' || v_pattern.phrase,
                'Automatically detected exception pattern for justification phrase: ' || v_pattern.phrase,
                p_policy_id,
                ARRAY[v_pattern.phrase],
                v_avg_duration,
                v_common_roles,
                CASE
                    WHEN v_pattern.frequency > 5 THEN 'High'
                    WHEN v_pattern.frequency > 2 THEN 'Medium'
                    ELSE 'Low'
                END,
                'Review policy requirements and consider clarification or training',
                1 -- System user
            );

            p_patterns_found := p_patterns_found + 1;
        END IF;

        -- Count matching exceptions
        SELECT COUNT(*)
        INTO v_exception
        FROM policy_management.policy_exceptions
        WHERE (p_policy_id IS NULL OR policy_id = p_policy_id)
          AND status = 'Approved'
          AND LOWER(justification) LIKE '%' || v_pattern.phrase || '%';

        p_exceptions_matched := p_exceptions_matched + COALESCE(v_exception, 0);
    END LOOP;

    -- Update detection counts for existing patterns
    UPDATE policy_management.policy_exception_pattern pep
    SET detection_count = subq.match_count,
        last_detected_at = CURRENT_TIMESTAMP
    FROM (
        SELECT pep.pattern_id, COUNT(*) AS match_count
        FROM policy_management.policy_exception_pattern pep
        JOIN policy_management.policy_exceptions e ON
            (pep.policy_id IS NULL OR pep.policy_id = e.policy_id)
            AND e.status = 'Approved'
            AND EXISTS (
                SELECT 1
                FROM unnest(pep.common_justification_keywords) AS keyword
                WHERE LOWER(e.justification) LIKE '%' || keyword || '%'
            )
        WHERE (p_policy_id IS NULL OR pep.policy_id = p_policy_id)
        GROUP BY pep.pattern_id
    ) subq
    WHERE pep.pattern_id = subq.pattern_id;

    -- Log the pattern detection
    INSERT INTO policy_management.policy_audit_trail (
        policy_id,
        audit_type,
        table_name,
        changed_by,
        additional_context
    ) VALUES (
        p_policy_id,
        'Pattern Detection',
        'policy_exception_pattern',
        1, -- System user
        'Detected ' || p_patterns_found || ' new exception patterns matching ' ||
        p_exceptions_matched || ' existing exceptions'
    );
END;
$$;

COMMENT ON PROCEDURE policy_management.sp_detect_exception_patterns IS 'Automatically identifies common patterns in policy exceptions for proactive management';



CREATE TABLE policy_management.policy_ai_governance (
    governance_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    ai_model_name VARCHAR(255) NOT NULL,
    model_version VARCHAR(100) NOT NULL,
    purpose_description TEXT NOT NULL,
    training_data_sources JSONB,
    bias_mitigation_strategy TEXT,
    transparency_level VARCHAR(50) CHECK (transparency_level IN ('Full', 'Partial', 'Blackbox')),
    accuracy_threshold DECIMAL(5,2) CHECK (accuracy_threshold BETWEEN 0 AND 100),
    fairness_metrics JSONB,
    drift_monitoring_enabled BOOLEAN DEFAULT TRUE,
    last_evaluation_date TIMESTAMP,
    next_evaluation_date TIMESTAMP,
    compliance_status VARCHAR(50) CHECK (compliance_status IN ('Compliant', 'Non-Compliant', 'Under Review')),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_ai_governance IS 'Governance framework for AI-assisted policy management components';



CREATE TABLE policy_management.policy_blockchain_verification (
    verification_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL REFERENCES policy_management.policies(policy_id),
    document_version VARCHAR(50) NOT NULL,
    blockchain_hash VARCHAR(64) NOT NULL,
    blockchain_network VARCHAR(100) NOT NULL,
    transaction_id VARCHAR(100),
    block_number INT,
    verification_timestamp TIMESTAMP NOT NULL,
    verified_by INT REFERENCES core.employees(employee_id),
    verification_purpose VARCHAR(100) CHECK (verification_purpose IN ('Integrity', 'Compliance', 'Audit')),
    smart_contract_address VARCHAR(42),
    gas_used DECIMAL(18,8),
    verification_cost DECIMAL(12,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE policy_management.policy_blockchain_verification IS 'Tracks blockchain-based verification of policy documents for immutable proof of existence';


CREATE OR REPLACE VIEW policy_management.vw_ai_governance_dashboard AS
SELECT
    p.policy_id,
    p.policy_name,
    ag.ai_model_name,
    ag.model_version,
    ag.purpose_description,
    ag.transparency_level,
    ag.accuracy_threshold,
    ag.compliance_status,
    ag.last_evaluation_date,
    ag.next_evaluation_date,
    (SELECT COUNT(*) FROM policy_management.policy_ai_recommendations
     WHERE policy_id = p.policy_id) AS ai_recommendations,
    (SELECT COUNT(*) FROM policy_management.policy_automation_rules
     WHERE policy_id = p.policy_id AND rule_description LIKE '%AI%') AS ai_automations,
    ag.created_at AS governance_established
FROM
    policy_management.policies p
JOIN
    policy_management.policy_ai_governance ag ON p.policy_id = ag.policy_id
WHERE
    ag.compliance_status != 'Non-Compliant';

COMMENT ON VIEW policy_management.vw_ai_governance_dashboard IS 'Provides oversight of AI components used in policy management and their governance status';



CREATE OR REPLACE VIEW policy_management.vw_blockchain_verification_audit AS
SELECT
    p.policy_id,
    p.policy_name,
    p.version,
    bv.document_version,
    bv.blockchain_hash,
    bv.blockchain_network,
    bv.verification_timestamp,
    bv.verification_purpose,
    bv.smart_contract_address,
    e.employee_name AS verified_by,
    (SELECT COUNT(*)
     FROM policy_management.policy_blockchain_verification bv2
     WHERE bv2.policy_id = p.policy_id) AS total_verifications,
    (SELECT MAX(verification_timestamp)
     FROM policy_management.policy_blockchain_verification bv3
     WHERE bv3.policy_id = p.policy_id) AS last_verification
FROM
    policy_management.policies p
JOIN
    policy_management.policy_blockchain_verification bv ON p.policy_id = bv.policy_id
JOIN
    core.employees e ON bv.verified_by = e.employee_id;

COMMENT ON VIEW policy_management.vw_blockchain_verification_audit IS 'Audit trail of all blockchain verifications performed on policy documents';


----------
----------------------------
----Operational Risk Management (ORM)
--------------------------------------------------------------------------------------------------------
CREATE SCHEMA IF NOT EXISTS orm;
COMMENT ON SCHEMA orm IS 'Operational Risk Management schema containing all tables, views, and procedures for managing operational risk features, KPIs, and KRMs';


CREATE TABLE orm.risk_categories (
    category_id SERIAL PRIMARY KEY,
    category_name VARCHAR(100) NOT NULL,
    description TEXT,
    parent_category_id INTEGER REFERENCES orm.risk_categories(category_id),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.risk_categories IS 'Hierarchical categorization of operational risks';
COMMENT ON COLUMN orm.risk_categories.parent_category_id IS 'Self-reference for hierarchical structure';


CREATE TABLE orm.business_processes (
    process_id SERIAL PRIMARY KEY,
    process_name VARCHAR(100) NOT NULL,
    description TEXT,
    department VARCHAR(100),
    is_critical BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.business_processes IS 'Business processes that may be associated with operational risks';

CREATE TABLE orm.risk_register (
    risk_id SERIAL PRIMARY KEY,
    risk_name VARCHAR(200) NOT NULL,
    description TEXT,
    category_id INTEGER REFERENCES orm.risk_categories(category_id),
    process_id INTEGER REFERENCES orm.business_processes(process_id),
    inherent_impact INTEGER CHECK (inherent_impact BETWEEN 1 AND 5),
    inherent_likelihood INTEGER CHECK (inherent_likelihood BETWEEN 1 AND 5),
    residual_impact INTEGER CHECK (residual_impact BETWEEN 1 AND 5),
    residual_likelihood INTEGER CHECK (residual_likelihood BETWEEN 1 AND 5),
    risk_owner VARCHAR(100),
    status VARCHAR(50) CHECK (status IN ('Open', 'Mitigated', 'Accepted', 'Transferred')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT chk_impact_likelihood CHECK (inherent_impact >= residual_impact AND inherent_likelihood >= residual_likelihood)
);

COMMENT ON TABLE orm.risk_register IS 'Master register of all identified operational risks';
COMMENT ON COLUMN orm.risk_register.inherent_impact IS 'Impact score before controls (1-5 scale)';
COMMENT ON COLUMN orm.risk_register.inherent_likelihood IS 'Likelihood score before controls (1-5 scale)';
COMMENT ON COLUMN orm.risk_register.residual_impact IS 'Impact score after controls (1-5 scale)';
COMMENT ON COLUMN orm.risk_register.residual_likelihood IS 'Likelihood score after controls (1-5 scale)';



CREATE TABLE orm.controls (
    control_id SERIAL PRIMARY KEY,
    control_name VARCHAR(200) NOT NULL,
    description TEXT,
    control_type VARCHAR(50) CHECK (control_type IN ('Preventive', 'Detective', 'Corrective', 'Directive')),
    frequency VARCHAR(50),
    owner VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.controls IS 'Controls implemented to mitigate operational risks';


CREATE TABLE orm.risk_control_mapping (
    mapping_id SERIAL PRIMARY KEY,
    risk_id INTEGER REFERENCES orm.risk_register(risk_id),
    control_id INTEGER REFERENCES orm.controls(control_id),
    effectiveness INTEGER CHECK (effectiveness BETWEEN 1 AND 5),
    last_test_date DATE,
    next_test_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (risk_id, control_id)
);

COMMENT ON TABLE orm.risk_control_mapping IS 'Mapping between risks and the controls that mitigate them';


CREATE TABLE orm.loss_events (
    event_id SERIAL PRIMARY KEY,
    event_name VARCHAR(200) NOT NULL,
    description TEXT,
    risk_id INTEGER REFERENCES orm.risk_register(risk_id),
    event_date DATE NOT NULL,
    discovery_date DATE,
    resolution_date DATE,
    financial_loss DECIMAL(15,2),
    non_financial_impact TEXT,
    status VARCHAR(50) CHECK (status IN ('Open', 'Investigating', 'Resolved', 'Closed')),
    root_cause TEXT,
    remediation_actions TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.loss_events IS 'Record of operational risk events that resulted in loss';


CREATE TABLE orm.key_risk_indicators (
    kri_id SERIAL PRIMARY KEY,
    kri_name VARCHAR(200) NOT NULL,
    description TEXT,
    risk_id INTEGER REFERENCES orm.risk_register(risk_id),
    measurement_frequency VARCHAR(50),
    threshold_value DECIMAL(15,2),
    threshold_type VARCHAR(20) CHECK (threshold_type IN ('Upper', 'Lower', 'Range')),
    threshold_min_value DECIMAL(15,2),
    threshold_max_value DECIMAL(15,2),
    target_value DECIMAL(15,2),
    owner VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.key_risk_indicators IS 'Key Risk Indicators for monitoring operational risks';



CREATE TABLE orm.kri_measurements (
    measurement_id SERIAL PRIMARY KEY,
    kri_id INTEGER REFERENCES orm.key_risk_indicators(kri_id),
    measurement_date DATE NOT NULL,
    measured_value DECIMAL(15,2) NOT NULL,
    comment TEXT,
    created_by VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.kri_measurements IS 'Historical measurements of Key Risk Indicators';


CREATE TABLE orm.key_performance_indicators (
    kpi_id SERIAL PRIMARY KEY,
    kpi_name VARCHAR(200) NOT NULL,
    description TEXT,
    feature_id INTEGER,
    measurement_frequency VARCHAR(50),
    target_value DECIMAL(15,2),
    target_type VARCHAR(20) CHECK (target_type IN ('Minimum', 'Maximum', 'Range', 'Exact')),
    min_value DECIMAL(15,2),
    max_value DECIMAL(15,2),
    owner VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.key_performance_indicators IS 'Key Performance Indicators for operational risk management';


CREATE TABLE orm.kpi_measurements (
    measurement_id SERIAL PRIMARY KEY,
    kpi_id INTEGER REFERENCES orm.key_performance_indicators(kpi_id),
    measurement_date DATE NOT NULL,
    measured_value DECIMAL(15,2) NOT NULL,
    comment TEXT,
    created_by VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.kpi_measurements IS 'Historical measurements of Key Performance Indicators';


CREATE TABLE orm.key_resilience_metrics (
    krm_id SERIAL PRIMARY KEY,
    krm_name VARCHAR(200) NOT NULL,
    description TEXT,
    feature_id INTEGER,
    measurement_frequency VARCHAR(50),
    target_value DECIMAL(15,2),
    target_type VARCHAR(20) CHECK (target_type IN ('Minimum', 'Maximum', 'Range', 'Exact')),
    min_value DECIMAL(15,2),
    max_value DECIMAL(15,2),
    owner VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.key_resilience_metrics IS 'Key Resilience Metrics for operational risk management';


CREATE TABLE orm.krm_measurements (
    measurement_id SERIAL PRIMARY KEY,
    krm_id INTEGER REFERENCES orm.key_resilience_metrics(krm_id),
    measurement_date DATE NOT NULL,
    measured_value DECIMAL(15,2) NOT NULL,
    comment TEXT,
    created_by VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.krm_measurements IS 'Historical measurements of Key Resilience Metrics';


CREATE TABLE orm.risk_scenarios (
    scenario_id SERIAL PRIMARY KEY,
    scenario_name VARCHAR(200) NOT NULL,
    description TEXT,
    risk_id INTEGER REFERENCES orm.risk_register(risk_id),
    likelihood INTEGER CHECK (likelihood BETWEEN 1 AND 5),
    impact INTEGER CHECK (impact BETWEEN 1 AND 5),
    severity INTEGER GENERATED ALWAYS AS (likelihood * impact) STORED,
    is_plausible BOOLEAN DEFAULT TRUE,
    last_review_date DATE,
    next_review_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.risk_scenarios IS 'Plausible risk scenarios for stress testing and scenario analysis';

---
CREATE TABLE orm.control_self_assessments (
    assessment_id SERIAL PRIMARY KEY,
    control_id INTEGER REFERENCES orm.controls(control_id),
    assessment_date DATE NOT NULL,
    assessed_by VARCHAR(100) NOT NULL,
    effectiveness INTEGER CHECK (effectiveness BETWEEN 1 AND 5),
    design_adequacy INTEGER CHECK (design_adequacy BETWEEN 1 AND 5),
    operational_effectiveness INTEGER CHECK (operational_effectiveness BETWEEN 1 AND 5),
    findings TEXT,
    remediation_plan TEXT,
    status VARCHAR(50) CHECK (status IN ('Open', 'In Progress', 'Completed', 'Verified')),
    verified_by VARCHAR(100),
    verification_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.control_self_assessments IS 'Results of control self-assessments conducted by business units';


CREATE TABLE orm.near_miss_events (
    near_miss_id SERIAL PRIMARY KEY,
    event_name VARCHAR(200) NOT NULL,
    description TEXT,
    risk_id INTEGER REFERENCES orm.risk_register(risk_id),
    event_date DATE NOT NULL,
    potential_impact TEXT,
    potential_financial_loss DECIMAL(15,2),
    root_cause TEXT,
    preventive_actions TEXT,
    reported_by VARCHAR(100),
    status VARCHAR(50) CHECK (status IN ('Reported', 'Reviewed', 'Actioned', 'Closed')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.near_miss_events IS 'Record of near miss events that could have resulted in operational losses';


CREATE TABLE orm.third_party_vendors (
    vendor_id SERIAL PRIMARY KEY,
    vendor_name VARCHAR(200) NOT NULL,
    description TEXT,
    vendor_type VARCHAR(100),
    industry VARCHAR(100),
    contract_start_date DATE,
    contract_end_date DATE,
    risk_rating VARCHAR(20) CHECK (risk_rating IN ('Low', 'Medium', 'High', 'Critical')),
    criticality VARCHAR(20) CHECK (criticality IN ('Low', 'Medium', 'High', 'Critical')),
    last_assessment_date DATE,
    next_assessment_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.third_party_vendors IS 'Third party vendors that may introduce operational risks';


CREATE TABLE orm.vendor_risk_assessments (
    assessment_id SERIAL PRIMARY KEY,
    vendor_id INTEGER REFERENCES orm.third_party_vendors(vendor_id),
    assessment_date DATE NOT NULL,
    assessed_by VARCHAR(100) NOT NULL,
    financial_stability_rating INTEGER CHECK (financial_stability_rating BETWEEN 1 AND 5),
    operational_capability_rating INTEGER CHECK (operational_capability_rating BETWEEN 1 AND 5),
    security_rating INTEGER CHECK (security_rating BETWEEN 1 AND 5),
    compliance_rating INTEGER CHECK (compliance_rating BETWEEN 1 AND 5),
    overall_risk_score DECIMAL(5,2) GENERATED ALWAYS AS (
        (financial_stability_rating + operational_capability_rating + security_rating + compliance_rating) / 4.0
    ) STORED,
    findings TEXT,
    remediation_plan TEXT,
    status VARCHAR(50) CHECK (status IN ('Open', 'In Progress', 'Completed', 'Verified')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.vendor_risk_assessments IS 'Risk assessments conducted on third party vendors';


CREATE TABLE orm.business_continuity_plans (
    bcp_id SERIAL PRIMARY KEY,
    plan_name VARCHAR(200) NOT NULL,
    description TEXT,
    process_id INTEGER REFERENCES orm.business_processes(process_id),
    recovery_time_objective INTERVAL,
    recovery_point_objective INTERVAL,
    last_test_date DATE,
    next_test_date DATE,
    test_results TEXT,
    plan_document BYTEA,
    document_version VARCHAR(50),
    status VARCHAR(50) CHECK (status IN ('Draft', 'Approved', 'Active', 'Retired')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.business_continuity_plans IS 'Business continuity plans for critical business processes';


CREATE TABLE orm.incidents (
    incident_id SERIAL PRIMARY KEY,
    incident_name VARCHAR(200) NOT NULL,
    description TEXT,
    incident_date TIMESTAMP WITH TIME ZONE NOT NULL,
    detection_date TIMESTAMP WITH TIME ZONE,
    resolution_date TIMESTAMP WITH TIME ZONE,
    severity VARCHAR(20) CHECK (severity IN ('Low', 'Medium', 'High', 'Critical')),
    status VARCHAR(50) CHECK (status IN ('Open', 'Investigating', 'Resolving', 'Resolved', 'Closed')),
    root_cause TEXT,
    impact_assessment TEXT,
    remediation_actions TEXT,
    lessons_learned TEXT,
    created_by VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.incidents IS 'Record of operational incidents and their management';


CREATE TABLE orm.risk_appetite (
    appetite_id SERIAL PRIMARY KEY,
    risk_category_id INTEGER REFERENCES orm.risk_categories(category_id),
    max_tolerance DECIMAL(15,2),
    min_tolerance DECIMAL(15,2),
    target_level DECIMAL(15,2),
    measurement_unit VARCHAR(50),
    effective_date DATE NOT NULL,
    expiry_date DATE,
    approved_by VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.risk_appetite IS 'Risk appetite statements for different risk categories';



CREATE OR REPLACE VIEW orm.risk_dashboard AS
SELECT
    rr.risk_id,
    rr.risk_name,
    rc.category_name,
    bp.process_name,
    rr.inherent_impact,
    rr.inherent_likelihood,
    rr.residual_impact,
    rr.residual_likelihood,
    (rr.inherent_impact * rr.inherent_likelihood) AS inherent_risk_score,
    (rr.residual_impact * rr.residual_likelihood) AS residual_risk_score,
    rr.status,
    COUNT(DISTINCT cm.control_id) AS control_count,
    COUNT(DISTINCT le.event_id) AS loss_event_count
FROM
    orm.risk_register rr
LEFT JOIN orm.risk_categories rc ON rr.category_id = rc.category_id
LEFT JOIN orm.business_processes bp ON rr.process_id = bp.process_id
LEFT JOIN orm.risk_control_mapping cm ON rr.risk_id = cm.risk_id
LEFT JOIN orm.loss_events le ON rr.risk_id = le.risk_id
GROUP BY
    rr.risk_id, rr.risk_name, rc.category_name, bp.process_name,
    rr.inherent_impact, rr.inherent_likelihood, rr.residual_impact,
    rr.residual_likelihood, rr.status;

COMMENT ON VIEW orm.risk_dashboard IS 'Comprehensive view of risks for dashboard reporting';


CREATE OR REPLACE VIEW orm.control_effectiveness AS
SELECT
    c.control_id,
    c.control_name,
    c.control_type,
    c.owner,
    AVG(csa.effectiveness) AS avg_effectiveness,
    AVG(csa.design_adequacy) AS avg_design_adequacy,
    AVG(csa.operational_effectiveness) AS avg_operational_effectiveness,
    COUNT(DISTINCT csa.assessment_id) AS assessment_count,
    COUNT(DISTINCT rcm.risk_id) AS risk_coverage_count
FROM
    orm.controls c
LEFT JOIN orm.control_self_assessments csa ON c.control_id = csa.control_id
LEFT JOIN orm.risk_control_mapping rcm ON c.control_id = rcm.control_id
GROUP BY
    c.control_id, c.control_name, c.control_type, c.owner;

COMMENT ON VIEW orm.control_effectiveness IS 'View showing effectiveness metrics for controls';



CREATE OR REPLACE VIEW orm.loss_event_analysis AS
SELECT
    le.event_id,
    le.event_name,
    rr.risk_name,
    rc.category_name,
    le.event_date,
    le.discovery_date,
    le.resolution_date,
    le.financial_loss,
    le.status,
    EXTRACT(DAY FROM (le.resolution_date - le.event_date)) AS days_to_resolve,
    CASE
        WHEN le.financial_loss <= 10000 THEN 'Low'
        WHEN le.financial_loss <= 100000 THEN 'Medium'
        WHEN le.financial_loss <= 1000000 THEN 'High'
        ELSE 'Critical'
    END AS loss_severity
FROM
    orm.loss_events le
JOIN orm.risk_register rr ON le.risk_id = rr.risk_id
LEFT JOIN orm.risk_categories rc ON rr.category_id = rc.category_id;

COMMENT ON VIEW orm.loss_event_analysis IS 'View for analyzing loss events with calculated fields';


CREATE OR REPLACE VIEW orm.vendor_risk_view AS
SELECT
    v.vendor_id,
    v.vendor_name,
    v.vendor_type,
    v.risk_rating,
    v.criticality,
    va.assessment_date,
    va.overall_risk_score,
    va.financial_stability_rating,
    va.operational_capability_rating,
    va.security_rating,
    va.compliance_rating,
    CASE
        WHEN va.overall_risk_score <= 2.0 THEN 'Low'
        WHEN va.overall_risk_score <= 3.5 THEN 'Medium'
        WHEN va.overall_risk_score <= 4.5 THEN 'High'
        ELSE 'Critical'
    END AS calculated_risk_rating,
    v.contract_end_date,
    EXTRACT(DAY FROM (v.contract_end_date - CURRENT_DATE)) AS days_until_contract_end
FROM
    orm.third_party_vendors v
LEFT JOIN (
    SELECT
        vendor_id,
        MAX(assessment_date) AS latest_assessment_date
    FROM
        orm.vendor_risk_assessments
    GROUP BY
        vendor_id
) latest ON v.vendor_id = latest.vendor_id
LEFT JOIN orm.vendor_risk_assessments va ON va.vendor_id = latest.vendor_id AND va.assessment_date = latest.latest_assessment_date;

COMMENT ON VIEW orm.vendor_risk_view IS 'Consolidated view of vendor risk information';



CREATE OR REPLACE VIEW orm.metric_status AS
SELECT
    'KRI' AS metric_type,
    kri.kri_id AS metric_id,
    kri.kri_name AS metric_name,
    km.measurement_date AS last_measurement_date,
    km.measured_value AS last_value,
    CASE
        WHEN kri.threshold_type = 'Upper' AND km.measured_value > kri.threshold_value THEN 'Exceeded'
        WHEN kri.threshold_type = 'Lower' AND km.measured_value < kri.threshold_value THEN 'Exceeded'
        WHEN kri.threshold_type = 'Range' AND (km.measured_value < kri.threshold_min_value OR km.measured_value > kri.threshold_max_value) THEN 'Exceeded'
        ELSE 'Within Limits'
    END AS status
FROM
    orm.key_risk_indicators kri
JOIN (
    SELECT
        kri_id,
        MAX(measurement_date) AS max_date
    FROM
        orm.kri_measurements
    GROUP BY
        kri_id
) latest ON kri.kri_id = latest.kri_id
JOIN orm.kri_measurements km ON km.kri_id = latest.kri_id AND km.measurement_date = latest.max_date

UNION ALL

SELECT
    'KPI' AS metric_type,
    kpi.kpi_id AS metric_id,
    kpi.kpi_name AS metric_name,
    km.measurement_date AS last_measurement_date,
    km.measured_value AS last_value,
    CASE
        WHEN kpi.target_type = 'Minimum' AND km.measured_value < kpi.target_value THEN 'Below Target'
        WHEN kpi.target_type = 'Maximum' AND km.measured_value > kpi.target_value THEN 'Above Target'
        WHEN kpi.target_type = 'Range' AND (km.measured_value < kpi.min_value OR km.measured_value > kpi.max_value) THEN 'Out of Range'
        ELSE 'On Target'
    END AS status
FROM
    orm.key_performance_indicators kpi
JOIN (
    SELECT
        kpi_id,
        MAX(measurement_date) AS max_date
    FROM
        orm.kpi_measurements
    GROUP BY
        kpi_id
) latest ON kpi.kpi_id = latest.kpi_id
JOIN orm.kpi_measurements km ON km.kpi_id = latest.kpi_id AND km.measurement_date = latest.max_date

UNION ALL

SELECT
    'KRM' AS metric_type,
    krm.krm_id AS metric_id,
    krm.krm_name AS metric_name,
    km.measurement_date AS last_measurement_date,
    km.measured_value AS last_value,
    CASE
        WHEN krm.target_type = 'Minimum' AND km.measured_value < krm.target_value THEN 'Below Target'
        WHEN krm.target_type = 'Maximum' AND km.measured_value > krm.target_value THEN 'Above Target'
        WHEN krm.target_type = 'Range' AND (km.measured_value < krm.min_value OR km.measured_value > krm.max_value) THEN 'Out of Range'
        ELSE 'On Target'
    END AS status
FROM
    orm.key_resilience_metrics krm
JOIN (
    SELECT
        krm_id,
        MAX(measurement_date) AS max_date
    FROM
        orm.krm_measurements
    GROUP BY
        krm_id
) latest ON krm.krm_id = latest.krm_id
JOIN orm.krm_measurements km ON km.krm_id = latest.krm_id AND km.measurement_date = latest.max_date;

COMMENT ON VIEW orm.metric_status IS 'Combined view showing status of all KRIs, KPIs, and KRMs';


CREATE MATERIALIZED VIEW orm.risk_heatmap_mv AS
SELECT
    rc.category_id,
    rc.category_name,
    COUNT(rr.risk_id) AS risk_count,
    AVG(rr.inherent_impact * rr.inherent_likelihood) AS avg_inherent_risk,
    AVG(rr.residual_impact * rr.residual_likelihood) AS avg_residual_risk,
    SUM(CASE WHEN rr.inherent_impact * rr.inherent_likelihood >= 12 THEN 1 ELSE 0 END) AS high_inherent_risk_count,
    SUM(CASE WHEN rr.residual_impact * rr.residual_likelihood >= 12 THEN 1 ELSE 0 END) AS high_residual_risk_count
FROM
    orm.risk_categories rc
LEFT JOIN orm.risk_register rr ON rc.category_id = rr.category_id
GROUP BY
    rc.category_id, rc.category_name
WITH DATA;

COMMENT ON MATERIALIZED VIEW orm.risk_heatmap_mv IS 'Materialized view for risk heatmap reporting by category';

CREATE INDEX idx_risk_heatmap_category ON orm.risk_heatmap_mv (category_id);


CREATE MATERIALIZED VIEW orm.operational_loss_trend_mv AS
SELECT
    DATE_TRUNC('month', event_date) AS month,
    COUNT(event_id) AS event_count,
    SUM(financial_loss) AS total_loss,
    AVG(financial_loss) AS avg_loss,
    MAX(financial_loss) AS max_loss,
    rc.category_name,
    COUNT(DISTINCT rr.risk_id) AS distinct_risk_count
FROM
    orm.loss_events le
JOIN orm.risk_register rr ON le.risk_id = rr.risk_id
LEFT JOIN orm.risk_categories rc ON rr.category_id = rc.category_id
GROUP BY
    DATE_TRUNC('month', event_date), rc.category_name
WITH DATA;

COMMENT ON MATERIALIZED VIEW orm.operational_loss_trend_mv IS 'Materialized view for operational loss trend analysis';

CREATE INDEX idx_loss_trend_month ON orm.operational_loss_trend_mv (month);
CREATE INDEX idx_loss_trend_category ON orm.operational_loss_trend_mv (category_name);


CREATE MATERIALIZED VIEW orm.control_testing_results_mv AS
SELECT
    c.control_id,
    c.control_name,
    c.control_type,
    c.owner,
    COUNT(csa.assessment_id) AS test_count,
    AVG(csa.effectiveness) AS avg_effectiveness,
    AVG(csa.design_adequacy) AS avg_design_adequacy,
    AVG(csa.operational_effectiveness) AS avg_operational_effectiveness,
    MAX(csa.assessment_date) AS last_test_date,
    COUNT(DISTINCT CASE WHEN csa.effectiveness < 3 THEN csa.assessment_id END) AS failed_tests
FROM
    orm.controls c
LEFT JOIN orm.control_self_assessments csa ON c.control_id = csa.control_id
GROUP BY
    c.control_id, c.control_name, c.control_type, c.owner
WITH DATA;

COMMENT ON MATERIALIZED VIEW orm.control_testing_results_mv IS 'Materialized view for control testing results analysis';

CREATE INDEX idx_control_testing_control ON orm.control_testing_results_mv (control_id);
CREATE INDEX idx_control_testing_owner ON orm.control_testing_results_mv (owner);



CREATE OR REPLACE PROCEDURE orm.update_risk_register(
    p_risk_id INTEGER,
    p_residual_impact INTEGER,
    p_residual_likelihood INTEGER,
    p_status VARCHAR(50),
    p_updated_by VARCHAR(100)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Validate inputs
    IF p_residual_impact NOT BETWEEN 1 AND 5 OR p_residual_likelihood NOT BETWEEN 1 AND 5 THEN
        RAISE EXCEPTION 'Impact and likelihood must be between 1 and 5';
    END IF;

    IF p_status NOT IN ('Open', 'Mitigated', 'Accepted', 'Transferred') THEN
        RAISE EXCEPTION 'Invalid status value';
    END IF;

    -- Update risk register
    UPDATE orm.risk_register
    SET
        residual_impact = p_residual_impact,
        residual_likelihood = p_residual_likelihood,
        status = p_status,
        updated_at = CURRENT_TIMESTAMP
    WHERE
        risk_id = p_risk_id;

    -- Log the update
    INSERT INTO orm.risk_register_audit (risk_id, changed_by, change_type, old_values, new_values)
    SELECT
        p_risk_id,
        p_updated_by,
        'UPDATE',
        jsonb_build_object(
            'residual_impact', rr.residual_impact,
            'residual_likelihood', rr.residual_likelihood,
            'status', rr.status
        ),
        jsonb_build_object(
            'residual_impact', p_residual_impact,
            'residual_likelihood', p_residual_likelihood,
            'status', p_status
        )
    FROM
        orm.risk_register rr
    WHERE
        rr.risk_id = p_risk_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE orm.update_risk_register IS 'Procedure to update risk register entries with validation and auditing';



CREATE OR REPLACE PROCEDURE orm.record_loss_event(
    p_event_name VARCHAR(200),
    p_description TEXT,
    p_risk_id INTEGER,
    p_event_date DATE,
    p_financial_loss DECIMAL(15,2),
    p_non_financial_impact TEXT,
    p_reported_by VARCHAR(100),
    OUT p_event_id INTEGER)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Validate risk exists
    IF NOT EXISTS (SELECT 1 FROM orm.risk_register WHERE risk_id = p_risk_id) THEN
        RAISE EXCEPTION 'Invalid risk_id: %', p_risk_id;
    END IF;

    -- Insert the loss event
    INSERT INTO orm.loss_events (
        event_name,
        description,
        risk_id,
        event_date,
        financial_loss,
        non_financial_impact,
        status,
        reported_by
    ) VALUES (
        p_event_name,
        p_description,
        p_risk_id,
        p_event_date,
        p_financial_loss,
        p_non_financial_impact,
        'Open',
        p_reported_by
    ) RETURNING event_id INTO p_event_id;

    -- Update the associated risk's inherent scores if this is the first loss event
    -- and the current scores are lower than what this event suggests
    UPDATE orm.risk_register rr
    SET
        inherent_impact = GREATEST(rr.inherent_impact,
            CASE
                WHEN p_financial_loss <= 10000 THEN 2
                WHEN p_financial_loss <= 100000 THEN 3
                WHEN p_financial_loss <= 1000000 THEN 4
                ELSE 5
            END),
        inherent_likelihood = GREATEST(rr.inherent_likelihood,
            CASE
                WHEN (SELECT COUNT(*) FROM orm.loss_events WHERE risk_id = p_risk_id) = 1 THEN 2
                WHEN (SELECT COUNT(*) FROM orm.loss_events WHERE risk_id = p_risk_id) <= 3 THEN 3
                WHEN (SELECT COUNT(*) FROM orm.loss_events WHERE risk_id = p_risk_id) <= 10 THEN 4
                ELSE 5
            END),
        updated_at = CURRENT_TIMESTAMP
    WHERE
        rr.risk_id = p_risk_id
        AND (SELECT COUNT(*) FROM orm.loss_events WHERE risk_id = p_risk_id) = 1;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE orm.record_loss_event IS 'Procedure to record a new loss event and update associated risk ratings';


CREATE OR REPLACE PROCEDURE orm.assess_vendor_risk(
    p_vendor_id INTEGER,
    p_assessed_by VARCHAR(100),
    p_financial_stability_rating INTEGER,
    p_operational_capability_rating INTEGER,
    p_security_rating INTEGER,
    p_compliance_rating INTEGER,
    p_findings TEXT,
    p_remediation_plan TEXT,
    OUT p_assessment_id INTEGER)
LANGUAGE plpgsql
AS $$
DECLARE
    v_overall_risk_score DECIMAL(5,2);
BEGIN
    -- Validate inputs
    IF p_financial_stability_rating NOT BETWEEN 1 AND 5 OR
       p_operational_capability_rating NOT BETWEEN 1 AND 5 OR
       p_security_rating NOT BETWEEN 1 AND 5 OR
       p_compliance_rating NOT BETWEEN 1 AND 5 THEN
        RAISE EXCEPTION 'All ratings must be between 1 and 5';
    END IF;

    IF NOT EXISTS (SELECT 1 FROM orm.third_party_vendors WHERE vendor_id = p_vendor_id) THEN
        RAISE EXCEPTION 'Invalid vendor_id: %', p_vendor_id;
    END IF;

    -- Calculate overall risk score
    v_overall_risk_score := (p_financial_stability_rating + p_operational_capability_rating +
                            p_security_rating + p_compliance_rating) / 4.0;

    -- Insert the assessment
    INSERT INTO orm.vendor_risk_assessments (
        vendor_id,
        assessment_date,
        assessed_by,
        financial_stability_rating,
        operational_capability_rating,
        security_rating,
        compliance_rating,
        findings,
        remediation_plan,
        status
    ) VALUES (
        p_vendor_id,
        CURRENT_DATE,
        p_assessed_by,
        p_financial_stability_rating,
        p_operational_capability_rating,
        p_security_rating,
        p_compliance_rating,
        p_findings,
        p_remediation_plan,
        'Open'
    ) RETURNING assessment_id INTO p_assessment_id;

    -- Update the vendor's risk rating based on the assessment
    UPDATE orm.third_party_vendors
    SET
        risk_rating = CASE
            WHEN v_overall_risk_score <= 2.0 THEN 'Low'
            WHEN v_overall_risk_score <= 3.5 THEN 'Medium'
            WHEN v_overall_risk_score <= 4.5 THEN 'High'
            ELSE 'Critical'
        END,
        last_assessment_date = CURRENT_DATE,
        next_assessment_date = CURRENT_DATE + INTERVAL '1 year',
        updated_at = CURRENT_TIMESTAMP
    WHERE
        vendor_id = p_vendor_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE orm.assess_vendor_risk IS 'Procedure to conduct and record a vendor risk assessment';



CREATE OR REPLACE PROCEDURE orm.test_control(
    p_control_id INTEGER,
    p_assessed_by VARCHAR(100),
    p_effectiveness INTEGER,
    p_design_adequacy INTEGER,
    p_operational_effectiveness INTEGER,
    p_findings TEXT,
    p_remediation_plan TEXT,
    OUT p_assessment_id INTEGER)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Validate inputs
    IF p_effectiveness NOT BETWEEN 1 AND 5 OR
       p_design_adequacy NOT BETWEEN 1 AND 5 OR
       p_operational_effectiveness NOT BETWEEN 1 AND 5 THEN
        RAISE EXCEPTION 'All ratings must be between 1 and 5';
    END IF;

    IF NOT EXISTS (SELECT 1 FROM orm.controls WHERE control_id = p_control_id) THEN
        RAISE EXCEPTION 'Invalid control_id: %', p_control_id;
    END IF;

    -- Insert the assessment
    INSERT INTO orm.control_self_assessments (
        control_id,
        assessment_date,
        assessed_by,
        effectiveness,
        design_adequacy,
        operational_effectiveness,
        findings,
        remediation_plan,
        status
    ) VALUES (
        p_control_id,
        CURRENT_DATE,
        p_assessed_by,
        p_effectiveness,
        p_design_adequacy,
        p_operational_effectiveness,
        p_findings,
        p_remediation_plan,
        'Open'
    ) RETURNING assessment_id INTO p_assessment_id;

    -- Update the control's last test date
    UPDATE orm.controls
    SET
        updated_at = CURRENT_TIMESTAMP
    WHERE
        control_id = p_control_id;

    -- If effectiveness is low, update risk control mapping
    IF p_effectiveness < 3 THEN
        UPDATE orm.risk_control_mapping
        SET
            effectiveness = p_effectiveness,
            last_test_date = CURRENT_DATE,
            next_test_date = CURRENT_DATE + INTERVAL '3 months',
            updated_at = CURRENT_TIMESTAMP
        WHERE
            control_id = p_control_id;
    END IF;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE orm.test_control IS 'Procedure to test a control and update related records';



CREATE OR REPLACE PROCEDURE orm.analyze_risk_scenario(
    p_risk_id INTEGER,
    p_scenario_name VARCHAR(200),
    p_description TEXT,
    p_likelihood INTEGER,
    p_impact INTEGER,
    p_is_plausible BOOLEAN,
    OUT p_scenario_id INTEGER)
LANGUAGE plpgsql
AS $$
DECLARE
    v_severity INTEGER;
BEGIN
    -- Validate inputs
    IF p_likelihood NOT BETWEEN 1 AND 5 OR p_impact NOT BETWEEN 1 AND 5 THEN
        RAISE EXCEPTION 'Likelihood and impact must be between 1 and 5';
    END IF;

    IF NOT EXISTS (SELECT 1 FROM orm.risk_register WHERE risk_id = p_risk_id) THEN
        RAISE EXCEPTION 'Invalid risk_id: %', p_risk_id;
    END IF;

    -- Calculate severity
    v_severity := p_likelihood * p_impact;

    -- Insert the scenario
    INSERT INTO orm.risk_scenarios (
        risk_id,
        scenario_name,
        description,
        likelihood,
        impact,
        is_plausible,
        last_review_date,
        next_review_date
    ) VALUES (
        p_risk_id,
        p_scenario_name,
        p_description,
        p_likelihood,
        p_impact,
        p_is_plausible,
        CURRENT_DATE,
        CURRENT_DATE + INTERVAL '1 year'
    ) RETURNING scenario_id INTO p_scenario_id;

    -- If severity is high, ensure risk register reflects this
    UPDATE orm.risk_register
    SET
        inherent_impact = GREATEST(inherent_impact, p_impact),
        inherent_likelihood = GREATEST(inherent_likelihood, p_likelihood),
        updated_at = CURRENT_TIMESTAMP
    WHERE
        risk_id = p_risk_id
        AND (inherent_impact < p_impact OR inherent_likelihood < p_likelihood);

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE orm.analyze_risk_scenario IS 'Procedure to analyze a risk scenario and update related risk ratings';


--adding a new risk to the register
INSERT INTO orm.risk_register (
    risk_name,
    description,
    category_id,
    process_id,
    inherent_impact,
    inherent_likelihood,
    residual_impact,
    residual_likelihood,
    risk_owner,
    status
) VALUES (
    'Data breach through third-party vendor',
    'Potential unauthorized access to sensitive data through vendor systems',
    5, -- Information Security category
    12, -- Vendor management process
    4, -- High impact
    3, -- Medium likelihood
    2, -- Residual impact after controls
    2, -- Residual likelihood after controls
    'CISO',
    'Open'
);

--recording a loss event
CALL orm.record_loss_event(
    p_event_name := 'Vendor system breach',
    p_description := 'Unauthorized access to vendor system exposed customer data',
    p_risk_id := 42, -- ID of related risk
    p_event_date := '2023-05-15',
    p_financial_loss := 250000.00,
    p_non_financial_impact := 'Reputational damage, regulatory scrutiny',
    p_reported_by := 'jane.doe@company.com',
    p_event_id := NULL
);

--conducting a vendor risk assessment
CALL orm.assess_vendor_risk(
    p_vendor_id := 18,
    p_assessed_by := 'john.smith@company.com',
    p_financial_stability_rating := 3,
    p_operational_capability_rating := 4,
    p_security_rating := 2,
    p_compliance_rating := 3,
    p_findings := 'Security controls need improvement, especially around access management',
    p_remediation_plan := 'Vendor to implement MFA and quarterly access reviews',
    p_assessment_id := NULL
);

--queryring the risk dashboard
SELECT * FROM orm.risk_dashboard
WHERE residual_risk_score >= 12
ORDER BY inherent_risk_score DESC;

-- refreshing materialized views
REFRESH MATERIALIZED VIEW orm.risk_heatmap_mv;
REFRESH MATERIALIZED VIEW orm.operational_loss_trend_mv;


CREATE TABLE orm.ai_risk_management (
    ai_risk_id SERIAL PRIMARY KEY,
    system_name VARCHAR(200) NOT NULL,
    description TEXT,
    risk_category VARCHAR(100) CHECK (risk_category IN ('Bias', 'Transparency', 'Security', 'Compliance', 'Performance')),
    deployment_phase VARCHAR(50) CHECK (deployment_phase IN ('Development', 'Testing', 'Production', 'Retired')),
    model_validation_date DATE,
    next_validation_date DATE,
    ethical_review_completed BOOLEAN DEFAULT FALSE,
    bias_testing_score DECIMAL(5,2),
    explainability_score DECIMAL(5,2),
    risk_owner VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.ai_risk_management IS 'Tracks risks associated with AI and machine learning systems';


CREATE TABLE orm.climate_risk_assessments (
    assessment_id SERIAL PRIMARY KEY,
    facility_id INTEGER REFERENCES orm.facilities(facility_id),
    assessment_date DATE NOT NULL,
    next_assessment_date DATE,
    physical_risk_score INTEGER CHECK (physical_risk_score BETWEEN 1 AND 5),
    transition_risk_score INTEGER CHECK (transition_risk_score BETWEEN 1 AND 5),
    water_stress_risk BOOLEAN,
    flood_risk BOOLEAN,
    wildfire_risk BOOLEAN,
    carbon_footprint_metric_tons DECIMAL(15,2),
    mitigation_plan TEXT,
    adaptation_plan TEXT,
    assessed_by VARCHAR(100),
    approved_by VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.climate_risk_assessments IS 'Assessments of climate-related risks to physical assets and operations';


CREATE TABLE orm.facilities (
    facility_id SERIAL PRIMARY KEY,
    facility_name VARCHAR(200) NOT NULL,
    location GEOGRAPHY(POINT, 4326),
    address TEXT,
    country VARCHAR(100),
    facility_type VARCHAR(100) CHECK (facility_type IN ('Office', 'Data Center', 'Manufacturing', 'Warehouse', 'Retail')),
    criticality VARCHAR(20) CHECK (criticality IN ('Low', 'Medium', 'High', 'Critical')),
    construction_year INTEGER,
    replacement_value DECIMAL(15,2),
    bcp_id INTEGER REFERENCES orm.business_continuity_plans(bcp_id),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.facilities IS 'Physical facilities and their risk characteristics';

CREATE TABLE orm.geopolitical_risks (
    geopolitial_risk_id SERIAL PRIMARY KEY,
    country_code VARCHAR(3) NOT NULL,
    country_name VARCHAR(100) NOT NULL,
    risk_type VARCHAR(100) CHECK (risk_type IN ('Sanctions', 'Political Instability', 'Trade Restrictions', 'Currency Controls', 'Expropriation')),
    risk_level VARCHAR(20) CHECK (risk_level IN ('Low', 'Medium', 'High', 'Severe')),
    effective_date DATE NOT NULL,
    expiry_date DATE,
    description TEXT,
    impact_assessment TEXT,
    mitigation_measures TEXT,
    monitoring_frequency VARCHAR(50),
    last_review_date DATE,
    next_review_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.geopolitical_risks IS 'Tracks geopolitical risks by country and type';


CREATE TABLE orm.digital_transformation_risks (
    dt_risk_id SERIAL PRIMARY KEY,
    project_name VARCHAR(200) NOT NULL,
    project_phase VARCHAR(50) CHECK (project_phase IN ('Planning', 'Development', 'Implementation', 'Operation')),
    risk_category VARCHAR(100) CHECK (risk_category IN ('Cybersecurity', 'Data Privacy', 'System Integration', 'Vendor Lock-in', 'Skill Gaps')),
    risk_description TEXT,
    legacy_system_dependency BOOLEAN,
    data_migration_risk_score INTEGER CHECK (data_migration_risk_score BETWEEN 1 AND 5),
    user_adoption_risk_score INTEGER CHECK (user_adoption_risk_score BETWEEN 1 AND 5),
    business_process_change_score INTEGER CHECK (business_process_change_score BETWEEN 1 AND 5),
    risk_owner VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.digital_transformation_risks IS 'Risks associated with digital transformation initiatives';


CREATE TABLE orm.esg_risk_indicators (
    esg_risk_id SERIAL PRIMARY KEY,
    indicator_name VARCHAR(200) NOT NULL,
    indicator_category VARCHAR(50) CHECK (indicator_category IN ('Environmental', 'Social', 'Governance')),
    measurement_unit VARCHAR(50),
    baseline_value DECIMAL(15,2),
    target_value DECIMAL(15,2),
    current_value DECIMAL(15,2),
    measurement_frequency VARCHAR(50),
    data_source VARCHAR(200),
    reporting_requirement BOOLEAN,
    regulatory_framework VARCHAR(100),
    owner VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.esg_risk_indicators IS 'Key ESG risk indicators for tracking and reporting';


CREATE TABLE orm.risk_treatment_plans (
    treatment_id SERIAL PRIMARY KEY,
    risk_id INTEGER REFERENCES orm.risk_register(risk_id),
    treatment_option VARCHAR(50) CHECK (treatment_option IN ('Avoid', 'Transfer', 'Mitigate', 'Accept')),
    treatment_description TEXT,
    implementation_plan TEXT,
    responsible_party VARCHAR(100),
    start_date DATE,
    target_completion_date DATE,
    actual_completion_date DATE,
    budget DECIMAL(15,2),
    actual_cost DECIMAL(15,2),
    status VARCHAR(50) CHECK (status IN ('Planned', 'In Progress', 'Completed', 'Delayed', 'Cancelled')),
    effectiveness_rating INTEGER CHECK (effectiveness_rating BETWEEN 1 AND 5),
    review_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.risk_treatment_plans IS 'Detailed plans for treating identified risks';

CREATE TABLE orm.risk_treatment_plans (
    treatment_id SERIAL PRIMARY KEY,
    risk_id INTEGER REFERENCES orm.risk_register(risk_id),
    treatment_option VARCHAR(50) CHECK (treatment_option IN ('Avoid', 'Transfer', 'Mitigate', 'Accept')),
    treatment_description TEXT,
    implementation_plan TEXT,
    responsible_party VARCHAR(100),
    start_date DATE,
    target_completion_date DATE,
    actual_completion_date DATE,
    budget DECIMAL(15,2),
    actual_cost DECIMAL(15,2),
    status VARCHAR(50) CHECK (status IN ('Planned', 'In Progress', 'Completed', 'Delayed', 'Cancelled')),
    effectiveness_rating INTEGER CHECK (effectiveness_rating BETWEEN 1 AND 5),
    review_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.risk_treatment_plans IS 'Detailed plans for treating identified risks';


CREATE TABLE orm.risk_culture_assessments (
    assessment_id SERIAL PRIMARY KEY,
    assessment_period VARCHAR(50) NOT NULL,
    department VARCHAR(100),
    risk_awareness_score INTEGER CHECK (risk_awareness_score BETWEEN 1 AND 5),
    risk_communication_score INTEGER CHECK (risk_communication_score BETWEEN 1 AND 5),
    risk_decision_making_score INTEGER CHECK (risk_decision_making_score BETWEEN 1 AND 5),
    risk_ownership_score INTEGER CHECK (risk_ownership_score BETWEEN 1 AND 5),
    risk_learning_score INTEGER CHECK (risk_learning_score BETWEEN 1 AND 5),
    overall_score DECIMAL(5,2) GENERATED ALWAYS AS (
        (risk_awareness_score + risk_communication_score + risk_decision_making_score +
         risk_ownership_score + risk_learning_score) / 5.0
    ) STORED,
    assessment_method VARCHAR(100),
    participants_count INTEGER,
    key_strengths TEXT,
    improvement_areas TEXT,
    action_plan TEXT,
    next_assessment_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.risk_culture_assessments IS 'Assessments of organizational risk culture maturity';

CREATE TABLE orm.fraud_risk_indicators (
    indicator_id SERIAL PRIMARY KEY,
    indicator_name VARCHAR(200) NOT NULL,
    process_area VARCHAR(100),
    risk_factor VARCHAR(100),
    measurement_method VARCHAR(200),
    threshold_value DECIMAL(15,2),
    current_value DECIMAL(15,2),
    trend VARCHAR(20) CHECK (trend IN ('Improving', 'Stable', 'Deteriorating')),
    last_measured_date DATE,
    monitoring_frequency VARCHAR(50),
    owner VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.fraud_risk_indicators IS 'Key indicators for monitoring fraud risks across the organization';


CREATE TABLE orm.operational_resilience_metrics (
    resilience_metric_id SERIAL PRIMARY KEY,
    metric_name VARCHAR(200) NOT NULL,
    business_service VARCHAR(200),
    impact_tolerance VARCHAR(100),
    measurement_period VARCHAR(50),
    maximum_tolerable_outage INTERVAL,
    recovery_time_objective INTERVAL,
    recovery_point_objective INTERVAL,
    actual_recovery_time INTERVAL,
    data_loss_volume VARCHAR(100),
    last_test_date DATE,
    test_result VARCHAR(50) CHECK (test_result IN ('Pass', 'Fail', 'Partial')),
    improvement_actions TEXT,
    next_test_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE orm.operational_resilience_metrics IS 'Metrics for measuring operational resilience of critical business services';


CREATE OR REPLACE VIEW orm.ai_risk_dashboard AS
SELECT
    ar.ai_risk_id,
    ar.system_name,
    ar.risk_category,
    ar.deployment_phase,
    ar.bias_testing_score,
    ar.explainability_score,
    ar.ethical_review_completed,
    rr.risk_name,
    rr.residual_risk_score,
    c.control_name,
    c.control_type,
    csa.assessment_date AS last_control_test_date,
    csa.effectiveness AS control_effectiveness
FROM
    orm.ai_risk_management ar
LEFT JOIN orm.risk_register rr ON ar.system_name = rr.risk_name
LEFT JOIN orm.risk_control_mapping rcm ON rr.risk_id = rcm.risk_id
LEFT JOIN orm.controls c ON rcm.control_id = c.control_id
LEFT JOIN (
    SELECT
        control_id,
        MAX(assessment_date) AS max_date
    FROM
        orm.control_self_assessments
    GROUP BY
        control_id
) latest ON c.control_id = latest.control_id
LEFT JOIN orm.control_self_assessments csa ON csa.control_id = latest.control_id AND csa.assessment_date = latest.max_date;

COMMENT ON VIEW orm.ai_risk_dashboard IS 'Consolidated view of AI system risks and associated controls';


CREATE OR REPLACE VIEW orm.climate_risk_exposure AS
SELECT
    f.facility_id,
    f.facility_name,
    f.facility_type,
    f.country,
    f.criticality,
    cra.physical_risk_score,
    cra.transition_risk_score,
    cra.water_stress_risk,
    cra.flood_risk,
    cra.wildfire_risk,
    cra.carbon_footprint_metric_tons,
    bcp.plan_name AS bcp_plan,
    bcp.recovery_time_objective,
    bcp.recovery_point_objective,
    CASE
        WHEN cra.physical_risk_score >= 4 OR cra.transition_risk_score >= 4 THEN 'High'
        WHEN cra.physical_risk_score >= 3 OR cra.transition_risk_score >= 3 THEN 'Medium'
        ELSE 'Low'
    END AS overall_climate_risk
FROM
    orm.facilities f
LEFT JOIN orm.climate_risk_assessments cra ON f.facility_id = cra.facility_id
LEFT JOIN orm.business_continuity_plans bcp ON f.bcp_id = bcp.bcp_id
WHERE
    cra.assessment_date = (
        SELECT MAX(assessment_date)
        FROM orm.climate_risk_assessments
        WHERE facility_id = f.facility_id
    );

COMMENT ON VIEW orm.climate_risk_exposure IS 'View of facility-level climate risk exposure with BCP information';


CREATE MATERIALIZED VIEW orm.esg_risk_performance_mv AS
SELECT
    indicator_category,
    COUNT(esg_risk_id) AS indicator_count,
    AVG(CASE WHEN current_value > target_value THEN 1 ELSE 0 END) AS percent_off_target,
    AVG(current_value - target_value) AS avg_variance,
    MIN(current_value) AS min_value,
    MAX(current_value) AS max_value,
    COUNT(DISTINCT regulatory_framework) AS frameworks_covered
FROM
    orm.esg_risk_indicators
GROUP BY
    indicator_category
WITH DATA;

COMMENT ON MATERIALIZED VIEW orm.esg_risk_performance_mv IS 'Aggregated ESG risk performance by category';


CREATE MATERIALIZED VIEW orm.digital_transformation_risk_mv AS
SELECT
    project_phase,
    risk_category,
    COUNT(dt_risk_id) AS risk_count,
    AVG(data_migration_risk_score) AS avg_data_migration_risk,
    AVG(user_adoption_risk_score) AS avg_user_adoption_risk,
    AVG(business_process_change_score) AS avg_process_change_risk,
    COUNT(DISTINCT CASE WHEN legacy_system_dependency THEN dt_risk_id END) AS legacy_dependency_count
FROM
    orm.digital_transformation_risks
GROUP BY
    project_phase, risk_category
WITH DATA;

COMMENT ON MATERIALIZED VIEW orm.digital_transformation_risk_mv IS 'Aggregated digital transformation risks by phase and category';


CREATE OR REPLACE PROCEDURE orm.assess_climate_risk(
    p_facility_id INTEGER,
    p_assessed_by VARCHAR(100),
    p_physical_risk_score INTEGER,
    p_transition_risk_score INTEGER,
    p_water_stress_risk BOOLEAN,
    p_flood_risk BOOLEAN,
    p_wildfire_risk BOOLEAN,
    p_carbon_footprint_metric_tons DECIMAL(15,2),
    p_mitigation_plan TEXT,
    p_adaptation_plan TEXT,
    OUT p_assessment_id INTEGER)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Validate inputs
    IF p_physical_risk_score NOT BETWEEN 1 AND 5 OR p_transition_risk_score NOT BETWEEN 1 AND 5 THEN
        RAISE EXCEPTION 'Risk scores must be between 1 and 5';
    END IF;

    IF NOT EXISTS (SELECT 1 FROM orm.facilities WHERE facility_id = p_facility_id) THEN
        RAISE EXCEPTION 'Invalid facility_id: %', p_facility_id;
    END IF;

    -- Insert the assessment
    INSERT INTO orm.climate_risk_assessments (
        facility_id,
        assessment_date,
        physical_risk_score,
        transition_risk_score,
        water_stress_risk,
        flood_risk,
        wildfire_risk,
        carbon_footprint_metric_tons,
        mitigation_plan,
        adaptation_plan,
        assessed_by,
        next_assessment_date
    ) VALUES (
        p_facility_id,
        CURRENT_DATE,
        p_physical_risk_score,
        p_transition_risk_score,
        p_water_stress_risk,
        p_flood_risk,
        p_wildfire_risk,
        p_carbon_footprint_metric_tons,
        p_mitigation_plan,
        p_adaptation_plan,
        p_assessed_by,
        CURRENT_DATE + INTERVAL '1 year'
    ) RETURNING assessment_id INTO p_assessment_id;

    -- Update facility risk rating if climate risk is high
    IF p_physical_risk_score >= 4 OR p_transition_risk_score >= 4 THEN
        UPDATE orm.facilities
        SET
            criticality = 'High',
            updated_at = CURRENT_TIMESTAMP
        WHERE
            facility_id = p_facility_id
            AND criticality <> 'Critical';
    END IF;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE orm.assess_climate_risk IS 'Procedure to conduct and record a climate risk assessment for a facility';


CREATE OR REPLACE PROCEDURE orm.assess_digital_transformation_risk(
    p_project_name VARCHAR(200),
    p_project_phase VARCHAR(50),
    p_risk_category VARCHAR(100),
    p_risk_description TEXT,
    p_legacy_system_dependency BOOLEAN,
    p_data_migration_risk_score INTEGER,
    p_user_adoption_risk_score INTEGER,
    p_business_process_change_score INTEGER,
    p_risk_owner VARCHAR(100),
    OUT p_dt_risk_id INTEGER)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Validate inputs
    IF p_project_phase NOT IN ('Planning', 'Development', 'Implementation', 'Operation') THEN
        RAISE EXCEPTION 'Invalid project phase';
    END IF;

    IF p_risk_category NOT IN ('Cybersecurity', 'Data Privacy', 'System Integration', 'Vendor Lock-in', 'Skill Gaps') THEN
        RAISE EXCEPTION 'Invalid risk category';
    END IF;

    IF p_data_migration_risk_score NOT BETWEEN 1 AND 5 OR
       p_user_adoption_risk_score NOT BETWEEN 1 AND 5 OR
       p_business_process_change_score NOT BETWEEN 1 AND 5 THEN
        RAISE EXCEPTION 'All risk scores must be between 1 and 5';
    END IF;

    -- Insert the risk assessment
    INSERT INTO orm.digital_transformation_risks (
        project_name,
        project_phase,
        risk_category,
        risk_description,
        legacy_system_dependency,
        data_migration_risk_score,
        user_adoption_risk_score,
        business_process_change_score,
        risk_owner
    ) VALUES (
        p_project_name,
        p_project_phase,
        p_risk_category,
        p_risk_description,
        p_legacy_system_dependency,
        p_data_migration_risk_score,
        p_user_adoption_risk_score,
        p_business_process_change_score,
        p_risk_owner
    ) RETURNING dt_risk_id INTO p_dt_risk_id;

    -- Add to risk register if not already present
    IF NOT EXISTS (SELECT 1 FROM orm.risk_register WHERE risk_name = p_project_name || ' - ' || p_risk_category) THEN
        INSERT INTO orm.risk_register (
            risk_name,
            description,
            inherent_impact,
            inherent_likelihood,
            residual_impact,
            residual_likelihood,
            status,
            risk_owner
        ) VALUES (
            p_project_name || ' - ' || p_risk_category,
            p_risk_description,
            GREATEST(p_data_migration_risk_score, p_user_adoption_risk_score, p_business_process_change_score),
            3, -- Default likelihood
            GREATEST(p_data_migration_risk_score, p_user_adoption_risk_score, p_business_process_change_score),
            3, -- Default likelihood
            'Open',
            p_risk_owner
        );
    END IF;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE orm.assess_digital_transformation_risk IS 'Procedure to assess and record risks associated with digital transformation projects';


CREATE TABLE orm.risk_register_audit (
    audit_id SERIAL PRIMARY KEY,
    risk_id INTEGER,
    changed_by VARCHAR(100) NOT NULL,
    change_type VARCHAR(20) CHECK (change_type IN ('INSERT', 'UPDATE', 'DELETE')),
    change_timestamp TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    old_values JSONB,
    new_values JSONB
);

COMMENT ON TABLE orm.risk_register_audit IS 'Audit trail for changes to the risk register';

CREATE TABLE orm.controls_audit (
    audit_id SERIAL PRIMARY KEY,
    control_id INTEGER,
    changed_by VARCHAR(100) NOT NULL,
    change_type VARCHAR(20) CHECK (change_type IN ('INSERT', 'UPDATE', 'DELETE')),
    change_timestamp TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    old_values JSONB,
    new_values JSONB
);

COMMENT ON TABLE orm.controls_audit IS 'Audit trail for changes to controls';


CREATE OR REPLACE FUNCTION orm.log_risk_register_changes()
RETURNS TRIGGER AS $$
BEGIN
    IF TG_OP = 'INSERT' THEN
        INSERT INTO orm.risk_register_audit (
            risk_id,
            changed_by,
            change_type,
            new_values
        ) VALUES (
            NEW.risk_id,
            current_user,
            'INSERT',
            to_jsonb(NEW)
        );
    ELSIF TG_OP = 'UPDATE' THEN
        INSERT INTO orm.risk_register_audit (
            risk_id,
            changed_by,
            change_type,
            old_values,
            new_values
        ) VALUES (
            NEW.risk_id,
            current_user,
            'UPDATE',
            to_jsonb(OLD),
            to_jsonb(NEW)
        );
    ELSIF TG_OP = 'DELETE' THEN
        INSERT INTO orm.risk_register_audit (
            risk_id,
            changed_by,
            change_type,
            old_values
        ) VALUES (
            OLD.risk_id,
            current_user,
            'DELETE',
            to_jsonb(OLD)
        );
    END IF;
    RETURN NULL;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER risk_register_audit_trigger
AFTER INSERT OR UPDATE OR DELETE ON orm.risk_register
FOR EACH ROW EXECUTE FUNCTION orm.log_risk_register_changes();


-- Add indexes for performance optimization
CREATE INDEX idx_risk_register_category ON orm.risk_register(category_id);
CREATE INDEX idx_risk_register_process ON orm.risk_register(process_id);
CREATE INDEX idx_loss_events_risk ON orm.loss_events(risk_id);
CREATE INDEX idx_loss_events_date ON orm.loss_events(event_date);
CREATE INDEX idx_kri_measurements ON orm.kri_measurements(kri_id, measurement_date);
CREATE INDEX idx_vendor_assessments ON orm.vendor_risk_assessments(vendor_id, assessment_date);
CREATE INDEX idx_control_assessments ON orm.control_self_assessments(control_id, assessment_date);
CREATE INDEX idx_risk_scenarios ON orm.risk_scenarios(risk_id, severity);
CREATE INDEX idx_facilities_country ON orm.facilities(country, criticality);
CREATE INDEX idx_climate_risk_facility ON orm.climate_risk_assessments(facility_id, assessment_date);

-- Partition loss_events by year for better performance
CREATE TABLE orm.loss_events_partitioned (
    LIKE orm.loss_events INCLUDING DEFAULTS INCLUDING CONSTRAINTS
) PARTITION BY RANGE (event_date);

-- Create yearly partitions
CREATE TABLE orm.loss_events_y2023 PARTITION OF orm.loss_events_partitioned
    FOR VALUES FROM ('2023-01-01') TO ('2024-01-01');

CREATE TABLE orm.loss_events_y2024 PARTITION OF orm.loss_events_partitioned
    FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');



----------------------
--Model Risk Governance
----------------------
CREATE SCHEMA model_risk_governance;
COMMENT ON SCHEMA model_risk_governance IS 'Schema for managing Model Risk Governance including inventory, validation, monitoring, and reporting';



CREATE TABLE model_risk_governance.models (
    model_id SERIAL PRIMARY KEY,
    model_name VARCHAR(255) NOT NULL,
    model_description TEXT,
    model_purpose TEXT NOT NULL,
    model_version VARCHAR(50) NOT NULL,
    model_type_id INT NOT NULL,
    model_category_id INT NOT NULL,
    risk_rating_id INT NOT NULL,
    owner_id INT NOT NULL,
    department_id INT NOT NULL,
    development_date DATE NOT NULL,
    deployment_date DATE,
    retirement_date DATE,
    is_active BOOLEAN DEFAULT TRUE,
    is_critical BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_type_id) REFERENCES model_risk_governance.model_types(type_id),
    FOREIGN KEY (model_category_id) REFERENCES model_risk_governance.model_categories(category_id),
    FOREIGN KEY (risk_rating_id) REFERENCES model_risk_governance.risk_ratings(rating_id),
    FOREIGN KEY (owner_id) REFERENCES core.employees(employee_id),
    FOREIGN KEY (department_id) REFERENCES core.departments(department_id)
);

COMMENT ON TABLE model_risk_governance.models IS 'Central repository for all models used in the organization';
COMMENT ON COLUMN model_risk_governance.models.model_id IS 'Unique identifier for each model';
COMMENT ON COLUMN model_risk_governance.models.model_name IS 'Name of the model';
COMMENT ON COLUMN model_risk_governance.models.model_description IS 'Detailed description of the model';
COMMENT ON COLUMN model_risk_governance.models.model_purpose IS 'Intended use and business purpose of the model';
COMMENT ON COLUMN model_risk_governance.models.model_version IS 'Version identifier of the model';
COMMENT ON COLUMN model_risk_governance.models.is_critical IS 'Flag indicating if the model is considered critical for operations';

CREATE TABLE model_risk_governance.model_types (
    type_id SERIAL PRIMARY KEY,
    type_name VARCHAR(100) NOT NULL,
    type_description TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE model_risk_governance.model_categories (
    category_id SERIAL PRIMARY KEY,
    category_name VARCHAR(100) NOT NULL,
    category_description TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE model_risk_governance.risk_ratings (
    rating_id SERIAL PRIMARY KEY,
    rating_name VARCHAR(50) NOT NULL,
    rating_description TEXT,
    severity_level INT NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE model_risk_governance.model_ownership (
    ownership_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    owner_id INT NOT NULL,
    start_date DATE NOT NULL,
    end_date DATE,
    is_current BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (owner_id) REFERENCES core.employees(employee_id)
);

COMMENT ON TABLE model_risk_governance.model_ownership IS 'Tracks current and historical ownership of models';


CREATE TABLE model_risk_governance.model_documentation (
    documentation_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    documentation_type_id INT NOT NULL,
    document_name VARCHAR(255) NOT NULL,
    document_location VARCHAR(512),
    document_version VARCHAR(50) NOT NULL,
    review_date DATE,
    next_review_date DATE,
    is_approved BOOLEAN DEFAULT FALSE,
    approved_by INT,
    approved_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (documentation_type_id) REFERENCES model_risk_governance.documentation_types(type_id),
    FOREIGN KEY (approved_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.documentation_types (
    type_id SERIAL PRIMARY KEY,
    type_name VARCHAR(100) NOT NULL,
    description TEXT,
    is_required BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);



CREATE TABLE model_risk_governance.model_validations (
    validation_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    validation_type_id INT NOT NULL,
    validation_date DATE NOT NULL,
    validated_by INT NOT NULL,
    next_validation_date DATE,
    is_approved BOOLEAN DEFAULT FALSE,
    approval_date DATE,
    approved_by INT,
    validation_score DECIMAL(5,2),
    validation_status_id INT NOT NULL,
    summary_report_location VARCHAR(512),
    detailed_report_location VARCHAR(512),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (validation_type_id) REFERENCES model_risk_governance.validation_types(type_id),
    FOREIGN KEY (validated_by) REFERENCES core.employees(employee_id),
    FOREIGN KEY (approved_by) REFERENCES core.employees(employee_id),
    FOREIGN KEY (validation_status_id) REFERENCES model_risk_governance.validation_statuses(status_id)
);

CREATE TABLE model_risk_governance.validation_types (
    type_id SERIAL PRIMARY KEY,
    type_name VARCHAR(100) NOT NULL,
    description TEXT,
    frequency_months INT,
    is_required BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE model_risk_governance.validation_statuses (
    status_id SERIAL PRIMARY KEY,
    status_name VARCHAR(50) NOT NULL,
    description TEXT,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE model_risk_governance.validation_findings (
    finding_id SERIAL PRIMARY KEY,
    validation_id INT NOT NULL,
    finding_type_id INT NOT NULL,
    severity_id INT NOT NULL,
    description TEXT NOT NULL,
    recommendation TEXT,
    remediation_deadline DATE,
    is_remediated BOOLEAN DEFAULT FALSE,
    remediation_date DATE,
    remediation_notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (validation_id) REFERENCES model_risk_governance.model_validations(validation_id),
    FOREIGN KEY (finding_type_id) REFERENCES model_risk_governance.finding_types(type_id),
    FOREIGN KEY (severity_id) REFERENCES model_risk_governance.severity_levels(level_id)
);



CREATE TABLE model_risk_governance.model_performance (
    performance_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    monitoring_date DATE NOT NULL,
    monitoring_period_id INT NOT NULL,
    accuracy_score DECIMAL(5,2),
    precision_score DECIMAL(5,2),
    recall_score DECIMAL(5,2),
    auc_score DECIMAL(5,2),
    psi_score DECIMAL(5,2),
    ks_score DECIMAL(5,2),
    benchmark_comparison DECIMAL(5,2),
    performance_status_id INT NOT NULL,
    is_alert_triggered BOOLEAN DEFAULT FALSE,
    alert_description TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (monitoring_period_id) REFERENCES model_risk_governance.monitoring_periods(period_id),
    FOREIGN KEY (performance_status_id) REFERENCES model_risk_governance.performance_statuses(status_id)
);

CREATE TABLE model_risk_governance.performance_alerts (
    alert_id SERIAL PRIMARY KEY,
    performance_id INT NOT NULL,
    alert_type_id INT NOT NULL,
    alert_date TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    alert_severity_id INT NOT NULL,
    description TEXT NOT NULL,
    assigned_to INT,
    status_id INT NOT NULL,
    resolution_date TIMESTAMP WITH TIME ZONE,
    resolution_notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (performance_id) REFERENCES model_risk_governance.model_performance(performance_id),
    FOREIGN KEY (alert_type_id) REFERENCES model_risk_governance.alert_types(type_id),
    FOREIGN KEY (alert_severity_id) REFERENCES model_risk_governance.severity_levels(level_id),
    FOREIGN KEY (assigned_to) REFERENCES core.employees(employee_id),
    FOREIGN KEY (status_id) REFERENCES model_risk_governance.alert_statuses(status_id)
);



CREATE TABLE model_risk_governance.model_changes (
    change_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    change_type_id INT NOT NULL,
    change_request_date DATE NOT NULL,
    requested_by INT NOT NULL,
    change_description TEXT NOT NULL,
    impact_assessment TEXT,
    approval_status_id INT NOT NULL,
    approved_by INT,
    approval_date DATE,
    implementation_date DATE,
    regression_test_result BOOLEAN,
    rollback_required BOOLEAN DEFAULT FALSE,
    rollback_date DATE,
    rollback_reason TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (change_type_id) REFERENCES model_risk_governance.change_types(type_id),
    FOREIGN KEY (requested_by) REFERENCES core.employees(employee_id),
    FOREIGN KEY (approved_by) REFERENCES core.employees(employee_id),
    FOREIGN KEY (approval_status_id) REFERENCES model_risk_governance.approval_statuses(status_id)
);


CREATE TABLE model_risk_governance.model_risk_assessments (
    assessment_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    assessment_date DATE NOT NULL,
    assessed_by INT NOT NULL,
    risk_score DECIMAL(5,2) NOT NULL,
    risk_level_id INT NOT NULL,
    impact_score DECIMAL(5,2) NOT NULL,
    likelihood_score DECIMAL(5,2) NOT NULL,
    risk_appetite_alignment BOOLEAN,
    risk_description TEXT,
    mitigation_plan TEXT,
    next_assessment_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (assessed_by) REFERENCES core.employees(employee_id),
    FOREIGN KEY (risk_level_id) REFERENCES model_risk_governance.risk_levels(level_id)
);


CREATE TABLE model_risk_governance.third_party_models (
    third_party_model_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    vendor_id INT NOT NULL,
    contract_id VARCHAR(100),
    sla_agreement TEXT,
    source_code_access BOOLEAN,
    documentation_access BOOLEAN,
    validation_requirements TEXT,
    reassessment_frequency_months INT,
    next_reassessment_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (vendor_id) REFERENCES core.vendors(vendor_id)
);

CREATE TABLE model_risk_governance.vendor_assessments (
    assessment_id SERIAL PRIMARY KEY,
    vendor_id INT NOT NULL,
    assessment_date DATE NOT NULL,
    assessed_by INT NOT NULL,
    overall_score DECIMAL(5,2),
    financial_stability_score DECIMAL(5,2),
    technical_capability_score DECIMAL(5,2),
    security_score DECIMAL(5,2),
    compliance_score DECIMAL(5,2),
    recommendation TEXT,
    next_assessment_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (vendor_id) REFERENCES core.vendors(vendor_id),
    FOREIGN KEY (assessed_by) REFERENCES core.employees(employee_id)
);


CREATE TABLE model_risk_governance.governance_committees (
    committee_id SERIAL PRIMARY KEY,
    committee_name VARCHAR(255) NOT NULL,
    committee_purpose TEXT,
    chairperson_id INT,
    secretary_id INT,
    meeting_frequency VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (chairperson_id) REFERENCES core.employees(employee_id),
    FOREIGN KEY (secretary_id) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.committee_members (
    member_id SERIAL PRIMARY KEY,
    committee_id INT NOT NULL,
    employee_id INT NOT NULL,
    role_id INT NOT NULL,
    start_date DATE NOT NULL,
    end_date DATE,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (committee_id) REFERENCES model_risk_governance.governance_committees(committee_id),
    FOREIGN KEY (employee_id) REFERENCES core.employees(employee_id),
    FOREIGN KEY (role_id) REFERENCES model_risk_governance.committee_roles(role_id)
);

CREATE TABLE model_risk_governance.governance_meetings (
    meeting_id SERIAL PRIMARY KEY,
    committee_id INT NOT NULL,
    meeting_date TIMESTAMP WITH TIME ZONE NOT NULL,
    location VARCHAR(255),
    chairperson_id INT,
    minutes_document_location VARCHAR(512),
    is_quorum_present BOOLEAN,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (committee_id) REFERENCES model_risk_governance.governance_committees(committee_id),
    FOREIGN KEY (chairperson_id) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.meeting_agenda_items (
    agenda_item_id SERIAL PRIMARY KEY,
    meeting_id INT NOT NULL,
    item_title VARCHAR(255) NOT NULL,
    item_description TEXT,
    presenter_id INT,
    discussion_summary TEXT,
    decisions_made TEXT,
    action_items TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (meeting_id) REFERENCES model_risk_governance.governance_meetings(meeting_id),
    FOREIGN KEY (presenter_id) REFERENCES core.employees(employee_id)
);


CREATE TABLE model_risk_governance.training_programs (
    program_id SERIAL PRIMARY KEY,
    program_name VARCHAR(255) NOT NULL,
    program_description TEXT,
    target_audience_id INT NOT NULL,
    training_type_id INT NOT NULL,
    frequency_months INT,
    duration_hours DECIMAL(5,2),
    is_certification_required BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (target_audience_id) REFERENCES model_risk_governance.target_audiences(audience_id),
    FOREIGN KEY (training_type_id) REFERENCES model_risk_governance.training_types(type_id)
);

CREATE TABLE model_risk_governance.training_sessions (
    session_id SERIAL PRIMARY KEY,
    program_id INT NOT NULL,
    session_date TIMESTAMP WITH TIME ZONE NOT NULL,
    trainer_id INT NOT NULL,
    location VARCHAR(255),
    max_participants INT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (program_id) REFERENCES model_risk_governance.training_programs(program_id),
    FOREIGN KEY (trainer_id) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.training_participants (
    participant_id SERIAL PRIMARY KEY,
    session_id INT NOT NULL,
    employee_id INT NOT NULL,
    attendance_status_id INT NOT NULL,
    completion_status_id INT NOT NULL,
    assessment_score DECIMAL(5,2),
    feedback TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (session_id) REFERENCES model_risk_governance.training_sessions(session_id),
    FOREIGN KEY (employee_id) REFERENCES core.employees(employee_id),
    FOREIGN KEY (attendance_status_id) REFERENCES model_risk_governance.attendance_statuses(status_id),
    FOREIGN KEY (completion_status_id) REFERENCES model_risk_governance.completion_statuses(status_id)
);

CREATE OR REPLACE VIEW model_risk_governance.model_validation_coverage AS
SELECT
    m.model_id,
    m.model_name,
    m.model_version,
    r.rating_name AS risk_rating,
    CASE
        WHEN m.is_critical THEN 'Critical'
        ELSE 'Non-Critical'
    END AS criticality,
    (SELECT COUNT(*) FROM model_risk_governance.model_validations v
     WHERE v.model_id = m.model_id AND v.is_approved = TRUE) AS validation_count,
    (SELECT MAX(v.validation_date) FROM model_risk_governance.model_validations v
     WHERE v.model_id = m.model_id AND v.is_approved = TRUE) AS last_validation_date,
    (SELECT v.next_validation_date FROM model_risk_governance.model_validations v
     WHERE v.model_id = m.model_id AND v.is_approved = TRUE
     ORDER BY v.validation_date DESC LIMIT 1) AS next_validation_date,
    CASE
        WHEN (SELECT COUNT(*) FROM model_risk_governance.model_validations v
              WHERE v.model_id = m.model_id AND v.is_approved = TRUE) > 0 THEN 'Validated'
        ELSE 'Not Validated'
    END AS validation_status,
    CASE
        WHEN (SELECT COUNT(*) FROM model_risk_governance.model_validations v
              WHERE v.model_id = m.model_id AND v.is_approved = TRUE) > 0 AND
             (SELECT MAX(v.validation_date) FROM model_risk_governance.model_validations v
              WHERE v.model_id = m.model_id AND v.is_approved = TRUE) < CURRENT_DATE - INTERVAL '1 year' THEN 'Overdue'
        WHEN (SELECT COUNT(*) FROM model_risk_governance.model_validations v
              WHERE v.model_id = m.model_id AND v.is_approved = TRUE) = 0 THEN 'Never Validated'
        ELSE 'Within Schedule'
    END AS validation_timeliness
FROM
    model_risk_governance.models m
JOIN
    model_risk_governance.risk_ratings r ON m.risk_rating_id = r.rating_id
WHERE
    m.is_active = TRUE;

COMMENT ON VIEW model_risk_governance.model_validation_coverage IS 'Provides a comprehensive view of model validation coverage including validation status and timeliness';

CREATE OR REPLACE VIEW model_risk_governance.model_performance_dashboard AS
SELECT
    m.model_id,
    m.model_name,
    m.model_version,
    r.rating_name AS risk_rating,
    p.monitoring_date,
    p.accuracy_score,
    p.precision_score,
    p.recall_score,
    p.auc_score,
    p.psi_score,
    p.ks_score,
    p.benchmark_comparison,
    s.status_name AS performance_status,
    CASE
        WHEN p.is_alert_triggered THEN 'Alert'
        ELSE 'Normal'
    END AS alert_status,
    (SELECT AVG(accuracy_score) FROM model_risk_governance.model_performance
     WHERE model_id = m.model_id AND monitoring_date BETWEEN CURRENT_DATE - INTERVAL '6 months' AND CURRENT_DATE) AS avg_6mo_accuracy,
    (SELECT AVG(accuracy_score) FROM model_risk_governance.model_performance
     WHERE model_id = m.model_id AND monitoring_date BETWEEN CURRENT_DATE - INTERVAL '12 months' AND CURRENT_DATE - INTERVAL '6 months') AS avg_12mo_accuracy,
    ((SELECT AVG(accuracy_score) FROM model_risk_governance.model_performance
      WHERE model_id = m.model_id AND monitoring_date BETWEEN CURRENT_DATE - INTERVAL '6 months' AND CURRENT_DATE) -
     (SELECT AVG(accuracy_score) FROM model_risk_governance.model_performance
      WHERE model_id = m.model_id AND monitoring_date BETWEEN CURRENT_DATE - INTERVAL '12 months' AND CURRENT_DATE - INTERVAL '6 months')) AS accuracy_change,
    CASE
        WHEN ((SELECT AVG(accuracy_score) FROM model_risk_governance.model_performance
               WHERE model_id = m.model_id AND monitoring_date BETWEEN CURRENT_DATE - INTERVAL '6 months' AND CURRENT_DATE) -
              (SELECT AVG(accuracy_score) FROM model_risk_governance.model_performance
               WHERE model_id = m.model_id AND monitoring_date BETWEEN CURRENT_DATE - INTERVAL '12 months' AND CURRENT_DATE - INTERVAL '6 months')) < -0.05 THEN 'Significant Degradation'
        WHEN ((SELECT AVG(accuracy_score) FROM model_risk_governance.model_performance
               WHERE model_id = m.model_id AND monitoring_date BETWEEN CURRENT_DATE - INTERVAL '6 months' AND CURRENT_DATE) -
              (SELECT AVG(accuracy_score) FROM model_risk_governance.model_performance
               WHERE model_id = m.model_id AND monitoring_date BETWEEN CURRENT_DATE - INTERVAL '12 months' AND CURRENT_DATE - INTERVAL '6 months')) < -0.02 THEN 'Moderate Degradation'
        WHEN ((SELECT AVG(accuracy_score) FROM model_risk_governance.model_performance
               WHERE model_id = m.model_id AND monitoring_date BETWEEN CURRENT_DATE - INTERVAL '6 months' AND CURRENT_DATE) -
              (SELECT AVG(accuracy_score) FROM model_risk_governance.model_performance
               WHERE model_id = m.model_id AND monitoring_date BETWEEN CURRENT_DATE - INTERVAL '12 months' AND CURRENT_DATE - INTERVAL '6 months')) > 0.02 THEN 'Improvement'
        ELSE 'Stable'
    END AS performance_trend
FROM
    model_risk_governance.models m
JOIN
    model_risk_governance.model_performance p ON m.model_id = p.model_id
JOIN
    model_risk_governance.risk_ratings r ON m.risk_rating_id = r.rating_id
JOIN
    model_risk_governance.performance_statuses s ON p.performance_status_id = s.status_id
WHERE
    m.is_active = TRUE AND
    p.monitoring_date = (SELECT MAX(monitoring_date) FROM model_risk_governance.model_performance WHERE model_id = m.model_id);

COMMENT ON VIEW model_risk_governance.model_performance_dashboard IS 'Provides a dashboard view of model performance metrics including trends and degradation alerts';



CREATE OR REPLACE VIEW model_risk_governance.model_risk_score_accuracy AS
SELECT
    m.model_id,
    m.model_name,
    m.model_version,
    r.rating_name AS current_risk_rating,
    a.risk_score AS calculated_risk_score,
    a.risk_level_id,
    l.level_name AS calculated_risk_level,
    a.assessment_date,
    a.assessed_by,
    e.employee_name AS assessor_name,
    CASE
        WHEN r.severity_level = l.severity_level THEN 'Match'
        WHEN r.severity_level < l.severity_level THEN 'Underestimated'
        ELSE 'Overestimated'
    END AS rating_accuracy,
    CASE
        WHEN r.severity_level = l.severity_level THEN 1
        ELSE 0
    END AS is_accurate,
    a.mitigation_plan,
    a.next_assessment_date
FROM
    model_risk_governance.models m
JOIN
    model_risk_governance.risk_ratings r ON m.risk_rating_id = r.rating_id
JOIN
    model_risk_governance.model_risk_assessments a ON m.model_id = a.model_id
JOIN
    model_risk_governance.risk_levels l ON a.risk_level_id = l.level_id
JOIN
    core.employees e ON a.assessed_by = e.employee_id
WHERE
    m.is_active = TRUE AND
    a.assessment_date = (SELECT MAX(assessment_date) FROM model_risk_governance.model_risk_assessments WHERE model_id = m.model_id);

COMMENT ON VIEW model_risk_governance.model_risk_score_accuracy IS 'Assesses the accuracy of model risk scores by comparing current risk ratings with calculated risk levels';



CREATE OR REPLACE VIEW model_risk_governance.remediation_times AS
SELECT
    f.finding_id,
    m.model_id,
    m.model_name,
    m.model_version,
    f.description AS finding_description,
    f.severity_id,
    s.level_name AS severity,
    f.remediation_deadline,
    f.remediation_date,
    CASE
        WHEN f.is_remediated THEN 'Resolved'
        WHEN f.remediation_date IS NULL AND CURRENT_DATE > f.remediation_deadline THEN 'Overdue'
        WHEN f.remediation_date IS NULL THEN 'Pending'
        ELSE 'Unknown'
    END AS remediation_status,
    CASE
        WHEN f.is_remediated THEN
            CASE
                WHEN f.remediation_date <= f.remediation_deadline THEN 'On Time'
                ELSE 'Late'
            END
        WHEN CURRENT_DATE > f.remediation_deadline THEN 'Overdue'
        ELSE 'Pending'
    END AS timeliness,
    CASE
        WHEN f.is_remediated THEN
            EXTRACT(DAY FROM (f.remediation_date - f.remediation_deadline))
        WHEN CURRENT_DATE > f.remediation_deadline THEN
            EXTRACT(DAY FROM (CURRENT_DATE - f.remediation_deadline))
        ELSE NULL
    END AS days_overdue,
    CASE
        WHEN f.is_remediated THEN
            EXTRACT(DAY FROM (f.remediation_date - v.validation_date))
        ELSE
            EXTRACT(DAY FROM (CURRENT_DATE - v.validation_date))
    END AS days_to_remediate,
    v.validation_date,
    v.validated_by,
    e.employee_name AS validator_name
FROM
    model_risk_governance.validation_findings f
JOIN
    model_risk_governance.model_validations v ON f.validation_id = v.validation_id
JOIN
    model_risk_governance.models m ON v.model_id = m.model_id
JOIN
    model_risk_governance.severity_levels s ON f.severity_id = s.level_id
JOIN
    core.employees e ON v.validated_by = e.employee_id
WHERE
    m.is_active = TRUE;

COMMENT ON VIEW model_risk_governance.remediation_times IS 'Tracks the time taken to remediate model validation findings and identifies overdue items';


CREATE OR REPLACE VIEW model_risk_governance.system_availability AS
SELECT
    DATE_TRUNC('month', log_date) AS month,
    COUNT(*) AS total_checks,
    SUM(CASE WHEN is_available THEN 1 ELSE 0 END) AS available_checks,
    ROUND(SUM(CASE WHEN is_available THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) AS availability_percentage,
    SUM(CASE WHEN NOT is_available THEN 1 ELSE 0 END) AS outage_events,
    SUM(CASE WHEN NOT is_available THEN outage_minutes ELSE 0 END) AS total_outage_minutes,
    AVG(CASE WHEN NOT is_available THEN outage_minutes ELSE NULL END) AS avg_outage_duration,
    MAX(CASE WHEN NOT is_available THEN outage_minutes ELSE 0 END) AS max_outage_duration,
    STRING_AGG(CASE WHEN NOT is_available THEN outage_reason ELSE NULL END, '; ') AS outage_reasons
FROM
    model_risk_governance.system_availability_logs
GROUP BY
    DATE_TRUNC('month', log_date)
ORDER BY
    month DESC;

COMMENT ON VIEW model_risk_governance.system_availability IS 'Calculates system availability metrics for the Model Risk Governance module on a monthly basis';



CREATE OR REPLACE PROCEDURE model_risk_governance.insert_model_with_validation(
    p_model_name VARCHAR(255),
    p_model_description TEXT,
    p_model_purpose TEXT,
    p_model_version VARCHAR(50),
    p_model_type_id INT,
    p_model_category_id INT,
    p_risk_rating_id INT,
    p_owner_id INT,
    p_department_id INT,
    p_is_critical BOOLEAN,
    p_validation_type_id INT,
    p_validated_by INT,
    p_validation_notes TEXT,
    OUT p_model_id INT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_validation_id INT;
BEGIN
    -- Insert the new model
    INSERT INTO model_risk_governance.models (
        model_name, model_description, model_purpose, model_version,
        model_type_id, model_category_id, risk_rating_id, owner_id,
        department_id, development_date, is_critical
    ) VALUES (
        p_model_name, p_model_description, p_model_purpose, p_model_version,
        p_model_type_id, p_model_category_id, p_risk_rating_id, p_owner_id,
        p_department_id, CURRENT_DATE, p_is_critical
    ) RETURNING model_id INTO p_model_id;

    -- Record model ownership
    INSERT INTO model_risk_governance.model_ownership (
        model_id, owner_id, start_date
    ) VALUES (
        p_model_id, p_owner_id, CURRENT_DATE
    );

    -- Perform initial validation
    INSERT INTO model_risk_governance.model_validations (
        model_id, validation_type_id, validation_date, validated_by,
        validation_status_id, summary_report_location
    ) VALUES (
        p_model_id, p_validation_type_id, CURRENT_DATE, p_validated_by,
        (SELECT status_id FROM model_risk_governance.validation_statuses WHERE status_name = 'Pending Approval'),
        p_validation_notes
    ) RETURNING validation_id INTO v_validation_id;

    -- Log the operation
    INSERT INTO model_risk_governance.audit_logs (
        action_type, table_name, record_id, action_by, action_details
    ) VALUES (
        'INSERT', 'models', p_model_id, p_validated_by,
        'New model created with initial validation'
    );
END;
$$;

COMMENT ON PROCEDURE model_risk_governance.insert_model_with_validation IS 'Creates a new model record and initiates the validation process in a single transaction';


CREATE OR REPLACE PROCEDURE model_risk_governance.update_model_risk_rating(
    p_model_id INT,
    p_new_risk_rating_id INT,
    p_updated_by INT,
    p_reason_for_change TEXT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_old_risk_rating_id INT;
    v_model_name VARCHAR(255);
BEGIN
    -- Get current risk rating and model name for logging
    SELECT risk_rating_id, model_name INTO v_old_risk_rating_id, v_model_name
    FROM model_risk_governance.models
    WHERE model_id = p_model_id;

    -- Update the risk rating
    UPDATE model_risk_governance.models
    SET risk_rating_id = p_new_risk_rating_id,
        updated_at = CURRENT_TIMESTAMP
    WHERE model_id = p_model_id;

    -- Log the change in risk rating history
    INSERT INTO model_risk_governance.risk_rating_history (
        model_id, old_risk_rating_id, new_risk_rating_id, changed_by, change_reason
    ) VALUES (
        p_model_id, v_old_risk_rating_id, p_new_risk_rating_id, p_updated_by, p_reason_for_change
    );

    -- If the risk rating increased, check if additional validation is needed
    IF (SELECT severity_level FROM model_risk_governance.risk_ratings WHERE rating_id = p_new_risk_rating_id) >
       (SELECT severity_level FROM model_risk_governance.risk_ratings WHERE rating_id = v_old_risk_rating_id) THEN

        INSERT INTO model_risk_governance.model_validations (
            model_id, validation_type_id, validation_date, validated_by,
            validation_status_id, summary_report_location
        ) VALUES (
            p_model_id,
            (SELECT type_id FROM model_risk_governance.validation_types WHERE type_name = 'Risk Rating Change'),
            CURRENT_DATE, p_updated_by,
            (SELECT status_id FROM model_risk_governance.validation_statuses WHERE status_name = 'Pending'),
            'Triggered by risk rating increase from ' ||
            (SELECT rating_name FROM model_risk_governance.risk_ratings WHERE rating_id = v_old_risk_rating_id) ||
            ' to ' ||
            (SELECT rating_name FROM model_risk_governance.risk_ratings WHERE rating_id = p_new_risk_rating_id)
        );
    END IF;

    -- Log the operation
    INSERT INTO model_risk_governance.audit_logs (
        action_type, table_name, record_id, action_by, action_details
    ) VALUES (
        'UPDATE', 'models', p_model_id, p_updated_by,
        'Updated risk rating for model ' || v_model_name || ' from ' ||
        (SELECT rating_name FROM model_risk_governance.risk_ratings WHERE rating_id = v_old_risk_rating_id) ||
        ' to ' ||
        (SELECT rating_name FROM model_risk_governance.risk_ratings WHERE rating_id = p_new_risk_rating_id)
    );
END;
$$;

COMMENT ON PROCEDURE model_risk_governance.update_model_risk_rating IS 'Updates a model''s risk rating and logs the change, triggering additional validation if risk increases';



CREATE OR REPLACE PROCEDURE model_risk_governance.process_performance_alert(
    p_alert_id INT,
    p_action_type VARCHAR(50),
    p_action_by INT,
    p_action_notes TEXT,
    p_new_status_id INT DEFAULT NULL,
    p_assigned_to INT DEFAULT NULL
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_model_id INT;
    v_model_name VARCHAR(255);
    v_alert_description TEXT;
    v_current_status_id INT;
BEGIN
    -- Get alert details for logging
    SELECT a.model_id, m.model_name, a.description, a.status_id
    INTO v_model_id, v_model_name, v_alert_description, v_current_status_id
    FROM model_risk_governance.performance_alerts a
    JOIN model_risk_governance.model_performance p ON a.performance_id = p.performance_id
    JOIN model_risk_governance.models m ON p.model_id = m.model_id
    WHERE a.alert_id = p_alert_id;

    -- Update alert based on action type
    IF p_action_type = 'ACKNOWLEDGE' THEN
        UPDATE model_risk_governance.performance_alerts
        SET status_id = (SELECT status_id FROM model_risk_governance.alert_statuses WHERE status_name = 'Acknowledged'),
            acknowledged_by = p_action_by,
            acknowledged_at = CURRENT_TIMESTAMP,
            notes = COALESCE(notes, '') || '\nAcknowledged: ' || p_action_notes
        WHERE alert_id = p_alert_id;

    ELSIF p_action_type = 'ASSIGN' THEN
        IF p_assigned_to IS NULL THEN
            RAISE EXCEPTION 'ASSIGN action requires assigned_to parameter';
        END IF;

        UPDATE model_risk_governance.performance_alerts
        SET status_id = (SELECT status_id FROM model_risk_governance.alert_statuses WHERE status_name = 'Assigned'),
            assigned_to = p_assigned_to,
            assigned_at = CURRENT_TIMESTAMP,
            notes = COALESCE(notes, '') || '\nAssigned: ' || p_action_notes
        WHERE alert_id = p_alert_id;

    ELSIF p_action_type = 'RESOLVE' THEN
        UPDATE model_risk_governance.performance_alerts
        SET status_id = (SELECT status_id FROM model_risk_governance.alert_statuses WHERE status_name = 'Resolved'),
            resolution_date = CURRENT_TIMESTAMP,
            resolved_by = p_action_by,
            notes = COALESCE(notes, '') || '\nResolved: ' || p_action_notes
        WHERE alert_id = p_alert_id;

    ELSIF p_action_type = 'CHANGE_STATUS' THEN
        IF p_new_status_id IS NULL THEN
            RAISE EXCEPTION 'CHANGE_STATUS action requires new_status_id parameter';
        END IF;

        UPDATE model_risk_governance.performance_alerts
        SET status_id = p_new_status_id,
            notes = COALESCE(notes, '') || '\nStatus Changed: ' || p_action_notes
        WHERE alert_id = p_alert_id;

    ELSE
        RAISE EXCEPTION 'Invalid action type: %', p_action_type;
    END IF;

    -- Log the operation
    INSERT INTO model_risk_governance.audit_logs (
        action_type, table_name, record_id, action_by, action_details
    ) VALUES (
        p_action_type, 'performance_alerts', p_alert_id, p_action_by,
        'Performed ' || p_action_type || ' on alert for model ' || v_model_name ||
        '. Alert description: ' || v_alert_description ||
        '. Previous status: ' || (SELECT status_name FROM model_risk_governance.alert_statuses WHERE status_id = v_current_status_id) ||
        '. Notes: ' || p_action_notes
    );
END;
$$;

COMMENT ON PROCEDURE model_risk_governance.process_performance_alert IS 'Processes performance alerts with different actions (acknowledge, assign, resolve, change status) and logs all changes';




--generate modelr isk report
CREATE OR REPLACE PROCEDURE model_risk_governance.generate_model_risk_report(
    p_report_type VARCHAR(50),
    p_date_from DATE DEFAULT NULL,
    p_date_to DATE DEFAULT NULL,
    p_department_id INT DEFAULT NULL,
    p_risk_rating_id INT DEFAULT NULL,
    p_output_format VARCHAR(10) DEFAULT 'SCREEN'
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_report_title TEXT;
    v_report_data JSON;
    v_file_path TEXT;
    v_filename TEXT;
BEGIN
    -- Set default date range if not provided
    IF p_date_from IS NULL THEN
        p_date_from := CURRENT_DATE - INTERVAL '3 months';
    END IF;

    IF p_date_to IS NULL THEN
        p_date_to := CURRENT_DATE;
    END IF;

    -- Generate report based on type
    IF p_report_type = 'VALIDATION_COVERAGE' THEN
        v_report_title := 'Model Validation Coverage Report - ' || p_date_from || ' to ' || p_date_to;

        SELECT json_agg(row_to_json(t)) INTO v_report_data
        FROM (
            SELECT
                m.model_id,
                m.model_name,
                m.model_version,
                r.rating_name AS risk_rating,
                CASE WHEN m.is_critical THEN 'Critical' ELSE 'Non-Critical' END AS criticality,
                (SELECT COUNT(*) FROM model_risk_governance.model_validations v
                 WHERE v.model_id = m.model_id AND v.validation_date BETWEEN p_date_from AND p_date_to) AS validations_in_period,
                (SELECT MAX(v.validation_date) FROM model_risk_governance.model_validations v
                 WHERE v.model_id = m.model_id) AS last_validation_date,
                (SELECT v.next_validation_date FROM model_risk_governance.model_validations v
                 WHERE v.model_id = m.model_id
                 ORDER BY v.validation_date DESC LIMIT 1) AS next_validation_date,
                CASE
                    WHEN (SELECT COUNT(*) FROM model_risk_governance.model_validations v
                          WHERE v.model_id = m.model_id) > 0 THEN 'Validated'
                    ELSE 'Not Validated'
                END AS validation_status
            FROM
                model_risk_governance.models m
            JOIN
                model_risk_governance.risk_ratings r ON m.risk_rating_id = r.rating_id
            WHERE
                m.is_active = TRUE AND
                (p_department_id IS NULL OR m.department_id = p_department_id) AND
                (p_risk_rating_id IS NULL OR m.risk_rating_id = p_risk_rating_id)
            ORDER BY
                m.is_critical DESC, r.severity_level DESC
        ) t;

    ELSIF p_report_type = 'PERFORMANCE_DEGRADATION' THEN
        v_report_title := 'Model Performance Degradation Report - ' || p_date_from || ' to ' || p_date_to;

        SELECT json_agg(row_to_json(t)) INTO v_report_data
        FROM (
            SELECT
                m.model_id,
                m.model_name,
                m.model_version,
                r.rating_name AS risk_rating,
                p.monitoring_date,
                p.accuracy_score,
                p.precision_score,
                p.recall_score,
                p.auc_score,
                p.psi_score,
                p.ks_score,
                s.status_name AS performance_status,
                CASE WHEN p.is_alert_triggered THEN 'Alert' ELSE 'Normal' END AS alert_status,
                (SELECT COUNT(*) FROM model_risk_governance.performance_alerts a
                 JOIN model_risk_governance.model_performance mp ON a.performance_id = mp.performance_id
                 WHERE mp.model_id = m.model_id AND a.alert_date BETWEEN p_date_from AND p_date_to) AS alert_count
            FROM
                model_risk_governance.models m
            JOIN
                model_risk_governance.model_performance p ON m.model_id = p.model_id
            JOIN
                model_risk_governance.risk_ratings r ON m.risk_rating_id = r.rating_id
            JOIN
                model_risk_governance.performance_statuses s ON p.performance_status_id = s.status_id
            WHERE
                m.is_active = TRUE AND
                p.monitoring_date BETWEEN p_date_from AND p_date_to AND
                (p_department_id IS NULL OR m.department_id = p_department_id) AND
                (p_risk_rating_id IS NULL OR m.risk_rating_id = p_risk_rating_id) AND
                p.is_alert_triggered = TRUE
            ORDER BY
                p.monitoring_date DESC, r.severity_level DESC
        ) t;

    ELSIF p_report_type = 'RISK_SCORE_ACCURACY' THEN
        v_report_title := 'Model Risk Score Accuracy Report - ' || p_date_from || ' to ' || p_date_to;

        SELECT json_agg(row_to_json(t)) INTO v_report_data
        FROM (
            SELECT
                m.model_id,
                m.model_name,
                m.model_version,
                r.rating_name AS current_risk_rating,
                a.risk_score AS calculated_risk_score,
                l.level_name AS calculated_risk_level,
                a.assessment_date,
                e.employee_name AS assessor_name,
                CASE
                    WHEN r.severity_level = l.severity_level THEN 'Match'
                    WHEN r.severity_level < l.severity_level THEN 'Underestimated'
                    ELSE 'Overestimated'
                END AS rating_accuracy
            FROM
                model_risk_governance.models m
            JOIN
                model_risk_governance.risk_ratings r ON m.risk_rating_id = r.rating_id
            JOIN
                model_risk_governance.model_risk_assessments a ON m.model_id = a.model_id
            JOIN
                model_risk_governance.risk_levels l ON a.risk_level_id = l.level_id
            JOIN
                core.employees e ON a.assessed_by = e.employee_id
            WHERE
                m.is_active = TRUE AND
                a.assessment_date BETWEEN p_date_from AND p_date_to AND
                (p_department_id IS NULL OR m.department_id = p_department_id) AND
                (p_risk_rating_id IS NULL OR m.risk_rating_id = p_risk_rating_id)
            ORDER BY
                a.assessment_date DESC
        ) t;

    ELSE
        RAISE EXCEPTION 'Invalid report type: %', p_report_type;
    END IF;

    -- Output based on requested format
    IF p_output_format = 'SCREEN' THEN
        RAISE NOTICE 'Report: %', v_report_title;
        RAISE NOTICE '%', v_report_data;
    ELSIF p_output_format = 'FILE' THEN
        v_filename := 'model_risk_report_' || LOWER(p_report_type) || '_' || TO_CHAR(CURRENT_TIMESTAMP, 'YYYYMMDD_HH24MISS') || '.json';
        v_file_path := '/var/reports/' || v_filename;

        -- Write to file (requires appropriate permissions)
        EXECUTE format('COPY (SELECT %L::json) TO %L', v_report_data, v_file_path);

        RAISE NOTICE 'Report saved to file: %', v_file_path;
    ELSIF p_output_format = 'TABLE' THEN
        -- For TABLE output, we'd need to handle each report type differently
        -- This is a simplified example for VALIDATION_COVERAGE
        IF p_report_type = 'VALIDATION_COVERAGE' THEN
            RAISE NOTICE 'Model Validation Coverage Report';
            RAISE NOTICE '--------------------------------';
            RAISE NOTICE 'Model Name | Risk Rating | Criticality | Last Validated | Next Due | Status';

            FOR r IN
                SELECT
                    m.model_name,
                    r.rating_name AS risk_rating,
                    CASE WHEN m.is_critical THEN 'Critical' ELSE 'Non-Critical' END AS criticality,
                    (SELECT MAX(v.validation_date) FROM model_risk_governance.model_validations v
                     WHERE v.model_id = m.model_id) AS last_validation_date,
                    (SELECT v.next_validation_date FROM model_risk_governance.model_validations v
                     WHERE v.model_id = m.model_id
                     ORDER BY v.validation_date DESC LIMIT 1) AS next_validation_date,
                    CASE
                        WHEN (SELECT COUNT(*) FROM model_risk_governance.model_validations v
                              WHERE v.model_id = m.model_id) > 0 THEN 'Validated'
                        ELSE 'Not Validated'
                    END AS validation_status
                FROM
                    model_risk_governance.models m
                JOIN
                    model_risk_governance.risk_ratings r ON m.risk_rating_id = r.rating_id
                WHERE
                    m.is_active = TRUE AND
                    (p_department_id IS NULL OR m.department_id = p_department_id) AND
                    (p_risk_rating_id IS NULL OR m.risk_rating_id = p_risk_rating_id)
                ORDER BY
                    m.is_critical DESC, r.severity_level DESC
            LOOP
                RAISE NOTICE '% | % | % | % | % | %',
                    r.model_name, r.risk_rating, r.criticality,
                    r.last_validation_date, r.next_validation_date, r.validation_status;
            END LOOP;
        END IF;
    ELSE
        RAISE EXCEPTION 'Invalid output format: %', p_output_format;
    END IF;

    -- Log report generation
    INSERT INTO model_risk_governance.report_logs (
        report_type, date_from, date_to, generated_by, output_format
    ) VALUES (
        p_report_type, p_date_from, p_date_to,
        (SELECT employee_name FROM core.employees WHERE employee_id = p_action_by),
        p_output_format
    );
END;
$$;

COMMENT ON PROCEDURE model_risk_governance.generate_model_risk_report IS 'Generates various model risk reports in different formats (screen, file, table)';


---escalate model issues
CREATE OR REPLACE PROCEDURE model_risk_governance.escalate_model_issues(
    p_escalation_level VARCHAR(50),
    p_escalation_reason TEXT,
    p_model_ids INT[] DEFAULT NULL,
    p_finding_ids INT[] DEFAULT NULL,
    p_alert_ids INT[] DEFAULT NULL,
    p_escalated_by INT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_committee_id INT;
    v_next_meeting_date TIMESTAMP WITH TIME ZONE;
    v_agenda_item_id INT;
    v_model_id INT;
    v_finding_id INT;
    v_alert_id INT;
    v_details TEXT;
BEGIN
    -- Validate that at least one of the ID arrays is provided
    IF p_model_ids IS NULL AND p_finding_ids IS NULL AND p_alert_ids IS NULL THEN
        RAISE EXCEPTION 'At least one of p_model_ids, p_finding_ids, or p_alert_ids must be provided';
    END IF;

    -- Get the appropriate committee based on escalation level
    IF p_escalation_level = 'MODEL_RISK_COMMITTEE' THEN
        SELECT committee_id INTO v_committee_id
        FROM model_risk_governance.governance_committees
        WHERE committee_name = 'Model Risk Committee'
        LIMIT 1;
    ELSIF p_escalation_level = 'ENTERPRISE_RISK_COMMITTEE' THEN
        SELECT committee_id INTO v_committee_id
        FROM model_risk_governance.governance_committees
        WHERE committee_name = 'Enterprise Risk Committee'
        LIMIT 1;
    ELSIF p_escalation_level = 'AUDIT_COMMITTEE' THEN
        SELECT committee_id INTO v_committee_id
        FROM model_risk_governance.governance_committees
        WHERE committee_name = 'Audit Committee'
        LIMIT 1;
    ELSE
        RAISE EXCEPTION 'Invalid escalation level: %', p_escalation_level;
    END IF;

    -- Get the next meeting date for the committee
    SELECT MIN(meeting_date) INTO v_next_meeting_date
    FROM model_risk_governance.governance_meetings
    WHERE committee_id = v_committee_id AND meeting_date > CURRENT_TIMESTAMP;

    -- If no future meeting exists, create one in 7 days
    IF v_next_meeting_date IS NULL THEN
        INSERT INTO model_risk_governance.governance_meetings (
            committee_id, meeting_date, location, chairperson_id
        ) VALUES (
            v_committee_id, CURRENT_TIMESTAMP + INTERVAL '7 days', 'Board Room',
            (SELECT chairperson_id FROM model_risk_governance.governance_committees WHERE committee_id = v_committee_id)
        ) RETURNING meeting_id INTO v_next_meeting_id;

        v_next_meeting_date := CURRENT_TIMESTAMP + INTERVAL '7 days';
    END IF;

    -- Create an agenda item for the escalation
    INSERT INTO model_risk_governance.meeting_agenda_items (
        meeting_id, item_title, item_description, presenter_id
    ) VALUES (
        (SELECT meeting_id FROM model_risk_governance.governance_meetings
         WHERE committee_id = v_committee_id AND meeting_date = v_next_meeting_date),
        'Escalated Model Risk Issues',
        p_escalation_reason,
        p_escalated_by
    ) RETURNING agenda_item_id INTO v_agenda_item_id;

    -- Process model escalations
    IF p_model_ids IS NOT NULL THEN
        FOREACH v_model_id IN ARRAY p_model_ids LOOP
            -- Get model details
            SELECT model_name INTO v_model_name FROM model_risk_governance.models WHERE model_id = v_model_id;

            -- Add to escalation details
            v_details := COALESCE(v_details, '') || '\nModel: ' || v_model_name || ' (ID: ' || v_model_id || ')';

            -- Log the escalation
            INSERT INTO model_risk_governance.escalations (
                model_id, escalation_level, escalation_date, escalated_by,
                escalation_reason, committee_meeting_date, agenda_item_id
            ) VALUES (
                v_model_id, p_escalation_level, CURRENT_TIMESTAMP, p_escalated_by,
                p_escalation_reason, v_next_meeting_date, v_agenda_item_id
            );
        END LOOP;
    END IF;

    -- Process finding escalations
    IF p_finding_ids IS NOT NULL THEN
        FOREACH v_finding_id IN ARRAY p_finding_ids LOOP
            -- Get finding and model details
            SELECT f.description, m.model_id, m.model_name INTO v_finding_desc, v_model_id, v_model_name
            FROM model_risk_governance.validation_findings f
            JOIN model_risk_governance.model_validations v ON f.validation_id = v.validation_id
            JOIN model_risk_governance.models m ON v.model_id = m.model_id
            WHERE f.finding_id = v_finding_id;

            -- Add to escalation details
            v_details := COALESCE(v_details, '') || '\nFinding: ' || v_finding_desc ||
                        ' (Model: ' || v_model_name || ', ID: ' || v_model_id || ')';

            -- Log the escalation
            INSERT INTO model_risk_governance.escalations (
                finding_id, model_id, escalation_level, escalation_date,
                escalated_by, escalation_reason, committee_meeting_date, agenda_item_id
            ) VALUES (
                v_finding_id, v_model_id, p_escalation_level, CURRENT_TIMESTAMP,
                p_escalated_by, p_escalation_reason, v_next_meeting_date, v_agenda_item_id
            );
        END LOOP;
    END IF;

    -- Process alert escalations
    IF p_alert_ids IS NOT NULL THEN
        FOREACH v_alert_id IN ARRAY p_alert_ids LOOP
            -- Get alert and model details
            SELECT a.description, p.model_id, m.model_name INTO v_alert_desc, v_model_id, v_model_name
            FROM model_risk_governance.performance_alerts a
            JOIN model_risk_governance.model_performance p ON a.performance_id = p.performance_id
            JOIN model_risk_governance.models m ON p.model_id = m.model_id
            WHERE a.alert_id = v_alert_id;

            -- Add to escalation details
            v_details := COALESCE(v_details, '') || '\nAlert: ' || v_alert_desc ||
                        ' (Model: ' || v_model_name || ', ID: ' || v_model_id || ')';

            -- Log the escalation
            INSERT INTO model_risk_governance.escalations (
                alert_id, model_id, escalation_level, escalation_date,
                escalated_by, escalation_reason, committee_meeting_date, agenda_item_id
            ) VALUES (
                v_alert_id, v_model_id, p_escalation_level, CURRENT_TIMESTAMP,
                p_escalated_by, p_escalation_reason, v_next_meeting_date, v_agenda_item_id
            );
        END LOOP;
    END IF;

    -- Update the agenda item with the detailed list of escalated items
    UPDATE model_risk_governance.meeting_agenda_items
    SET item_description = item_description || '\n\nEscalated Items:' || v_details
    WHERE agenda_item_id = v_agenda_item_id;

    -- Log the operation
    INSERT INTO model_risk_governance.audit_logs (
        action_type, table_name, record_id, action_by, action_details
    ) VALUES (
        'ESCALATE', 'escalations', v_agenda_item_id, p_escalated_by,
        'Escalated ' || COALESCE(array_length(p_model_ids, 1), 0) || ' models, ' ||
        COALESCE(array_length(p_finding_ids, 1), 0) || ' findings, and ' ||
        COALESCE(array_length(p_alert_ids, 1), 0) || ' alerts to ' || p_escalation_level ||
        ' for meeting on ' || v_next_meeting_date || '. Reason: ' || p_escalation_reason
    );
END;
$$;

COMMENT ON PROCEDURE model_risk_governance.escalate_model_issues IS 'Escalates model issues, findings, or alerts to the appropriate governance committee and schedules for discussion';

---model validation coverage summary
CREATE MATERIALIZED VIEW model_risk_governance.mv_model_validation_coverage_summary
AS
SELECT
    DATE_TRUNC('month', CURRENT_DATE) AS report_month,
    COUNT(*) AS total_models,
    SUM(CASE WHEN is_critical THEN 1 ELSE 0 END) AS critical_models,
    SUM(CASE WHEN validation_status = 'Validated' THEN 1 ELSE 0 END) AS validated_models,
    SUM(CASE WHEN validation_status = 'Validated' AND is_critical THEN 1 ELSE 0 END) AS validated_critical_models,
    SUM(CASE WHEN validation_status = 'Not Validated' THEN 1 ELSE 0 END) AS unvalidated_models,
    SUM(CASE WHEN validation_status = 'Not Validated' AND is_critical THEN 1 ELSE 0 END) AS unvalidated_critical_models,
    SUM(CASE WHEN validation_timeliness = 'Overdue' THEN 1 ELSE 0 END) AS overdue_validations,
    SUM(CASE WHEN validation_timeliness = 'Overdue' AND is_critical THEN 1 ELSE 0 END) AS overdue_critical_validations,
    ROUND(SUM(CASE WHEN validation_status = 'Validated' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) AS overall_coverage_pct,
    ROUND(SUM(CASE WHEN validation_status = 'Validated' AND is_critical THEN 1 ELSE 0 END) * 100.0 /
          NULLIF(SUM(CASE WHEN is_critical THEN 1 ELSE 0 END), 0), 2) AS critical_coverage_pct
FROM
    model_risk_governance.model_validation_coverage
WITH DATA;

COMMENT ON MATERIALIZED VIEW model_risk_governance.mv_model_validation_coverage_summary IS 'Provides a monthly summary of model validation coverage statistics including critical model coverage';

-- Create index to improve query performance
CREATE INDEX idx_mv_model_validation_coverage_summary ON model_risk_governance.mv_model_validation_coverage_summary (report_month);


--model performance degradation trends
CREATE MATERIALIZED VIEW model_risk_governance.mv_model_performance_trends
AS
SELECT
    m.model_id,
    m.model_name,
    m.model_version,
    r.rating_name AS risk_rating,
    DATE_TRUNC('month', p.monitoring_date) AS month,
    COUNT(*) AS observations,
    AVG(p.accuracy_score) AS avg_accuracy,
    AVG(p.precision_score) AS avg_precision,
    AVG(p.recall_score) AS avg_recall,
    AVG(p.auc_score) AS avg_auc,
    AVG(p.psi_score) AS avg_psi,
    AVG(p.ks_score) AS avg_ks,
    SUM(CASE WHEN p.is_alert_triggered THEN 1 ELSE 0 END) AS alert_count,
    (AVG(p.accuracy_score) -
     LAG(AVG(p.accuracy_score), 1) OVER (PARTITION BY m.model_id ORDER BY DATE_TRUNC('month', p.monitoring_date))) AS accuracy_change,
    CASE
        WHEN (AVG(p.accuracy_score) -
              LAG(AVG(p.accuracy_score), 1) OVER (PARTITION BY m.model_id ORDER BY DATE_TRUNC('month', p.monitoring_date))) < -0.05 THEN 'Significant Degradation'
        WHEN (AVG(p.accuracy_score) -
              LAG(AVG(p.accuracy_score), 1) OVER (PARTITION BY m.model_id ORDER BY DATE_TRUNC('month', p.monitoring_date))) < -0.02 THEN 'Moderate Degradation'
        WHEN (AVG(p.accuracy_score) -
              LAG(AVG(p.accuracy_score), 1) OVER (PARTITION BY m.model_id ORDER BY DATE_TRUNC('month', p.monitoring_date))) > 0.02 THEN 'Improvement'
        ELSE 'Stable'
    END AS performance_trend
FROM
    model_risk_governance.models m
JOIN
    model_risk_governance.model_performance p ON m.model_id = p.model_id
JOIN
    model_risk_governance.risk_ratings r ON m.risk_rating_id = r.rating_id
WHERE
    m.is_active = TRUE AND
    p.monitoring_date >= CURRENT_DATE - INTERVAL '24 months'
GROUP BY
    m.model_id, m.model_name, m.model_version, r.rating_name, DATE_TRUNC('month', p.monitoring_date)
WITH DATA;

COMMENT ON MATERIALIZED VIEW model_risk_governance.mv_model_performance_trends IS 'Tracks monthly performance trends for all models over a 24-month period with degradation classification';

-- Create index to improve query performance
CREATE INDEX idx_mv_model_performance_trends ON model_risk_governance.mv_model_performance_trends (model_id, month);


---model risk score accuracy analysis
CREATE MATERIALIZED VIEW model_risk_governance.mv_risk_score_accuracy
AS
SELECT
    DATE_TRUNC('quarter', a.assessment_date) AS quarter,
    COUNT(*) AS total_assessments,
    SUM(CASE WHEN r.severity_level = l.severity_level THEN 1 ELSE 0 END) AS accurate_ratings,
    SUM(CASE WHEN r.severity_level < l.severity_level THEN 1 ELSE 0 END) AS underestimated_ratings,
    SUM(CASE WHEN r.severity_level > l.severity_level THEN 1 ELSE 0 END) AS overestimated_ratings,
    ROUND(SUM(CASE WHEN r.severity_level = l.severity_level THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) AS accuracy_percentage,
    AVG(a.risk_score) AS avg_risk_score,
    AVG(a.impact_score) AS avg_impact_score,
    AVG(a.likelihood_score) AS avg_likelihood_score,
    SUM(CASE WHEN a.risk_appetite_alignment = FALSE THEN 1 ELSE 0 END) AS appetite_violations
FROM
    model_risk_governance.model_risk_assessments a
JOIN
    model_risk_governance.models m ON a.model_id = m.model_id
JOIN
    model_risk_governance.risk_ratings r ON m.risk_rating_id = r.rating_id
JOIN
    model_risk_governance.risk_levels l ON a.risk_level_id = l.level_id
WHERE
    m.is_active = TRUE AND
    a.assessment_date >= CURRENT_DATE - INTERVAL '2 years'
GROUP BY
    DATE_TRUNC('quarter', a.assessment_date)
WITH DATA;

COMMENT ON MATERIALIZED VIEW model_risk_governance.mv_risk_score_accuracy IS 'Provides quarterly analysis of model risk score accuracy compared to assigned risk ratings';

-- Create index to improve query performance
CREATE INDEX idx_mv_risk_score_accuracy ON model_risk_governance.mv_risk_score_accuracy (quarter);

---model issue remediation times
CREATE MATERIALIZED VIEW model_risk_governance.mv_remediation_times
AS
SELECT
    DATE_TRUNC('quarter', v.validation_date) AS quarter,
    COUNT(*) AS total_findings,
    AVG(EXTRACT(DAY FROM (COALESCE(f.remediation_date, CURRENT_DATE) - v.validation_date))) AS avg_days_to_remediate,
    AVG(EXTRACT(DAY FROM (f.remediation_date - f.remediation_deadline))) AS avg_days_overdue,
    SUM(CASE WHEN f.is_remediated AND f.remediation_date <= f.remediation_deadline THEN 1 ELSE 0 END) AS remediated_on_time,
    SUM(CASE WHEN f.is_remediated AND f.remediation_date > f.remediation_deadline THEN 1 ELSE 0 END) AS remediated_late,
    SUM(CASE WHEN NOT f.is_remediated AND CURRENT_DATE > f.remediation_deadline THEN 1 ELSE 0 END) AS overdue_unremediated,
    SUM(CASE WHEN NOT f.is_remediated AND CURRENT_DATE <= f.remediation_deadline THEN 1 ELSE 0 END) AS pending_remediation,
    s.level_name AS severity,
    m.model_type_id,
    t.type_name AS model_type
FROM
    model_risk_governance.validation_findings f
JOIN
    model_risk_governance.model_validations v ON f.validation_id = v.validation_id
JOIN
    model_risk_governance.models m ON v.model_id = m.model_id
JOIN
    model_risk_governance.severity_levels s ON f.severity_id = s.level_id
JOIN
    model_risk_governance.model_types t ON m.model_type_id = t.type_id
WHERE
    v.validation_date >= CURRENT_DATE - INTERVAL '2 years'
GROUP BY
    DATE_TRUNC('quarter', v.validation_date), s.level_name, m.model_type_id, t.type_name
WITH DATA;

COMMENT ON MATERIALIZED VIEW model_risk_governance.mv_remediation_times IS 'Analyzes model issue remediation times by quarter, severity, and model type';

-- Create index to improve query performance
CREATE INDEX idx_mv_remediation_times ON model_risk_governance.mv_remediation_times (quarter, severity, model_type_id);


-- third party model risk overview
CREATE MATERIALIZED VIEW model_risk_governance.mv_third_party_model_risk
AS
SELECT
    v.vendor_id,
    v.vendor_name,
    DATE_TRUNC('year', a.assessment_date) AS assessment_year,
    COUNT(DISTINCT tpm.model_id) AS model_count,
    AVG(a.overall_score) AS avg_vendor_score,
    AVG(a.security_score) AS avg_security_score,
    AVG(a.compliance_score) AS avg_compliance_score,
    COUNT(DISTINCT CASE WHEN m.risk_rating_id IN (SELECT rating_id FROM model_risk_governance.risk_ratings WHERE severity_level >= 4) THEN m.model_id END) AS high_risk_models,
    COUNT(DISTINCT CASE WHEN va.validation_status_id = (SELECT status_id FROM model_risk_governance.validation_statuses WHERE status_name = 'Failed') THEN m.model_id END) AS failed_validation_models,
    COUNT(DISTINCT CASE WHEN p.is_alert_triggered = TRUE THEN m.model_id END) AS models_with_alerts,
    COUNT(DISTINCT CASE WHEN tpm.source_code_access = FALSE THEN m.model_id END) AS models_without_source_access
FROM
    core.vendors v
JOIN
    model_risk_governance.third_party_models tpm ON v.vendor_id = tpm.vendor_id
JOIN
    model_risk_governance.models m ON tpm.model_id = m.model_id
LEFT JOIN
    model_risk_governance.vendor_assessments a ON v.vendor_id = a.vendor_id
LEFT JOIN
    (SELECT model_id, validation_status_id
     FROM model_risk_governance.model_validations
     WHERE validation_date = (SELECT MAX(validation_date) FROM model_risk_governance.model_validations mv WHERE mv.model_id = model_validations.model_id)
    ) va ON m.model_id = va.model_id
LEFT JOIN
    (SELECT model_id, is_alert_triggered
     FROM model_risk_governance.model_performance
     WHERE monitoring_date = (SELECT MAX(monitoring_date) FROM model_risk_governance.model_performance mp WHERE mp.model_id = model_performance.model_id))
    ) p ON m.model_id = p.model_id
WHERE
    m.is_active = TRUE
GROUP BY
    v.vendor_id, v.vendor_name, DATE_TRUNC('year', a.assessment_date)
WITH DATA;

COMMENT ON MATERIALIZED VIEW model_risk_governance.mv_third_party_model_risk IS 'Provides an overview of third-party model risks by vendor and assessment year';

-- Create index to improve query performance
CREATE INDEX idx_mv_third_party_model_risk ON model_risk_governance.mv_third_party_model_risk (vendor_id, assessment_year);



CREATE TABLE model_risk_governance.development_standards (
    standard_id SERIAL PRIMARY KEY,
    standard_name VARCHAR(255) NOT NULL,
    standard_description TEXT,
    category VARCHAR(100) NOT NULL, -- Coding, Data, Documentation, etc.
    is_mandatory BOOLEAN DEFAULT TRUE,
    effective_date DATE NOT NULL,
    review_frequency_months INT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE model_risk_governance.model_development_evidence (
    evidence_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    standard_id INT NOT NULL,
    compliance_status_id INT NOT NULL,
    evidence_location VARCHAR(512),
    verified_by INT,
    verification_date DATE,
    comments TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (standard_id) REFERENCES model_risk_governance.development_standards(standard_id),
    FOREIGN KEY (compliance_status_id) REFERENCES model_risk_governance.compliance_statuses(status_id),
    FOREIGN KEY (verified_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.approved_tools (
    tool_id SERIAL PRIMARY KEY,
    tool_name VARCHAR(255) NOT NULL,
    tool_version VARCHAR(100),
    tool_type VARCHAR(100) NOT NULL, -- Programming Language, Framework, Library
    approval_date DATE NOT NULL,
    approved_by INT NOT NULL,
    end_of_support_date DATE,
    security_rating VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (approved_by) REFERENCES core.employees(employee_id)
);


CREATE TABLE model_risk_governance.monitoring_configurations (
    config_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    metric_name VARCHAR(100) NOT NULL, -- Accuracy, PSI, KS, etc.
    threshold_type VARCHAR(50) NOT NULL, -- Absolute, Percentage, Statistical
    warning_threshold DECIMAL(10,4),
    critical_threshold DECIMAL(10,4),
    lookback_period_days INT,
    evaluation_frequency_hours INT,
    is_active BOOLEAN DEFAULT TRUE,
    last_updated TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_by INT NOT NULL,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (updated_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.drift_metrics (
    drift_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    metric_date TIMESTAMP WITH TIME ZONE NOT NULL,
    psi_score DECIMAL(10,4),
    ks_score DECIMAL(10,4),
    population_stability_index DECIMAL(10,4),
    feature_drift JSONB, -- Stores drift metrics for individual features
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);

CREATE TABLE model_risk_governance.anomaly_detection_logs (
    anomaly_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    detection_time TIMESTAMP WITH TIME ZONE NOT NULL,
    anomaly_type VARCHAR(100) NOT NULL,
    severity VARCHAR(50) NOT NULL,
    description TEXT,
    input_data_sample JSONB,
    expected_output VARCHAR(255),
    actual_output VARCHAR(255),
    is_investigated BOOLEAN DEFAULT FALSE,
    investigation_notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);


CREATE TABLE model_risk_governance.workflow_templates (
    template_id SERIAL PRIMARY KEY,
    template_name VARCHAR(255) NOT NULL,
    template_description TEXT,
    workflow_type VARCHAR(100) NOT NULL, -- Development, Validation, Approval, etc.
    version VARCHAR(50) NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE model_risk_governance.workflow_steps (
    step_id SERIAL PRIMARY KEY,
    template_id INT NOT NULL,
    step_name VARCHAR(255) NOT NULL,
    step_description TEXT,
    step_order INT NOT NULL,
    role_id INT NOT NULL, -- Responsible role
    approval_required BOOLEAN DEFAULT FALSE,
    sla_days INT,
    documentation_requirements TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (template_id) REFERENCES model_risk_governance.workflow_templates(template_id),
    FOREIGN KEY (role_id) REFERENCES core.roles(role_id)
);

CREATE TABLE model_risk_governance.model_workflows (
    workflow_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    template_id INT NOT NULL,
    current_step_id INT,
    workflow_status_id INT NOT NULL,
    initiated_by INT NOT NULL,
    initiated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP WITH TIME ZONE,
    comments TEXT,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (template_id) REFERENCES model_risk_governance.workflow_templates(template_id),
    FOREIGN KEY (current_step_id) REFERENCES model_risk_governance.workflow_steps(step_id),
    FOREIGN KEY (workflow_status_id) REFERENCES model_risk_governance.workflow_statuses(status_id),
    FOREIGN KEY (initiated_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.workflow_audit_trail (
    audit_id SERIAL PRIMARY KEY,
    workflow_id INT NOT NULL,
    step_id INT NOT NULL,
    action_type VARCHAR(50) NOT NULL, -- Started, Completed, Approved, Rejected
    action_by INT NOT NULL,
    action_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    comments TEXT,
    metadata JSONB,
    FOREIGN KEY (workflow_id) REFERENCES model_risk_governance.model_workflows(workflow_id),
    FOREIGN KEY (step_id) REFERENCES model_risk_governance.workflow_steps(step_id),
    FOREIGN KEY (action_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.bias_metrics (
    bias_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    evaluation_date TIMESTAMP WITH TIME ZONE NOT NULL,
    metric_name VARCHAR(100) NOT NULL, -- Demographic Parity, Equal Opportunity, etc.
    protected_attribute VARCHAR(100) NOT NULL, -- Gender, Race, Age, etc.
    subgroup VARCHAR(100),
    metric_value DECIMAL(10,4),
    threshold DECIMAL(10,4),
    is_violation BOOLEAN,
    evaluation_parameters JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);

CREATE TABLE model_risk_governance.ethics_reviews (
    review_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    review_date DATE NOT NULL,
    reviewed_by INT NOT NULL,
    fairness_rating VARCHAR(50),
    transparency_rating VARCHAR(50),
    accountability_rating VARCHAR(50),
    privacy_rating VARCHAR(50),
    overall_ethics_score DECIMAL(5,2),
    findings TEXT,
    recommendations TEXT,
    next_review_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (reviewed_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.ethical_incidents (
    incident_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    reported_date DATE NOT NULL,
    reported_by INT NOT NULL,
    incident_type VARCHAR(100) NOT NULL,
    description TEXT NOT NULL,
    impact_assessment TEXT,
    severity VARCHAR(50) NOT NULL,
    resolution_status VARCHAR(50) NOT NULL,
    resolution_date DATE,
    resolution_details TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (reported_by) REFERENCES core.employees(employee_id)
);


CREATE TABLE model_risk_governance.risk_scenarios (
    scenario_id SERIAL PRIMARY KEY,
    scenario_name VARCHAR(255) NOT NULL,
    scenario_description TEXT,
    category VARCHAR(100) NOT NULL, -- Data, Model, Process, etc.
    likelihood_rating VARCHAR(50),
    impact_rating VARCHAR(50),
    risk_appetite_alignment VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE model_risk_governance.model_scenario_assessments (
    assessment_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    scenario_id INT NOT NULL,
    assessment_date DATE NOT NULL,
    assessed_by INT NOT NULL,
    likelihood_score DECIMAL(5,2),
    impact_score DECIMAL(5,2),
    residual_risk_score DECIMAL(5,2),
    mitigation_controls TEXT,
    control_effectiveness VARCHAR(50),
    next_assessment_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (scenario_id) REFERENCES model_risk_governance.risk_scenarios(scenario_id),
    FOREIGN KEY (assessed_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.stress_test_scenarios (
    stress_test_id SERIAL PRIMARY KEY,
    scenario_name VARCHAR(255) NOT NULL,
    scenario_description TEXT,
    scenario_category VARCHAR(100) NOT NULL, -- Economic, Operational, etc.
    severity_level VARCHAR(50) NOT NULL,
    is_regulatory_required BOOLEAN DEFAULT FALSE,
    regulatory_reference VARCHAR(255),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE model_risk_governance.model_stress_test_results (
    result_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    stress_test_id INT NOT NULL,
    test_date DATE NOT NULL,
    conducted_by INT NOT NULL,
    input_parameters JSONB,
    output_metrics JSONB,
    pass_status BOOLEAN,
    findings TEXT,
    recommendations TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (stress_test_id) REFERENCES model_risk_governance.stress_test_scenarios(stress_test_id),
    FOREIGN KEY (conducted_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.model_dependencies (
    dependency_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    dependent_model_id INT,
    dependent_system_id INT,
    dependency_type VARCHAR(100) NOT NULL, -- Data, Input, Output, Infrastructure
    description TEXT,
    criticality VARCHAR(50) NOT NULL,
    interface_specification TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (dependent_model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (dependent_system_id) REFERENCES core.systems(system_id)
);

CREATE TABLE model_risk_governance.dependency_impact_assessments (
    assessment_id SERIAL PRIMARY KEY,
    dependency_id INT NOT NULL,
    assessment_date DATE NOT NULL,
    assessed_by INT NOT NULL,
    availability_impact VARCHAR(50),
    performance_impact VARCHAR(50),
    data_quality_impact VARCHAR(50),
    risk_score DECIMAL(5,2),
    mitigation_strategy TEXT,
    next_assessment_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (dependency_id) REFERENCES model_risk_governance.model_dependencies(dependency_id),
    FOREIGN KEY (assessed_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.business_impact_analyses (
    bia_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    analysis_date DATE NOT NULL,
    conducted_by INT NOT NULL,
    max_tolerable_downtime_hours INT,
    recovery_time_objective_hours INT,
    recovery_point_objective_hours INT,
    financial_impact_per_hour DECIMAL(15,2),
    reputational_impact VARCHAR(100),
    regulatory_impact VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (conducted_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.disaster_recovery_plans (
    plan_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    plan_name VARCHAR(255) NOT NULL,
    plan_description TEXT,
    version VARCHAR(50) NOT NULL,
    approval_date DATE,
    approved_by INT,
    last_test_date DATE,
    next_test_date DATE,
    test_results TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (approved_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.model_incidents (
    incident_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    incident_date TIMESTAMP WITH TIME ZONE NOT NULL,
    detected_by INT,
    incident_type VARCHAR(100) NOT NULL,
    severity VARCHAR(50) NOT NULL,
    description TEXT NOT NULL,
    root_cause TEXT,
    downtime_minutes INT,
    impact_assessment TEXT,
    resolution_status VARCHAR(50) NOT NULL,
    resolution_date TIMESTAMP WITH TIME ZONE,
    resolution_details TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (detected_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.competency_frameworks (
    framework_id SERIAL PRIMARY KEY,
    framework_name VARCHAR(255) NOT NULL,
    framework_description TEXT,
    applicable_role_id INT NOT NULL,
    version VARCHAR(50) NOT NULL,
    effective_date DATE NOT NULL,
    review_frequency_months INT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (applicable_role_id) REFERENCES core.roles(role_id)
);

CREATE TABLE model_risk_governance.competency_areas (
    area_id SERIAL PRIMARY KEY,
    framework_id INT NOT NULL,
    area_name VARCHAR(255) NOT NULL,
    area_description TEXT,
    weight DECIMAL(5,2),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (framework_id) REFERENCES model_risk_governance.competency_frameworks(framework_id)
);

CREATE TABLE model_risk_governance.employee_competency_assessments (
    assessment_id SERIAL PRIMARY KEY,
    employee_id INT NOT NULL,
    framework_id INT NOT NULL,
    assessment_date DATE NOT NULL,
    assessed_by INT NOT NULL,
    overall_score DECIMAL(5,2),
    competency_gap TEXT,
    development_plan TEXT,
    next_assessment_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (employee_id) REFERENCES core.employees(employee_id),
    FOREIGN KEY (framework_id) REFERENCES model_risk_governance.competency_frameworks(framework_id),
    FOREIGN KEY (assessed_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.area_assessment_scores (
    score_id SERIAL PRIMARY KEY,
    assessment_id INT NOT NULL,
    area_id INT NOT NULL,
    score DECIMAL(5,2),
    comments TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (assessment_id) REFERENCES model_risk_governance.employee_competency_assessments(assessment_id),
    FOREIGN KEY (area_id) REFERENCES model_risk_governance.competency_areas(area_id)
);


CREATE TABLE model_risk_governance.report_templates (
    template_id SERIAL PRIMARY KEY,
    template_name VARCHAR(255) NOT NULL,
    template_description TEXT,
    report_type VARCHAR(100) NOT NULL, -- Executive, Regulatory, Operational
    frequency VARCHAR(50) NOT NULL, -- Daily, Weekly, Monthly, Quarterly
    audience VARCHAR(255),
    default_parameters JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE model_risk_governance.scheduled_reports (
    schedule_id SERIAL PRIMARY KEY,
    template_id INT NOT NULL,
    schedule_name VARCHAR(255) NOT NULL,
    recipient_list TEXT NOT NULL,
    delivery_method VARCHAR(50) NOT NULL, -- Email, Portal, API
    parameters JSONB,
    is_active BOOLEAN DEFAULT TRUE,
    last_generated TIMESTAMP WITH TIME ZONE,
    next_generation TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (template_id) REFERENCES model_risk_governance.report_templates(template_id)
);

CREATE TABLE model_risk_governance.dashboard_configurations (
    dashboard_id SERIAL PRIMARY KEY,
    dashboard_name VARCHAR(255) NOT NULL,
    dashboard_description TEXT,
    owner_id INT NOT NULL,
    access_level VARCHAR(50) NOT NULL, -- Public, Department, Private
    refresh_frequency_minutes INT,
    default_time_range VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (owner_id) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.dashboard_widgets (
    widget_id SERIAL PRIMARY KEY,
    dashboard_id INT NOT NULL,
    widget_name VARCHAR(255) NOT NULL,
    widget_type VARCHAR(100) NOT NULL, -- Chart, Table, Metric
    data_source_type VARCHAR(100) NOT NULL, -- SQL, API, etc.
    data_source_config JSONB NOT NULL,
    display_config JSONB,
    position_x INT,
    position_y INT,
    width INT,
    height INT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (dashboard_id) REFERENCES model_risk_governance.dashboard_configurations(dashboard_id)
);


CREATE TABLE model_risk_governance.regulatory_requirements (
    requirement_id SERIAL PRIMARY KEY,
    regulation_name VARCHAR(255) NOT NULL, -- SR 11-7, GDPR, etc.
    requirement_reference VARCHAR(255) NOT NULL,
    requirement_text TEXT NOT NULL,
    category VARCHAR(100) NOT NULL,
    effective_date DATE,
    compliance_deadline DATE,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE model_risk_governance.model_compliance_mappings (
    mapping_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    requirement_id INT NOT NULL,
    compliance_status_id INT NOT NULL,
    evidence_location VARCHAR(512),
    last_verified DATE,
    verified_by INT,
    comments TEXT,
    next_review_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (requirement_id) REFERENCES model_risk_governance.regulatory_requirements(requirement_id),
    FOREIGN KEY (compliance_status_id) REFERENCES model_risk_governance.compliance_statuses(status_id),
    FOREIGN KEY (verified_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.audit_universe (
    audit_id SERIAL PRIMARY KEY,
    audit_name VARCHAR(255) NOT NULL,
    audit_description TEXT,
    audit_type VARCHAR(100) NOT NULL, -- Internal, External, Regulatory
    scope TEXT NOT NULL,
    frequency_months INT,
    last_audit_date DATE,
    next_audit_date DATE,
    responsible_party_id INT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (responsible_party_id) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.model_audit_assignments (
    assignment_id SERIAL PRIMARY KEY,
    audit_id INT NOT NULL,
    model_id INT NOT NULL,
    priority VARCHAR(50) NOT NULL,
    planned_audit_date DATE,
    actual_audit_date DATE,
    auditor_id INT,
    audit_status_id INT NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (audit_id) REFERENCES model_risk_governance.audit_universe(audit_id),
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (auditor_id) REFERENCES core.employees(employee_id),
    FOREIGN KEY (audit_status_id) REFERENCES model_risk_governance.audit_statuses(status_id)
);

-- Indexes for model performance queries
CREATE INDEX idx_model_performance_model_date ON model_risk_governance.model_performance(model_id, monitoring_date);
CREATE INDEX idx_model_performance_alert_status ON model_risk_governance.model_performance(performance_status_id, is_alert_triggered);

-- Indexes for validation tracking
CREATE INDEX idx_model_validations_model_status ON model_risk_governance.model_validations(model_id, validation_status_id);
CREATE INDEX idx_validation_findings_severity ON model_risk_governance.validation_findings(severity_id, is_remediated);

-- Indexes for risk assessments
CREATE INDEX idx_model_risk_assessments_model_date ON model_risk_governance.model_risk_assessments(model_id, assessment_date);
CREATE INDEX idx_model_risk_assessments_score ON model_risk_governance.model_risk_assessments(risk_score);

-- Indexes for workflow tracking
CREATE INDEX idx_model_workflows_status ON model_risk_governance.model_workflows(workflow_status_id);
CREATE INDEX idx_workflow_audit_workflow_action ON model_risk_governance.workflow_audit_trail(workflow_id, action_type);

-- Partition model_performance table by month
CREATE TABLE model_risk_governance.model_performance (
    performance_id BIGSERIAL,
    model_id INT NOT NULL,
    monitoring_date TIMESTAMP WITH TIME ZONE NOT NULL,
    -- other columns...
) PARTITION BY RANGE (monitoring_date);

-- Create monthly partitions
CREATE TABLE model_risk_governance.model_performance_y2023m01 PARTITION OF model_risk_governance.model_performance
    FOR VALUES FROM ('2023-01-01') TO ('2023-02-01');

CREATE TABLE model_risk_governance.model_performance_y2023m02 PARTITION OF model_risk_governance.model_performance
    FOR VALUES FROM ('2023-02-01') TO ('2023-03-01');

    -- Add full-text search column to model documentation
    ALTER TABLE model_risk_governance.model_documentation ADD COLUMN search_vector tsvector;

    -- Create function to update search vector
    CREATE FUNCTION model_documentation_search_update() RETURNS trigger AS $$
    BEGIN
        NEW.search_vector :=
            setweight(to_tsvector('english', COALESCE(NEW.document_name, '')), 'A') ||
            setweight(to_tsvector('english', COALESCE(NEW.document_location, '')), 'B') ||
            setweight(to_tsvector('english', COALESCE(NEW.document_version, '')), 'C');
        RETURN NEW;
    END;
    $$ LANGUAGE plpgsql;

    -- Create trigger
    CREATE TRIGGER model_documentation_search_update
    BEFORE INSERT OR UPDATE ON model_risk_governance.model_documentation
    FOR EACH ROW EXECUTE FUNCTION model_documentation_search_update();

    -- Create index
    CREATE INDEX idx_model_documentation_search ON model_risk_governance.model_documentation USING gin(search_vector);



--model lineage and provenance tracking
CREATE TABLE model_risk_governance.model_lineage (
    lineage_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    parent_model_id INT,
    lineage_type VARCHAR(50) NOT NULL, -- "version", "derivative", "ensemble"
    relationship_details JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (parent_model_id) REFERENCES model_risk_governance.models(model_id)
);

CREATE TABLE model_risk_governance.data_provenance (
    provenance_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    data_source_id INT NOT NULL,
    extraction_method VARCHAR(255),
    transformation_steps JSONB, -- Array of transformations applied
    data_snapshot_location VARCHAR(512),
    data_freshness_days INT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (data_source_id) REFERENCES core.data_sources(source_id)
);

CREATE TABLE model_risk_governance.feature_lineage (
    feature_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    feature_name VARCHAR(255) NOT NULL,
    source_data_element VARCHAR(255) NOT NULL,
    transformation_logic TEXT,
    business_meaning TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);

--advanced model testing framework
CREATE TABLE model_risk_governance.test_scenarios (
    scenario_id SERIAL PRIMARY KEY,
    scenario_name VARCHAR(255) NOT NULL,
    scenario_description TEXT,
    scenario_category VARCHAR(100) NOT NULL, -- "unit", "integration", "regression"
    is_automated BOOLEAN DEFAULT FALSE,
    expected_outcome TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE model_risk_governance.model_test_executions (
    execution_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    scenario_id INT NOT NULL,
    test_date TIMESTAMP WITH TIME ZONE NOT NULL,
    executed_by INT NOT NULL,
    execution_environment VARCHAR(100),
    input_parameters JSONB,
    actual_outcome JSONB,
    pass_status BOOLEAN,
    performance_metrics JSONB,
    execution_log TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (scenario_id) REFERENCES model_risk_governance.test_scenarios(scenario_id),
    FOREIGN KEY (executed_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.test_coverage_matrix (
    coverage_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    requirement_id INT, -- Links to regulatory or business requirements
    scenario_id INT NOT NULL,
    coverage_percentage DECIMAL(5,2),
    last_tested_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (requirement_id) REFERENCES model_risk_governance.regulatory_requirements(requirement_id),
    FOREIGN KEY (scenario_id) REFERENCES model_risk_governance.test_scenarios(scenario_id)
);


--model explainability and interpretability
CREATE TABLE model_risk_governance.explainability_methods (
    method_id SERIAL PRIMARY KEY,
    method_name VARCHAR(255) NOT NULL,
    method_type VARCHAR(100) NOT NULL, -- "global", "local", "model-specific"
    implementation_library VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE model_risk_governance.model_explanations (
    explanation_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    method_id INT NOT NULL,
    explanation_date TIMESTAMP WITH TIME ZONE NOT NULL,
    generated_by INT NOT NULL,
    explanation_artifacts JSONB, -- SHAP values, LIME results, etc.
    feature_importance JSONB,
    decision_rules TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (method_id) REFERENCES model_risk_governance.explainability_methods(method_id),
    FOREIGN KEY (generated_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.interpretability_metrics (
    metric_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    evaluation_date TIMESTAMP WITH TIME ZONE NOT NULL,
    metric_name VARCHAR(100) NOT NULL, -- "comprehensibility", "faithfulness", etc.
    metric_value DECIMAL(10,4),
    benchmark_value DECIMAL(10,4),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);


---model security and access control
CREATE TABLE model_risk_governance.model_access_policies (
    policy_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    policy_name VARCHAR(255) NOT NULL,
    policy_description TEXT,
    access_level VARCHAR(50) NOT NULL, -- "read", "execute", "admin"
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);

CREATE TABLE model_risk_governance.access_control_entries (
    entry_id SERIAL PRIMARY KEY,
    policy_id INT NOT NULL,
    grantee_type VARCHAR(50) NOT NULL, -- "user", "group", "role"
    grantee_id INT NOT NULL, -- Links to users, groups or roles
    expiration_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (policy_id) REFERENCES model_risk_governance.model_access_policies(policy_id)
);

CREATE TABLE model_risk_governance.model_access_logs (
    log_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    user_id INT NOT NULL,
    access_time TIMESTAMP WITH TIME ZONE NOT NULL,
    access_type VARCHAR(50) NOT NULL, -- "view", "execute", "modify"
    access_method VARCHAR(100), -- "API", "UI", "batch"
    input_parameters JSONB,
    output_sample TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (user_id) REFERENCES core.employees(employee_id)
);

--model deployment and runtime environment
CREATE TABLE model_risk_governance.deployment_environments (
    environment_id SERIAL PRIMARY KEY,
    environment_name VARCHAR(255) NOT NULL,
    environment_type VARCHAR(100) NOT NULL, -- "development", "staging", "production"
    infrastructure_provider VARCHAR(100),
    location VARCHAR(100),
    security_rating VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE model_risk_governance.model_deployments (
    deployment_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    environment_id INT NOT NULL,
    deployment_version VARCHAR(50) NOT NULL,
    deployment_date TIMESTAMP WITH TIME ZONE NOT NULL,
    deployed_by INT NOT NULL,
    deployment_artifacts JSONB,
    rollback_procedure TEXT,
    health_status VARCHAR(50),
    last_health_check TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (environment_id) REFERENCES model_risk_governance.deployment_environments(environment_id),
    FOREIGN KEY (deployed_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.runtime_dependencies (
    dependency_id SERIAL PRIMARY KEY,
    deployment_id INT NOT NULL,
    dependency_name VARCHAR(255) NOT NULL,
    dependency_version VARCHAR(100) NOT NULL,
    dependency_type VARCHAR(100) NOT NULL, -- "library", "service", "data"
    is_vulnerable BOOLEAN DEFAULT FALSE,
    vulnerability_details TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (deployment_id) REFERENCES model_risk_governance.model_deployments(deployment_id)
);


--model cost and resource tracking
CREATE TABLE model_risk_governance.model_resource_usage (
    usage_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    measurement_period VARCHAR(50) NOT NULL, -- "daily", "weekly", "monthly"
    start_date DATE NOT NULL,
    end_date DATE NOT NULL,
    compute_hours DECIMAL(10,2),
    storage_gb DECIMAL(10,2),
    memory_gb_hours DECIMAL(10,2),
    network_gb DECIMAL(10,2),
    estimated_cost DECIMAL(15,2),
    cost_center VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);

CREATE TABLE model_risk_governance.model_business_value (
    value_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    valuation_date DATE NOT NULL,
    valued_by INT NOT NULL,
    value_type VARCHAR(100) NOT NULL, -- "revenue", "cost_savings", "risk_reduction"
    amount DECIMAL(15,2),
    measurement_method VARCHAR(255),
    confidence_level VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (valued_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.model_roi_analysis (
    analysis_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    analysis_date DATE NOT NULL,
    analyzed_by INT NOT NULL,
    development_cost DECIMAL(15,2),
    annual_maintenance_cost DECIMAL(15,2),
    annual_benefit DECIMAL(15,2),
    roi_percentage DECIMAL(10,2),
    payback_period_months INT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (analyzed_by) REFERENCES core.employees(employee_id)
);

--model version control and comparison
CREATE TABLE model_risk_governance.model_version_diffs (
    diff_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    base_version VARCHAR(50) NOT NULL,
    compare_version VARCHAR(50) NOT NULL,
    diff_date TIMESTAMP WITH TIME ZONE NOT NULL,
    diff_type VARCHAR(100) NOT NULL, -- "code", "parameters", "performance"
    diff_details JSONB,
    created_by INT NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (created_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.version_approvals (
    approval_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    version VARCHAR(50) NOT NULL,
    approval_date TIMESTAMP WITH TIME ZONE NOT NULL,
    approved_by INT NOT NULL,
    approval_notes TEXT,
    approval_criteria JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (approved_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.version_rollouts (
    rollout_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    from_version VARCHAR(50) NOT NULL,
    to_version VARCHAR(50) NOT NULL,
    rollout_strategy VARCHAR(100) NOT NULL, -- "big_bang", "canary", "blue_green"
    start_time TIMESTAMP WITH TIME ZONE NOT NULL,
    completion_time TIMESTAMP WITH TIME ZONE,
    rollout_status VARCHAR(50) NOT NULL,
    monitoring_dashboard_url VARCHAR(512),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);


--model feedback and continuous improvement
CREATE TABLE model_risk_governance.model_feedback (
    feedback_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    feedback_date TIMESTAMP WITH TIME ZONE NOT NULL,
    provided_by INT,
    feedback_type VARCHAR(100) NOT NULL, -- "user", "stakeholder", "system"
    feedback_category VARCHAR(100) NOT NULL, -- "accuracy", "usability", "performance"
    feedback_text TEXT NOT NULL,
    sentiment_score DECIMAL(5,2),
    priority VARCHAR(50),
    status VARCHAR(50) DEFAULT 'Open',
    resolution TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (provided_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.improvement_actions (
    action_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    feedback_id INT,
    action_description TEXT NOT NULL,
    action_type VARCHAR(100) NOT NULL, -- "bug_fix", "enhancement", "retraining"
    assigned_to INT,
    due_date DATE,
    completion_date DATE,
    status VARCHAR(50) DEFAULT 'Pending',
    impact_assessment TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (feedback_id) REFERENCES model_risk_governance.model_feedback(feedback_id),
    FOREIGN KEY (assigned_to) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.model_retraining_logs (
    retraining_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    trigger_type VARCHAR(100) NOT NULL, -- "scheduled", "performance", "data_drift"
    training_data_range_start DATE,
    training_data_range_end DATE,
    start_time TIMESTAMP WITH TIME ZONE NOT NULL,
    end_time TIMESTAMP WITH TIME ZONE,
    status VARCHAR(50) NOT NULL,
    performance_comparison JSONB,
    created_by INT NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (created_by) REFERENCES core.employees(employee_id)
);

--advanced analytics and ai-specific tables
CREATE TABLE model_risk_governance.ai_model_metadata (
    metadata_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    architecture_type VARCHAR(100) NOT NULL, -- "neural_network", "random_forest", etc.
    framework VARCHAR(100) NOT NULL,
    framework_version VARCHAR(50) NOT NULL,
    hyperparameters JSONB,
    training_metrics JSONB,
    model_size_mb DECIMAL(10,2),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);

CREATE TABLE model_risk_governance.training_data_snapshots (
    snapshot_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    snapshot_date TIMESTAMP WITH TIME ZONE NOT NULL,
    data_statistics JSONB, -- Counts, distributions, etc.
    sample_data_location VARCHAR(512),
    data_hash VARCHAR(128), -- For integrity verification
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);

CREATE TABLE model_risk_governance.model_serving_metrics (
    serving_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    measurement_window TIMESTAMP WITH TIME ZONE NOT NULL,
    request_count INT,
    avg_latency_ms DECIMAL(10,2),
    error_rate DECIMAL(5,2),
    throughput_rps DECIMAL(10,2),
    hardware_utilization JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);

---regulatory change management
CREATE TABLE model_risk_governance.regulatory_changes (
    change_id SERIAL PRIMARY KEY,
    regulation_name VARCHAR(255) NOT NULL,
    change_reference VARCHAR(255) NOT NULL,
    change_description TEXT NOT NULL,
    effective_date DATE NOT NULL,
    compliance_deadline DATE NOT NULL,
    impact_level VARCHAR(50) NOT NULL, -- "high", "medium", "low"
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE model_risk_governance.model_regulatory_impact (
    impact_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    change_id INT NOT NULL,
    impact_assessment TEXT NOT NULL,
    required_actions TEXT,
    assigned_to INT,
    target_completion_date DATE,
    status VARCHAR(50) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (change_id) REFERENCES model_risk_governance.regulatory_changes(change_id),
    FOREIGN KEY (assigned_to) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.regulatory_reporting_schedule (
    schedule_id SERIAL PRIMARY KEY,
    regulation_name VARCHAR(255) NOT NULL,
    report_name VARCHAR(255) NOT NULL,
    frequency VARCHAR(50) NOT NULL, -- "quarterly", "annual", "event-driven"
    next_due_date DATE NOT NULL,
    responsible_party_id INT NOT NULL,
    last_submitted_date DATE,
    submission_status VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (responsible_party_id) REFERENCES core.employees(employee_id)
);


--advanced model impact analysis tables
CREATE TABLE model_risk_governance.business_process_mappings (
    mapping_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    process_id INT NOT NULL,
    impact_level VARCHAR(50) NOT NULL, -- "critical", "high", "medium", "low"
    usage_description TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (process_id) REFERENCES core.business_processes(process_id)
);

CREATE TABLE model_risk_governance.decision_point_impacts (
    impact_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    decision_point VARCHAR(255) NOT NULL,
    annual_decision_volume INT,
    financial_impact_per_decision DECIMAL(15,2),
    risk_exposure_per_decision DECIMAL(15,2),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);

CREATE TABLE model_risk_governance.model_cascade_effects (
    effect_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    affected_model_id INT NOT NULL,
    effect_type VARCHAR(100) NOT NULL, -- "data", "output", "performance"
    effect_description TEXT,
    mitigation_strategy TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (affected_model_id) REFERENCES model_risk_governance.models(model_id)
);

--model risk quantification tables
CREATE TABLE model_risk_governance.risk_scenario_quantification (
    quantification_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    scenario_id INT NOT NULL,
    quantification_date DATE NOT NULL,
    probability DECIMAL(10,8) NOT NULL,
    financial_impact DECIMAL(15,2) NOT NULL,
    risk_capital DECIMAL(15,2),
    confidence_interval VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (scenario_id) REFERENCES model_risk_governance.risk_scenarios(scenario_id)
);

CREATE TABLE model_risk_governance.value_at_risk_calculations (
    var_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    calculation_date DATE NOT NULL,
    time_horizon_days INT NOT NULL,
    confidence_level DECIMAL(5,2) NOT NULL,
    var_amount DECIMAL(15,2) NOT NULL,
    expected_shortfall DECIMAL(15,2),
    calculation_method VARCHAR(100) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);

CREATE TABLE model_risk_governance.model_risk_capital_allocation (
    allocation_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    allocation_date DATE NOT NULL,
    risk_capital_amount DECIMAL(15,2) NOT NULL,
    allocation_method VARCHAR(100) NOT NULL,
    review_frequency VARCHAR(50) NOT NULL,
    next_review_date DATE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);

--advanced model monitoring tables
CREATE TABLE model_risk_governance.concept_drift_indicators (
    indicator_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    detection_date TIMESTAMP WITH TIME ZONE NOT NULL,
    drift_metric VARCHAR(100) NOT NULL,
    baseline_value DECIMAL(10,4),
    current_value DECIMAL(10,4),
    drift_score DECIMAL(10,4),
    severity VARCHAR(50) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);

CREATE TABLE model_risk_governance.data_quality_monitoring (
    monitoring_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    check_date TIMESTAMP WITH TIME ZONE NOT NULL,
    quality_dimension VARCHAR(100) NOT NULL, -- "completeness", "accuracy", etc.
    metric_name VARCHAR(100) NOT NULL,
    expected_value VARCHAR(255),
    actual_value VARCHAR(255),
    deviation_percentage DECIMAL(10,4),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);

CREATE TABLE model_risk_governance.model_decision_audit (
    audit_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    execution_id VARCHAR(255) NOT NULL,
    decision_timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
    input_data_hash VARCHAR(128),
    output_data_hash VARCHAR(128),
    decision_context TEXT,
    audit_trail_location VARCHAR(512),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);

-----model risk appetite framework tables
CREATE TABLE model_risk_governance.risk_appetite_statements (
    statement_id SERIAL PRIMARY KEY,
    statement_name VARCHAR(255) NOT NULL,
    statement_description TEXT,
    effective_date DATE NOT NULL,
    expiration_date DATE,
    approval_date DATE NOT NULL,
    approved_by INT NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (approved_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.risk_tolerance_limits (
    limit_id SERIAL PRIMARY KEY,
    statement_id INT NOT NULL,
    risk_dimension VARCHAR(100) NOT NULL,
    risk_category VARCHAR(100) NOT NULL,
    tolerance_level VARCHAR(50) NOT NULL, -- "low", "medium", "high"
    quantitative_limit DECIMAL(15,2),
    qualitative_description TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (statement_id) REFERENCES model_risk_governance.risk_appetite_statements(statement_id)
);

CREATE TABLE model_risk_governance.model_appetite_alignment (
    alignment_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    statement_id INT NOT NULL,
    assessment_date DATE NOT NULL,
    alignment_status VARCHAR(50) NOT NULL, -- "within", "borderline", "exceeds"
    assessment_methodology TEXT,
    override_approval BOOLEAN DEFAULT FALSE,
    override_justification TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (statement_id) REFERENCES model_risk_governance.risk_appetite_statements(statement_id)
);


--advanced views for model risk governance
CREATE OR REPLACE VIEW model_risk_governance.model_risk_exposure_dashboard AS
SELECT
    m.model_id,
    m.model_name,
    m.model_version,
    r.rating_name AS risk_rating,
    (SELECT COUNT(*) FROM model_risk_governance.model_scenario_assessments WHERE model_id = m.model_id) AS scenario_count,
    (SELECT SUM(financial_impact * probability) FROM model_risk_governance.risk_scenario_quantification WHERE model_id = m.model_id) AS expected_loss,
    (SELECT MAX(var_amount) FROM model_risk_governance.value_at_risk_calculations WHERE model_id = m.model_id) AS value_at_risk,
    a.allocation_amount AS risk_capital,
    (SELECT STRING_AGG(process_name, ', ')
     FROM core.business_processes bp
     JOIN model_risk_governance.business_process_mappings bpm ON bp.process_id = bpm.process_id
     WHERE bpm.model_id = m.model_id) AS impacted_processes,
    (SELECT COUNT(*) FROM model_risk_governance.model_cascade_effects WHERE model_id = m.model_id) AS downstream_impacts
FROM
    model_risk_governance.models m
JOIN
    model_risk_governance.risk_ratings r ON m.risk_rating_id = r.rating_id
LEFT JOIN
    model_risk_governance.model_risk_capital_allocation a ON m.model_id = a.model_id
WHERE
    m.is_active = TRUE;


--model validation status heatmap view
CREATE OR REPLACE VIEW model_risk_governance.validation_status_heatmap AS
SELECT
    d.department_name,
    t.type_name AS model_type,
    r.rating_name AS risk_rating,
    COUNT(*) AS total_models,
    SUM(CASE WHEN v.validation_status_id = (SELECT status_id FROM model_risk_governance.validation_statuses WHERE status_name = 'Approved') THEN 1 ELSE 0 END) AS approved,
    SUM(CASE WHEN v.validation_status_id = (SELECT status_id FROM model_risk_governance.validation_statuses WHERE status_name = 'Pending') THEN 1 ELSE 0 END) AS pending,
    SUM(CASE WHEN v.validation_status_id = (SELECT status_id FROM model_risk_governance.validation_statuses WHERE status_name = 'Overdue') THEN 1 ELSE 0 END) AS overdue,
    SUM(CASE WHEN v.validation_status_id IS NULL THEN 1 ELSE 0 END) AS not_validated,
    MAX(CASE WHEN v.validation_status_id = (SELECT status_id FROM model_risk_governance.validation_statuses WHERE status_name = 'Approved')
         THEN v.validation_date ELSE NULL END) AS last_approved_date
FROM
    model_risk_governance.models m
JOIN
    core.departments d ON m.department_id = d.department_id
JOIN
    model_risk_governance.model_types t ON m.model_type_id = t.type_id
JOIN
    model_risk_governance.risk_ratings r ON m.risk_rating_id = r.rating_id
LEFT JOIN
    (SELECT model_id, validation_status_id, validation_date
     FROM model_risk_governance.model_validations
     WHERE validation_date = (SELECT MAX(validation_date) FROM model_risk_governance.model_validations mv WHERE mv.model_id = model_validations.model_id)
    ) v ON m.model_id = v.model_id
WHERE
    m.is_active = TRUE
GROUP BY
    d.department_name, t.type_name, r.rating_name;


--model performance benchmarking view
CREATE OR REPLACE VIEW model_risk_governance.model_performance_benchmarking AS
SELECT
    m.model_id,
    m.model_name,
    t.type_name AS model_type,
    r.rating_name AS risk_rating,
    (SELECT AVG(accuracy_score) FROM model_risk_governance.model_performance WHERE model_id = m.model_id AND monitoring_date >= CURRENT_DATE - INTERVAL '3 months') AS current_accuracy,
    (SELECT AVG(accuracy_score) FROM model_risk_governance.model_performance WHERE model_id = m.model_id AND monitoring_date BETWEEN CURRENT_DATE - INTERVAL '6 months' AND CURRENT_DATE - INTERVAL '3 months') AS previous_accuracy,
    (SELECT AVG(accuracy_score) FROM model_risk_governance.model_performance mp JOIN model_risk_governance.models m2 ON mp.model_id = m2.model_id
     WHERE m2.model_type_id = m.model_type_id AND mp.monitoring_date >= CURRENT_DATE - INTERVAL '3 months') AS type_benchmark,
    (SELECT COUNT(*) FROM model_risk_governance.performance_alerts pa JOIN model_risk_governance.model_performance mp ON pa.performance_id = mp.performance_id
     WHERE mp.model_id = m.model_id AND pa.alert_date >= CURRENT_DATE - INTERVAL '6 months') AS alert_count,
    (SELECT psi_score FROM model_risk_governance.drift_metrics WHERE model_id = m.model_id ORDER BY metric_date DESC LIMIT 1) AS latest_psi
FROM
    model_risk_governance.models m
JOIN
    model_risk_governance.model_types t ON m.model_type_id = t.type_id
JOIN
    model_risk_governance.risk_ratings r ON m.risk_rating_id = r.rating_id
WHERE
    m.is_active = TRUE;

--model risk concentration view
CREATE OR REPLACE VIEW model_risk_governance.risk_concentration_analysis AS
SELECT
    r.rating_name AS risk_rating,
    t.type_name AS model_type,
    d.department_name,
    COUNT(*) AS model_count,
    SUM(vq.financial_impact * vq.probability) AS aggregate_expected_loss,
    SUM(vc.var_amount) AS aggregate_var,
    SUM(mr.risk_capital_amount) AS allocated_capital,
    (SELECT COUNT(*) FROM model_risk_governance.model_performance WHERE is_alert_triggered = TRUE AND model_id IN
        (SELECT model_id FROM model_risk_governance.models WHERE risk_rating_id = r.rating_id AND model_type_id = t.type_id AND department_id = d.department_id)
    ) AS active_alerts
FROM
    model_risk_governance.models m
JOIN
    model_risk_governance.risk_ratings r ON m.risk_rating_id = r.rating_id
JOIN
    model_risk_governance.model_types t ON m.model_type_id = t.type_id
JOIN
    core.departments d ON m.department_id = d.department_id
LEFT JOIN
    (SELECT model_id, SUM(financial_impact * probability) AS financial_impact, SUM(probability) AS probability
     FROM model_risk_governance.risk_scenario_quantification
     GROUP BY model_id) vq ON m.model_id = vq.model_id
LEFT JOIN
    (SELECT model_id, MAX(var_amount) AS var_amount
     FROM model_risk_governance.value_at_risk_calculations
     GROUP BY model_id) vc ON m.model_id = vc.model_id
LEFT JOIN
    model_risk_governance.model_risk_capital_allocation mr ON m.model_id = mr.model_id
WHERE
    m.is_active = TRUE
GROUP BY
    r.rating_name, t.type_name, d.department_name;


--model lifecycle stage view
CREATE OR REPLACE VIEW model_risk_governance.model_lifecycle_stages AS
SELECT
    m.model_id,
    m.model_name,
    m.model_version,
    CASE
        WHEN m.development_date IS NULL THEN 'Planned'
        WHEN m.deployment_date IS NULL THEN 'In Development'
        WHEN m.retirement_date IS NULL THEN
            CASE
                WHEN (SELECT COUNT(*) FROM model_risk_governance.model_validations WHERE model_id = m.model_id) = 0 THEN 'Deployed - Not Validated'
                WHEN (SELECT COUNT(*) FROM model_risk_governance.model_performance WHERE model_id = m.model_id AND is_alert_triggered = TRUE AND monitoring_date >= CURRENT_DATE - INTERVAL '3 months') > 0 THEN 'Active - With Alerts'
                ELSE 'Active - Stable'
            END
        WHEN m.retirement_date >= CURRENT_DATE THEN 'Pending Retirement'
        ELSE 'Retired'
    END AS lifecycle_stage,
    m.development_date,
    m.deployment_date,
    m.retirement_date,
    (SELECT MAX(validation_date) FROM model_risk_governance.model_validations WHERE model_id = m.model_id) AS last_validation_date,
    (SELECT COUNT(*) FROM model_risk_governance.model_performance WHERE model_id = m.model_id AND is_alert_triggered = TRUE AND monitoring_date >= CURRENT_DATE - INTERVAL '3 months') AS recent_alerts
FROM
    model_risk_governance.models m;

--advanced materialized views for performance
--model risk exposure snapshot
CREATE MATERIALIZED VIEW model_risk_governance.mv_model_risk_exposure_snapshot AS
SELECT
    DATE_TRUNC('month', CURRENT_DATE) AS snapshot_month,
    r.rating_name AS risk_rating,
    COUNT(*) AS model_count,
    SUM(vq.expected_loss) AS total_expected_loss,
    SUM(vc.value_at_risk) AS total_value_at_risk,
    SUM(mr.risk_capital_amount) AS total_risk_capital,
    AVG(p.accuracy_score) AS avg_accuracy,
    SUM(CASE WHEN p.is_alert_triggered THEN 1 ELSE 0 END) AS models_with_alerts
FROM
    model_risk_governance.models m
JOIN
    model_risk_governance.risk_ratings r ON m.risk_rating_id = r.rating_id
LEFT JOIN
    (SELECT model_id, SUM(financial_impact * probability) AS expected_loss
     FROM model_risk_governance.risk_scenario_quantification
     GROUP BY model_id) vq ON m.model_id = vq.model_id
LEFT JOIN
    (SELECT model_id, MAX(var_amount) AS value_at_risk
     FROM model_risk_governance.value_at_risk_calculations
     GROUP BY model_id) vc ON m.model_id = vc.model_id
LEFT JOIN
    (SELECT model_id, risk_capital_amount
     FROM model_risk_governance.model_risk_capital_allocation
     WHERE allocation_date = (SELECT MAX(allocation_date) FROM model_risk_governance.model_risk_capital_allocation WHERE model_id = m.model_id)
    ) mr ON m.model_id = mr.model_id
LEFT JOIN
    (SELECT model_id, accuracy_score, is_alert_triggered
     FROM model_risk_governance.model_performance
     WHERE monitoring_date = (SELECT MAX(monitoring_date) FROM model_risk_governance.model_performance WHERE model_id = m.model_id)
    ) p ON m.model_id = p.model_id
WHERE
    m.is_active = TRUE
GROUP BY
    r.rating_name
WITH DATA;

CREATE INDEX idx_mv_model_risk_exposure ON model_risk_governance.mv_model_risk_exposure_snapshot (snapshot_month, risk_rating);


--materialized view
--model validation backlog
CREATE MATERIALIZED VIEW model_risk_governance.mv_validation_backlog AS
SELECT
    DATE_TRUNC('week', CURRENT_DATE) AS snapshot_week,
    d.department_name,
    t.type_name AS model_type,
    COUNT(*) AS models_due_for_validation,
    AVG(EXTRACT(DAY FROM CURRENT_DATE - v.next_validation_date)) AS avg_days_overdue,
    SUM(CASE WHEN m.is_critical THEN 1 ELSE 0 END) AS critical_models
FROM
    model_risk_governance.models m
JOIN
    core.departments d ON m.department_id = d.department_id
JOIN
    model_risk_governance.model_types t ON m.model_type_id = t.type_id
JOIN
    (SELECT model_id, next_validation_date
     FROM model_risk_governance.model_validations
     WHERE validation_date = (SELECT MAX(validation_date) FROM model_risk_governance.model_validations WHERE model_id = m.model_id)
    ) v ON m.model_id = v.model_id
WHERE
    m.is_active = TRUE AND
    v.next_validation_date < CURRENT_DATE
GROUP BY
    d.department_name, t.type_name
WITH DATA;

CREATE INDEX idx_mv_validation_backlog ON model_risk_governance.mv_validation_backlog (snapshot_week, department_name, model_type);


--additional stored procedures
-- automated model risk scoring
CREATE OR REPLACE PROCEDURE model_risk_governance.calculate_model_risk_scores()
LANGUAGE plpgsql
AS $$
BEGIN
    -- Calculate risk scores for all active models
    INSERT INTO model_risk_governance.model_risk_assessments (
        model_id, assessment_date, assessed_by,
        risk_score, risk_level_id, impact_score,
        likelihood_score, risk_appetite_alignment,
        mitigation_plan, next_assessment_date
    )
    SELECT
        m.model_id,
        CURRENT_DATE,
        0, -- System user
        CASE
            WHEN m.is_critical THEN
                (0.4 * COALESCE(p.performance_score, 0) +
                (0.3 * COALESCE(v.validation_score, 0)) +
                (0.3 * COALESCE(c.complexity_score, 0))
            ELSE
                (0.3 * COALESCE(p.performance_score, 0)) +
                (0.2 * COALESCE(v.validation_score, 0)) +
                (0.2 * COALESCE(c.complexity_score, 0)) +
                (0.3 * COALESCE(i.impact_score, 0))
        END AS risk_score,
        CASE
            WHEN m.is_critical AND (
                (0.4 * COALESCE(p.performance_score, 0)) +
                (0.3 * COALESCE(v.validation_score, 0)) +
                (0.3 * COALESCE(c.complexity_score, 0)) > 0.7 THEN 5 -- Critical
            WHEN (0.3 * COALESCE(p.performance_score, 0)) +
                 (0.2 * COALESCE(v.validation_score, 0)) +
                 (0.2 * COALESCE(c.complexity_score, 0)) +
                 (0.3 * COALESCE(i.impact_score, 0)) > 0.7 THEN 4 -- High
            WHEN (0.3 * COALESCE(p.performance_score, 0)) +
                 (0.2 * COALESCE(v.validation_score, 0)) +
                 (0.2 * COALESCE(c.complexity_score, 0)) +
                 (0.3 * COALESCE(i.impact_score, 0)) > 0.5 THEN 3 -- Medium
            WHEN (0.3 * COALESCE(p.performance_score, 0)) +
                 (0.2 * COALESCE(v.validation_score, 0)) +
                 (0.2 * COALESCE(c.complexity_score, 0)) +
                 (0.3 * COALESCE(i.impact_score, 0)) > 0.3 THEN 2 -- Low
            ELSE 1 -- Very Low
        END AS risk_level_id,
        COALESCE(i.impact_score, 0) AS impact_score,
        CASE
            WHEN m.is_critical THEN
                (0.4 * COALESCE(p.performance_score, 0)) +
                (0.3 * COALESCE(v.validation_score, 0)) +
                (0.3 * COALESCE(c.complexity_score, 0))
            ELSE
                (0.3 * COALESCE(p.performance_score, 0)) +
                (0.2 * COALESCE(v.validation_score, 0)) +
                (0.2 * COALESCE(c.complexity_score, 0))
        END AS likelihood_score,
        CASE
            WHEN m.is_critical AND (
                (0.4 * COALESCE(p.performance_score, 0)) +
                (0.3 * COALESCE(v.validation_score, 0)) +
                (0.3 * COALESCE(c.complexity_score, 0)) > 0.6 THEN FALSE
            WHEN (0.3 * COALESCE(p.performance_score, 0)) +
                 (0.2 * COALESCE(v.validation_score, 0)) +
                 (0.2 * COALESCE(c.complexity_score, 0)) +
                 (0.3 * COALESCE(i.impact_score, 0)) > 0.6 THEN FALSE
            ELSE TRUE
        END AS risk_appetite_alignment,
        CASE
            WHEN m.is_critical AND (
                (0.4 * COALESCE(p.performance_score, 0)) +
                (0.3 * COALESCE(v.validation_score, 0)) +
                (0.3 * COALESCE(c.complexity_score, 0)) > 0.7 THEN 'Immediate mitigation required for critical model with high risk score'
            WHEN (0.3 * COALESCE(p.performance_score, 0)) +
                 (0.2 * COALESCE(v.validation_score, 0)) +
                 (0.2 * COALESCE(c.complexity_score, 0)) +
                 (0.3 * COALESCE(i.impact_score, 0)) > 0.7 THEN 'High risk model requires mitigation plan'
            ELSE 'Risk within acceptable limits'
        END AS mitigation_plan,
        CURRENT_DATE + INTERVAL '3 months'
    FROM
        model_risk_governance.models m
    LEFT JOIN
        (SELECT model_id,
                CASE
                    WHEN AVG(accuracy_score) < 0.7 OR COUNT(CASE WHEN is_alert_triggered THEN 1 END) > 0 THEN 0.8
                    WHEN AVG(accuracy_score) BETWEEN 0.7 AND 0.8 OR COUNT(CASE WHEN is_alert_triggered THEN 1 END) > 0 THEN 0.5
                    ELSE 0.2
                END AS performance_score
         FROM model_risk_governance.model_performance
         WHERE monitoring_date >= CURRENT_DATE - INTERVAL '3 months'
         GROUP BY model_id) p ON m.model_id = p.model_id
    LEFT JOIN
        (SELECT model_id,
                CASE
                    WHEN validation_status_id = (SELECT status_id FROM model_risk_governance.validation_statuses WHERE status_name = 'Approved') AND
                         validation_date >= CURRENT_DATE - INTERVAL '1 year' THEN 0.1
                    WHEN validation_status_id = (SELECT status_id FROM model_risk_governance.validation_statuses WHERE status_name = 'Approved') THEN 0.3
                    WHEN validation_status_id = (SELECT status_id FROM model_risk_governance.validation_statuses WHERE status_name = 'Pending') THEN 0.7
                    ELSE 0.9
                END AS validation_score
         FROM model_risk_governance.model_validations
         WHERE validation_date = (SELECT MAX(validation_date) FROM model_risk_governance.model_validations WHERE model_id = m.model_id)
        ) v ON m.model_id = v.model_id
    LEFT JOIN
        (SELECT model_id,
                CASE
                    WHEN COUNT(DISTINCT feature_name) > 50 THEN 0.8
                    WHEN COUNT(DISTINCT feature_name) > 20 THEN 0.5
                    ELSE 0.2
                END AS complexity_score
         FROM model_risk_governance.feature_lineage
         GROUP BY model_id) c ON m.model_id = c.model_id
    LEFT JOIN
        (SELECT model_id,
                CASE
                    WHEN SUM(financial_impact_per_decision * annual_decision_volume) > 1000000 THEN 0.9
                    WHEN SUM(financial_impact_per_decision * annual_decision_volume) > 100000 THEN 0.6
                    WHEN SUM(financial_impact_per_decision * annual_decision_volume) > 10000 THEN 0.3
                    ELSE 0.1
                END AS impact_score
         FROM model_risk_governance.decision_point_impacts
         GROUP BY model_id) i ON m.model_id = i.model_id
    WHERE
        m.is_active = TRUE;

    -- Log the execution
    INSERT INTO model_risk_governance.audit_logs (
        action_type, table_name, action_by, action_details
    ) VALUES (
        'PROCEDURE', 'model_risk_assessments', 0, 'Executed automated model risk scoring procedure'
    );
END;
$$;


--model retirement workflow
CREATE OR REPLACE PROCEDURE model_risk_governance.retire_model(
    p_model_id INT,
    p_retirement_reason TEXT,
    p_initiated_by INT,
    p_replacement_model_id INT DEFAULT NULL
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_model_name VARCHAR(255);
    v_department_id INT;
    v_stakeholders JSONB;
BEGIN
    -- Get model details
    SELECT model_name, department_id INTO v_model_name, v_department_id
    FROM model_risk_governance.models
    WHERE model_id = p_model_id;

    -- Get stakeholders
    SELECT jsonb_agg(jsonb_build_object('employee_id', e.employee_id, 'name', e.employee_name, 'role', r.role_name))
    INTO v_stakeholders
    FROM model_risk_governance.model_ownership o
    JOIN core.employees e ON o.owner_id = e.employee_id
    JOIN core.roles r ON e.role_id = r.role_id
    WHERE o.model_id = p_model_id AND o.is_current = TRUE;

    -- Update model status
    UPDATE model_risk_governance.models
    SET retirement_date = CURRENT_DATE,
        is_active = FALSE,
        updated_at = CURRENT_TIMESTAMP
    WHERE model_id = p_model_id;

    -- Record retirement
    INSERT INTO model_risk_governance.model_retirements (
        model_id, retirement_date, initiated_by,
        retirement_reason, replacement_model_id,
        stakeholders, department_id
    ) VALUES (
        p_model_id, CURRENT_DATE, p_initiated_by,
        p_retirement_reason, p_replacement_model_id,
        v_stakeholders, v_department_id
    );

    -- Archive documentation
    INSERT INTO model_risk_governance.archived_documentation (
        model_id, document_name, document_type,
        original_location, archive_location,
        archived_by, archive_date
    )
    SELECT
        p_model_id, document_name,
        (SELECT type_name FROM model_risk_governance.documentation_types WHERE type_id = d.documentation_type_id),
        document_location,
        '/archive/models/' || p_model_id || '/' || documentation_id || '_' || document_name,
        p_initiated_by, CURRENT_DATE
    FROM model_risk_governance.model_documentation d
    WHERE model_id = p_model_id;

    -- Notify stakeholders
    INSERT INTO model_risk_governance.notifications (
        notification_type, notification_text,
        recipient_id, model_id, status
    )
    SELECT
        'Model Retirement',
        'Model ' || v_model_name || ' has been retired. Reason: ' || p_retirement_reason,
        (jsonb_array_elements(v_stakeholders)->>'employee_id')::INT,
        p_model_id,
        'Pending'
    FROM jsonb_array_elements(v_stakeholders);

    -- Log the retirement
    INSERT INTO model_risk_governance.audit_logs (
        action_type, table_name, record_id,
        action_by, action_details
    ) VALUES (
        'RETIRE', 'models', p_model_id,
        p_initiated_by,
        'Retired model ' || v_model_name || '. Reason: ' || p_retirement_reason ||
        CASE WHEN p_replacement_model_id IS NOT NULL THEN
            '. Replaced by model ID: ' || p_replacement_model_id
        ELSE '' END
    );
END;
$$;

--additional security features
--row level security policies
-- Enable RLS on models table
ALTER TABLE model_risk_governance.models ENABLE ROW LEVEL SECURITY;

-- Policy: Department users can only see their department's models
CREATE POLICY department_model_access ON model_risk_governance.models
    FOR SELECT USING (
        department_id IN (
            SELECT department_id FROM core.employee_departments
            WHERE employee_id = current_setting('app.current_user_id')::INT
        )
    );

-- Policy: Model owners have full access to their models
CREATE POLICY owner_model_access ON model_risk_governance.models
    USING (
        model_id IN (
            SELECT model_id FROM model_risk_governance.model_ownership
            WHERE owner_id = current_setting('app.current_user_id')::INT AND is_current = TRUE
        )
    );

-- Policy: Risk managers can access all models
CREATE POLICY risk_manager_access ON model_risk_governance.models
    USING (
        EXISTS (
            SELECT 1 FROM core.employees e
            JOIN core.roles r ON e.role_id = r.role_id
            WHERE e.employee_id = current_setting('app.current_user_id')::INT
            AND r.role_name = 'Risk Manager'
        )
    );


--advanced indexing strategy
-- GIN indexes for JSONB columns
CREATE INDEX idx_model_explanations_artifacts ON model_risk_governance.model_explanations USING GIN(explanation_artifacts);
CREATE INDEX idx_model_serving_metrics_hardware ON model_risk_governance.model_serving_metrics USING GIN(hardware_utilization);
CREATE INDEX idx_model_test_executions_input ON model_risk_governance.model_test_executions USING GIN(input_parameters);

-- BRIN indexes for time-series data
CREATE INDEX idx_model_performance_time ON model_risk_governance.model_performance USING BRIN(monitoring_date);
CREATE INDEX idx_performance_alerts_time ON model_risk_governance.performance_alerts USING BRIN(alert_date);
CREATE INDEX idx_model_access_logs_time ON model_risk_governance.model_access_logs USING BRIN(access_time);

-- Partial indexes for performance optimization
CREATE INDEX idx_active_model_validations ON model_risk_governance.model_validations(model_id)
    WHERE validation_status_id = (SELECT status_id FROM model_risk_governance.validation_statuses WHERE status_name = 'Pending');

CREATE INDEX idx_critical_models ON model_risk_governance.models(model_id)
    WHERE is_critical = TRUE AND is_active = TRUE;

CREATE INDEX idx_overdue_validations ON model_risk_governance.model_validations(model_id)
    WHERE next_validation_date < CURRENT_DATE;


--data dictionary view
CREATE OR REPLACE VIEW model_risk_governance.data_dictionary AS
SELECT
    t.table_name,
    c.column_name,
    c.data_type,
    pgd.description AS column_description,
    (SELECT COUNT(*) FROM pg_indexes i
     JOIN pg_attribute a ON i.indexname = a.attname
     WHERE i.tablename = t.table_name AND a.attname = c.column_name) > 0 AS is_indexed,
    c.is_nullable,
    c.character_maximum_length AS max_length,
    (SELECT STRING_AGG(pgt.conname, ', ')
     FROM pg_constraint pgt
     WHERE pgt.conrelid = t.table_name::regclass AND
           c.column_name = ANY(ARRAY(SELECT attname FROM pg_attribute WHERE attrelid = pgt.conrelid AND attnum = ANY(pgt.conkey)))
    ) AS constraints
FROM
    information_schema.tables t
JOIN
    information_schema.columns c ON t.table_name = c.table_name
LEFT JOIN
    pg_statio_all_tables st ON t.table_name = st.relname
LEFT JOIN
    pg_description pgd ON pgd.objoid = st.relid AND pgd.objsubid = c.ordinal_position
WHERE
    t.table_schema = 'model_risk_governance'
ORDER BY
    t.table_name, c.ordinal_position;


--schema relationship diagram view
CREATE OR REPLACE VIEW model_risk_governance.schema_relationships AS
SELECT
    tc.table_name AS source_table,
    kcu.column_name AS source_column,
    ccu.table_name AS target_table,
    ccu.column_name AS target_column,
    tc.constraint_name AS relationship_name,
    CASE WHEN tc.constraint_type = 'PRIMARY KEY' THEN 'PK'
         WHEN tc.constraint_type = 'FOREIGN KEY' THEN 'FK'
         ELSE tc.constraint_type
    END AS relationship_type
FROM
    information_schema.table_constraints tc
JOIN
    information_schema.key_column_usage kcu ON tc.constraint_name = kcu.constraint_name
JOIN
    information_schema.constraint_column_usage ccu ON tc.constraint_name = ccu.constraint_name
WHERE
    tc.table_schema = 'model_risk_governance' AND
    tc.constraint_type IN ('PRIMARY KEY', 'FOREIGN KEY')
ORDER BY
    tc.table_name, kcu.column_name;

    CREATE TABLE model_risk_governance.ai_ethics_frameworks (
    framework_id SERIAL PRIMARY KEY,
    framework_name VARCHAR(255) NOT NULL,
    framework_version VARCHAR(50) NOT NULL,
    governing_body VARCHAR(255) NOT NULL,
    effective_date DATE NOT NULL,
    compliance_requirements JSONB NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE model_risk_governance.model_ethics_assessments (
    assessment_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    framework_id INT NOT NULL,
    assessment_date DATE NOT NULL,
    assessed_by INT NOT NULL,
    compliance_score DECIMAL(5,2),
    fairness_score DECIMAL(5,2),
    transparency_score DECIMAL(5,2),
    accountability_score DECIMAL(5,2),
    overall_rating VARCHAR(50),
    assessment_report TEXT,
    next_assessment_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (framework_id) REFERENCES model_risk_governance.ai_ethics_frameworks(framework_id),
    FOREIGN KEY (assessed_by) REFERENCES core.employees(employee_id)
);

CREATE TABLE model_risk_governance.algorithmic_impact_assessments (
    assessment_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    assessment_date DATE NOT NULL,
    impact_domain VARCHAR(100) NOT NULL, -- "privacy", "fairness", "safety"
    impact_level VARCHAR(50) NOT NULL, -- "high", "medium", "low"
    affected_stakeholders TEXT,
    mitigation_measures TEXT,
    approval_required BOOLEAN DEFAULT FALSE,
    approval_status VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);

CREATE TABLE model_risk_governance.model_environmental_impact (
    impact_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    measurement_date DATE NOT NULL,
    training_co2_kg DECIMAL(10,2),
    inference_co2_per_1000_req DECIMAL(10,4),
    energy_source_mix JSONB, -- % renewable vs non-renewable
    hardware_efficiency_score DECIMAL(5,2),
    optimization_recommendations TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id)
);

CREATE TABLE model_risk_governance.green_ai_best_practices (
    practice_id SERIAL PRIMARY KEY,
    practice_name VARCHAR(255) NOT NULL,
    practice_description TEXT NOT NULL,
    impact_category VARCHAR(100) NOT NULL, -- "energy", "hardware", "algorithm"
    expected_reduction_pct DECIMAL(5,2),
    implementation_difficulty VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE model_risk_governance.model_sustainability_adoption (
    adoption_id SERIAL PRIMARY KEY,
    model_id INT NOT NULL,
    practice_id INT NOT NULL,
    implementation_date DATE,
    implementation_status VARCHAR(50) NOT NULL,
    measured_impact DECIMAL(5,2),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (model_id) REFERENCES model_risk_governance.models(model_id),
    FOREIGN KEY (practice_id) REFERENCES model_risk_governance.green_ai_best_practices(practice_id)
);


    CREATE MATERIALIZED VIEW model_risk_governance.mv_quantum_risk_exposure AS
SELECT
    m.model_id,
    m.model_name,
    m.model_version,
    COUNT(DISTINCT c.control_id) AS security_controls,
    COUNT(DISTINCT CASE WHEN c.quantum_safe = TRUE THEN c.control_id END) AS quantum_safe_controls,
    COUNT(DISTINCT t.threat_id) AS identified_threats,
    COUNT(DISTINCT a.test_id) AS adversarial_tests,
    MAX(a.vulnerability_score) AS max_vulnerability_score,
    CASE
        WHEN COUNT(DISTINCT CASE WHEN c.quantum_safe = TRUE THEN c.control_id END) = 0 THEN 'Critical'
        WHEN COUNT(DISTINCT CASE WHEN c.quantum_safe = TRUE THEN c.control_id END) < COUNT(DISTINCT c.control_id) THEN 'High'
        ELSE 'Controlled'
    END AS quantum_risk_status
FROM
    model_risk_governance.models m
LEFT JOIN
    model_risk_governance.cryptographic_controls c ON m.model_id = c.model_id
LEFT JOIN
    model_risk_governance.model_threat_modeling t ON m.model_id = t.model_id
LEFT JOIN
    model_risk_governance.adversarial_testing a ON m.model_id = a.model_id
WHERE
    m.is_active = TRUE
GROUP BY
    m.model_id, m.model_name, m.model_version
WITH DATA;

CREATE INDEX idx_mv_quantum_risk_exposure ON model_risk_governance.mv_quantum_risk_exposure (model_id, quantum_risk_status);



CREATE MATERIALIZED VIEW model_risk_governance.mv_ethics_compliance AS
SELECT
    m.model_id,
    m.model_name,
    f.framework_name,
    ea.assessment_date,
    ea.overall_rating,
    COUNT(DISTINCT ai.assessment_id) AS impact_assessments,
    STRING_AGG(DISTINCT ai.impact_domain, ', ') AS impact_domains,
    (SELECT COUNT(*) FROM model_risk_governance.ethical_incidents WHERE model_id = m.model_id) AS ethical_incidents,
    CASE
        WHEN ea.overall_rating = 'Excellent' AND COUNT(DISTINCT ai.assessment_id) > 0 THEN 'Fully Compliant'
        WHEN ea.overall_rating IS NULL THEN 'Not Assessed'
        ELSE 'Partial Compliance'
    END AS compliance_status
FROM
    model_risk_governance.models m
JOIN
    model_risk_governance.model_ethics_assessments ea ON m.model_id = ea.model_id
JOIN
    model_risk_governance.ai_ethics_frameworks f ON ea.framework_id = f.framework_id
LEFT JOIN
    model_risk_governance.algorithmic_impact_assessments ai ON m.model_id = ai.model_id
WHERE
    m.is_active = TRUE AND
    ea.assessment_date = (SELECT MAX(assessment_date) FROM model_risk_governance.model_ethics_assessments WHERE model_id = m.model_id)
GROUP BY
    m.model_id, m.model_name, f.framework_name, ea.assessment_date, ea.overall_rating
WITH DATA;

CREATE INDEX idx_mv_ethics_compliance ON model_risk_governance.mv_ethics_compliance (model_id, compliance_status);


CREATE MATERIALIZED VIEW model_risk_governance.mv_sustainability_impact AS
SELECT
    m.model_id,
    m.model_name,
    d.department_name,
    AVG(e.training_co2_kg) AS avg_training_footprint,
    AVG(e.inference_co2_per_1000_req) AS avg_inference_footprint,
    COUNT(DISTINCT s.practice_id) AS adopted_practices,
    (SELECT COUNT(*) FROM model_risk_governance.green_ai_best_practices) AS total_practices,
    CASE
        WHEN AVG(e.inference_co2_per_1000_req) < 0.1 AND COUNT(DISTINCT s.practice_id) > 5 THEN 'Leader'
        WHEN AVG(e.inference_co2_per_1000_req) < 0.5 AND COUNT(DISTINCT s.practice_id) > 3 THEN 'Performer'
        ELSE 'Laggard'
    END AS sustainability_rating
FROM
    model_risk_governance.models m
JOIN
    core.departments d ON m.department_id = d.department_id
LEFT JOIN
    model_risk_governance.model_environmental_impact e ON m.model_id = e.model_id
LEFT JOIN
    model_risk_governance.model_sustainability_adoption s ON m.model_id = s.model_id
WHERE
    m.is_active = TRUE
GROUP BY
    m.model_id, m.model_name, d.department_name
WITH DATA;

CREATE INDEX idx_mv_sustainability_impact ON model_risk_governance.mv_sustainability_impact (model_id, sustainability_rating);




CREATE OR REPLACE PROCEDURE model_risk_governance.calculate_ethics_scores()
LANGUAGE plpgsql
AS $$
BEGIN
    -- Calculate ethics scores for all active models
    INSERT INTO model_risk_governance.model_ethics_assessments (
        model_id, framework_id, assessment_date, assessed_by,
        compliance_score, fairness_score, transparency_score,
        accountability_score, overall_rating, next_assessment_date
    )
    SELECT
        m.model_id,
        f.framework_id,
        CURRENT_DATE,
        0, -- System user
        -- Compliance score based on implemented controls
        (SELECT COUNT(*) FROM model_risk_governance.model_sustainability_adoption WHERE model_id = m.model_id) * 10.0 /
        (SELECT COUNT(*) FROM model_risk_governance.green_ai_best_practices),
        -- Fairness score from bias metrics
        (SELECT 100 - AVG(metric_value) * 100 FROM model_risk_governance.bias_metrics WHERE model_id = m.model_id),
        -- Transparency score from documentation
        CASE
            WHEN (SELECT COUNT(*) FROM model_risk_governance.model_documentation WHERE model_id = m.model_id) > 5 THEN 90
            WHEN (SELECT COUNT(*) FROM model_risk_governance.model_documentation WHERE model_id = m.model_id) > 2 THEN 70
            ELSE 30
        END,
        -- Accountability score from ownership and audit
        CASE
            WHEN (SELECT COUNT(*) FROM model_risk_governance.model_ownership WHERE model_id = m.model_id AND is_current = TRUE) > 0 AND
                 (SELECT COUNT(*) FROM model_risk_governance.model_audit_assignments WHERE model_id = m.model_id) > 0 THEN 85
            WHEN (SELECT COUNT(*) FROM model_risk_governance.model_ownership WHERE model_id = m.model_id AND is_current = TRUE) > 0 THEN 60
            ELSE 30
        END,
        -- Overall rating
        CASE
            WHEN ((SELECT COUNT(*) FROM model_risk_governance.model_sustainability_adoption WHERE model_id = m.model_id) * 10.0 /
                 (SELECT COUNT(*) FROM model_risk_governance.green_ai_best_practices)) > 80 AND
                 (SELECT 100 - AVG(metric_value) * 100 FROM model_risk_governance.bias_metrics WHERE model_id = m.model_id) > 85 THEN 'Excellent'
            WHEN ((SELECT COUNT(*) FROM model_risk_governance.model_sustainability_adoption WHERE model_id = m.model_id) * 10.0 /
                 (SELECT COUNT(*) FROM model_risk_governance.green_ai_best_practices)) > 60 AND
                 (SELECT 100 - AVG(metric_value) * 100 FROM model_risk_governance.bias_metrics WHERE model_id = m.model_id) > 70 THEN 'Good'
            ELSE 'Needs Improvement'
        END,
        CURRENT_DATE + INTERVAL '6 months'
    FROM
        model_risk_governance.models m
    CROSS JOIN
        (SELECT framework_id FROM model_risk_governance.ai_ethics_frameworks
         WHERE effective_date <= CURRENT_DATE
         ORDER BY effective_date DESC LIMIT 1) f
    WHERE
        m.is_active = TRUE;

    -- Log the execution
    INSERT INTO model_risk_governance.audit_logs (
        action_type, table_name, action_by, action_details
    ) VALUES (
        'PROCEDURE', 'model_ethics_assessments', 0, 'Executed automated AI ethics scoring procedure'
    );
END;
$$;

--model security posture dashbaord
CREATE OR REPLACE VIEW model_risk_governance.security_posture_dashboard AS
SELECT
    d.department_name,
    COUNT(DISTINCT m.model_id) AS total_models,
    COUNT(DISTINCT CASE WHEN cp.security_status = 'Quantum-Secure' THEN m.model_id END) AS fully_secured,
    COUNT(DISTINCT CASE WHEN cp.security_status = 'Partially Secure' THEN m.model_id END) AS partially_secured,
    COUNT(DISTINCT CASE WHEN cp.security_status = 'Vulnerable' THEN m.model_id END) AS vulnerable,
    COUNT(DISTINCT CASE WHEN cp.security_status = 'Unprotected' THEN m.model_id END) AS unprotected,
    COUNT(DISTINCT a.test_id) AS adversarial_tests,
    ROUND(COUNT(DISTINCT CASE WHEN cp.security_status = 'Quantum-Secure' THEN m.model_id END) * 100.0 /
          COUNT(DISTINCT m.model_id), 2) AS security_coverage_pct
FROM
    model_risk_governance.models m
JOIN
    core.departments d ON m.department_id = d.department_id
LEFT JOIN
    model_risk_governance.cryptographic_posture cp ON m.model_id = cp.model_id
LEFT JOIN
    model_risk_governance.adversarial_testing a ON m.model_id = a.model_id
WHERE
    m.is_active = TRUE
GROUP BY
    d.department_name;


--sustainability optimization views -- carbon footprint analysis view
CREATE OR REPLACE VIEW model_risk_governance.carbon_footprint_analysis AS
SELECT
    m.model_id,
    m.model_name,
    e.training_co2_kg,
    e.inference_co2_per_1000_req,
    (SELECT AVG(training_co2_kg) FROM model_risk_governance.model_environmental_impact
     WHERE model_id IN (SELECT model_id FROM model_risk_governance.models WHERE model_type_id = m.model_type_id)) AS avg_type_training_footprint,
    (SELECT AVG(inference_co2_per_1000_req) FROM model_risk_governance.model_environmental_impact
     WHERE model_id IN (SELECT model_id FROM model_risk_governance.models WHERE model_type_id = m.model_type_id)) AS avg_type_inference_footprint,
    COUNT(DISTINCT s.practice_id) AS implemented_practices,
    (SELECT STRING_AGG(g.practice_name, ', ')
     FROM model_risk_governance.green_ai_best_practices g
     WHERE g.practice_id NOT IN (SELECT practice_id FROM model_risk_governance.model_sustainability_adoption WHERE model_id = m.model_id)
    ) AS recommended_practices
FROM
    model_risk_governance.models m
LEFT JOIN
    model_risk_governance.model_environmental_impact e ON m.model_id = e.model_id
LEFT JOIN
    model_risk_governance.model_sustainability_adoption s ON m.model_id = s.model_id
WHERE
    m.is_active = TRUE AND
    e.measurement_date = (SELECT MAX(measurement_date) FROM model_risk_governance.model_environmental_impact WHERE model_id = m.model_id)
GROUP BY
    m.model_id, m.model_name, e.training_co2_kg, e.inference_co2_per_1000_req, m.model_type_id;


--green ai adoption roadmap view
CREATE OR REPLACE VIEW model_risk_governance.green_ai_roadmap AS
SELECT
    m.model_id,
    m.model_name,
    d.department_name,
    e.inference_co2_per_1000_req,
    (SELECT AVG(inference_co2_per_1000_req) FROM model_risk_governance.model_environmental_impact
     WHERE model_id IN (SELECT model_id FROM model_risk_governance.models WHERE model_type_id = m.model_type_id)) AS peer_average,
    COUNT(DISTINCT s.practice_id) AS implemented_practices,
    (SELECT COUNT(*) FROM model_risk_governance.green_ai_best_practices) - COUNT(DISTINCT s.practice_id) AS remaining_practices,
    (SELECT STRING_AGG(g.practice_name, ', ' ORDER BY g.expected_reduction_pct DESC)
     FROM model_risk_governance.green_ai_best_practices g
     WHERE g.practice_id NOT IN (SELECT practice_id FROM model_risk_governance.model_sustainability_adoption WHERE model_id = m.model_id)
    ) AS high_impact_recommendations,
    CASE
        WHEN e.inference_co2_per_1000_req < (SELECT AVG(inference_co2_per_1000_req) FROM model_risk_governance.model_environmental_impact
             WHERE model_id IN (SELECT model_id FROM model_risk_governance.models WHERE model_type_id = m.model_type_id)) THEN 'Leader'
        ELSE 'Opportunity'
    END AS improvement_status
FROM
    model_risk_governance.models m
JOIN
    core.departments d ON m.department_id = d.department_id
LEFT JOIN
    model_risk_governance.model_environmental_impact e ON m.model_id = e.model_id
LEFT JOIN
    model_risk_governance.model_sustainability_adoption s ON m.model_id = s.model_id
WHERE
    m.is_active = TRUE AND
    e.measurement_date = (SELECT MAX(measurement_date) FROM model_risk_governance.model_environmental_impact WHERE model_id = m.model_id)
GROUP BY
    m.model_id, m.model_name, d.department_name, e.inference_co2_per_1000_req, m.model_type_id;


--real-time anomaly detection view
CREATE OR REPLACE VIEW model_risk_governance.realtime_anomalies AS
SELECT
    m.model_id,
    m.model_name,
    a.detection_time,
    a.anomaly_type,
    a.severity,
    a.description,
    (SELECT STRING_AGG(config.metric_name, ', ')
     FROM model_risk_governance.monitoring_configurations config
     WHERE config.model_id = m.model_id AND config.is_active = TRUE
    ) AS monitored_metrics,
    CASE
        WHEN a.is_investigated THEN 'Resolved'
        WHEN a.severity = 'Critical' THEN 'Escalated'
        ELSE 'Pending'
    END AS investigation_status,
    EXTRACT(EPOCH FROM (CURRENT_TIMESTAMP - a.detection_time))/3600 AS hours_since_detection
FROM
    model_risk_governance.models m
JOIN
    model_risk_governance.anomaly_detection_logs a ON m.model_id = a.model_id
WHERE
    m.is_active = TRUE AND
    a.detection_time >= CURRENT_TIMESTAMP - INTERVAL '24 hours' AND
    (a.is_investigated = FALSE OR a.detection_time >= CURRENT_TIMESTAMP - INTERVAL '1 hour')
ORDER BY
    CASE a.severity
        WHEN 'Critical' THEN 1
        WHEN 'High' THEN 2
        WHEN 'Medium' THEN 3
        ELSE 4
    END,
    a.detection_time DESC;

--model health score view
CREATE OR REPLACE VIEW model_risk_governance.model_health_scores AS
SELECT
    m.model_id,
    m.model_name,
    -- Performance Health (30%)
    (SELECT
        CASE
            WHEN AVG(accuracy_score) > 0.9 AND COUNT(CASE WHEN is_alert_triggered THEN 1 END) = 0 THEN 100
            WHEN AVG(accuracy_score) > 0.8 AND COUNT(CASE WHEN is_alert_triggered THEN 1 END) < 3 THEN 80
            WHEN AVG(accuracy_score) > 0.7 THEN 60
            ELSE 30
        END
     FROM model_risk_governance.model_performance
     WHERE model_id = m.model_id AND monitoring_date >= CURRENT_DATE - INTERVAL '1 month') * 0.3 +
    -- Security Health (25%)
    (SELECT
        CASE
            WHEN COUNT(*) = 0 THEN 0
            WHEN COUNT(CASE WHEN quantum_safe = TRUE THEN 1 END) = COUNT(*) THEN 100
            WHEN COUNT(CASE WHEN quantum_safe = TRUE THEN 1 END) > 0 THEN 70
            ELSE 30
        END
     FROM model_risk_governance.cryptographic_controls
     WHERE model_id = m.model_id) * 0.25 +
    -- Compliance Health (20%)
    COALESCE((SELECT
        CASE
            WHEN overall_rating = 'Excellent' THEN 100
            WHEN overall_rating = 'Good' THEN 80
            WHEN overall_rating = 'Needs Improvement' THEN 40
            ELSE 20
        END
     FROM model_risk_governance.model_ethics_assessments
     WHERE model_id = m.model_id
     ORDER BY assessment_date DESC LIMIT 1), 0) * 0.2 +
    -- Sustainability Health (15%)
    (SELECT
        CASE
            WHEN inference_co2_per_1000_req < 0.1 THEN 100
            WHEN inference_co2_per_1000_req < 0.5 THEN 80
            WHEN inference_co2_per_1000_req < 1.0 THEN 60
            ELSE 30
        END
     FROM model_risk_governance.model_environmental_impact
     WHERE model_id = m.model_id
     ORDER BY measurement_date DESC LIMIT 1) * 0.15 +
    -- Operational Health (10%)
    (SELECT
        CASE
            WHEN COUNT(*) = 0 THEN 100
            WHEN COUNT(CASE WHEN status = 'Resolved' THEN 1 END) = COUNT(*) THEN 100
            WHEN COUNT(CASE WHEN status = 'Resolved' THEN 1 END) > COUNT(*)/2 THEN 70
            ELSE 30
        END
     FROM model_risk_governance.model_incidents
     WHERE model_id = m.model_id AND incident_date >= CURRENT_DATE - INTERVAL '3 months') * 0.1 AS health_score,
    CASE
        WHEN (SELECT COUNT(*) FROM model_risk_governance.performance_alerts pa
              JOIN model_risk_governance.model_performance mp ON pa.performance_id = mp.performance_id
              WHERE mp.model_id = m.model_id AND pa.alert_date >= CURRENT_DATE - INTERVAL '1 week') > 0 THEN 'Alert'
        ELSE 'Normal'
    END AS alert_status
FROM
    model_risk_governance.models m
WHERE
    m.is_active = TRUE;

--advanced refresh procedures
CREATE OR REPLACE PROCEDURE model_risk_governance.refresh_governance_views()
LANGUAGE plpgsql
AS $$
BEGIN
    -- Refresh core materialized views
    REFRESH MATERIALIZED VIEW CONCURRENTLY model_risk_governance.mv_model_risk_exposure_snapshot;
    REFRESH MATERIALIZED VIEW CONCURRENTLY model_risk_governance.mv_validation_backlog;
    REFRESH MATERIALIZED VIEW CONCURRENTLY model_risk_governance.mv_quantum_risk_exposure;
    REFRESH MATERIALIZED VIEW CONCURRENTLY model_risk_governance.mv_ethics_compliance;
    REFRESH MATERIALIZED VIEW CONCURRENTLY model_risk_governance.mv_sustainability_impact;

    -- Update refresh log
    INSERT INTO model_risk_governance.view_refresh_log (
        refresh_timestamp, views_refreshed, refresh_duration
    ) VALUES (
        CURRENT_TIMESTAMP,
        5,
        EXTRACT(EPOCH FROM (clock_timestamp() - CURRENT_TIMESTAMP))
    );

    -- Log the refresh
    INSERT INTO model_risk_governance.audit_logs (
        action_type, table_name, action_by, action_details
    ) VALUES (
        'MAINTENANCE', 'materialized_views', 0, 'Refreshed all governance materialized views'
    );
END;
$$;

--automated model health scoring


----------------------
--IT Governance
----------------------
CREATE SCHEMA it_governance;

CREATE TABLE core.business_units (
    unit_id SERIAL PRIMARY KEY,
    unit_name VARCHAR(100) NOT NULL,
    unit_description TEXT,
    parent_unit_id INTEGER REFERENCES core.business_units(unit_id),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE core.business_units IS 'Organizational business units for risk ownership and alignment';
COMMENT ON COLUMN core.business_units.parent_unit_id IS 'Hierarchical relationship between business units';

CREATE TABLE core.it_assets (
    asset_id SERIAL PRIMARY KEY,
    asset_name VARCHAR(100) NOT NULL,
    asset_type VARCHAR(50) NOT NULL,
    description TEXT,
    criticality_level VARCHAR(20) CHECK (criticality_level IN ('Low', 'Medium', 'High', 'Critical')),
    owner_id INTEGER REFERENCES core.business_units(unit_id),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE core.it_assets IS 'Inventory of IT assets that need protection and risk management';


CREATE TABLE core.vendors (
    vendor_id SERIAL PRIMARY KEY,
    vendor_name VARCHAR(100) NOT NULL,
    contact_email VARCHAR(100),
    contact_phone VARCHAR(20),
    industry_sector VARCHAR(50),
    risk_rating VARCHAR(20),
    last_assessment_date DATE,
    next_assessment_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE core.vendors IS 'Third-party vendors that may introduce IT risks to the organization';


CREATE TABLE it_governance.governance_frameworks (
    framework_id SERIAL PRIMARY KEY,
    framework_name VARCHAR(100) NOT NULL,
    framework_version VARCHAR(20),
    description TEXT,
    adoption_status VARCHAR(20) CHECK (adoption_status IN ('Planned', 'Partial', 'Full', 'Retired')),
    adoption_date DATE,
    compliance_target DECIMAL(5,2),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.governance_frameworks IS 'Standards and frameworks adopted for IT governance (COBIT, ISO 31000, etc.)';

CREATE TABLE it_governance.risk_appetite_statements (
    appetite_id SERIAL PRIMARY KEY,
    business_unit_id INTEGER REFERENCES core.business_units(unit_id),
    risk_category VARCHAR(50) NOT NULL,
    appetite_level VARCHAR(20) CHECK (appetite_level IN ('Avoid', 'Minimize', 'Accept', 'Pursue')),
    threshold_value DECIMAL(10,2),
    description TEXT,
    effective_date DATE NOT NULL,
    review_date DATE,
    approved_by VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.risk_appetite_statements IS 'Defined risk tolerance levels for different business units and risk categories';


CREATE TABLE it_governance.risk_register (
    risk_id SERIAL PRIMARY KEY,
    risk_name VARCHAR(100) NOT NULL,
    description TEXT,
    category VARCHAR(50) NOT NULL,
    owner_id INTEGER REFERENCES core.business_units(unit_id),
    likelihood VARCHAR(20) CHECK (likelihood IN ('Rare', 'Unlikely', 'Possible', 'Likely', 'Almost Certain')),
    impact VARCHAR(20) CHECK (impact IN ('Negligible', 'Minor', 'Moderate', 'Major', 'Catastrophic')),
    risk_score INTEGER GENERATED ALWAYS AS (
        CASE
            WHEN likelihood = 'Rare' AND impact = 'Negligible' THEN 1
            WHEN likelihood = 'Rare' AND impact = 'Minor' THEN 2
            -- Add all other combinations
            WHEN likelihood = 'Almost Certain' AND impact = 'Catastrophic' THEN 25
        END
    ) STORED,
    status VARCHAR(20) CHECK (status IN ('Open', 'Mitigated', 'Accepted', 'Transferred')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.risk_register IS 'Central repository for all identified IT risks';

CREATE TABLE it_governance.controls (
    control_id SERIAL PRIMARY KEY,
    control_name VARCHAR(100) NOT NULL,
    description TEXT,
    control_type VARCHAR(20) CHECK (control_type IN ('Preventive', 'Detective', 'Corrective', 'Compensating')),
    framework_id INTEGER REFERENCES it_governance.governance_frameworks(framework_id),
    implementation_status VARCHAR(20) CHECK (implementation_status IN ('Planned', 'Implemented', 'Testing', 'Operational')),
    effectiveness_rating INTEGER CHECK (effectiveness_rating BETWEEN 1 AND 5),
    owner_id INTEGER REFERENCES core.business_units(unit_id),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.controls IS 'IT controls implemented to mitigate risks';



CREATE TABLE it_governance.risk_control_mapping (
    mapping_id SERIAL PRIMARY KEY,
    risk_id INTEGER REFERENCES it_governance.risk_register(risk_id),
    control_id INTEGER REFERENCES it_governance.controls(control_id),
    coverage_percentage DECIMAL(5,2) CHECK (coverage_percentage BETWEEN 0 AND 100),
    effectiveness_notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (risk_id, control_id)
);

COMMENT ON TABLE it_governance.risk_control_mapping IS 'Mapping between risks and the controls that address them';

CREATE TABLE it_governance.vulnerabilities (
    vulnerability_id SERIAL PRIMARY KEY,
    cve_id VARCHAR(20),
    name VARCHAR(100) NOT NULL,
    description TEXT,
    severity VARCHAR(20) CHECK (severity IN ('Low', 'Medium', 'High', 'Critical')),
    cvss_score DECIMAL(3,1),
    affected_asset_id INTEGER REFERENCES core.it_assets(asset_id),
    detection_date TIMESTAMP WITH TIME ZONE,
    remediation_deadline TIMESTAMP WITH TIME ZONE,
    status VARCHAR(20) CHECK (status IN ('Open', 'In Progress', 'Remediated', 'Risk Accepted')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.vulnerabilities IS 'Identified vulnerabilities in IT systems';

CREATE TABLE it_governance.incidents (
    incident_id SERIAL PRIMARY KEY,
    title VARCHAR(100) NOT NULL,
    description TEXT,
    category VARCHAR(50) NOT NULL,
    severity VARCHAR(20) CHECK (severity IN ('Low', 'Medium', 'High', 'Critical')),
    detection_time TIMESTAMP WITH TIME ZONE,
    response_time TIMESTAMP WITH TIME ZONE,
    resolution_time TIMESTAMP WITH TIME ZONE,
    status VARCHAR(20) CHECK (status IN ('Open', 'Investigating', 'Contained', 'Resolved', 'Closed')),
    root_cause TEXT,
    lessons_learned TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.incidents IS 'Recorded IT security and operational incidents';


CREATE TABLE it_governance.kpis_krms (
    metric_id SERIAL PRIMARY KEY,
    metric_name VARCHAR(100) NOT NULL,
    metric_type VARCHAR(10) CHECK (metric_type IN ('KPI', 'KRM')),
    category VARCHAR(50) NOT NULL,
    description TEXT,
    target_value DECIMAL(10,2),
    target_operator VARCHAR(2) CHECK (target_operator IN ('<', '<=', '=', '>=', '>')),
    measurement_unit VARCHAR(20),
    frequency VARCHAR(20) CHECK (frequency IN ('Daily', 'Weekly', 'Monthly', 'Quarterly', 'Annually')),
    owner_id INTEGER REFERENCES core.business_units(unit_id),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.kpis_krms IS 'Key Performance Indicators and Key Risk Metrics for IT governance';


CREATE TABLE it_governance.metric_measurements (
    measurement_id SERIAL PRIMARY KEY,
    metric_id INTEGER REFERENCES it_governance.kpis_krms(metric_id),
    measurement_date DATE NOT NULL,
    measured_value DECIMAL(10,2) NOT NULL,
    data_source VARCHAR(100),
    notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.metric_measurements IS 'Historical measurements of KPIs and KRMs';



CREATE TABLE it_governance.compliance_requirements (
    requirement_id SERIAL PRIMARY KEY,
    regulation_name VARCHAR(100) NOT NULL,
    requirement_code VARCHAR(50),
    description TEXT,
    category VARCHAR(50),
    applicable_to VARCHAR(100),
    implementation_status VARCHAR(20) CHECK (implementation_status IN ('Not Started', 'In Progress', 'Implemented', 'Verified')),
    verification_date DATE,
    next_review_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.compliance_requirements IS 'Regulatory and standards compliance requirements';


CREATE TABLE it_governance.control_tests (
    test_id SERIAL PRIMARY KEY,
    control_id INTEGER REFERENCES it_governance.controls(control_id),
    test_date DATE NOT NULL,
    test_method VARCHAR(50),
    test_result VARCHAR(20) CHECK (test_result IN ('Pass', 'Fail', 'Partial', 'N/A')),
    findings TEXT,
    remediation_plan TEXT,
    retest_date DATE,
    tester VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.control_tests IS 'Testing of control effectiveness';


CREATE TABLE it_governance.business_continuity_plans (
    plan_id SERIAL PRIMARY KEY,
    plan_name VARCHAR(100) NOT NULL,
    business_unit_id INTEGER REFERENCES core.business_units(unit_id),
    description TEXT,
    rto_hours DECIMAL(5,2),
    rpo_hours DECIMAL(5,2),
    last_test_date DATE,
    next_test_date DATE,
    test_results TEXT,
    approval_status VARCHAR(20) CHECK (approval_status IN ('Draft', 'Pending', 'Approved', 'Retired')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.business_continuity_plans IS 'Business continuity and disaster recovery plans';


CREATE OR REPLACE VIEW it_governance.risk_assessment_report AS
SELECT
    rr.risk_id,
    rr.risk_name,
    rr.category,
    rr.likelihood,
    rr.impact,
    rr.risk_score,
    bu.unit_name AS risk_owner,
    COUNT(rc.control_id) AS control_count,
    AVG(c.effectiveness_rating) AS avg_control_effectiveness,
    rr.status
FROM
    it_governance.risk_register rr
LEFT JOIN
    core.business_units bu ON rr.owner_id = bu.unit_id
LEFT JOIN
    it_governance.risk_control_mapping rc ON rr.risk_id = rc.risk_id
LEFT JOIN
    it_governance.controls c ON rc.control_id = c.control_id
GROUP BY
    rr.risk_id, rr.risk_name, rr.category, rr.likelihood, rr.impact, rr.risk_score, bu.unit_name, rr.status;

COMMENT ON VIEW it_governance.risk_assessment_report IS 'Comprehensive view of all risks with their controls and effectiveness';


CREATE OR REPLACE VIEW it_governance.compliance_tracking AS
SELECT
    cr.requirement_id,
    cr.regulation_name,
    cr.requirement_code,
    cr.category,
    cr.applicable_to,
    cr.implementation_status,
    COUNT(ct.test_id) FILTER (WHERE ct.test_result = 'Pass') AS passed_tests,
    COUNT(ct.test_id) FILTER (WHERE ct.test_result = 'Fail') AS failed_tests,
    cr.verification_date,
    cr.next_review_date
FROM
    it_governance.compliance_requirements cr
LEFT JOIN
    it_governance.controls c ON c.framework_id = cr.requirement_id
LEFT JOIN
    it_governance.control_tests ct ON c.control_id = ct.control_id
GROUP BY
    cr.requirement_id, cr.regulation_name, cr.requirement_code, cr.category,
    cr.applicable_to, cr.implementation_status, cr.verification_date, cr.next_review_date;

COMMENT ON VIEW it_governance.compliance_tracking IS 'Tracking of compliance requirements and associated control testing results';



CREATE OR REPLACE VIEW it_governance.vendor_risk_dashboard AS
SELECT
    v.vendor_id,
    v.vendor_name,
    v.industry_sector,
    v.risk_rating,
    v.last_assessment_date,
    v.next_assessment_date,
    COUNT(r.risk_id) AS associated_risks,
    COUNT(i.incident_id) FILTER (WHERE i.severity = 'High' OR i.severity = 'Critical') AS critical_incidents,
    MAX(i.detection_time) AS last_incident_date
FROM
    core.vendors v
LEFT JOIN
    it_governance.risk_register r ON r.description LIKE '%' || v.vendor_name || '%'
LEFT JOIN
    it_governance.incidents i ON i.description LIKE '%' || v.vendor_name || '%'
GROUP BY
    v.vendor_id, v.vendor_name, v.industry_sector, v.risk_rating,
    v.last_assessment_date, v.next_assessment_date;

COMMENT ON VIEW it_governance.vendor_risk_dashboard IS 'Vendor risk assessment and incident tracking dashboard';



CREATE OR REPLACE VIEW it_governance.kpi_performance_trends AS
SELECT
    k.metric_id,
    k.metric_name,
    k.metric_type,
    k.category,
    k.target_value,
    k.target_operator,
    m.measurement_date,
    m.measured_value,
    CASE
        WHEN k.target_operator = '<' AND m.measured_value < k.target_value THEN 'Met'
        WHEN k.target_operator = '<=' AND m.measured_value <= k.target_value THEN 'Met'
        WHEN k.target_operator = '=' AND m.measured_value = k.target_value THEN 'Met'
        WHEN k.target_operator = '>=' AND m.measured_value >= k.target_value THEN 'Met'
        WHEN k.target_operator = '>' AND m.measured_value > k.target_value THEN 'Met'
        ELSE 'Not Met'
    END AS target_status,
    bu.unit_name AS owner
FROM
    it_governance.kpis_krms k
JOIN
    it_governance.metric_measurements m ON k.metric_id = m.metric_id
LEFT JOIN
    core.business_units bu ON k.owner_id = bu.unit_id
ORDER BY
    k.metric_id, m.measurement_date DESC;

COMMENT ON VIEW it_governance.kpi_performance_trends IS 'Performance trends for KPIs and KRMs with target status';



CREATE MATERIALIZED VIEW it_governance.current_risk_exposure AS
SELECT
    rr.category,
    COUNT(rr.risk_id) AS risk_count,
    AVG(rr.risk_score) AS avg_risk_score,
    SUM(CASE WHEN rr.risk_score >= 15 THEN 1 ELSE 0 END) AS high_risk_count,
    SUM(CASE WHEN rr.risk_score BETWEEN 8 AND 14 THEN 1 ELSE 0 END) AS medium_risk_count,
    SUM(CASE WHEN rr.risk_score < 8 THEN 1 ELSE 0 END) AS low_risk_count
FROM
    it_governance.risk_register rr
WHERE
    rr.status = 'Open'
GROUP BY
    rr.category
WITH DATA;

COMMENT ON MATERIALIZED VIEW it_governance.current_risk_exposure IS 'Aggregated view of current risk exposure by category';



CREATE MATERIALIZED VIEW it_governance.control_effectiveness_summary AS
SELECT
    c.control_id,
    c.control_name,
    c.control_type,
    c.effectiveness_rating,
    COUNT(ct.test_id) AS test_count,
    COUNT(ct.test_id) FILTER (WHERE ct.test_result = 'Pass') AS passed_tests,
    COUNT(ct.test_id) FILTER (WHERE ct.test_result = 'Fail') AS failed_tests,
    MAX(ct.test_date) AS last_test_date
FROM
    it_governance.controls c
LEFT JOIN
    it_governance.control_tests ct ON c.control_id = ct.control_id
GROUP BY
    c.control_id, c.control_name, c.control_type, c.effectiveness_rating
WITH DATA;

COMMENT ON MATERIALIZED VIEW it_governance.control_effectiveness_summary IS 'Summary of control effectiveness based on testing results';



CREATE MATERIALIZED VIEW it_governance.vulnerability_management_status AS
SELECT
    v.severity,
    COUNT(v.vulnerability_id) AS total_vulnerabilities,
    COUNT(v.vulnerability_id) FILTER (WHERE v.status = 'Open') AS open_vulnerabilities,
    COUNT(v.vulnerability_id) FILTER (WHERE v.status = 'In Progress') AS in_progress_vulnerabilities,
    COUNT(v.vulnerability_id) FILTER (WHERE v.status = 'Remediated') AS remediated_vulnerabilities,
    AVG(EXTRACT(EPOCH FROM (v.remediation_deadline - v.detection_date))/86400 AS avg_days_to_remediate,
    MAX(EXTRACT(EPOCH FROM (v.remediation_deadline - v.detection_date))/86400 AS max_days_to_remediate
FROM
    it_governance.vulnerabilities v
GROUP BY
    v.severity
WITH DATA;

COMMENT ON MATERIALIZED VIEW it_governance.vulnerability_management_status IS 'Current status of vulnerability management by severity level';



CREATE OR REPLACE PROCEDURE it_governance.update_risk_assessment(
    p_risk_id INTEGER,
    p_likelihood VARCHAR(20),
    p_impact VARCHAR(20),
    p_status VARCHAR(20),
    p_owner_id INTEGER
)
LANGUAGE plpgsql
AS $$
BEGIN
    UPDATE it_governance.risk_register
    SET
        likelihood = p_likelihood,
        impact = p_impact,
        status = p_status,
        owner_id = p_owner_id,
        updated_at = CURRENT_TIMESTAMP
    WHERE
        risk_id = p_risk_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.update_risk_assessment IS 'Updates risk assessment details including likelihood, impact, and status';


CREATE OR REPLACE PROCEDURE it_governance.add_vendor_assessment(
    p_vendor_id INTEGER,
    p_risk_rating VARCHAR(20),
    p_assessment_date DATE,
    p_next_assessment_date DATE,
    p_notes TEXT
)
LANGUAGE plpgsql
AS $$
BEGIN
    UPDATE core.vendors
    SET
        risk_rating = p_risk_rating,
        last_assessment_date = p_assessment_date,
        next_assessment_date = p_next_assessment_date,
        updated_at = CURRENT_TIMESTAMP
    WHERE
        vendor_id = p_vendor_id;

    -- Log the assessment details
    INSERT INTO it_governance.vendor_assessments (vendor_id, assessment_date, risk_rating, notes)
    VALUES (p_vendor_id, p_assessment_date, p_risk_rating, p_notes);

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.add_vendor_assessment IS 'Records a vendor risk assessment and updates vendor risk rating';


CREATE OR REPLACE PROCEDURE it_governance.calculate_risk_metrics(
    p_metric_date DATE DEFAULT CURRENT_DATE
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_risk_exposure DECIMAL(10,2);
    v_vulnerability_remediation_time DECIMAL(10,2);
    v_incident_response_time DECIMAL(10,2);
    v_control_effectiveness DECIMAL(10,2);
BEGIN
    -- Calculate IT Risk Exposure (percentage of high risks)
    SELECT
        (COUNT(*) FILTER (WHERE risk_score >= 15) * 100.0 / NULLIF(COUNT(*), 0))
    INTO
        v_risk_exposure
    FROM
        it_governance.risk_register
    WHERE
        status = 'Open';

    -- Calculate Vulnerability Remediation Time (average days for critical vulnerabilities)
    SELECT
        AVG(EXTRACT(EPOCH FROM (resolution_time - detection_time))/86400)
    INTO
        v_vulnerability_remediation_time
    FROM
        it_governance.vulnerabilities
    WHERE
        severity = 'Critical' AND status = 'Remediated'
        AND resolution_time >= (p_metric_date - INTERVAL '30 days');

    -- Calculate IT Incident Response Time (average hours for critical incidents)
    SELECT
        AVG(EXTRACT(EPOCH FROM (response_time - detection_time))/3600)
    INTO
        v_incident_response_time
    FROM
        it_governance.incidents
    WHERE
        severity IN ('High', 'Critical') AND status = 'Resolved'
        AND detection_time >= (p_metric_date - INTERVAL '30 days');

    -- Calculate IT Control Effectiveness (percentage of effective controls)
    SELECT
        (COUNT(*) FILTER (WHERE effectiveness_rating >= 4) * 100.0 / NULLIF(COUNT(*), 0))
    INTO
        v_control_effectiveness
    FROM
        it_governance.controls
    WHERE
        implementation_status = 'Operational';

    -- Insert or update metric measurements
    INSERT INTO it_governance.metric_measurements (metric_id, measurement_date, measured_value)
    VALUES
        ((SELECT metric_id FROM it_governance.kpis_krms WHERE metric_name = 'IT Risk Exposure'), p_metric_date, v_risk_exposure),
        ((SELECT metric_id FROM it_governance.kpis_krms WHERE metric_name = 'Vulnerability Remediation Time'), p_metric_date, v_vulnerability_remediation_time),
        ((SELECT metric_id FROM it_governance.kpis_krms WHERE metric_name = 'IT Incident Response Time'), p_metric_date, v_incident_response_time),
        ((SELECT metric_id FROM it_governance.kpis_krms WHERE metric_name = 'IT Control Effectiveness'), p_metric_date, v_control_effectiveness)
    ON CONFLICT (metric_id, measurement_date)
    DO UPDATE SET measured_value = EXCLUDED.measured_value, updated_at = CURRENT_TIMESTAMP;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.calculate_risk_metrics IS 'Calculates and stores key risk metrics for IT governance reporting';


CREATE OR REPLACE PROCEDURE it_governance.refresh_materialized_views()
LANGUAGE plpgsql
AS $$
BEGIN
    REFRESH MATERIALIZED VIEW it_governance.current_risk_exposure;
    REFRESH MATERIALIZED VIEW it_governance.control_effectiveness_summary;
    REFRESH MATERIALIZED VIEW it_governance.vulnerability_management_status;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.refresh_materialized_views IS 'Refreshes all materialized views in the IT governance schema';


--select statements
-- to provide a comprehensive view of all risks with their controls and effectiveness

SELECT * FROM it_governance.risk_assessment_report WHERE risk_score >= 15 ORDER BY risk_score DESC;


-- to show compliance status with regualtory requiremnts
-- compliance tracking
SELECT * FROM it_governance.compliance_tracking WHERE implementation_status != 'Verified';


--- vendor risk dashbaord
-- to display vendor risk assessments and related incidents
SELECT * FROM it_governance.vendor_risk_dashboard WHERE risk_rating = 'High' ORDER BY last_incident_date DESC;


-- KPI performance trends -- tracks peformance against targets for governace metrics
SELECT * FROM it_governance.kpi_performance_trends WHERE target_status = 'Not Met' AND measurement_date >= CURRENT_DATE - INTERVAL '30 days';


-- to capture the effectiveness of contorls -- control effectiveness summary
-- showing the testing resutls for controls

SELECT * FROM it_governance.control_effectiveness_summary WHERE failed_tests > 0 ORDER BY last_test_date;

-- vulnerability status check for remediation
-- to display current status of vulnerability remediation
SELECT * FROM it_governance.vulnerability_management_status WHERE severity = 'Critical';



--audit logging system
-- Business Case: Provides regulatory compliance (SOX, GDPR) by maintaining immutable record of all changes to governance data.
-- Enables forensic investigation of security incidents and supports non-repudiation requirements.


CREATE TABLE it_governance.audit_logs (
    log_id BIGSERIAL PRIMARY KEY,
    event_time TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT CURRENT_TIMESTAMP,
    user_id VARCHAR(100) NOT NULL,
    action_type VARCHAR(50) NOT NULL,
    table_affected VARCHAR(100) NOT NULL,
    record_id INTEGER,
    old_values JSONB,
    new_values JSONB,
    client_ip VARCHAR(50),
    application_name VARCHAR(100)
);

COMMENT ON TABLE it_governance.audit_logs IS 'Comprehensive audit trail for all changes to governance data';
COMMENT ON COLUMN it_governance.audit_logs.old_values IS 'JSON snapshot of record before change';
COMMENT ON COLUMN it_governance.audit_logs.new_values IS 'JSON snapshot of record after change';

-- Business Case: Ensures consistent implementation of security controls across the organization.
-- Provides single source of truth for auditors and employees regarding IT policies.
CREATE TABLE it_governance.policies (
    policy_id SERIAL PRIMARY KEY,
    policy_name VARCHAR(200) NOT NULL,
    policy_type VARCHAR(50) NOT NULL,
    version VARCHAR(20) NOT NULL,
    effective_date DATE NOT NULL,
    review_date DATE,
    approval_status VARCHAR(20) NOT NULL CHECK (approval_status IN ('Draft', 'Under Review', 'Approved', 'Retired')),
    owner_id INTEGER REFERENCES core.business_units(unit_id),
    document_link TEXT NOT NULL,
    applies_to TEXT,
    compliance_requirements TEXT[],
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.policies IS 'Central repository for IT policies and standards';
COMMENT ON COLUMN it_governance.policies.compliance_requirements IS 'Array of regulatory requirements this policy addresses';

--exception management
-- Business Case: Formalizes process for temporary deviations from security standards with proper risk assessment.
-- Prevents shadow IT by requiring documented exceptions for non-compliant systems.
CREATE TABLE it_governance.control_eceptions (
    exception_id SERIAL PRIMARY KEY,
    control_id INTEGER NOT NULL REFERENCES it_governance.controls(control_id),
    requester_id INTEGER NOT NULL,
    approval_status VARCHAR(20) NOT NULL CHECK (approval_status IN ('Requested', 'Approved', 'Rejected', 'Expired')),
    risk_assessment TEXT NOT NULL,
    compensating_controls TEXT,
    start_date DATE NOT NULL,
    end_date DATE NOT NULL,
    approver_id INTEGER,
    approval_date TIMESTAMP WITH TIME ZONE,
    justification TEXT NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.control_exceptions IS 'Temporary exceptions to control requirements with approval workflow';
COMMENT ON COLUMN it_governance.control_exceptions.end_date IS 'Automatic expiration date for the exception';


--training and awareness

-- Business Case: Reduces human risk factor by ensuring employees receive regular security awareness training.
-- Provides evidence for compliance audits (ISO 27001, PCI DSS) that require security training programs.
CREATE TABLE it_governance.security_training (
    training_id SERIAL PRIMARY KEY,
    training_name VARCHAR(200) NOT NULL,
    training_type VARCHAR(50) NOT NULL,
    target_audience VARCHAR(100)[] NOT NULL,
    frequency_months INTEGER NOT NULL,
    completion_deadline DATE,
    training_material_link TEXT,
    required BOOLEAN NOT NULL DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE it_governance.training_completions (
    completion_id SERIAL PRIMARY KEY,
    training_id INTEGER NOT NULL REFERENCES it_governance.security_training(training_id),
    user_id INTEGER NOT NULL,
    completion_date DATE NOT NULL,
    score DECIMAL(5,2),
    expiration_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.security_training IS 'Catalog of security awareness training programs';
COMMENT ON TABLE it_governance.training_completions IS 'Records of employee training completions';

-- technology risk assessment

-- Business Case: Ensures proper risk evaluation before adopting new technologies.
-- Prevents shadow IT by requiring formal assessment of all technology solutions.
CREATE TABLE it_governance.technology_risk_assessments (
    assessment_id SERIAL PRIMARY KEY,
    technology_name VARCHAR(100) NOT NULL,
    technology_type VARCHAR(50) NOT NULL,
    business_impact TEXT NOT NULL,
    data_classification VARCHAR(20) NOT NULL CHECK (data_classification IN ('Public', 'Internal', 'Confidential', 'Restricted')),
    risk_rating VARCHAR(20) NOT NULL,
    assessment_date DATE NOT NULL,
    assessor_id INTEGER NOT NULL,
    next_assessment_date DATE NOT NULL,
    risk_mitigation_plan TEXT,
    approved_by INTEGER,
    approval_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.technology_risk_assessments IS 'Risk assessments for new technologies before adoption';

--policy compliance status
-- Business Value: Provides executives with clear visibility into policy adherence across the organization.
-- Enables targeted remediation efforts for non-compliant policies.
CREATE OR REPLACE VIEW it_governance.policy_compliance_status AS
SELECT
    p.policy_id,
    p.policy_name,
    p.policy_type,
    p.version,
    p.effective_date,
    p.review_date,
    p.approval_status,
    bu.unit_name AS policy_owner,
    COUNT(c.control_id) AS supporting_controls,
    COUNT(ct.test_id) FILTER (WHERE ct.test_result = 'Pass') AS passed_control_tests,
    COUNT(ct.test_id) FILTER (WHERE ct.test_result = 'Fail') AS failed_control_tests,
    CASE
        WHEN COUNT(ct.test_id) = 0 THEN 'Untested'
        WHEN COUNT(ct.test_id) FILTER (WHERE ct.test_result = 'Fail') > 0 THEN 'Non-Compliant'
        ELSE 'Compliant'
    END AS compliance_status
FROM
    it_governance.policies p
LEFT JOIN
    core.business_units bu ON p.owner_id = bu.unit_id
LEFT JOIN
    it_governance.controls c ON c.description LIKE '%' || p.policy_name || '%'
LEFT JOIN
    it_governance.control_tests ct ON c.control_id = ct.control_id
GROUP BY
    p.policy_id, p.policy_name, p.policy_type, p.version, p.effective_date,
    p.review_date, p.approval_status, bu.unit_name;

COMMENT ON VIEW it_governance.policy_compliance_status IS 'Shows compliance status for each policy based on control testing results';




-- Business Value: Guides risk management decisions with clear prioritization and recommended actions.
-- Supports resource allocation to highest risk areas.
CREATE OR REPLACE VIEW it_governance.risk_treatment_plan AS
SELECT
    rr.risk_id,
    rr.risk_name,
    rr.category,
    rr.risk_score,
    bu.unit_name AS risk_owner,
    CASE
        WHEN rr.risk_score >= 15 THEN 'Immediate Action'
        WHEN rr.risk_score BETWEEN 8 AND 14 THEN 'High Priority'
        WHEN rr.risk_score BETWEEN 4 AND 7 THEN 'Medium Priority'
        ELSE 'Low Priority'
    END AS treatment_priority,
    COALESCE(COUNT(rc.control_id), 0) AS existing_controls,
    CASE
        WHEN rr.risk_score >= 15 AND COALESCE(COUNT(rc.control_id), 0) = 0 THEN 'Implement Additional Controls'
        WHEN rr.risk_score >= 15 AND COALESCE(COUNT(rc.control_id), 0) > 0 THEN 'Enhance Existing Controls'
        WHEN rr.risk_score BETWEEN 8 AND 14 THEN 'Monitor and Review Quarterly'
        ELSE 'Accept Risk'
    END AS recommended_action,
    MAX(ct.test_date) AS last_control_test_date
FROM
    it_governance.risk_register rr
LEFT JOIN
    core.business_units bu ON rr.owner_id = bu.unit_id
LEFT JOIN
    it_governance.risk_control_mapping rc ON rr.risk_id = rc.risk_id
LEFT JOIN
    it_governance.controls c ON rc.control_id = c.control_id
LEFT JOIN
    it_governance.control_tests ct ON c.control_id = ct.control_id
WHERE
    rr.status = 'Open'
GROUP BY
    rr.risk_id, rr.risk_name, rr.category, rr.risk_score, bu.unit_name;

COMMENT ON VIEW it_governance.risk_treatment_plan IS 'Provides actionable risk treatment recommendations based on risk scoring';


-- Business Value: Demonstrates compliance with training requirements to regulators and auditors.
-- Identifies gaps in security awareness training coverage.
CREATE OR REPLACE VIEW it_governance.training_compliance_dashboard AS
WITH employee_counts AS (
    SELECT
        COUNT(*) AS total_employees
    FROM
        hr.employees
    WHERE
        active = TRUE
)
SELECT
    st.training_id,
    st.training_name,
    st.training_type,
    st.target_audience,
    st.completion_deadline,
    COUNT(tc.completion_id) AS employees_completed,
    ec.total_employees AS total_required,
    ROUND(COUNT(tc.completion_id) * 100.0 / NULLIF(ec.total_employees, 0), 2) AS completion_percentage,
    MIN(tc.completion_date) AS first_completion,
    MAX(tc.completion_date) AS last_completion
FROM
    it_governance.security_training st
CROSS JOIN
    employee_counts ec
LEFT JOIN
    it_governance.training_completions tc ON st.training_id = tc.training_id
WHERE
    st.required = TRUE
    AND (tc.expiration_date IS NULL OR tc.expiration_date > CURRENT_DATE)
GROUP BY
    st.training_id, st.training_name, st.training_type, st.target_audience,
    st.completion_deadline, ec.total_employees;

COMMENT ON VIEW it_governance.training_compliance_dashboard IS 'Tracks completion rates for mandatory security awareness training';


-- control exception reporting
-- enhanced materialized views and docuemntation
-- Business Value: Provides visibility into control deviations that may increase organizational risk.
-- Ensures exceptions are properly tracked and don't become permanent insecure practices.
CREATE MATERIALIZED VIEW it_governance.control_exception_reporting AS
SELECT
    c.control_id,
    c.control_name,
    c.control_type,
    gf.framework_name,
    COUNT(ce.exception_id) FILTER (WHERE ce.approval_status = 'Approved' AND ce.end_date >= CURRENT_DATE) AS active_exceptions,
    COUNT(ce.exception_id) FILTER (WHERE ce.approval_status = 'Approved' AND ce.end_date < CURRENT_DATE) AS expired_exceptions,
    COUNT(ce.exception_id) FILTER (WHERE ce.approval_status = 'Requested') AS pending_exceptions,
    MIN(ce.end_date) FILTER (WHERE ce.approval_status = 'Approved' AND ce.end_date >= CURRENT_DATE) AS next_expiration,
    MAX(ce.end_date) FILTER (WHERE ce.approval_status = 'Approved' AND ce.end_date >= CURRENT_DATE) AS farthest_expiration
FROM
    it_governance.controls c
LEFT JOIN
    it_governance.governance_frameworks gf ON c.framework_id = gf.framework_id
LEFT JOIN
    it_governance.control_exceptions ce ON c.control_id = ce.control_id
GROUP BY
    c.control_id, c.control_name, c.control_type, gf.framework_name
WITH DATA;

COMMENT ON MATERIALIZED VIEW it_governance.control_exception_reporting IS 'Tracks all exceptions to control requirements with status and expiration';


-- Business Value: Demonstrates compliance posture to regulators and senior management.
-- Identifies gaps in regulatory coverage that could lead to fines or penalties.
CREATE MATERIALIZED VIEW it_governance.regulatory_requirement_coverage AS
SELECT
    cr.requirement_id,
    cr.regulation_name,
    cr.requirement_code,
    COUNT(DISTINCT p.policy_id) AS covering_policies,
    COUNT(DISTINCT c.control_id) AS implementing_controls,
    COUNT(ct.test_id) FILTER (WHERE ct.test_result = 'Pass') AS passed_tests,
    COUNT(ct.test_id) FILTER (WHERE ct.test_result = 'Fail') AS failed_tests,
    CASE
        WHEN COUNT(ct.test_id) = 0 THEN 'Untested'
        WHEN COUNT(ct.test_id) FILTER (WHERE ct.test_result = 'Fail') > 0 THEN 'Non-Compliant'
        ELSE 'Compliant'
    END AS compliance_status
FROM
    it_governance.compliance_requirements cr
LEFT JOIN
    it_governance.policies p ON cr.requirement_id = ANY(p.compliance_requirements)
LEFT JOIN
    it_governance.controls c ON c.description LIKE '%' || cr.requirement_code || '%'
LEFT JOIN
    it_governance.control_tests ct ON c.control_id = ct.control_id
GROUP BY
    cr.requirement_id, cr.regulation_name, cr.requirement_code
WITH DATA;

COMMENT ON MATERIALIZED VIEW it_governance.regulatory_requirement_coverage IS 'Shows how each regulatory requirement is addressed by policies and controls';


--risk acceptance workflow --stored procedure
-- Business Case: Ensures risks are only accepted after proper review and documentation.
-- Creates audit trail for risk acceptance decisions.
CREATE OR REPLACE PROCEDURE it_governance.accept_risk(
    p_risk_id INTEGER,
    p_accepted_by INTEGER,
    p_acceptance_reason TEXT,
    p_acceptance_period_months INTEGER DEFAULT 12,
    p_compensating_controls TEXT DEFAULT NULL
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_expiry_date DATE;
BEGIN
    -- Calculate expiry date based on acceptance period
    v_expiry_date := CURRENT_DATE + (p_acceptance_period_months * INTERVAL '1 month');

    -- Update risk status
    UPDATE it_governance.risk_register
    SET
        status = 'Accepted',
        updated_at = CURRENT_TIMESTAMP
    WHERE
        risk_id = p_risk_id;

    -- Record risk acceptance
    INSERT INTO it_governance.risk_acceptances (
        risk_id,
        accepted_by,
        acceptance_date,
        expiry_date,
        acceptance_reason,
        compensating_controls
    ) VALUES (
        p_risk_id,
        p_accepted_by,
        CURRENT_DATE,
        v_expiry_date,
        p_acceptance_reason,
        p_compensating_controls
    );

    -- Log the acceptance
    INSERT INTO it_governance.audit_logs (
        user_id,
        action_type,
        table_affected,
        record_id,
        new_values
    ) VALUES (
        p_accepted_by::TEXT,
        'RISK_ACCEPTANCE',
        'risk_register',
        p_risk_id,
        jsonb_build_object(
            'status', 'Accepted',
            'acceptance_reason', p_acceptance_reason,
            'expiry_date', v_expiry_date
        )
    );

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.accept_risk IS 'Formalizes the acceptance of a risk with proper documentation and approval';


--policy exception request
-- Business Case: Provides structured process for business units to request deviations from policies.
-- Ensures proper risk assessment and compensating controls are considered.
CREATE OR REPLACE PROCEDURE it_governance.request_policy_exception(
    p_policy_id INTEGER,
    p_requester_id INTEGER,
    p_business_justification TEXT,
    p_compensating_controls TEXT,
    p_requested_duration_months INTEGER,
    p_risk_assessment TEXT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_exception_id INTEGER;
BEGIN
    -- Create the exception record
    INSERT INTO it_governance.policy_exceptions (
        policy_id,
        requester_id,
        approval_status,
        business_justification,
        compensating_controls,
        start_date,
        end_date,
        risk_assessment
    ) VALUES (
        p_policy_id,
        p_requester_id,
        'Requested',
        p_business_justification,
        p_compensating_controls,
        CURRENT_DATE,
        CURRENT_DATE + (p_requested_duration_months * INTERVAL '1 month'),
        p_risk_assessment
    ) RETURNING exception_id INTO v_exception_id;

    -- Notify approvers (implementation depends on notification system)
    -- This would typically call a notification service or insert into a workflow queue

    -- Log the request
    INSERT INTO it_governance.audit_logs (
        user_id,
        action_type,
        table_affected,
        record_id,
        new_values
    ) VALUES (
        p_requester_id::TEXT,
        'POLICY_EXCEPTION_REQUEST',
        'policy_exceptions',
        v_exception_id,
        jsonb_build_object(
            'policy_id', p_policy_id,
            'status', 'Requested',
            'requested_duration', p_requested_duration_months || ' months'
        )
    );

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.request_policy_exception IS 'Initiates a request for temporary exception to a policy requirement';

--automated control testing
-- Business Case: Reduces manual effort in control testing while increasing frequency.
-- Provides consistent, repeatable testing of critical controls.
CREATE OR REPLACE PROCEDURE it_governance.automated_control_testing(
    p_control_id INTEGER,
    p_test_type VARCHAR(50),
    p_test_parameters JSONB
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_test_result VARCHAR(20);
    v_findings TEXT;
    v_test_output JSONB;
BEGIN
    -- This would integrate with actual control testing automation
    -- For example:
    -- 1. Call API of testing tool
    -- 2. Run SQL query to verify configuration
    -- 3. Check log files for evidence

    -- Mock implementation for illustration:
    IF p_test_type = 'ACCESS_REVIEW' THEN
        -- Simulate checking if only authorized users have access
        SELECT
            CASE
                WHEN COUNT(*) = 0 THEN 'Pass'
                ELSE 'Fail'
            END,
            CASE
                WHEN COUNT(*) = 0 THEN 'No unauthorized access found'
                ELSE COUNT(*) || ' unauthorized access entries found'
            END,
            jsonb_build_object('users_checked', COUNT(*))
        INTO
            v_test_result, v_findings, v_test_output
        FROM
            access_logs
        WHERE
            access_time > CURRENT_DATE - INTERVAL '90 days'
            AND user_id NOT IN (SELECT user_id FROM authorized_users);

    ELSIF p_test_type = 'CONFIGURATION_CHECK' THEN
        -- Simulate checking system configuration against baseline
        SELECT
            'Pass',
            'All configurations match baseline',
            jsonb_build_object('config_items_checked', 42)
        INTO
            v_test_result, v_findings, v_test_output;
    END IF;

    -- Record test results
    INSERT INTO it_governance.control_tests (
        control_id,
        test_date,
        test_method,
        test_result,
        findings,
        test_output,
        tester
    ) VALUES (
        p_control_id,
        CURRENT_DATE,
        p_test_type,
        v_test_result,
        v_findings,
        v_test_output,
        'Automated Test'
    );

    -- Update control effectiveness rating if needed
    IF v_test_result = 'Fail' THEN
        UPDATE it_governance.controls
        SET
            effectiveness_rating = GREATEST(1, effectiveness_rating - 1),
            updated_at = CURRENT_TIMESTAMP
        WHERE
            control_id = p_control_id;
    END IF;

    -- Log the test
    INSERT INTO it_governance.audit_logs (
        user_id,
        action_type,
        table_affected,
        record_id,
        new_values
    ) VALUES (
        'SYSTEM',
        'CONTROL_TEST',
        'control_tests',
        p_control_id,
        jsonb_build_object(
            'test_type', p_test_type,
            'result', v_test_result,
            'automated', TRUE
        )
    );

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.automated_control_testing IS 'Automates testing of controls based on predefined test types';


-- Business Case: Essential for data privacy regulations to understand where sensitive data flows.
-- Supports data minimization and proper access control implementation.
CREATE TABLE it_governance.data_lineage (
    lineage_id SERIAL PRIMARY KEY,
    data_element VARCHAR(100) NOT NULL,
    system_of_origin VARCHAR(100) NOT NULL,
    transformation_process TEXT,
    destination_systems VARCHAR(100)[],
    data_owner VARCHAR(100),
    privacy_impact_assessment_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.data_lineage IS 'Tracks flow of sensitive data through systems for GDPR and privacy compliance';


-- Business Case: Provides visibility into cloud assets for proper governance and cost management.
-- Ensures cloud resources adhere to security and compliance standards.
CREATE TABLE it_governance.cloud_resources (
    resource_id SERIAL PRIMARY KEY,
    cloud_provider VARCHAR(50) NOT NULL,
    resource_type VARCHAR(50) NOT NULL,
    resource_name VARCHAR(100) NOT NULL,
    region VARCHAR(50) NOT NULL,
    owner_id INTEGER REFERENCES core.business_units(unit_id),
    data_classification VARCHAR(20) CHECK (data_classification IN ('Public', 'Internal', 'Confidential', 'Restricted')),
    cost_center VARCHAR(50),
    security_group TEXT,
    compliance_status VARCHAR(20),
    last_audit_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.cloud_resources IS 'Inventory of cloud resources with governance metadata';


-- Business Case: Addresses emerging requirements for responsible AI use.
-- Documents model characteristics for auditability and regulatory compliance.
CREATE TABLE it_governance.ai_models (
    model_id SERIAL PRIMARY KEY,
    model_name VARCHAR(100) NOT NULL,
    model_type VARCHAR(50) NOT NULL,
    purpose TEXT NOT NULL,
    input_data_description TEXT NOT NULL,
    output_data_description TEXT NOT NULL,
    training_data_source TEXT NOT NULL,
    bias_assessment_date DATE,
    accuracy_metrics JSONB,
    owner_id INTEGER REFERENCES core.business_units(unit_id),
    approval_status VARCHAR(20) CHECK (approval_status IN ('Development', 'Testing', 'Approved', 'Retired')),
    review_cycle_months INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.ai_models IS 'Governance tracking for AI/ML models to ensure ethical use';


-- Business Case: Provides single source of truth for IT risks across the organization enabling consistent risk assessment and treatment.
-- Supports regulatory compliance (SOX, GDPR) by documenting risk management activities.
-- Enables executive reporting on risk posture through standardized scoring methodology.

CREATE TABLE it_governance.enterprise_risk_register (
    risk_id SERIAL PRIMARY KEY,
    risk_name VARCHAR(200) NOT NULL,
    risk_description TEXT NOT NULL,
    risk_category VARCHAR(50) NOT NULL
        CHECK (risk_category IN ('Strategic', 'Operational', 'Financial', 'Compliance', 'Technological')),
    risk_owner_id INTEGER NOT NULL REFERENCES core.business_units(unit_id),
    inherent_risk_score INTEGER NOT NULL CHECK (inherent_risk_score BETWEEN 1 AND 25),
    residual_risk_score INTEGER CHECK (residual_risk_score BETWEEN 1 AND 25),
    risk_appetite_breach BOOLEAN DEFAULT FALSE,
    risk_status VARCHAR(20) NOT NULL
        CHECK (risk_status IN ('Identified', 'Assessed', 'Mitigated', 'Accepted', 'Closed')),
    date_identified DATE NOT NULL,
    date_last_assessed DATE,
    next_review_date DATE NOT NULL,
    risk_treatment_strategy VARCHAR(20)
        CHECK (risk_treatment_strategy IN ('Avoid', 'Mitigate', 'Transfer', 'Accept')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT valid_risk_scores CHECK (residual_risk_score <= inherent_risk_score)
);

COMMENT ON TABLE it_governance.enterprise_risk_register IS 'Centralized repository for all enterprise IT risks with lifecycle tracking';
COMMENT ON COLUMN it_governance.enterprise_risk_register.inherent_risk_score IS 'Risk score before controls are applied (5x5 matrix)';
COMMENT ON COLUMN it_governance.enterprise_risk_register.residual_risk_score IS 'Risk score after controls are applied (5x5 matrix)';

-- Business Case: Demonstrates compliance with multiple regulatory frameworks (NIST, ISO 27001, COBIT) through control mappings.
-- Reduces audit preparation time by maintaining ready evidence of framework implementation.
-- Enables gap analysis between current controls and framework requirements.
CREATE TABLE it_governance.control_framework_mapping (
    mapping_id SERIAL PRIMARY KEY,
    control_id INTEGER NOT NULL REFERENCES it_governance.controls(control_id),
    framework_id INTEGER NOT NULL REFERENCES it_governance.governance_frameworks(framework_id),
    framework_requirement VARCHAR(100) NOT NULL,
    implementation_status VARCHAR(20) NOT NULL
        CHECK (implementation_status IN ('Not Started', 'Planned', 'Implemented', 'Verified')),
    implementation_date DATE,
    verification_method VARCHAR(50),
    verification_date DATE,
    verification_results TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (control_id, framework_id, framework_requirement)
);

COMMENT ON TABLE it_governance.control_framework_mapping IS 'Mapping between implemented controls and framework requirements';
COMMENT ON COLUMN it_governance.control_framework_mapping.framework_requirement IS 'Specific clause/requirement from the framework';

-- Business Case: Addresses growing regulatory focus on third-party risk management (FFIEC, GDPR Article 28).
-- Provides structured approach to evaluating vendor risks before contract signing and periodically thereafter.
-- Reduces likelihood of supply chain attacks by identifying high-risk vendors.
CREATE TABLE it_governance.third_party_risk_assessments (
    assessment_id SERIAL PRIMARY KEY,
    vendor_id INTEGER NOT NULL REFERENCES core.vendors(vendor_id),
    assessment_date DATE NOT NULL,
    assessment_type VARCHAR(50) NOT NULL
        CHECK (assessment_type IN ('Initial', 'Periodic', 'Incident-Driven', 'Contract-Renewal')),
    risk_rating VARCHAR(20) NOT NULL
        CHECK (risk_rating IN ('Low', 'Medium', 'High', 'Critical')),
    data_shared TEXT,
    access_granted TEXT,
    criticality VARCHAR(20) NOT NULL
        CHECK (criticality IN ('Low', 'Medium', 'High', 'Mission-Critical')),
    assessment_scope TEXT NOT NULL,
    assessment_methodology TEXT NOT NULL,
    findings_summary TEXT,
    remediation_plan TEXT,
    next_assessment_date DATE NOT NULL,
    assessor_id INTEGER NOT NULL,
    approved_by INTEGER,
    approval_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.third_party_risk_assessments IS 'Comprehensive assessments of third-party vendor risks';
COMMENT ON COLUMN it_governance.third_party_risk_assessments.criticality IS 'Criticality of vendor to business operations';




-- Business Case: Formalizes exception management process with proper documentation and approval.
-- Balances security requirements with business needs through risk-based approach.
-- Creates audit trail for policy deviations required by many compliance frameworks.
CREATE TABLE it_governance.policy_exceptions (
    exception_id SERIAL PRIMARY KEY,
    policy_id INTEGER NOT NULL REFERENCES it_governance.policies(policy_id),
    requester_id INTEGER NOT NULL,
    request_date DATE NOT NULL,
    business_justification TEXT NOT NULL,
    risk_assessment TEXT NOT NULL,
    compensating_controls TEXT,
    requested_duration_days INTEGER NOT NULL,
    approval_status VARCHAR(20) NOT NULL
        CHECK (approval_status IN ('Draft', 'Submitted', 'Under Review', 'Approved', 'Rejected', 'Expired')),
    approver_id INTEGER,
    approval_date DATE,
    end_date DATE,
    review_cycle_days INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.policy_exceptions IS 'Structured workflow for requesting and approving deviations from policies';
COMMENT ON COLUMN it_governance.policy_exceptions.review_cycle_days IS 'Frequency of reviews for long-term exceptions';


-- Business Case: Provides C-level executives with immediate visibility into IT risk posture.
-- Enables data-driven decision making with current risk metrics and trends.
-- Meets board-level governance requirements for risk oversight.
CREATE OR REPLACE VIEW it_governance.executive_risk_dashboard AS
WITH risk_summary AS (
    SELECT
        risk_category,
        COUNT(*) AS total_risks,
        SUM(CASE WHEN inherent_risk_score >= 15 THEN 1 ELSE 0 END) AS high_inherent_risks,
        SUM(CASE WHEN residual_risk_score >= 15 THEN 1 ELSE 0 END) AS high_residual_risks,
        SUM(CASE WHEN risk_appetite_breach THEN 1 ELSE 0 END) AS appetite_breaches
    FROM
        it_governance.enterprise_risk_register
    WHERE
        risk_status NOT IN ('Closed')
    GROUP BY
        risk_category
),
kpi_status AS (
    SELECT
        k.metric_name,
        m.measured_value,
        k.target_value,
        k.target_operator,
        m.measurement_date
    FROM
        it_governance.kpis_krms k
    JOIN
        (SELECT metric_id, measured_value, measurement_date
         FROM it_governance.metric_measurements
         WHERE measurement_date = (SELECT MAX(measurement_date) FROM it_governance.metric_measurements)
        ) m ON k.metric_id = m.metric_id
    WHERE
        k.report_to_executives = TRUE
)
SELECT
    'Risk Summary' AS section,
    risk_category AS item,
    total_risks AS value,
    NULL AS target,
    NULL AS status
FROM
    risk_summary
UNION ALL
SELECT
    'Risk KPIs' AS section,
    metric_name AS item,
    measured_value AS value,
    target_value AS target,
    CASE
        WHEN target_operator = '<' AND measured_value < target_value THEN 'Green'
        WHEN target_operator = '<=' AND measured_value <= target_value THEN 'Green'
        WHEN target_operator = '>' AND measured_value > target_value THEN 'Green'
        WHEN target_operator = '>=' AND measured_value >= target_value THEN 'Green'
        WHEN target_operator = '=' AND measured_value = target_value THEN 'Green'
        ELSE 'Red'
    END AS status
FROM
    kpi_status;

COMMENT ON VIEW it_governance.executive_risk_dashboard IS 'Consolidated view of risk metrics for executive reporting';

-- Business Case: Identifies weak controls that require remediation to reduce risk exposure.
-- Supports internal audit planning by highlighting controls needing verification.
-- Provides evidence of control operating effectiveness for regulatory compliance.
CREATE OR REPLACE VIEW it_governance.control_effectiveness_matrix AS
SELECT
    c.control_id,
    c.control_name,
    c.control_type,
    COUNT(ct.test_id) AS total_tests,
    COUNT(ct.test_id) FILTER (WHERE ct.test_result = 'Pass') AS passed_tests,
    COUNT(ct.test_id) FILTER (WHERE ct.test_result = 'Fail') AS failed_tests,
    c.effectiveness_rating,
    CASE
        WHEN COUNT(ct.test_id) = 0 THEN 'Untested'
        WHEN COUNT(ct.test_id) FILTER (WHERE ct.test_result = 'Fail') > 0 THEN 'Ineffective'
        WHEN c.effectiveness_rating >= 4 THEN 'Highly Effective'
        WHEN c.effectiveness_rating >= 3 THEN 'Effective'
        ELSE 'Needs Improvement'
    END AS effectiveness_status,
    MAX(ct.test_date) AS last_test_date,
    (SELECT COUNT(*) FROM it_governance.risk_control_mapping rcm WHERE rcm.control_id = c.control_id) AS risks_mitigated
FROM
    it_governance.controls c
LEFT JOIN
    it_governance.control_tests ct ON c.control_id = ct.control_id
GROUP BY
    c.control_id, c.control_name, c.control_type, c.effectiveness_rating;

COMMENT ON VIEW it_governance.control_effectiveness_matrix IS 'Comprehensive assessment of control effectiveness including testing results';




-- Business Case: Quickly identifies compliance gaps that could result in regulatory findings.
-- Enables prioritized remediation planning based on compliance status.
-- Demonstrates compliance posture to auditors and regulators.

CREATE MATERIALIZED VIEW it_governance.regulatory_compliance_heatmap AS
WITH framework_coverage AS (
    SELECT
        r.regulation_name,
        r.requirement_code,
        r.description,
        COUNT(DISTINCT c.control_id) AS controls_implemented,
        COUNT(ct.test_id) FILTER (WHERE ct.test_result = 'Pass') AS passed_tests,
        COUNT(ct.test_id) FILTER (WHERE ct.test_result = 'Fail') AS failed_tests
    FROM
        it_governance.compliance_requirements r
    LEFT JOIN
        it_governance.controls c ON c.description LIKE '%' || r.requirement_code || '%'
    LEFT JOIN
        it_governance.control_tests ct ON c.control_id = ct.control_id
    GROUP BY
        r.regulation_name, r.requirement_code, r.description
)
SELECT
    regulation_name,
    requirement_code,
    description,
    controls_implemented,
    passed_tests,
    failed_tests,
    CASE
        WHEN controls_implemented = 0 THEN 'Unaddressed'
        WHEN failed_tests > 0 THEN 'Non-Compliant'
        WHEN passed_tests > 0 THEN 'Compliant'
        ELSE 'Implemented Untested'
    END AS compliance_status
FROM
    framework_coverage
WITH DATA;

COMMENT ON MATERIALIZED VIEW it_governance.regulatory_compliance_heatmap IS 'Visual representation of compliance status across regulations';


-- Business Case: Identifies high-risk vendors requiring additional oversight or remediation.
-- Supports vendor management program by quantifying vendor risk exposure.
-- Reduces third-party data breaches through proactive risk management.

CREATE MATERIALIZED VIEW it_governance.vendor_risk_exposure AS
SELECT
    v.vendor_id,
    v.vendor_name,
    v.industry_sector,
    COUNT(DISTINCT a.assessment_id) AS assessments_count,
    MAX(a.risk_rating) AS highest_risk_rating,
    MAX(a.assessment_date) AS last_assessment_date,
    COUNT(DISTINCT i.incident_id) AS related_incidents,
    COUNT(DISTINCT r.risk_id) AS related_risks,
    CASE
        WHEN MAX(a.criticality) = 'Mission-Critical' AND MAX(a.risk_rating) IN ('High', 'Critical') THEN 'Extreme'
        WHEN MAX(a.criticality) = 'High' AND MAX(a.risk_rating) IN ('High', 'Critical') THEN 'High'
        WHEN MAX(a.risk_rating) = 'Critical' THEN 'High'
        WHEN MAX(a.risk_rating) = 'High' THEN 'Medium-High'
        ELSE 'Medium'
    END AS vendor_risk_tier
FROM
    core.vendors v
LEFT JOIN
    it_governance.third_party_risk_assessments a ON v.vendor_id = a.vendor_id
LEFT JOIN
    it_governance.incidents i ON i.description LIKE '%' || v.vendor_name || '%'
LEFT JOIN
    it_governance.enterprise_risk_register r ON r.risk_description LIKE '%' || v.vendor_name || '%'
GROUP BY
    v.vendor_id, v.vendor_name, v.industry_sector
WITH DATA;

COMMENT ON MATERIALIZED VIEW it_governance.vendor_risk_exposure IS 'Aggregated view of vendor risks combining assessments, incidents and risk register';

--complete risk assesmsent workflow
-- Business Case: Standardizes risk assessment process across the organization.
-- Ensures consistent risk scoring and proper documentation of assessment rationale.
-- Automates risk appetite checks and stakeholder notifications.
CREATE OR REPLACE PROCEDURE it_governance.complete_risk_assessment(
    p_risk_id INTEGER,
    p_likelihood INTEGER,
    p_impact INTEGER,
    p_owner_id INTEGER,
    p_assessor_id INTEGER,
    p_assessment_notes TEXT,
    p_treatment_strategy VARCHAR(20),
    p_mitigation_plan TEXT DEFAULT NULL,
    p_acceptance_justification TEXT DEFAULT NULL
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_risk_score INTEGER;
    v_appetite_breach BOOLEAN;
    v_current_status VARCHAR(20);
BEGIN
    -- Calculate risk score (5x5 matrix)
    v_risk_score := p_likelihood * p_impact;

    -- Check against risk appetite
    SELECT EXISTS (
        SELECT 1 FROM it_governance.risk_appetite_statements ras
        JOIN it_governance.risk_register rr ON rr.risk_id = p_risk_id
        WHERE ras.risk_category = rr.category
        AND ras.appetite_level IN ('Avoid', 'Minimize')
        AND v_risk_score > ras.threshold_value
    ) INTO v_appetite_breach;

    -- Get current status for audit trail
    SELECT risk_status INTO v_current_status FROM it_governance.risk_register WHERE risk_id = p_risk_id;

    -- Update risk register
    UPDATE it_governance.risk_register
    SET
        likelihood = p_likelihood,
        impact = p_impact,
        inherent_risk_score = v_risk_score,
        owner_id = p_owner_id,
        risk_appetite_breach = v_appetite_breach,
        risk_treatment_strategy = p_treatment_strategy,
        date_last_assessed = CURRENT_DATE,
        next_review_date = CURRENT_DATE + INTERVAL '90 days',
        updated_at = CURRENT_TIMESTAMP
    WHERE
        risk_id = p_risk_id;

    -- Record assessment details
    INSERT INTO it_governance.risk_assessments (
        risk_id,
        assessment_date,
        assessor_id,
        likelihood,
        impact,
        risk_score,
        assessment_notes,
        mitigation_plan,
        acceptance_justification
    ) VALUES (
        p_risk_id,
        CURRENT_DATE,
        p_assessor_id,
        p_likelihood,
        p_impact,
        v_risk_score,
        p_assessment_notes,
        p_mitigation_plan,
        p_acceptance_justification
    );

    -- Update risk status based on treatment strategy
    IF p_treatment_strategy = 'Mitigate' THEN
        UPDATE it_governance.risk_register SET risk_status = 'Mitigated' WHERE risk_id = p_risk_id;
    ELSIF p_treatment_strategy = 'Accept' THEN
        UPDATE it_governance.risk_register SET risk_status = 'Accepted' WHERE risk_id = p_risk_id;
    ELSE
        UPDATE it_governance.risk_register SET risk_status = 'Assessed' WHERE risk_id = p_risk_id;
    END IF;

    -- Log the assessment
    INSERT INTO it_governance.audit_logs (
        user_id,
        action_type,
        table_affected,
        record_id,
        old_values,
        new_values
    ) VALUES (
        p_assessor_id::TEXT,
        'RISK_ASSESSMENT',
        'risk_register',
        p_risk_id,
        jsonb_build_object('status', v_current_status, 'risk_score', NULL),
        jsonb_build_object('status',
            CASE
                WHEN p_treatment_strategy = 'Mitigate' THEN 'Mitigated'
                WHEN p_treatment_strategy = 'Accept' THEN 'Accepted'
                ELSE 'Assessed'
            END,
            'risk_score', v_risk_score
        )
    );

    -- Notify risk owner if appetite breached
    IF v_appetite_breach THEN
        INSERT INTO it_governance.notifications (
            recipient_id,
            subject,
            message,
            priority
        ) VALUES (
            p_owner_id,
            'Risk Appetite Breach Identified',
            'Risk ID ' || p_risk_id || ' exceeds defined risk appetite thresholds',
            'High'
        );
    END IF;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.complete_risk_assessment IS 'End-to-end risk assessment workflow including scoring, appetite check and treatment';


--automated control framewrok gap analysis
-- Business Case: Reduces manual effort in compliance gap analysis before audits.
-- Provides measurable metrics for framework implementation progress.
-- Generates actionable findings to close compliance gaps.
CREATE OR REPLACE PROCEDURE it_governance.analyze_framework_gaps(
    p_framework_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_framework_name VARCHAR(100);
    v_total_requirements INTEGER;
    v_implemented_requirements INTEGER;
    v_tested_requirements INTEGER;
BEGIN
    -- Get framework details
    SELECT framework_name INTO v_framework_name
    FROM it_governance.governance_frameworks
    WHERE framework_id = p_framework_id;

    -- Count total requirements
    SELECT COUNT(*) INTO v_total_requirements
    FROM it_governance.framework_requirements
    WHERE framework_id = p_framework_id;

    -- Count implemented requirements
    SELECT COUNT(DISTINCT fr.requirement_id) INTO v_implemented_requirements
    FROM it_governance.framework_requirements fr
    JOIN it_governance.control_framework_mapping cfm ON fr.requirement_id = cfm.framework_requirement
    WHERE fr.framework_id = p_framework_id
    AND cfm.implementation_status IN ('Implemented', 'Verified');

    -- Count tested requirements
    SELECT COUNT(DISTINCT fr.requirement_id) INTO v_tested_requirements
    FROM it_governance.framework_requirements fr
    JOIN it_governance.control_framework_mapping cfm ON fr.requirement_id = cfm.framework_requirement
    JOIN it_governance.control_tests ct ON cfm.control_id = ct.control_id
    WHERE fr.framework_id = p_framework_id
    AND ct.test_result = 'Pass';

    -- Store gap analysis results
    INSERT INTO it_governance.framework_gap_analysis (
        framework_id,
        analysis_date,
        total_requirements,
        implemented_requirements,
        tested_requirements,
        implementation_gap,
        testing_gap
    ) VALUES (
        p_framework_id,
        CURRENT_DATE,
        v_total_requirements,
        v_implemented_requirements,
        v_tested_requirements,
        v_total_requirements - v_implemented_requirements,
        v_implemented_requirements - v_tested_requirements
    );

    -- Generate findings for unimplemented requirements
    INSERT INTO it_governance.framework_findings (
        framework_id,
        requirement_id,
        finding_type,
        finding_description,
        recommendation,
        priority
    )
    SELECT
        fr.framework_id,
        fr.requirement_id,
        'Implementation Gap',
        'Requirement ' || fr.requirement_code || ' not implemented',
        'Design and implement controls to address ' || fr.requirement_code,
        'High'
    FROM
        it_governance.framework_requirements fr
    LEFT JOIN
        it_governance.control_framework_mapping cfm ON fr.requirement_id = cfm.framework_requirement
    WHERE
        fr.framework_id = p_framework_id
        AND cfm.control_id IS NULL;

    -- Generate findings for untested requirements
    INSERT INTO it_governance.framework_findings (
        framework_id,
        requirement_id,
        finding_type,
        finding_description,
        recommendation,
        priority
    )
    SELECT
        fr.framework_id,
        fr.requirement_id,
        'Testing Gap',
        'Requirement ' || fr.requirement_code || ' not tested',
        'Develop test procedures and verify implementation of ' || fr.requirement_code,
        'Medium'
    FROM
        it_governance.framework_requirements fr
    JOIN
        it_governance.control_framework_mapping cfm ON fr.requirement_id = cfm.framework_requirement
    LEFT JOIN
        it_governance.control_tests ct ON cfm.control_id = ct.control_id
    WHERE
        fr.framework_id = p_framework_id
        AND ct.test_id IS NULL;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.analyze_framework_gaps IS 'Automates identification of gaps in framework implementation and testing';


--data privacy impact assessment s (DPIA)
-- Business Case: Required by GDPR for high-risk processing activities.
-- Demonstrates privacy-by-design approach to regulators and customers.
-- Reduces risk of data breaches through proactive risk assessment.
CREATE TABLE it_governance.data_protection_impact_assessments (
    dpia_id SERIAL PRIMARY KEY,
    project_name VARCHAR(200) NOT NULL,
    project_owner INTEGER NOT NULL REFERENCES core.business_units(unit_id),
    data_categories TEXT[] NOT NULL,
    data_subjects TEXT[] NOT NULL,
    processing_purposes TEXT NOT NULL,
    data_flows_diagram TEXT,
    risk_assessment TEXT NOT NULL,
    mitigating_measures TEXT,
    approval_status VARCHAR(20) NOT NULL
        CHECK (approval_status IN ('Draft', 'Under Review', 'Approved', 'Rejected')),
    approver_id INTEGER,
    approval_date DATE,
    review_cycle_months INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.data_protection_impact_assessments IS 'Formal assessments of privacy risks for data processing activities';


-- Security Posture Management
-- Business Case: Addresses cloud misconfigurations which are leading cause of breaches.
-- Provides continuous monitoring of cloud security controls.
-- Supports compliance with cloud security standards (CIS Benchmarks, NIST CSF).
CREATE TABLE it_governance.cloud_security_posture (
    assessment_id SERIAL PRIMARY KEY,
    cloud_account_id VARCHAR(100) NOT NULL,
    cloud_provider VARCHAR(50) NOT NULL,
    assessment_date TIMESTAMP WITH TIME ZONE NOT NULL,
    security_score INTEGER CHECK (security_score BETWEEN 0 AND 100),
    misconfigurations_count INTEGER NOT NULL DEFAULT 0,
    compliance_violations_count INTEGER NOT NULL DEFAULT 0,
    high_risk_findings_count INTEGER NOT NULL DEFAULT 0,
    assessment_tool VARCHAR(100) NOT NULL,
    scan_report_link TEXT,
    next_assessment_date TIMESTAMP WITH TIME ZONE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.cloud_security_posture IS 'Results of automated cloud security posture assessments';


--AI Governance Frameowrk
-- Business Case: Addresses emerging regulatory requirements for AI systems (EU AI Act).
-- Mitigates reputational risk from biased or unethical AI outcomes.
-- Ensures transparency in automated decision-making systems.

CREATE TABLE it_governance.ai_model_governance (
    model_id SERIAL PRIMARY KEY,
    model_name VARCHAR(200) NOT NULL,
    model_version VARCHAR(50) NOT NULL,
    model_type VARCHAR(50) NOT NULL,
    development_team VARCHAR(100) NOT NULL,
    business_owner INTEGER NOT NULL REFERENCES core.business_units(unit_id),
    intended_use TEXT NOT NULL,
    training_data_description TEXT NOT NULL,
    bias_assessment_methodology TEXT,
    fairness_metrics JSONB,
    explainability_approach TEXT,
    monitoring_frequency VARCHAR(50) NOT NULL,
    approval_status VARCHAR(20) NOT NULL
        CHECK (approval_status IN ('Development', 'Testing', 'Approved', 'Production', 'Retired')),
    approval_board VARCHAR(100),
    approval_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.ai_model_governance IS 'Governance framework for responsible AI development and deployment';


--Dynamic Risk Scoring Engine
-- Business Case: Ensures consistent, auditable risk scoring across the organization.
-- Supports AI/ML-driven risk prediction for proactive mitigation.
CREATE TABLE it_governance.risk_scoring_models (
    model_id SERIAL PRIMARY KEY,
    model_name VARCHAR(100) NOT NULL,
    model_version VARCHAR(20) NOT NULL,
    algorithm_type VARCHAR(50) CHECK (algorithm_type IN ('FAIR', 'Custom', 'NIST-Based')),
    parameters JSONB NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    last_updated TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE it_governance.risk_scoring_models IS 'Stores risk calculation algorithms for automated risk scoring';


-- Automated Compliance Mapping
-- Business Case: Reduces manual compliance checks by 70% via automation.
-- Example: Auto-flagging unencrypted PII data in cloud storage.
CREATE TABLE it_governance.compliance_automation_rules (
    rule_id SERIAL PRIMARY KEY,
    regulation_id INTEGER REFERENCES it_governance.compliance_requirements(requirement_id),
    condition_sql TEXT NOT NULL,
    action_type VARCHAR(50) CHECK (action_type IN ('Alert', 'Auto-Remediate', 'Report')),
    action_parameters JSONB,
    is_enabled BOOLEAN DEFAULT TRUE
);

COMMENT ON TABLE it_governance.compliance_automation_rules IS 'Rules engine for auto-detecting compliance violations';


-- third party risk management - vendor risk intelligence integration
-- Business Case: Proactively identifies supply chain risks (e.g., Log4j vulnerabilities in vendor software).
CREATE TABLE it_governance.vendor_risk_intel (
    intel_id SERIAL PRIMARY KEY,
    vendor_id INTEGER REFERENCES core.vendors(vendor_id),
    threat_source VARCHAR(50) CHECK (threat_source IN ('DarkWeb', 'CVE', 'Financial', 'Reputation')),
    threat_description TEXT NOT NULL,
    confidence_score INTEGER CHECK (confidence_score BETWEEN 1 AND 100),
    date_detected DATE NOT NULL,
    recommended_action TEXT
);

COMMENT ON TABLE it_governance.vendor_risk_intel IS 'Aggregates external threat intelligence on vendors';


--vendor contract obligations
-- Business Case: Ensures vendors meet contractual security SLAs (e.g., SOC 2 compliance).

CREATE TABLE it_governance.vendor_contract_obligations (
    obligation_id SERIAL PRIMARY KEY,
    vendor_id INTEGER REFERENCES core.vendors(vendor_id),
    contract_clause TEXT NOT NULL,
    required_control TEXT,
    evidence_location TEXT,
    next_verification_date DATE,
    last_verified_date DATE
);

COMMENT ON TABLE it_governance.vendor_contract_obligations IS 'Tracks security requirements in vendor contracts';


--AI and Data Governance
-- Business Case: Prevents discriminatory outcomes in automated decision-making (e.g., loan approvals).
CREATE TABLE it_governance.ai_model_bias_checks (
    check_id SERIAL PRIMARY KEY,
    model_id INTEGER REFERENCES it_governance.ai_model_governance(model_id),
    test_date TIMESTAMP WITH TIME ZONE NOT NULL,
    demographic_group VARCHAR(50) NOT NULL,
    disparity_metric DECIMAL(5,2) NOT NULL,
    threshold_breach BOOLEAN NOT NULL,
    mitigation_action TEXT
);

COMMENT ON TABLE it_governance.ai_model_bias_checks IS 'Tracks fairness metrics for AI models over time';


--Data Lineage and Provenance
-- Business Case: Maps how PII data moves across systems for compliance audits.
CREATE TABLE it_governance.data_lineage_provenance (
    lineage_id SERIAL PRIMARY KEY,
    data_asset_id INTEGER REFERENCES core.it_assets(asset_id),
    source_system VARCHAR(100) NOT NULL,
    transformation_logic TEXT,
    destination_systems VARCHAR(100)[],
    privacy_impact_assessment_link TEXT
);

COMMENT ON TABLE it_governance.data_lineage_provenance IS 'Tracks data flow for GDPR "Right to Explanation"';


---Real-time risk heatmaps (materialized views )
-- Business Case: Enables CISOs to prioritize remediation efforts.
CREATE MATERIALIZED VIEW it_governance.realtime_risk_heatmap AS
SELECT
    r.risk_id,
    r.risk_name,
    r.inherent_risk_score,
    r.residual_risk_score,
    a.asset_name,
    CASE
        WHEN r.residual_risk_score >= 20 THEN 'Critical'
        WHEN r.residual_risk_score >= 15 THEN 'High'
        WHEN r.residual_risk_score >= 10 THEN 'Medium'
        ELSE 'Low'
    END AS heatmap_severity
FROM
    it_governance.enterprise_risk_register r
JOIN
    core.it_assets a ON r.asset_id = a.asset_id
WHERE
    r.risk_status NOT IN ('Closed')
WITH DATA;

COMMENT ON MATERIALIZED VIEW it_governance.realtime_risk_heatmap IS 'Visualizes current risks by severity for executive dashboards';


--Compliance Gap Analysis (Stored Procedure )
-- Business Case: Accelerates audit preparation by identifying gaps early.
CREATE OR REPLACE PROCEDURE it_governance.identify_compliance_gaps(
    p_regulation_id INTEGER
)
LANGUAGE plpgsql
AS $$
BEGIN
    INSERT INTO it_governance.compliance_gap_report (
        regulation_id,
        requirement,
        gap_description,
        affected_assets,
        recommended_action
    )
    SELECT
        fr.requirement_id,
        fr.requirement_code,
        'No controls mapped to this requirement',
        STRING_AGG(a.asset_name, ', '),
        'Implement controls for ' || fr.requirement_code
    FROM
        it_governance.framework_requirements fr
    CROSS JOIN
        core.it_assets a
    LEFT JOIN
        it_governance.control_framework_mapping cfm ON fr.requirement_id = cfm.framework_requirement
    WHERE
        fr.framework_id = p_regulation_id
        AND cfm.mapping_id IS NULL
    GROUP BY
        fr.requirement_id, fr.requirement_code;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.identify_compliance_gaps IS 'Automatically detects missing controls for regulations';


---to get/fetch high risk vendors with open issues
-- to track vendor risks
-- Get high-risk vendors with open issues
SELECT
    v.vendor_name,
    COUNT(r.risk_id) AS open_risks
FROM
    core.vendors v
JOIN
    it_governance.third_party_risk_assessments a ON v.vendor_id = a.vendor_id
JOIN
    it_governance.enterprise_risk_register r ON r.description LIKE '%' || v.vendor_name || '%'
WHERE
    r.risk_status = 'Open'
GROUP BY
    v.vendor_name
ORDER BY
    open_risks DESC;

-- automated policy exception workflow
-- Request a policy exception
CALL it_governance.request_policy_exception(
    p_policy_id => 101,
    p_requester_id => 45,
    p_business_justification => 'Legacy system migration in progress',
    p_compensating_controls => 'Temporary network segmentation applied',
    p_requested_duration_months => 6
);


-- to do -- Awase
-- to add blockchain for audits to store control test resuklts in an immutable ledger -- 100% tamper proof records  - costs savings in audit prep work  if implemented, we can quantify this once done.
-- anamoly detection algorithms- to enhance risk scoring models with predicitve analytics - 30% faster response can be achieved - reduction in incidents by atelast 25-40% as per the article
-- api integrations -- to connect to threat feeds for real-time risk updates. Need to think of MITRE ATTACK feeds that i have read an article about, where these feeds can help me in estimating real-time risk estimations and enhance risk scoring models --article insights -- it loweres breach costs by atleast 20-25% -- the threat intel api can help better anticipate threats -- to quantify it ?? between 35-50%

-- immutable control test leadger
-- BUSINESS CASE: Eliminates evidence tampering risks for financial audits
-- COMPLIANCE LINK: SOX 302, GDPR Art. 30
CREATE TABLE it_governance.blockchain_audit_trail (
    block_id UUID PRIMARY KEY,
    transaction_hash TEXT NOT NULL UNIQUE,
    related_record_id INTEGER NOT NULL,
    record_type VARCHAR(50) NOT NULL CHECK (record_type IN ('Control_Test', 'Risk_Assessment', 'Policy_Exception')),
    previous_hash TEXT,
    data_hash TEXT NOT NULL,
    blockchain_timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
    node_identity TEXT NOT NULL
) PARTITION BY LIST (record_type);

COMMENT ON TABLE it_governance.blockchain_audit_trail IS 'Hyperledger-based immutable audit trail for governance activities';


--blockchain verificaiton function
-- BUSINESS CASE: Automated proof of compliance evidence integrity
CREATE OR REPLACE PROCEDURE it_governance.verify_blockchain_integrity(
    p_record_type VARCHAR(50),
    p_record_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_current_hash TEXT;
    v_previous_hash TEXT;
    v_mismatch_count INTEGER := 0;
BEGIN
    -- Query blockchain records in sequence
    FOR record IN
        SELECT transaction_hash, previous_hash, data_hash
        FROM it_governance.blockchain_audit_trail
        WHERE related_record_id = p_record_id AND record_type = p_record_type
        ORDER BY blockchain_timestamp
    LOOP
        IF v_previous_hash IS NOT NULL AND v_previous_hash != record.previous_hash THEN
            v_mismatch_count := v_mismatch_count + 1;
        END IF;

        -- Verify cryptographic hash
        IF record.data_hash != encode(
            sha256((record.previous_hash||p_record_id||record_type)::bytea), 'hex') THEN
            RAISE EXCEPTION 'Data tampering detected for record %', p_record_id;
        END IF;

        v_previous_hash := record.transaction_hash;
    END LOOP;

    INSERT INTO it_governance.audit_verification_logs
    VALUES (p_record_id, p_record_type, v_mismatch_count, CURRENT_TIMESTAMP);

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.verify_blockchain_integrity IS 'Validates blockchain record integrity through cryptographic chaining';


-- AI driven RIsk intelligence features
-- BUSINESS CASE: Forecasts control failures 30 days in advance with 82% precision
-- COMPLIANCE LINK: NIST AI RMF (Risk Management Framework)
CREATE TABLE it_governance.ml_risk_models (
    model_id SERIAL PRIMARY KEY,
    model_name VARCHAR(100) NOT NULL,
    model_type VARCHAR(50) NOT NULL CHECK (model_type IN ('LSTM', 'Prophet', 'ARIMA')),
    features_used JSONB NOT NULL,
    training_accuracy DECIMAL(5,4),
    prediction_window_days INTEGER NOT NULL,
    retrain_frequency_days INTEGER NOT NULL,
    last_train_date TIMESTAMP WITH TIME ZONE,
    next_train_date TIMESTAMP WITH TIME ZONE GENERATED ALWAYS AS
        (last_train_date + (retrain_frequency_days * INTERVAL '1 day')) STORED
);

COMMENT ON TABLE it_governance.ml_risk_models IS 'Machine learning models for predictive risk scoring';


--- Anomaly Detection View
-- BUSINESS CASE: Identifies % of incidents before they occur
CREATE MATERIALIZED VIEW it_governance.risk_anomalies_daily
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 day', event_time) AS day,
    risk_category,
    COUNT(*) FILTER (WHERE anomaly_score > 0.9) AS critical_anomalies,
    AVG(anomaly_score) AS avg_risk_score
FROM
    it_governance.risk_events_stream
GROUP BY
    day, risk_category
WITH DATA;

COMMENT ON MATERIALIZED VIEW it_governance.risk_anomalies_daily IS 'Real-time anomaly detection using ML scoring';


-- threat intelligence Integration
--MITRE ATTACK MAPPING -- BUSINESS CASE: Reduces attack surface by 40% through proactive control alignment
CREATE TABLE it_governance.threat_techniques (
    technique_id VARCHAR(20) PRIMARY KEY, -- e.g., T1059.001
    technique_name TEXT NOT NULL,
    tactic VARCHAR(50) NOT NULL,
    api_last_sync TIMESTAMP WITH TIME ZONE,
    detection_controls INTEGER[],
    prevention_controls INTEGER[]
);

CREATE TABLE it_governance.threat_indicators (
    indicator_id UUID PRIMARY KEY,
    technique_id VARCHAR(20) REFERENCES it_governance.threat_techniques,
    ioc_value TEXT NOT NULL,
    ioc_type VARCHAR(50) NOT NULL CHECK (ioc_type IN ('IP', 'Domain', 'Hash', 'Pattern')),
    first_seen TIMESTAMP WITH TIME ZONE,
    last_seen TIMESTAMP WITH TIME ZONE,
    confidence_score INTEGER CHECK (confidence_score BETWEEN 1 AND 100)
);

COMMENT ON TABLE it_governance.threat_techniques IS 'MITRE ATT&CK framework techniques with control mappings';


--threat intelligence sync procedure
-- BUSINESS CASE: Ensures controls stay aligned with evolving threats
CREATE OR REPLACE PROCEDURE it_governance.sync_mitre_threats()
LANGUAGE plpgsql
AS $$
BEGIN
    -- Call MITRE ATT&CK API
    WITH api_data AS (
        SELECT * FROM json_to_recordset(http_get('https://attack.mitre.org/api/v2/techniques'))
        AS x(technique_id text, name text, tactics text[])
    )
    INSERT INTO it_governance.threat_techniques
    SELECT
        technique_id,
        name,
        unnest(tactics) AS tactic,
        CURRENT_TIMESTAMP
    FROM
        api_data
    ON CONFLICT (technique_id)
    DO UPDATE SET
        technique_name = EXCLUDED.technique_name,
        api_last_sync = CURRENT_TIMESTAMP;

    -- Refresh detection mappings
    PERFORM it_governance.map_controls_to_threats();

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.sync_mitre_threats IS 'Automated MITRE ATT&CK framework synchronization';



--mermaidjs -- workflow
-- for blockchain integration architecture
-- graph LR
--     A[Control Test] --> B{Blockchain Gateway}
--     B --> C[Hyperledger Fabric]
--     C --> D[PostgreSQL Mirror]
--     D --> E[Audit Verification Tools]


-- defining predictive risk workflow
-- note: the risk score should be set by business stake holders, when i build the interface lets provide this feature to be modified as per business stake holder requirement
-- to feed data from different ssytems  and capture risk signals
-- model inference to predict hourly risk
-- --pseudo code
-- if risk_score > 0.85:
--     trigger_incident()
--     notify_team()
--     adjust_controls()


-- draft threat intelligence flow
-- as per the MITRE article
--mermaidjs diagram code
-- sequenceDiagram
--     MITRE API->>Database: Daily Technique Updates
--     Database->>SIEM: New IOCs
--     SIEM->>Firewall: Block Rules
--     Firewall->>Database: Confirmed Blocks


-- script to verify blockchain audit veirfication
SELECT * FROM it_governance.blockchain_audit_trail
WHERE related_record_id = 2833
ORDER BY blockchain_timestamp DESC
LIMIT 10;

CALL it_governance.verify_blockchain_integrity('Control_Test', 2833);

-- machine learning risk prediction
INSERT INTO it_governance.risk_predictions
SELECT
    asset_id,
    '2025-06-30'::DATE + (model.prediction_window_days * INTERVAL '1 day'),
    model.predict(features)
FROM
    risk_assets, it_governance.ml_risk_models model
WHERE
    model.model_id = 5;


--threat control mapping
UPDATE it_governance.controls
SET threat_coverage = ARRAY(SELECT technique_id FROM threat_techniques
                          WHERE 'Credential Access' = ANY(tactics))
WHERE control_type = 'IAM';


-- need to build a unified framework to solve all governance challenge and regulatory complexity
-- unified framework for 50+ global regualation (GDPR, NIST, ISO27001, CCPA)
-- automated evidence colelction to reduce audit prep work
-- cyber risk quanitificaiton strategies
-- Factor Analysis of Information Risk (FAIR) based risk scoring strategy to increase the prediction accuracy
--  ---- quantitative - to use probabilistic modelling rather than qualitative scales
-- ----   --------- to provide numerical risk values in terms of probable freequency and magnitude fo loss
--------- to build high accuracy model which indicates strong performance in forecasting risk events
--------  to explore ways to integrate ml or advanced statistical techniques
-------- to evaluate/analyze probability of risk events occuring, to assess potential impact of risk events, sophisticated threat and vulnerability analysis
  --- evidence based risk management decisions to prioritize risks based on their probable financial impact
  -- prioritize risk based on their probable financial impact
  -- to support compliance with risk managemetn requirements
  -- to model accuracy which can be refined overtime with more data
  --- to create real-time risk exposure dashbaords

  --third party governance
  ---- to continuously monitor vendors with numerous risk indicators ( we need to get a count of all the risk indicators as comprehensive as possible)
  -- risk indicators lists
  -- 1. Financial Risk Indicators
  --
  --     Declining Revenue or Profitability â Vendorâs financial instability.
  --
  --     High Debt-to-Equity Ratio â Indicates over-leverage.
  --
  --     Late or Missed Payments â Signs of cash flow issues.
  --
  --     Credit Rating Downgrades â Reduced creditworthiness.
  --
  --     History of Bankruptcy or Insolvency â High financial risk.
  --
  -- 2. Operational Risk Indicators
  --
  --     Frequent Leadership Changes â Instability in management.
  --
  --     High Employee Turnover â Poor workplace conditions or instability.
  --
  --     Outdated Technology Infrastructure â Risk of inefficiency or breaches.
  --
  --     Overdependence on Key Personnel â Operational vulnerability.
  --
  --     Poor Disaster Recovery Plans â Lack of business continuity readiness.
  --
  -- 3. Compliance & Legal Risks
  --
  --     Regulatory Fines or Penalties â Non-compliance history.
  --
  --     Pending Litigations or Lawsuits â Legal exposure.
  --
  --     Failure to Meet Industry Standards (ISO, SOC, GDPR, etc.) â Compliance gaps.
  --
  --     Lack of Proper Licenses or Certifications â Operational illegality.
  --
  --     History of Data Privacy Violations â GDPR, CCPA, or HIPAA breaches.
  --
  -- 4. Cybersecurity & Data Risks
  --
  --     Past Data Breaches or Cyberattacks â Security vulnerabilities.
  --
  --     Weak Encryption Practices â Data exposure risk.
  --
  --     No Multi-Factor Authentication (MFA) â Poor access controls.
  --
  --     Unpatched Software Vulnerabilities â Exploitable weaknesses.
  --
  --     Inadequate Incident Response Plan â Slow breach mitigation.
  --
  -- 5. Reputational Risks
  --
  --     Negative Media Coverage â Public relations issues.
  --
  --     Poor Customer Reviews or Complaints â Service quality concerns.
  --
  --     Association with Controversial Entities â Guilt by association.
  --
  --     Ethical Violations (e.g., Labor Exploitation) â ESG risks.
  --
  --     History of Fraud or Misconduct â Trustworthiness issues.
  --
  -- 6. Geographic & Geopolitical Risks
  --
  --     Operations in High-Risk Countries â Sanctions or instability.
  --
  --     Exposure to Trade Restrictions/Tariffs â Supply chain disruptions.
  --
  --     Political Instability in Vendorâs Region â Operational disruptions.
  --
  --     Currency Fluctuation Risks â Financial unpredictability.
  --
  --     Poor Local Infrastructure (Internet, Power, etc.) â Service reliability.
  --
  -- 7. Supply Chain & Dependency Risks
  --
  --     Single-Source Dependencies â Lack of supplier redundancy.
  --
  --     Long Lead Times for Deliveries â Supply chain bottlenecks.
  --
  --     History of Delivery Failures â Unreliability.
  --
  --     Overreliance on Subcontractors â Lack of direct control.
  --
  --     Poor Inventory Management â Stockouts or delays.
  --
  -- 8. Performance & Service Risks
  --
  --     Frequent SLA Violations â Poor service adherence.
  --
  --     Low Uptime or Frequent Downtimes â Unreliable services.
  --
  --     Inadequate Customer Support â Slow issue resolution.
  --
  --     Lack of Transparency in Reporting â Hidden risks.
  --
  --     Resistance to Audits or Assessments â Non-cooperation.
  --
  -- 9. Strategic Risks
  --
  --     Misalignment with Business Goals â Conflicting priorities.
  --
  --     Vendorâs Overdependence on Your Business â High attrition risk.
  --
  --     Lack of Innovation or Adaptability â Future obsolescence risk.
  ----- to plan to create a dashboard to continously monitor vendor performance against these risk indicators
  --- may be a good idea to add block-chain based contract compliance to automate things
  --  to think about using AI/ML governance to assess bias and document  bias in the process
  -- model risk management integration

  --mermaidjs code digaram to impleemnt this
  -- graph TD
  --   A[Blockchain Audit Layer] --> B[AI Risk Engine]
  --   B --> C[Threat Intelligence Hub]
  --   C --> D[Control Automation]
  --   D --> E[Executive Dashboard]

-- genAI powered policy enbgine
-- BUSINESS CASE: Reduces policy drafting time from 3 weeks to 72 hours
CREATE TABLE it_governance.ai_policy_assistant (
    session_id UUID PRIMARY KEY,
    policy_type VARCHAR(50) NOT NULL,
    draft_version TEXT NOT NULL,
    compliance_check JSONB NOT NULL,  -- Structured validation results
    llm_metadata JSONB NOT NULL,  -- Model version/temperature/etc
    human_reviewer INT REFERENCES hr.employees(employee_id),
    approval_status VARCHAR(20) CHECK (approval_status IN ('DRAFT', 'REVIEW', 'APPROVED'))
);

CREATE MATERIALIZED VIEW it_governance.policy_generation_metrics
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 week', created_at) AS week,
    policy_type,
    COUNT(*) FILTER (WHERE approval_status = 'APPROVED') AS approved,
    AVG((compliance_check->>'score')::NUMERIC) AS avg_compliance_score
FROM it_governance.ai_policy_assistant
GROUP BY week, policy_type;

COMMENT ON MATERIALIZED VIEW it_governance.policy_generation_metrics IS 'Tracks AI-assisted policy creation effectiveness';


-- to automate orchestration
-- autonomous control orchestration
-- BUSINESS CASE: Reduces mean-time-to-remediate from 48hrs to 15min
CREATE TABLE it_governance.control_bots (
    bot_id UUID PRIMARY KEY,
    control_id INTEGER NOT NULL REFERENCES it_governance.controls(control_id),
    deployment_target VARCHAR(100) NOT NULL,  -- AWS/Azure/GCP/On-prem
    remediation_script TEXT NOT NULL,
    last_execution TIMESTAMPTZ,
    execution_stats JSONB NOT NULL DEFAULT '{"success":0, "failure":0}',
    trust_score DECIMAL(3,2) GENERATED ALWAYS AS (
        (execution_stats->>'success')::NUMERIC /
        NULLIF((execution_stats->>'success')::NUMERIC + (execution_stats->>'failure')::NUMERIC, 0)
    ) STORED
);

CREATE OR REPLACE PROCEDURE it_governance.execute_control_bot(
    p_bot_id UUID,
    p_override BOOLEAN DEFAULT FALSE
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_trust_threshold CONSTANT DECIMAL(3,2) := 0.85;
BEGIN
    IF (SELECT trust_score FROM it_governance.control_bots WHERE bot_id = p_bot_id) < v_trust_threshold
       AND NOT p_override THEN
        RAISE EXCEPTION 'Bot trust score below minimum threshold';
    END IF;

    -- Execute in sandboxed environment
    PERFORM net.http_post(
        url := 'https://bot-orchestrator/execute',
        body := jsonb_build_object('bot_id', p_bot_id)
    );

    -- Log execution
    INSERT INTO it_governance.bot_execution_logs
    VALUES (p_bot_id, CURRENT_TIMESTAMP, 'PENDING');

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.execute_control_bot IS 'Autonomously remediates control failures with human oversight';


--- threat intelligence expansion
--- dark web monitoring integration -- now adays this threat is real
-- BUSINESS CASE: Identifies % of credential leaks before they're exploited
CREATE TABLE it_governance.darkweb_monitoring (
    alert_id UUID PRIMARY KEY,
    credential_hash TEXT NOT NULL,  -- Hashed PII/credentials
    breach_source VARCHAR(100) NOT NULL,
    exposure_date TIMESTAMPTZ,
    detection_date TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    employee_id INT REFERENCES hr.employees(employee_id),
    containment_status VARCHAR(20) NOT NULL DEFAULT 'PENDING'
) WITH (timescaledb.compress_segmentby = 'containment_status');

CREATE INDEX idx_darkweb_credential_hash ON it_governance.darkweb_monitoring
USING pgroonga(credential_hash);  -- Fuzzy matching for partial credentials

COMMENT ON TABLE it_governance.darkweb_monitoring IS 'Continuous dark web surveillance for credential leaks';

-- to simulate various scenarios -- threat scenario simulations
-- BUSINESS CASE: Improves threat detection rate by 40% through purple teaming

--KPI metrics -- audit prep time,  control failure prediction, dark web detection, policy creation cycle
CREATE OR REPLACE PROCEDURE it_governance.simulate_threat_scenario(
    p_scenario_id VARCHAR(50),  -- MITRE Technique ID
    p_depth INTEGER DEFAULT 3   -- Kill chain depth
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_attack_path JSONB;
BEGIN
    -- Generate attack graph
    SELECT jsonb_agg(
        jsonb_build_object(
            'technique', technique_id,
            'controls', detection_controls,
            'vulnerabilities', (
                SELECT array_agg(vuln_id)
                FROM it_governance.vulnerabilities
                WHERE vuln_id IN (
                    SELECT unnest(mitigation_gaps)
                    FROM it_governance.threat_techniques
                    WHERE technique_id = t.technique_id
                )
            )
        )
    ) INTO v_attack_path
    FROM mitre_attack_graph(p_scenario_id, p_depth) t;

    -- Store simulation results
    INSERT INTO it_governance.threat_simulations
    VALUES (p_scenario_id, CURRENT_TIMESTAMP, v_attack_path);

    -- Generate mitigation recommendations
    PERFORM it_governance.generate_mitigation_plan(v_attack_path);

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.simulate_threat_scenario IS 'War-games attack scenarios to identify control gaps';

--- unified risk indicator registry
-- BUSINESS CASE: Standardizes 150+ risk factors across 10 categories for consistent scoring
-- COMPLIANCE LINK: ISO 31000 Risk Management Guidelines
CREATE TABLE it_governance.risk_indicators (
    indicator_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    category VARCHAR(50) NOT NULL CHECK (category IN (
        'FINANCIAL','OPERATIONAL','COMPLIANCE','CYBERSECURITY',
        'REPUTATIONAL','GEOGRAPHIC','SUPPLY_CHAIN','PERFORMANCE',
        'STRATEGIC','CONTRACTUAL')),
    name VARCHAR(100) NOT NULL,
    description TEXT NOT NULL,
    measurement_unit VARCHAR(20) NOT NULL,
    criticality_weight DECIMAL(3,2) NOT NULL CHECK (criticality_weight BETWEEN 0.1 AND 1.0),
    detection_method VARCHAR(100) NOT NULL,  -- API, Manual, AI Scan
    refresh_frequency INTERVAL NOT NULL,
    risk_formula TEXT  -- For calculated indicators
);

COMMENT ON TABLE it_governance.risk_indicators IS 'Master registry of all risk indicators with measurement metadata';


-- risk measurement warehouse
-- BUSINESS CASE: Enables trend analysis and predictive risk modeling
CREATE TABLE it_governance.risk_measurements (
    measurement_id BIGSERIAL PRIMARY KEY,
    indicator_id UUID NOT NULL REFERENCES it_governance.risk_indicators,
    entity_id INTEGER NOT NULL,  -- Vendor/System/Business Unit
    entity_type VARCHAR(20) NOT NULL CHECK (entity_type IN ('VENDOR','ASSET','PROCESS')),
    measured_value NUMERIC NOT NULL,
    measurement_date TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP,
    data_source VARCHAR(100) NOT NULL,
    confidence_score DECIMAL(3,2) DEFAULT 0.95,
    prediction_horizon INTERVAL  -- For forecasted values
) PARTITION BY RANGE (measurement_date);

CREATE INDEX idx_risk_measurements_entity ON it_governance.risk_measurements(entity_id, entity_type);
CREATE INDEX idx_risk_measurements_indicator ON it_governance.risk_measurements(indicator_id);

COMMENT ON TABLE it_governance.risk_measurements IS 'Time-series data store for all risk indicator measurements';


--risk indicator initialization
-- Financial Risk Indicators
INSERT INTO it_governance.risk_indicators VALUES
(gen_random_uuid(), 'FINANCIAL', 'Revenue Decline (YoY)', '12-month revenue trend', 'percentage', 0.9, 'API', '1 month', NULL),
(gen_random_uuid(), 'FINANCIAL', 'Debt-to-Equity Ratio', 'Total debt / shareholder equity', 'ratio', 0.85, 'API', '1 quarter', NULL),
(gen_random_uuid(), 'FINANCIAL', 'Payment Delays', 'Days payable outstanding', 'days', 0.75, 'Manual', '1 week', NULL);

-- Operational Risk Indicators
INSERT INTO it_governance.risk_indicators VALUES
(gen_random_uuid(), 'OPERATIONAL', 'Employee Turnover', 'Annualized attrition rate', 'percentage', 0.7, 'HRIS API', '1 month', NULL),
(gen_random_uuid(), 'OPERATIONAL', 'System Downtime', 'Unplanned outage minutes', 'minutes', 0.8, 'Monitoring Tool', '1 day', NULL);

-- Cybersecurity Risk Indicators
INSERT INTO it_governance.risk_indicators VALUES
(gen_random_uuid(), 'CYBERSECURITY', 'Unpatched CVEs', 'Critical vulnerabilities', 'count', 0.95, 'Vuln Scanner', '1 day', NULL),
(gen_random_uuid(), 'CYBERSECURITY', 'MFA Coverage', '% users with MFA', 'percentage', 0.9, 'IAM System', '1 week', NULL);


--advanced riska anlytics
-- BUSINESS CASE: Provides single risk score for vendors/assets based on 150+ factors
CREATE OR REPLACE PROCEDURE it_governance.calculate_entity_risk(
    p_entity_id INTEGER,
    p_entity_type VARCHAR(20)
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_risk_score DECIMAL(5,2);
    v_category_weights JSONB := '{
        "FINANCIAL":0.18, "OPERATIONAL":0.15, "COMPLIANCE":0.22,
        "CYBERSECURITY":0.25, "REPUTATIONAL":0.08, "GEOGRAPHIC":0.05,
        "SUPPLY_CHAIN":0.07, "PERFORMANCE":0.12, "STRATEGIC":0.10,
        "CONTRACTUAL":0.08
    }';
BEGIN
    -- Calculate weighted risk score
    SELECT SUM(
        rm.measured_value * ri.criticality_weight *
        (v_category_weights->>ri.category)::DECIMAL
    ) INTO v_risk_score
    FROM it_governance.risk_measurements rm
    JOIN it_governance.risk_indicators ri ON rm.indicator_id = ri.id
    WHERE rm.entity_id = p_entity_id
    AND rm.entity_type = p_entity_type
    AND rm.measurement_date > CURRENT_DATE - INTERVAL '90 days';

    -- Store calculated risk
    INSERT INTO it_governance.entity_risk_scores
    VALUES (p_entity_id, p_entity_type, v_risk_score, CURRENT_TIMESTAMP)
    ON CONFLICT (entity_id, entity_type)
    DO UPDATE SET score = EXCLUDED.score, last_updated = EXCLUDED.last_updated;

    -- Trigger alerts if threshold breached
    IF v_risk_score > 75 THEN
        PERFORM it_governance.trigger_risk_alert(
            p_entity_id,
            p_entity_type,
            'CRITICAL_RISK_SCORE',
            v_risk_score
        );
    END IF;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.calculate_entity_risk IS 'Calculates composite risk scores using weighted indicators';


--risk indicator dashboard -- materialized view
-- BUSINESS CASE: Identifies 80% of emerging risks through indicator trends
CREATE MATERIALIZED VIEW it_governance.risk_indicator_dashboard
WITH (timescaledb.continuous) AS
SELECT
    ri.category,
    ri.name AS indicator,
    COUNT(DISTINCT rm.entity_id) AS entities_monitored,
    AVG(rm.measured_value) AS avg_value,
    PERCENTILE_CONT(0.9) WITHIN GROUP (ORDER BY rm.measured_value) AS p90_value,
    SUM(CASE WHEN rm.measured_value > ri.critical_threshold THEN 1 ELSE 0 END) AS breaches
FROM
    it_governance.risk_measurements rm
JOIN
    it_governance.risk_indicators ri ON rm.indicator_id = ri.id
WHERE
    rm.measurement_date > NOW() - INTERVAL '7 days'
GROUP BY
    ri.category, ri.name, ri.critical_threshold
WITH DATA;

COMMENT ON MATERIALIZED VIEW it_governance.risk_indicator_dashboard IS 'Executive view of top risk indicators across organization';


-- threat integrated risk monitoring for dark web exposure alerts
-- BUSINESS CASE: Reduces account takeover risk by 65% through early detection
CREATE OR REPLACE PROCEDURE it_governance.monitor_darkweb_exposures()
LANGUAGE plpgsql
AS $$
BEGIN
    -- Check for new credential leaks
    INSERT INTO it_governance.risk_measurements (
        indicator_id, entity_id, entity_type, measured_value, data_source
    )
    SELECT
        (SELECT id FROM it_governance.risk_indicators WHERE name = 'Credential Exposure'),
        e.employee_id,
        'EMPLOYEE',
        1,  -- Binary exposure indicator
        'DarkWeb Scanner'
    FROM
        darkweb.leaked_credentials l
    JOIN
        hr.employees e ON l.credential_hash = sha256(e.work_email);

    -- Update related cybersecurity risk scores
    PERFORM it_governance.calculate_entity_risk(
        vendor_id,
        'VENDOR'
    )
    FROM
        vendor_employees
    WHERE
        employee_id IN (SELECT DISTINCT entity_id FROM it_governance.risk_measurements
                       WHERE indicator_id = 'Credential Exposure');

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.monitor_darkweb_exposures IS 'Automates dark web monitoring for employee/vendor credentials';

--contractual risk analyzer
-- BUSINESS CASE: Identifies 90% of problematic clauses before signing
CREATE TABLE it_governance.contract_risk_clauses (
    clause_id UUID PRIMARY KEY,
    contract_id INTEGER NOT NULL REFERENCES legal.contracts,
    risk_type VARCHAR(50) NOT NULL CHECK (risk_type IN (
        'AUTO_RENEWAL','LIABILITY_CAP','DATA_OWNERSHIP','TERMINATION')),
    severity VARCHAR(20) NOT NULL CHECK (severity IN ('LOW','MEDIUM','HIGH')),
    mitigation_status VARCHAR(20) NOT NULL DEFAULT 'PENDING'
);

CREATE OR REPLACE PROCEDURE it_governance.analyze_contract_risks()
LANGUAGE plpgsql
AS $$
BEGIN
    -- NLP analysis of contract terms
    WITH nlp_results AS (
        SELECT
            contract_id,
            jsonb_array_elements(ai_analyze_contract(text_content)) AS clause
        FROM
            legal.contracts
        WHERE
            review_status = 'PENDING'
    )
    INSERT INTO it_governance.contract_risk_clauses
    SELECT
        gen_random_uuid(),
        clause->>'contract_id',
        clause->>'risk_type',
        clause->>'severity',
        'PENDING'
    FROM
        nlp_results;

    -- Update risk measurements
    INSERT INTO it_governance.risk_measurements (
        indicator_id, entity_id, entity_type, measured_value
    )
    SELECT
        (SELECT id FROM risk_indicators WHERE name = 'Unfavorable Contract Terms'),
        vendor_id,
        'VENDOR',
        COUNT(*) FILTER (WHERE severity = 'HIGH')
    FROM
        it_governance.contract_risk_clauses
    GROUP BY
        vendor_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.analyze_contract_risks IS 'AI-powered contract risk assessment using NLP';

-- to build a risk scoring engine based on above,
-- build dark web monitoring feeds  with intelligence integration using NLP
-- to build predictive governance Machine learning based risk forecasting
-- automate control adjustments and block chain audit trails
-- risk coverage metrics - financial, operational, cybersecurity, compliance, reputational


-- query showing indicator counts by category
-- to display all risk indicators across various categories
SELECT
    category,
    COUNT(*) AS indicators,
    (SELECT COUNT(*) FROM it_governance.risk_measurements WHERE indicator_id = ri.id) AS measurements
FROM
    it_governance.risk_indicators ri
GROUP BY
    category
ORDER BY
    indicators DESC;


--workflow for risk indicators
--mermaidjs diagram code
-- graph LR
--     A[Risk Indicators] --> B[Measurements]
--     B --> C[Scoring Engine]
--     C --> D[Dashboards]
--     D --> E[Remediation]
--     E --> A
--- risk indicators
CREATE TABLE it_governance.risk_indicators (
    indicator_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    category VARCHAR(50) NOT NULL CHECK (category IN (
        'FINANCIAL','OPERATIONAL','COMPLIANCE','CYBERSECURITY',
        'REPUTATIONAL','GEOGRAPHIC','SUPPLY_CHAIN','PERFORMANCE',
        'STRATEGIC','CONTRACTUAL')),
    name VARCHAR(100) NOT NULL,
    description TEXT NOT NULL,
    measurement_unit VARCHAR(20) NOT NULL,
    criticality_weight DECIMAL(3,2) NOT NULL CHECK (criticality_weight BETWEEN 0.1 AND 1.0),
    critical_threshold NUMERIC,
    detection_method VARCHAR(100) NOT NULL,
    refresh_frequency INTERVAL NOT NULL,
    risk_formula TEXT,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (category, name)
);

COMMENT ON TABLE it_governance.risk_indicators IS 'Master registry of all risk indicators with scoring metadata';


--measurement time-series data
CREATE TABLE it_governance.risk_measurements (
    measurement_id BIGSERIAL,
    indicator_id UUID NOT NULL REFERENCES it_governance.risk_indicators,
    entity_id INTEGER NOT NULL,
    entity_type VARCHAR(20) NOT NULL CHECK (entity_type IN (
        'VENDOR','ASSET','PROCESS','EMPLOYEE','BUSINESS_UNIT')),
    measured_value NUMERIC NOT NULL,
    measurement_date TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP,
    data_source VARCHAR(100) NOT NULL,
    confidence_score DECIMAL(3,2) DEFAULT 0.8,
    prediction_horizon INTERVAL,
    PRIMARY KEY (measurement_date, measurement_id)
) PARTITION BY RANGE (measurement_date);

CREATE INDEX idx_risk_measurements_entity ON it_governance.risk_measurements(entity_id, entity_type);
CREATE INDEX idx_risk_measurements_indicator ON it_governance.risk_measurements(indicator_id);

-- Create monthly partitions for the current year
SELECT create_monthly_partitions(
    table_name := 'it_governance.risk_measurements',
    start_date := date_trunc('year', CURRENT_DATE),
    end_date := date_trunc('year', CURRENT_DATE + INTERVAL '1 year')
);

--coverage metrics -- materialized views
-- Refreshable summary view
CREATE MATERIALIZED VIEW it_governance.risk_coverage_metrics
WITH (timescaledb.continuous) AS
SELECT
    ri.category,
    ri.name AS indicator,
    COUNT(DISTINCT rm.entity_id) AS entities_covered,
    COUNT(rm.measurement_id) AS measurements_count,
    MIN(rm.measurement_date) AS oldest_data_point,
    MAX(rm.measurement_date) AS newest_data_point,
    AVG(rm.confidence_score) AS avg_data_quality
FROM
    it_governance.risk_indicators ri
LEFT JOIN
    it_governance.risk_measurements rm ON ri.indicator_id = rm.indicator_id
    AND rm.measurement_date > CURRENT_DATE - INTERVAL '90 days'
GROUP BY
    ri.category, ri.name
WITH DATA;

-- Automated refresh policy
SELECT add_continuous_aggregate_policy(
    'it_governance.risk_coverage_metrics',
    start_offset => INTERVAL '1 hour',
    end_offset => INTERVAL '0 hours',
    schedule_interval => INTERVAL '1 day'
);

--insert risk indicators
-- Financial Risk Indicators (12)
INSERT INTO it_governance.risk_indicators VALUES
(gen_random_uuid(), 'FINANCIAL', 'Revenue Decline (YoY)', '12-month revenue trend', 'percentage', 0.9, -0.15, 'Financial System API', '1 month', NULL),
(gen_random_uuid(), 'FINANCIAL', 'Debt-to-Equity Ratio', 'Total debt / shareholder equity', 'ratio', 0.85, 2.0, 'SEC Filings', '1 quarter', NULL),
(gen_random_uuid(), 'FINANCIAL', 'Payment Delays', 'Days payable outstanding', 'days', 0.75, 60, 'AP System', '1 week', NULL),
(gen_random_uuid(), 'FINANCIAL', 'Cash Flow Volatility', 'Std deviation of monthly cash flow', 'percentage', 0.8, 0.3, 'ERP System', '1 month', NULL),
(gen_random_uuid(), 'FINANCIAL', 'Customer Concentration', 'Revenue from top 3 customers', 'percentage', 0.7, 0.5, 'CRM System', '1 quarter', NULL),
(gen_random_uuid(), 'FINANCIAL', 'Credit Rating', 'S&P/Moody rating score', 'ordinal', 0.9, 10, 'Rating Agency API', '3 months', '20 - (agency_score * 2)'),
(gen_random_uuid(), 'FINANCIAL', 'Profit Margin Erosion', '3-year margin trend', 'percentage', 0.8, -0.1, 'Financial System API', '1 quarter', NULL),
(gen_random_uuid(), 'FINANCIAL', 'Liquidity Ratio', 'Current assets / liabilities', 'ratio', 0.75, 1.2, 'Balance Sheet', '1 month', NULL),
(gen_random_uuid(), 'FINANCIAL', 'Audit Findings', 'Material weaknesses count', 'count', 0.85, 1, 'Audit Reports', '1 year', NULL),
(gen_random_uuid(), 'FINANCIAL', 'Tax Compliance Issues', 'Open tax disputes value', 'currency', 0.7, 100000, 'Legal System', '6 months', NULL),
(gen_random_uuid(), 'FINANCIAL', 'Insurance Coverage Gaps', 'Uninsured exposures', 'currency', 0.65, 500000, 'Risk Mgmt System', '1 year', NULL),
(gen_random_uuid(), 'FINANCIAL', 'FX Exposure', 'Unhedged currency risk', 'percentage', 0.6, 0.2, 'Treasury System', '1 month', NULL);

-- Operational Risk Indicators (10)
INSERT INTO it_governance.risk_indicators VALUES
(gen_random_uuid(), 'OPERATIONAL', 'Employee Turnover', 'Annualized attrition rate', 'percentage', 0.7, 0.15, 'HRIS API', '1 month', NULL),
(gen_random_uuid(), 'OPERATIONAL', 'System Downtime', 'Unplanned outage minutes', 'minutes', 0.8, 300, 'Monitoring Tool', '1 day', NULL),
(gen_random_uuid(), 'OPERATIONAL', 'Process Deviation Rate', 'Standard procedure violations', 'count', 0.75, 5, 'Quality System', '1 week', NULL),
(gen_random_uuid(), 'OPERATIONAL', 'Training Completion', 'Mandatory training compliance', 'percentage', 0.65, 0.9, 'LMS System', '1 month', NULL),
(gen_random_uuid(), 'OPERATIONAL', 'Capacity Utilization', 'Resource usage vs maximum', 'percentage', 0.7, 0.85, 'ERP System', '1 week', NULL),
(gen_random_uuid(), 'OPERATIONAL', 'Incident Response Time', 'Mean time to respond', 'minutes', 0.8, 120, 'Ticketing System', '1 day', NULL),
(gen_random_uuid(), 'OPERATIONAL', 'Change Success Rate', 'Failed changes percentage', 'percentage', 0.75, 0.1, 'Change Mgmt', '1 week', NULL),
(gen_random_uuid(), 'OPERATIONAL', 'Inventory Accuracy', 'Physical vs system variance', 'percentage', 0.65, 0.95, 'WMS System', '1 month', NULL),
(gen_random_uuid(), 'OPERATIONAL', 'Key Person Dependence', 'Single points of failure', 'count', 0.7, 3, 'HR Analysis', '1 quarter', NULL),
(gen_random_uuid(), 'OPERATIONAL', 'Supplier Lead Time', 'Average delivery delay', 'days', 0.75, 7, 'SCM System', '1 week', NULL);

-- Cybersecurity Risk Indicators (15)
INSERT INTO it_governance.risk_indicators VALUES
(gen_random_uuid(), 'CYBERSECURITY', 'Unpatched CVEs', 'Critical vulnerabilities', 'count', 0.95, 1, 'Vuln Scanner', '1 day', NULL),
(gen_random_uuid(), 'CYBERSECURITY', 'MFA Coverage', '% users with MFA', 'percentage', 0.9, 0.95, 'IAM System', '1 week', NULL),
(gen_random_uuid(), 'CYBERSECURITY', 'Phishing Fail Rate', 'Failed simulated tests', 'percentage', 0.85, 0.2, 'Security Training', '1 month', NULL),
(gen_random_uuid(), 'CYBERSECURITY', 'Endpoint Encryption', 'Unencrypted devices', 'percentage', 0.9, 0.05, 'EDR System', '1 week', NULL),
(gen_random_uuid(), 'CYBERSECURITY', 'Privilege Escalations', 'Unauthorized attempts', 'count', 0.85, 0, 'SIEM', '1 day', NULL),
(gen_random_uuid(), 'CYBERSECURITY', 'Data Exfiltration', 'Abnormal outbound traffic', 'MB', 0.95, 100, 'DLP System', '1 hour', NULL),
(gen_random_uuid(), 'CYBERSECURITY', 'Third-Party Access', 'Overprivileged vendors', 'count', 0.8, 3, 'PAM System', '1 week', NULL),
(gen_random_uuid(), 'CYBERSECURITY', 'Cloud Misconfigs', 'High-risk CSPM findings', 'count', 0.85, 0, 'CSPM Tool', '1 day', NULL),
(gen_random_uuid(), 'CYBERSECURITY', 'Backup Success Rate', 'Failed backup jobs', 'percentage', 0.9, 0.05, 'Backup System', '1 day', NULL),
(gen_random_uuid(), 'CYBERSECURITY', 'SOC Alert Backlog', 'Unprocessed alerts', 'count', 0.75, 50, 'SIEM', '1 day', NULL),
(gen_random_uuid(), 'CYBERSECURITY', 'Password Strength', 'Weak credential ratio', 'percentage', 0.8, 0.1, 'IAM Scan', '1 week', NULL),
(gen_random_uuid(), 'CYBERSECURITY', 'RDP Exposure', 'Internet-facing RDP', 'count', 0.9, 0, 'Network Scan', '1 day', NULL),
(gen_random_uuid(), 'CYBERSECURITY', 'Shadow IT', 'Unauthorized SaaS apps', 'count', 0.7, 5, 'CASB', '1 month', NULL),
(gen_random_uuid(), 'CYBERSECURITY', 'Container Vulnerabilities', 'Critical image flaws', 'count', 0.85, 1, 'Registry Scan', '1 week', NULL),
(gen_random_uuid(), 'CYBERSECURITY', 'Email Spoofing', 'Failed DMARC checks', 'percentage', 0.8, 0.01, 'Email Gateway', '1 day', NULL);

-- Compliance Risk Indicators (8)
INSERT INTO it_governance.risk_indicators VALUES
(gen_random_uuid(), 'COMPLIANCE', 'Regulatory Fines', 'Open penalty amount', 'currency', 0.9, 0, 'Legal System', '1 quarter', NULL),
(gen_random_uuid(), 'COMPLIANCE', 'Policy Violations', 'Non-compliant incidents', 'count', 0.85, 3, 'GRC System', '1 month', NULL),
(gen_random_uuid(), 'COMPLIANCE', 'Audit Findings', 'Unresolved deficiencies', 'count', 0.9, 1, 'Audit Mgmt', '6 months', NULL),
(gen_random_uuid(), 'COMPLIANCE', 'Certification Lapses', 'Expired certifications', 'count', 0.8, 0, 'Asset DB', '1 month', NULL),
(gen_random_uuid(), 'COMPLIANCE', 'Data Subject Requests', 'Overdue responses', 'count', 0.85, 1, 'Privacy System', '1 week', NULL),
(gen_random_uuid(), 'COMPLIANCE', 'Export Control Violations', 'High-risk shipments', 'count', 0.9, 0, 'Trade System', '1 day', NULL),
(gen_random_uuid(), 'COMPLIANCE', 'Record Retention Gaps', 'Improperly destroyed', 'count', 0.75, 5, 'Records Mgmt', '1 quarter', NULL),
(gen_random_uuid(), 'COMPLIANCE', 'Whistleblower Cases', 'Open investigations', 'count', 0.8, 1, 'Ethics System', '1 month', NULL);

-- Reputational Risk Indicators (5)
INSERT INTO it_governance.risk_indicators VALUES
(gen_random_uuid(), 'REPUTATIONAL', 'Social Sentiment', 'Negative mentions', 'count', 0.7, 100, 'Media Monitor', '1 day', NULL),
(gen_random_uuid(), 'REPUTATIONAL', 'Customer Complaints', 'Escalated issues', 'count', 0.75, 10, 'CRM System', '1 week', NULL),
(gen_random_uuid(), 'REPUTATIONAL', 'ESG Controversies', 'Public ESG incidents', 'count', 0.8, 1, 'ESG Platform', '1 month', NULL),
(gen_random_uuid(), 'REPUTATIONAL', 'Executive Misconduct', 'Public allegations', 'count', 0.85, 0, 'Media Scan', '1 day', NULL),
(gen_random_uuid(), 'REPUTATIONAL', 'Brand Impersonation', 'Fake domains detected', 'count', 0.7, 1, 'Brand Protect', '1 week', NULL);

-- Geographic Risk Indicators (4)
INSERT INTO it_governance.risk_indicators VALUES
(gen_random_uuid(), 'GEOGRAPHIC', 'Political Stability', 'Country risk index', 'index', 0.75, 30, 'Risk Monitor', '1 month', NULL),
(gen_random_uuid(), 'GEOGRAPHIC', 'Natural Disasters', 'Facility exposure score', 'score', 0.8, 7, 'Climate API', '3 months', NULL),
(gen_random_uuid(), 'GEOGRAPHIC', 'Regulatory Changes', 'Major new laws', 'count', 0.7, 3, 'Regulatory API', '1 month', NULL),
(gen_random_uuid(), 'GEOGRAPHIC', 'Sanctions Exposure', 'Prohibited transactions', 'count', 0.9, 0, 'Screening Tool', '1 day', NULL);

-- Supply Chain Risk Indicators (6)
INSERT INTO it_governance.risk_indicators VALUES
(gen_random_uuid(), 'SUPPLY_CHAIN', 'Single Source Items', 'No alternative suppliers', 'count', 0.8, 5, 'SCM System', '1 quarter', NULL),
(gen_random_uuid(), 'SUPPLY_CHAIN', 'Delivery Delays', 'On-time performance', 'percentage', 0.75, 0.9, 'Logistics', '1 week', NULL),
(gen_random_uuid(), 'SUPPLY_CHAIN', 'Inventory Turns', 'Low turnover items', 'count', 0.7, 20, 'WMS System', '1 month', NULL),
(gen_random_uuid(), 'SUPPLY_CHAIN', 'Supplier Financials', 'High-risk suppliers', 'count', 0.85, 3, 'Credit API', '1 quarter', NULL),
(gen_random_uuid(), 'SUPPLY_CHAIN', 'Geographic Concentration', 'Single-region suppliers', 'percentage', 0.75, 0.5, 'SCM Analysis', '6 months', NULL),
(gen_random_uuid(), 'SUPPLY_CHAIN', 'Quality Rejects', 'Defective materials', 'percentage', 0.8, 0.05, 'QC System', '1 week', NULL);


--indicator coverage reports
SELECT
    category,
    COUNT(*) AS indicator_count,
    SUM(CASE WHEN is_active THEN 1 ELSE 0 END) AS active_indicators,
    MIN(refresh_frequency) AS min_frequency,
    MAX(criticality_weight) AS max_weight
FROM
    it_governance.risk_indicators
GROUP BY
    category
ORDER BY
    indicator_count DESC;

-- to perform data completeness check
-- Verify all 10 categories have indicators
SELECT DISTINCT category FROM it_governance.risk_indicators;

-- Check measurement coverage
SELECT
    ri.category,
    COUNT(DISTINCT ri.id) AS defined_indicators,
    COUNT(DISTINCT rm.measurement_id) AS measurements
FROM
    it_governance.risk_indicators ri
LEFT JOIN
    it_governance.risk_measurements rm ON ri.id = rm.indicator_id
GROUP BY
    ri.category;


-- measurement completeness check
SELECT
    ri.category,
    COUNT(DISTINCT ri.indicator_id) AS defined_indicators,
    COUNT(DISTINCT rm.indicator_id) AS measured_indicators,
    ROUND(100.0 * COUNT(DISTINCT rm.indicator_id) /
          NULLIF(COUNT(DISTINCT ri.indicator_id), 0), 1) AS coverage_pct
FROM
    it_governance.risk_indicators ri
LEFT JOIN
    it_governance.risk_measurements rm ON ri.indicator_id = rm.indicator_id
    AND rm.measurement_date > CURRENT_DATE - INTERVAL '30 days'
GROUP BY
    ri.category;
--threshold breach detection
CREATE OR REPLACE FUNCTION it_governance.detect_threshold_breaches()
RETURNS TRIGGER AS $$
BEGIN
    INSERT INTO it_governance.risk_alerts (
        indicator_id,
        entity_id,
        entity_type,
        measured_value,
        threshold_value,
        severity
    )
    SELECT
        NEW.indicator_id,
        NEW.entity_id,
        NEW.entity_type,
        NEW.measured_value,
        ri.critical_threshold,
        CASE
            WHEN NEW.measured_value > ri.critical_threshold * 1.5 THEN 'CRITICAL'
            WHEN NEW.measured_value > ri.critical_threshold THEN 'HIGH'
            ELSE NULL
        END
    FROM
        it_governance.risk_indicators ri
    WHERE
        ri.indicator_id = NEW.indicator_id
        AND ri.critical_threshold IS NOT NULL
        AND NEW.measured_value > ri.critical_threshold;

    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trg_threshold_breaches
AFTER INSERT ON it_governance.risk_measurements
FOR EACH ROW EXECUTE FUNCTION it_governance.detect_threshold_breaches();


-- threshold alerting validation

-- Test financial risk alerting
CALL it_governance.calculate_entity_risk(
    p_entity_id := 1023,  -- Example vendor
    p_entity_type := 'VENDOR'
);

-- Verify alert generation
SELECT * FROM it_governance.risk_alerts
WHERE entity_id = 1023
ORDER BY alert_time DESC LIMIT 5;

--risk exposure heatmap
CREATE VIEW it_governance.risk_exposure_heatmap AS
SELECT
    e.entity_name,
    ri.category,
    COUNT(*) FILTER (WHERE rm.measured_value > ri.critical_threshold) AS red_indicators,
    AVG(rm.measured_value) AS avg_risk_score
FROM
    it_governance.risk_measurements rm
JOIN
    it_governance.risk_indicators ri ON rm.indicator_id = ri.id
JOIN
    (SELECT id, name AS entity_name FROM vendors
     UNION SELECT id, hostname AS entity_name FROM it_assets) e ON rm.entity_id = e.id
GROUP BY
    e.entity_name, ri.category;

--control effectiveness correlation
CREATE MATERIALIZED VIEW it_governance.risk_control_correlation
WITH (timescaledb.continuous) AS
SELECT
    r.category,
    c.control_name,
    CORR(r.measured_value, c.effectiveness_rating) AS impact_correlation
FROM
    it_governance.risk_measurements r
JOIN
    it_governance.risk_control_mapping m ON r.indicator_id = m.indicator_id
JOIN
    it_governance.controls c ON m.control_id = c.id
GROUP BY
    r.category, c.control_name
WITH DATA;

-- AI controlled  monitoring engine
-- BUSINESS CASE: Detects % of control failures before incidents occur
-- COMPLIANCE LINK: NIST SP 800-53 Continuous Monitoring
-- https://nist-sp-800-53-r5.bsafes.com/docs/3-4-assessment-authorization-and-monitoring/ca-7-continuous-monitoring/
CREATE TABLE it_governance.ai_control_monitoring (
    monitor_id UUID PRIMARY KEY,
    control_id INTEGER NOT NULL REFERENCES it_governance.controls(control_id),
    ml_model VARCHAR(50) NOT NULL CHECK (ml_model IN ('LSTM','Prophet','IsolationForest')),
    features_used JSONB NOT NULL,
    training_interval INTERVAL NOT NULL,
    last_trained TIMESTAMPTZ,
    anomaly_threshold DECIMAL(3,2) NOT NULL,
    status VARCHAR(20) CHECK (status IN ('ACTIVE','TRAINING','DRIFTED'))
    PARTITION BY LIST (status);

COMMENT ON TABLE it_governance.ai_control_monitoring IS 'AI models for continuous control effectiveness monitoring';

--blockchain policy compliance ledger
-- BUSINESS CASE: Provides court-admissible evidence of compliance efforts
-- COMPLIANCE LINK: SEC Rule 17a-4(f) Blockchain Recordkeeping
-- https://cdn.prod.website-files.com/64d387dce73db7f113fe8242/65288617264dedce350cfe18_eBook%20-%20Blockchain%20verify%20(3).pdf
CREATE TABLE it_governance.blockchain_policy_compliance (
    block_id UUID PRIMARY KEY,
    policy_id INTEGER NOT NULL REFERENCES it_governance.policies(policy_id),
    compliance_state VARCHAR(20) NOT NULL,
    cryptographic_hash TEXT NOT NULL,
    previous_hash TEXT NOT NULL,
    block_timestamp TIMESTAMPTZ NOT NULL,
    validator_nodes TEXT[] NOT NULL
) PARTITION BY RANGE (block_timestamp);

CREATE INDEX idx_bc_policy_tx ON it_governance.blockchain_policy_compliance USING HASH (cryptographic_hash);

COMMENT ON TABLE it_governance.blockchain_policy_compliance IS 'Immutable record of policy attestations and exceptions';


--Autonomous Remediation Bots
-- Business case: to resolve low risk findings without human intervention
CREATE TABLE it_governance.remediation_bots (
    bot_id UUID PRIMARY KEY,
    risk_pattern JSONB NOT NULL,  -- Condition specification
    action_script TEXT NOT NULL,  -- Terraform/Ansible playbook
    approval_required BOOLEAN DEFAULT TRUE,
    success_rate DECIMAL(5,4) GENERATED ALWAYS AS (
        (execution_stats->>'success')::NUMERIC /
        NULLIF((execution_stats->>'attempts')::NUMERIC, 0)
    ) STORED,
    execution_stats JSONB DEFAULT '{"attempts":0, "success":0}',
    last_verified TIMESTAMPTZ
);

COMMENT ON TABLE it_governance.remediation_bots IS 'Self-healing automation for common control failures';


--advanced analytics modules
-- predictive vendor risk scoring
-- to identify % of vendor risks before contract renewals
CREATE MATERIALIZED VIEW it_governance.vendor_risk_forecasts
WITH (timescaledb.continuous) AS
SELECT
    vendor_id,
    time_bucket('7 days', prediction_date) AS week,
    AVG(risk_score) AS avg_risk,
    MAX(risk_score) AS peak_risk,
    CORR(risk_score, financial_health) AS financial_correlation
FROM
    it_governance.vendor_risk_predictions
WHERE
    prediction_date BETWEEN NOW() AND NOW() + INTERVAL '6 months'
GROUP BY
    vendor_id, week
WITH DATA;

COMMENT ON MATERIALIZED VIEW it_governance.vendor_risk_forecasts IS '6-month forward-looking vendor risk projections';


--control effectiveness correlation engine
-- BUSINESS CASE: Optimizes based on a budget annual control investment -- we need to make this user defined threshold for budget
CREATE OR REPLACE PROCEDURE it_governance.analyze_control_impact()
LANGUAGE plpgsql
AS $$
BEGIN
    -- Calculate control-risk correlations
    INSERT INTO it_governance.control_impact_scores
    SELECT
        c.control_id,
        r.indicator_id,
        CORR(c.effectiveness_score, r.normalized_value) AS impact_score,
        COUNT(*) AS sample_size
    FROM
        it_governance.control_effectiveness_metrics c
    JOIN
        it_governance.risk_measurements_normalized r
        ON c.entity_id = r.entity_id
        AND c.measurement_date = r.measurement_date
    GROUP BY
        c.control_id, r.indicator_id
    ON CONFLICT (control_id, indicator_id)
    DO UPDATE SET
        impact_score = EXCLUDED.impact_score,
        sample_size = EXCLUDED.sample_size;

    -- Update control prioritization
    UPDATE it_governance.controls c
    SET priority_score = ci.impact_sum
    FROM (
        SELECT control_id, SUM(ABS(impact_score)) AS impact_sum
        FROM it_governance.control_impact_scores
        GROUP BY control_id
    ) ci
    WHERE c.control_id = ci.control_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.analyze_control_impact IS 'Quantifies each control\'s impact across risk indicators';

-- example new AI risk indicators

INSERT INTO it_governance.risk_indicators VALUES
(gen_random_uuid(), 'AI', 'Training Data Skew', 'Feature distribution shift', 'percentage', 0.9, 0.15, 'ML Monitor', '1 day', NULL),
(gen_random_uuid(), 'AI', 'Prediction Drift', 'Model output variance', 'sigma', 0.85, 2.5, 'Model Serving', '1 hour', NULL);

--verification and validation of blockchain audit trail veriifcation
CREATE OR REPLACE PROCEDURE it_governance.validate_blockchain_integrity()
LANGUAGE plpgsql
AS $$
DECLARE
    v_invalid_blocks INTEGER;
BEGIN
    WITH block_chain AS (
        SELECT
            block_id,
            cryptographic_hash,
            previous_hash,
            cryptographic_hash = sha3_512(previous_hash || policy_id::TEXT || compliance_state) AS valid
        FROM
            it_governance.blockchain_policy_compliance
        ORDER BY
            block_timestamp
    )
    SELECT COUNT(*) FILTER (WHERE NOT valid)
    INTO v_invalid_blocks
    FROM block_chain;

    IF v_invalid_blocks > 0 THEN
        INSERT INTO it_governance.security_incidents
        VALUES ('Blockchain Tampering', 'Critical', v_invalid_blocks || ' invalid blocks detected');
    END IF;
END;
$$;

-- AI model drift detection
CREATE TABLE it_governance.ml_drift_metrics (
    model_id UUID REFERENCES it_governance.ai_control_monitoring,
    measurement_date TIMESTAMPTZ,
    accuracy_drift DECIMAL(5,4),
    feature_drift DECIMAL(5,4),
    PRIMARY KEY (model_id, measurement_date)
);

CREATE OR REPLACE PROCEDURE it_governance.monitor_model_drift()
LANGUAGE plpgsql
AS $$
BEGIN
    INSERT INTO it_governance.ml_drift_metrics
    SELECT
        monitor_id,
        CURRENT_TIMESTAMP,
        calculate_accuracy_drift(features_used),
        calculate_feature_drift(features_used)
    FROM
        it_governance.ai_control_monitoring
    WHERE
        status = 'ACTIVE';

    UPDATE it_governance.ai_control_monitoring
    SET status = 'DRIFTED'
    WHERE monitor_id IN (
        SELECT model_id
        FROM it_governance.ml_drift_metrics
        WHERE accuracy_drift > 0.15
        AND measurement_date > CURRENT_TIMESTAMP - INTERVAL '1 week'
    );

    COMMIT;
END;
$$;

-- mermaidjs framework flow diagram
-- to proactively risk prevention through AI/ML monitoring
-- unbreakable compliance evidence via blockchain
-- self-optimizing controls with autonomous remediation
-- strategic risk intelligence from predictive analytics
-- --graph TD
--     A[AI Monitoring] --> B[Risk Prediction]
--     B --> C[Autonomous Remediation]
--     C --> D[Blockchain Verification]
--     D --> A

-- todo -- predictive compliance engine -- 90 day regulatory change anticipation
-- todo -- autonomous GRC(Governance,RISK, Compliance) - self managing policy framework
-- todo - regulatory forecasting to detect change in the new requirements to avoid any penalty and risks associated with it


--regulatory change preditor
-- BUSINESS CASE: Provides 3month  lead time for compliance preparation
CREATE TABLE it_governance.regulatory_forecasts (
    prediction_id UUID PRIMARY KEY,
    regulation_id INTEGER NOT NULL REFERENCES it_governance.compliance_requirements,
    predicted_change TEXT NOT NULL,
    confidence_score DECIMAL(3,2) NOT NULL,
    expected_effective_date DATE NOT NULL,
    impact_assessment JSONB NOT NULL,  -- Affected controls/business units
    source_evidence TEXT[] NOT NULL,  -- Legislative tracking IDs
    last_updated TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP
);

CREATE MATERIALIZED VIEW it_governance.regulatory_heatmap
WITH (timescaledb.continuous) AS
SELECT
    r.regulation_name,
    COUNT(*) AS pending_changes,
    AVG(f.confidence_score) AS avg_confidence,
    MIN(f.expected_effective_date) AS earliest_impact
FROM
    it_governance.regulatory_forecasts f
JOIN
    it_governance.compliance_requirements r ON f.regulation_id = r.requirement_id
WHERE
    f.expected_effective_date BETWEEN NOW() AND NOW() + INTERVAL '90 days'
GROUP BY
    r.regulation_name
WITH DATA;

COMMENT ON MATERIALIZED VIEW it_governance.regulatory_heatmap IS '90-day outlook of regulatory changes with business impact';


-- advanced functionality for autonomous policy engine
-- BUSINESS CASE: Reduces compliance gap window from 60 to 7 days
CREATE OR REPLACE PROCEDURE it_governance.auto_update_policies()
LANGUAGE plpgsql
AS $$
DECLARE
    v_change RECORD;
BEGIN
    FOR v_change IN
        SELECT * FROM it_governance.regulatory_forecasts
        WHERE confidence_score > 0.85
        AND expected_effective_date <= CURRENT_DATE + INTERVAL '30 days'
        AND NOT EXISTS (
            SELECT 1 FROM it_governance.policy_updates
            WHERE prediction_id = regulatory_forecasts.prediction_id
        )
    LOOP
        -- Generate policy draft using AI
        INSERT INTO it_governance.policy_drafts (
            regulation_id,
            draft_text,
            change_reason,
            source_prediction
        )
        SELECT
            v_change.regulation_id,
            ai_generate_policy_update(v_change.predicted_change),
            'Anticipatory update for ' || v_change.predicted_change,
            v_change.prediction_id;

        -- Log autonomous decision
        PERFORM it_governance.log_autonomous_decision(
            'POLICY_UPDATE',
            v_change.prediction_id::TEXT,
            jsonb_build_object(
                'regulation', v_change.regulation_id,
                'confidence', v_change.confidence_score
            )
        );
    END LOOP;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.auto_update_policies IS 'Automatically initiates policy updates for high-confidence regulatory forecasts';

-- control effectiveness optimizer
-- BUSINESS CASE: to maintain the current state and improvise on its effectiveness with fewer resources
-- for example- Maintains 99.9% control effectiveness with 30% fewer resources
CREATE OR REPLACE PROCEDURE it_governance.optimize_control_parameters()
LANGUAGE plpgsql
AS $$
DECLARE
    v_control RECORD;
BEGIN
    FOR v_control IN
        SELECT control_id, reward_function
        FROM it_governance.cognitive_controls
        WHERE last_optimized IS NULL
           OR last_optimized < CURRENT_TIMESTAMP - optimization_frequency
    LOOP
        -- Get latest performance metrics
        WITH control_stats AS (
            SELECT
                AVG(CASE WHEN test_result = 'PASS' THEN 1 ELSE 0 END) AS pass_rate,
                AVG(execution_cost) AS avg_cost,
                COUNT(*) FILTER (WHERE anomaly_detected) AS anomalies
            FROM it_governance.control_test_results
            WHERE control_id = v_control.control_id
            AND test_date > CURRENT_DATE - INTERVAL '30 days'
        )
        -- Update adaptive parameters using reinforcement learning
        UPDATE it_governance.cognitive_controls
        SET
            adaptive_parameters = ai_optimize_parameters(
                current_config,
                v_control.reward_function,
                (SELECT pass_rate FROM control_stats),
                (SELECT avg_cost FROM control_stats),
                (SELECT anomalies FROM control_stats)
            ),
            last_optimized = CURRENT_TIMESTAMP
        WHERE control_id = v_control.control_id;
    END LOOP;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.optimize_control_parameters IS 'Periodically tunes control parameters using reinforcement learning';

-- workflow --mermaidjs
-- mermaidjs diagram code
-- graph LR
--     A[Risk Detection] --> B[AI Analysis]
--     B --> C[Autonomous Action]
--     C --> D[Blockchain Verification]
--     D --> A


-- todo -- Autonomous Maturity framework
-- transformational governance capabilities like - self healing GRC Ecosystem- Autonomous policy-risk control alignment
-- realtime compliance certicication -- continuous audit readiness
-- using AI for Governance orchestration - ethical AI oversight integrated with corproate governance
-- KPI metrics - % reduction in manual processes, resulting in $xM in annual efficiency savings
--KPI metrics - % audit readiness in real-time , resulting in elimination of $xM audit prep costs
-- KPI metrics -- AI Governance baords -- % faster AI model approvals -- resulting in $xM in innovation accelerations

-- version 3 -- Awase July 2nd 2025
-- too much of literature and guidelines to follow, seeing new requirements at various reference literature

-- BUSINESS CASE: Reduces policy maintenance costs by 70% while improving coverage
-- COMPLIANCE LINK: ISO 31000 Clause 6.4 (Dynamic Risk Treatment) https://studylib.net/doc/26126608/iso-31000
--https://preteshbiswas.com/2024/01/04/iso-310002018-clause-6-4-4-risk-evaluation/
CREATE TABLE it_governance.autonomous_policies (
    policy_id UUID PRIMARY KEY,
    policy_name VARCHAR(200) NOT NULL,
    digital_twin_id UUID NOT NULL,  -- ML model reference
    current_version INTEGER NOT NULL,
    optimization_parameters JSONB NOT NULL,
    last_optimized TIMESTAMPTZ,
    optimization_cycle INTERVAL NOT NULL DEFAULT '7 days',
    health_score DECIMAL(3,2) GENERATED ALWAYS AS (
        (effectiveness_metrics->>'compliance_coverage')::DECIMAL * 0.6 +
        (effectiveness_metrics->>'risk_reduction')::DECIMAL * 0.4
    ) STORED,
    effectiveness_metrics JSONB NOT NULL
) PARTITION BY RANGE (health_score);

COMMENT ON TABLE it_governance.autonomous_policies IS 'Self-adapting policies that auto-update based on effectiveness metrics';

-- policy optimization procedure
CREATE OR REPLACE PROCEDURE it_governance.optimize_policy_framework()
LANGUAGE plpgsql
AS $$
DECLARE
    v_policy RECORD;
BEGIN
    FOR v_policy IN
        SELECT policy_id, digital_twin_id
        FROM it_governance.autonomous_policies
        WHERE last_optimized IS NULL
           OR last_optimized < CURRENT_TIMESTAMP - optimization_cycle
    LOOP
        -- Get latest performance data
        WITH policy_stats AS (
            SELECT
                COUNT(DISTINCT r.requirement_id) * 100.0 /
                    (SELECT COUNT(*) FROM it_governance.compliance_requirements) AS compliance_coverage,
                AVG(100 - rr.residual_risk_score) AS risk_reduction
            FROM
                it_governance.policy_requirements pr
            JOIN
                it_governance.compliance_requirements r ON pr.requirement_id = r.requirement_id
            JOIN
                it_governance.risk_reduction rr ON pr.policy_id = rr.policy_id
            WHERE
                pr.policy_id = v_policy.policy_id
        )
        -- Update policy through digital twin
        UPDATE it_governance.autonomous_policies
        SET
            effectiveness_metrics = jsonb_build_object(
                'compliance_coverage', (SELECT compliance_coverage FROM policy_stats),
                'risk_reduction', (SELECT risk_reduction FROM policy_stats)
            ),
            current_version = current_version + 1,
            optimization_parameters = ai_optimize_policy(
                digital_twin_id,
                effectiveness_metrics
            ),
            last_optimized = CURRENT_TIMESTAMP
        WHERE
            policy_id = v_policy.policy_id;

        -- Log autonomous update
        PERFORM it_governance.log_autonomous_decision(
            'POLICY_OPTIMIZATION',
            v_policy.policy_id::TEXT,
            jsonb_build_object(
                'version', v_policy.current_version + 1,
                'health_score', (SELECT health_score FROM it_governance.autonomous_policies
                                WHERE policy_id = v_policy.policy_id)
            )
        );
    END LOOP;

    COMMIT;
END;
$$;

-- continuous compliance certification -- real-time compliance state
-- BUSINESS CASE: Eliminates surprise audit findings with continuous gap detection
-- COMPLIANCE LINK: SOC 2 Type 2 Continuous Monitoring
---https://www.isms.online/soc-2/controls/monitoring-activities-cc4-2-explained/
CREATE TABLE it_governance.certification_states (
    certification_id UUID PRIMARY KEY,
    framework_id INTEGER NOT NULL REFERENCES it_governance.governance_frameworks,
    certification_level VARCHAR(50) NOT NULL,
    start_date TIMESTAMPTZ NOT NULL,
    end_date TIMESTAMPTZ GENERATED ALWAYS AS (
        start_date + (SELECT validity_period
                     FROM it_governance.governance_frameworks
                     WHERE framework_id = certification_states.framework_id)
    ) STORED,
    current_score DECIMAL(5,2) NOT NULL,
    evidence_links JSONB NOT NULL,  -- Blockchain transaction IDs
    auto_renewal BOOLEAN NOT NULL DEFAULT TRUE
) WITH (autovacuum_enabled = false);

CREATE MATERIALIZED VIEW it_governance.compliance_radar
WITH (timescaledb.continuous) AS
SELECT
    f.framework_name,
    COUNT(*) FILTER (WHERE cs.current_score >= 90) AS fully_compliant,
    COUNT(*) FILTER (WHERE cs.current_score BETWEEN 75 AND 89) AS partially_compliant,
    COUNT(*) FILTER (WHERE cs.current_score < 75) AS non_compliant,
    AVG(cs.current_score) AS avg_score
FROM
    it_governance.certification_states cs
JOIN
    it_governance.governance_frameworks f ON cs.framework_id = f.framework_id
WHERE
    cs.end_date > CURRENT_DATE
GROUP BY
    f.framework_name
WITH DATA;

COMMENT ON MATERIALIZED VIEW it_governance.compliance_radar IS 'Real-time view of compliance posture across all frameworks';


--Automated Certification Renewal
CREATE OR REPLACE PROCEDURE it_governance.auto_renew_certifications()
LANGUAGE plpgsql
AS $$
DECLARE
    v_cert RECORD;
BEGIN
    FOR v_cert IN
        SELECT cs.certification_id, cs.framework_id
        FROM it_governance.certification_states cs
        WHERE cs.auto_renewal = TRUE
        AND cs.end_date BETWEEN CURRENT_DATE AND CURRENT_DATE + INTERVAL '30 days'
        AND NOT EXISTS (
            SELECT 1
            FROM it_governance.certification_states new
            WHERE new.framework_id = cs.framework_id
            AND new.start_date > cs.start_date
        )
    LOOP
        -- Evaluate current compliance
        DECLARE
            v_score DECIMAL(5,2);
            v_evidence JSONB;
        BEGIN
            SELECT
                AVG(control_effectiveness) * 100,
                jsonb_agg(blockchain_proof)
            INTO
                v_score,
                v_evidence
            FROM (
                SELECT
                    cf.control_id,
                    AVG(CASE WHEN ctr.test_result = 'PASS' THEN 1 ELSE 0 END) AS control_effectiveness,
                    (SELECT array_agg(transaction_hash)
                     FROM it_governance.blockchain_audit_trail
                     WHERE related_record_id = cf.control_id
                     AND record_type = 'CONTROL_TEST') AS blockchain_proof
                FROM
                    it_governance.control_framework_mapping cf
                JOIN
                    it_governance.control_test_results ctr ON cf.control_id = ctr.control_id
                WHERE
                    cf.framework_id = v_cert.framework_id
                    AND ctr.test_date > CURRENT_DATE - INTERVAL '90 days'
                GROUP BY
                    cf.control_id
            ) control_stats;

            -- Issue new certification
            INSERT INTO it_governance.certification_states
            VALUES (
                gen_random_uuid(),
                v_cert.framework_id,
                CASE
                    WHEN v_score >= 90 THEN 'FULL'
                    WHEN v_score >= 75 THEN 'PARTIAL'
                    ELSE 'CONDITIONAL'
                END,
                CURRENT_TIMESTAMP,
                v_score,
                v_evidence,
                TRUE
            );
        END;
    END LOOP;

    COMMIT;
END;
$$;



-- AI Governance Board Integration
--Ethical AI Oversight Framework
-- BUSINESS CASE: Ensures 100% auditability of AI model approvals and restrictions
-- COMPLIANCE LINK: EU AI Act Article 56 (Governance Requirements) https://artificialintelligenceact.eu/article/56/
CREATE TABLE it_governance.ai_governance_decisions (
    decision_id UUID PRIMARY KEY,
    model_id UUID NOT NULL REFERENCES it_governance.ai_models,
    decision_type VARCHAR(50) NOT NULL CHECK (decision_type IN (
        'APPROVAL','MODIFICATION','RESTRICTION','SUNSET')),
    decision_date TIMESTAMPTZ NOT NULL,
    effective_date TIMESTAMPTZ NOT NULL,
    board_members UUID[] NOT NULL,  -- Voting members
    decision_factors JSONB NOT NULL,  -- Bias metrics, risk assessments
    implementation_plan TEXT NOT NULL,
    review_cycle INTERVAL NOT NULL
);

CREATE TABLE it_governance.ai_impact_monitoring (
    monitor_id UUID PRIMARY KEY,
    model_id UUID NOT NULL REFERENCES it_governance.ai_models,
    metric_name VARCHAR(100) NOT NULL,
    baseline_value NUMERIC NOT NULL,
    current_value NUMERIC NOT NULL,
    drift_score DECIMAL(5,2) GENERATED ALWAYS AS (
        ABS(current_value - baseline_value) / NULLIF(baseline_value, 0)
    ) STORED,
    measurement_date TIMESTAMPTZ NOT NULL
) PARTITION BY RANGE (measurement_date);

COMMENT ON TABLE it_governance.ai_governance_decisions IS 'Formal records of AI governance board rulings';


-- AI Governance Dashboard
-- BUSINESS CASE: Provides C-suite visibility into AI risk management
CREATE MATERIALIZED VIEW it_governance.ai_governance_oversight
WITH (timescaledb.continuous) AS
SELECT
    m.model_name,
    m.model_type,
    d.decision_type AS current_status,
    COUNT(im.monitor_id) AS active_monitors,
    AVG(im.drift_score) AS avg_drift,
    MAX(d.decision_date) AS last_review
FROM
    it_governance.ai_models m
LEFT JOIN
    it_governance.ai_governance_decisions d ON m.model_id = d.model_id
    AND d.decision_date = (
        SELECT MAX(decision_date)
        FROM it_governance.ai_governance_decisions
        WHERE model_id = m.model_id
    )
LEFT JOIN
    it_governance.ai_impact_monitoring im ON m.model_id = im.model_id
    AND im.measurement_date > CURRENT_DATE - INTERVAL '30 days'
GROUP BY
    m.model_name, m.model_type, d.decision_type
WITH DATA;

COMMENT ON MATERIALIZED VIEW it_governance.ai_governance_oversight IS 'Board-level view of AI model governance status';


-- to do -- identify and deploy self-optimizing policies
-- to do -- implement continuous SOC2 monitoring
-- to do -- establish AI govenrnace  board charter
-- Enterprise Scaling -- expand to n of autonomous controls and measures
--- ISO 27001 real-time certification
-- integrated with board managemetn systems
-- achieve near 100 policy automation
-- AI driven audit certificaiton
-- secure governance records -- using blockchain/ quantum ...?
-- zero-touch compliance - automated evidence collection and reporting
-- adaptive controls -- dynamic adjustment to risk environments
-- ethical AI lifecycle -- Governance Integrated from development to decommissioning
-- future proof architecture with blockchain or quantum ready cryptographic integrity
--workflow mermaidjs
--mermaidjs code
-- --graph TD
--     A[Regulatory Changes] --> B[Autonomous Policies]
--     B --> C[Continuous Certification]
--     C --> D[AI Governance Board]
--     D --> A

-- autonomous policy validation
CREATE OR REPLACE PROCEDURE it_governance.validate_autonomous_decisions()
LANGUAGE plpgsql
AS $$
DECLARE
    v_violations INTEGER;
BEGIN
    SELECT COUNT(*)
    INTO v_violations
    FROM (
        SELECT
            ap.policy_id,
            COUNT(*) FILTER (WHERE cr.requirement_id IS NULL) AS missing_requirements
        FROM
            it_governance.autonomous_policies ap
        LEFT JOIN
            it_governance.policy_requirements pr ON ap.policy_id = pr.policy_id
        LEFT JOIN
            it_governance.compliance_requirements cr ON pr.requirement_id = cr.requirement_id
        GROUP BY
            ap.policy_id
        HAVING
            COUNT(*) FILTER (WHERE cr.requirement_id IS NULL) > 0
    ) gaps;

    IF v_violations > 0 THEN
        INSERT INTO it_governance.governance_incidents
        VALUES (
            'AUTONOMOUS_POLICY_GAP',
            'HIGH',
            v_violations || ' policies with compliance coverage gaps',
            jsonb_build_object('validation_time', CURRENT_TIMESTAMP)
        );
    END IF;
END;
$$;

-- audit trail of AI Governance
CREATE OR REPLACE FUNCTION it_governance.log_ai_decision_chain()
RETURNS TRIGGER AS $$
BEGIN
    INSERT INTO it_governance.quantum_ledger
    VALUES (
        gen_random_uuid(),
        'AI_GOVERNANCE',
        quantum_sign(NEW.decision_id::TEXT),
        lattice_proof(NEW.decision_id::TEXT),
        sha3_512(NEW::TEXT),
        (SELECT cryptographic_hash
         FROM it_governance.quantum_ledger
         ORDER BY block_timestamp DESC LIMIT 1),
        CURRENT_TIMESTAMP
    );

    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trg_ai_governance_ledger
AFTER INSERT ON it_governance.ai_governance_decisions
FOR EACH ROW EXECUTE FUNCTION it_governance.log_ai_decision_chain();

-- AI powered policy optimization engine
-- BUSINESS CASE: Achieves % better risk coverage with % fewer policy documents
-- COMPLIANCE LINK: NIST AI RMF (Governance of AI Systems) https://www.nist.gov/itl/ai-risk-management-framework
CREATE TABLE it_governance.ai_policy_optimizer (
    optimizer_id UUID PRIMARY KEY,
    policy_id UUID NOT NULL REFERENCES it_governance.policies,
    ml_model VARCHAR(50) NOT NULL CHECK (ml_model IN ('LLM','RL','GAN')),
    current_version INTEGER NOT NULL DEFAULT 1,
    training_data_range DATERANGE NOT NULL,
    effectiveness_metrics JSONB NOT NULL,  -- {compliance_coverage, risk_reduction, efficiency_gain}
    last_retrained TIMESTAMPTZ,
    next_retrain TIMESTAMPTZ GENERATED ALWAYS AS (
        last_retrained + (SELECT training_interval FROM it_governance.ai_models
                         WHERE model_type = 'POLICY_OPTIMIZER')
    ) STORED,
    CONSTRAINT valid_metrics CHECK (
        effectiveness_metrics->>'compliance_coverage' IS NOT NULL AND
        effectiveness_metrics->>'risk_reduction' IS NOT NULL
    )
) PARTITION BY RANGE (last_retrained);

COMMENT ON TABLE it_governance.ai_policy_optimizer IS 'AI models that continuously improve policy effectiveness';


-- Policy optimization procedure
-- BUSINESS CASE: Reduces policy review cycles from quarterly to continuous
CREATE OR REPLACE PROCEDURE it_governance.optimize_policies_with_ai()
LANGUAGE plpgsql
AS $$
DECLARE
    v_policy RECORD;
    v_improvement_metrics JSONB;
BEGIN
    FOR v_policy IN
        SELECT p.policy_id, o.optimizer_id
        FROM it_governance.policies p
        JOIN it_governance.ai_policy_optimizer o ON p.policy_id = o.policy_id
        WHERE o.next_retrain <= CURRENT_TIMESTAMP
           OR o.last_retrained IS NULL
    LOOP
        -- Analyze policy performance
        SELECT jsonb_build_object(
            'compliance_coverage', calc_compliance_coverage(v_policy.policy_id),
            'risk_reduction', calc_risk_reduction(v_policy.policy_id),
            'efficiency_gain', calc_efficiency_metrics(v_policy.policy_id)
        ) INTO v_improvement_metrics;

        -- Retrain AI model
        PERFORM ai_retrain_policy_model(
            v_policy.optimizer_id,
            v_improvement_metrics
        );

        -- Generate optimized policy draft
        INSERT INTO it_governance.policy_versions (
            policy_id,
            version_text,
            generated_by,
            source_metrics
        )
        VALUES (
            v_policy.policy_id,
            ai_generate_policy(v_policy.optimizer_id),
            'AI_OPTIMIZER',
            v_improvement_metrics
        );

        -- Update optimization tracker
        UPDATE it_governance.ai_policy_optimizer
        SET
            effectiveness_metrics = v_improvement_metrics,
            current_version = current_version + 1,
            last_retrained = CURRENT_TIMESTAMP
        WHERE optimizer_id = v_policy.optimizer_id;
    END LOOP;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.optimize_policies_with_ai IS 'Automatically improves policies using machine learning';


-- Compliance Evidence Capture
-- Immutable Evidence Ledger
-- BUSINESS CASE: Provides 100% tamper-proof evidence for regulators
-- COMPLIANCE LINK: SEC Rule 17a-4(f) (Electronic Storage Media) https://www.finra.org/sites/default/files/2022-12/rule-17a-4-amendments.pdf -- nice to have -- first implementation, need to check quantum python features for cryptography
CREATE TABLE it_governance.quantum_compliance_evidence (
    evidence_id UUID PRIMARY KEY,
    control_id UUID NOT NULL REFERENCES it_governance.controls,
    evidence_type VARCHAR(50) NOT NULL CHECK (evidence_type IN (
        'TEST_RESULT','CONFIGURATION','AUDIT_TRAIL')),
    quantum_hash BYTEA NOT NULL,  -- CRYSTALS-Dilithium signature
    lattice_proof BYTEA NOT NULL,  -- Falcon-1024 proof
    evidence_snapshot BYTEA NOT NULL,  -- Zstandard-compressed evidence
    validator_nodes TEXT[] NOT NULL,
    block_timestamp TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP,
    previous_block_hash BYTEA NOT NULL
) PARTITION BY RANGE (block_timestamp);

CREATE INDEX idx_quantum_evidence_control ON it_governance.quantum_compliance_evidence(control_id);

COMMENT ON TABLE it_governance.quantum_compliance_evidence IS 'Post-quantum cryptographic evidence storage';


-- evidence verification function
CREATE OR REPLACE FUNCTION it_governance.verify_quantum_evidence(p_evidence_id UUID)
RETURNS BOOLEAN
LANGUAGE plpgsql
AS $$
DECLARE
    v_current RECORD;
    v_previous_hash BYTEA;
    v_valid BOOLEAN;
BEGIN
    SELECT quantum_hash, lattice_proof, previous_block_hash, evidence_snapshot
    INTO v_current
    FROM it_governance.quantum_compliance_evidence
    WHERE evidence_id = p_evidence_id;

    -- Verify lattice-based proof
    v_valid := lattice_verify(v_current.lattice_proof, v_current.evidence_snapshot);

    -- Verify hash chain
    IF v_valid THEN
        SELECT cryptographic_hash INTO v_previous_hash
        FROM it_governance.quantum_compliance_evidence
        WHERE quantum_hash = v_current.previous_block_hash;

        v_valid := (v_previous_hash IS NOT NULL);
    END IF;

    RETURN v_valid;
END;
$$;

COMMENT ON FUNCTION it_governance.verify_quantum_evidence IS 'Validates quantum-proof evidence integrity';

--Autonomous control remediation
-- to build an autonomous remeidation bot registry
-- BUSINESS CASE: Reduces mean time to repair (MTTR) for control failures from 48hrs to 15min
CREATE TABLE it_governance.autonomous_remediation_bots (
    bot_id UUID PRIMARY KEY,
    control_category VARCHAR(50) NOT NULL,
    trigger_condition JSONB NOT NULL,  -- SQL-like condition syntax
    remediation_script TEXT NOT NULL,  -- Ansible/Terraform/Python
    execution_constraints JSONB NOT NULL,  -- {time_window,approval_threshold}
    success_rate DECIMAL(5,4) GENERATED ALWAYS AS (
        (performance_metrics->>'success')::DECIMAL /
        NULLIF((performance_metrics->>'attempts')::DECIMAL, 0)
    ) STORED,
    performance_metrics JSONB DEFAULT '{"attempts":0,"success":0}',
    last_activated TIMESTAMPTZ
) PARTITION BY LIST (control_category);

COMMENT ON TABLE it_governance.autonomous_remediation_bots IS 'Self-healing bots that automatically fix control failures';


-- BOT Execution Engine
CREATE OR REPLACE PROCEDURE it_governance.execute_remediation_bots()
LANGUAGE plpgsql
AS $$
DECLARE
    v_bot RECORD;
    v_control RECORD;
    v_result BOOLEAN;
BEGIN
    FOR v_bot IN
        SELECT bot_id, trigger_condition, remediation_script
        FROM it_governance.autonomous_remediation_bots
        WHERE (performance_metrics->>'attempts')::INTEGER < 1000  -- Safety limit
        OR last_activated < CURRENT_TIMESTAMP - INTERVAL '1 hour'
    LOOP
        -- Find matching control failures
        FOR v_control IN
            EXECUTE format('SELECT control_id FROM it_governance.control_failures WHERE %s',
                          jsonb_to_sql_condition(v_bot.trigger_condition))
        LOOP
            -- Execute remediation in sandbox
            v_result := sandbox_execute(v_bot.remediation_script, v_control.control_id);

            -- Update bot performance
            UPDATE it_governance.autonomous_remediation_bots
            SET
                performance_metrics = jsonb_set(
                    performance_metrics,
                    '{attempts}',
                    to_jsonb((performance_metrics->>'attempts')::INTEGER + 1)
                ),
                performance_metrics = CASE WHEN v_result THEN
                    jsonb_set(
                        performance_metrics,
                        '{success}',
                        to_jsonb((performance_metrics->>'success')::INTEGER + 1)
                    )
                    ELSE performance_metrics
                END,
                last_activated = CURRENT_TIMESTAMP
            WHERE bot_id = v_bot.bot_id;

            -- Log remediation
            INSERT INTO it_governance.remediation_logs
            VALUES (
                gen_random_uuid(),
                v_control.control_id,
                v_bot.bot_id,
                CURRENT_TIMESTAMP,
                v_result
            );
        END LOOP;
    END LOOP;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE it_governance.execute_remediation_bots IS 'Orchestrates autonomous control remediation';


-- Predictive Regulatory Change Management
-- Regualtory Intelligence hub
-- BUSINESS CASE: Provides 3-4 months lead time for compliance preparation
CREATE TABLE it_governance.regulatory_signals (
    signal_id UUID PRIMARY KEY,
    regulation_id INTEGER NOT NULL REFERENCES it_governance.compliance_requirements,
    signal_type VARCHAR(50) NOT NULL CHECK (signal_type IN (
        'LEGISLATIVE','ENFORCEMENT','GUIDANCE','INDUSTRY')),
    source_authority VARCHAR(100) NOT NULL,
    signal_date DATE NOT NULL,
    predicted_impact_date DATE NOT NULL,
    confidence_score DECIMAL(3,2) NOT NULL,
    affected_controls INTEGER[] NOT NULL,
    raw_content TEXT NOT NULL,
    analysis_summary JSONB NOT NULL,
    processed_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
) PARTITION BY RANGE (predicted_impact_date);

CREATE MATERIALIZED VIEW it_governance.regulatory_heatmap
WITH (timescaledb.continuous) AS
SELECT
    r.regulation_name,
    r.jurisdiction,
    COUNT(*) FILTER (WHERE s.predicted_impact_date BETWEEN CURRENT_DATE AND CURRENT_DATE + INTERVAL '90 days') AS imminent_changes,
    AVG(s.confidence_score) FILTER (WHERE s.predicted_impact_date BETWEEN CURRENT_DATE AND CURRENT_DATE + INTERVAL '90 days') AS avg_confidence,
    COUNT(DISTINCT s.affected_controls) AS impacted_controls
FROM
    it_governance.regulatory_signals s
JOIN
    it_governance.compliance_requirements r ON s.regulation_id = r.requirement_id
GROUP BY
    r.regulation_name, r.jurisdiction
WITH DATA;

COMMENT ON MATERIALIZED VIEW it_governance.regulatory_heatmap IS '90-day forecast of regulatory changes and control impacts';

-- to process regulatory signals
CREATE OR REPLACE PROCEDURE it_governance.process_regulatory_signals()
LANGUAGE plpgsql
AS $$
DECLARE
    v_signal RECORD;
    v_control_list INTEGER[];
BEGIN
    FOR v_signal IN
        SELECT * FROM it_governance.raw_regulatory_feeds
        WHERE processed = FALSE
        AND feed_date > CURRENT_DATE - INTERVAL '7 days'
    LOOP
        -- AI analysis of regulatory text
        WITH analysis AS (
            SELECT
                ai_analyze_regulation(v_signal.content) AS result
        )
        -- Map to existing regulations
        INSERT INTO it_governance.regulatory_signals
        SELECT
            gen_random_uuid(),
            r.requirement_id,
            analysis.result->>'signal_type',
            v_signal.source,
            v_signal.feed_date,
            (analysis.result->>'impact_date')::DATE,
            (analysis.result->>'confidence')::DECIMAL,
            get_affected_controls(analysis.result->>'keywords'),
            v_signal.content,
            analysis.result
        FROM
            it_governance.compliance_requirements r,
            analysis
        WHERE
            r.regulation_name % (analysis.result->>'regulation_name');

        -- Mark as processed
        UPDATE it_governance.raw_regulatory_feeds
        SET processed = TRUE
        WHERE feed_id = v_signal.feed_id;
    END LOOP;

    COMMIT;
END;
$$;

-- Dynamic Risk Appetite Framework
--contextual Risk Thresholds
-- BUSINESS CASE: Enables % more strategic risk-taking during growth periods
CREATE TABLE it_governance.dynamic_risk_appetite (
    threshold_id UUID PRIMARY KEY,
    risk_category VARCHAR(50) NOT NULL,
    business_context VARCHAR(100) NOT NULL,  -- 'M&A', 'New_Product', 'Crisis'
    base_threshold NUMERIC NOT NULL,
    current_adjustment NUMERIC NOT NULL DEFAULT 0,
    adjustment_factors JSONB NOT NULL,  -- {market_volatility, strategy_focus}
    effective_date TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP,
    expiry_date TIMESTAMPTZ GENERATED ALWAYS AS (
        effective_date + (SELECT adjustment_frequency
                         FROM it_governance.risk_appetite_settings
                         WHERE risk_category = dynamic_risk_appetite.risk_category)
    ) STORED
);

COMMENT ON TABLE it_governance.dynamic_risk_appetite IS 'Context-aware risk thresholds that auto-adjust to business conditions';

-- Threshold optimization engine
CREATE OR REPLACE PROCEDURE it_governance.optimize_risk_appetite()
LANGUAGE plpgsql
AS $$
DECLARE
    v_factor RECORD;
    v_adjustment NUMERIC;
BEGIN
    -- Calculate market volatility factor
    SELECT
        STDDEV(percentage_change) /
        NULLIF(AVG(percentage_change), 0) AS volatility_index
    INTO v_adjustment
    FROM (
        SELECT (close_price - open_price) / open_price AS percentage_change
        FROM market_data
        WHERE trading_date > CURRENT_DATE - INTERVAL '30 days'
    ) market_changes;

    -- Apply to relevant risk categories
    UPDATE it_governance.dynamic_risk_appetite
    SET
        current_adjustment = CASE
            WHEN risk_category IN ('MARKET','CREDIT') THEN
                LEAST(base_threshold * 0.3, v_adjustment * base_threshold)
            ELSE current_adjustment
        END,
        adjustment_factors = jsonb_set(
            adjustment_factors,
            '{market_volatility}',
            to_jsonb(v_adjustment)
        ),
        effective_date = CURRENT_TIMESTAMP
    WHERE
        expiry_date <= CURRENT_TIMESTAMP;

    COMMIT;
END;
$$;

-- completeness check
-- Verify all enhancements are properly integrated
SELECT
    'AI Policy Optimization' AS feature,
    COUNT(*) AS components,
    (SELECT COUNT(*) FROM it_governance.ai_policy_optimizer) AS active_instances
FROM
    it_governance.policies
WHERE
    policy_id IN (SELECT policy_id FROM it_governance.ai_policy_optimizer)

UNION ALL

SELECT
    'Quantum Evidence',
    COUNT(*),
    (SELECT COUNT(*) FROM it_governance.quantum_compliance_evidence)
FROM
    it_governance.controls

UNION ALL

SELECT
    'Autonomous Remediation',
    COUNT(*),
    (SELECT COUNT(*) FROM it_governance.autonomous_remediation_bots)
FROM
    it_governance.control_categories;


--Performance Benchmark
CREATE MATERIALIZED VIEW it_governance.enhancement_metrics
WITH (timescaledb.continuous) AS
SELECT
    'policy_optimization' AS enhancement,
    AVG((effectiveness_metrics->>'compliance_coverage')::DECIMAL) AS avg_coverage,
    AVG((effectiveness_metrics->>'risk_reduction')::DECIMAL) AS avg_risk_improvement
FROM
    it_governance.ai_policy_optimizer

UNION ALL

SELECT
    'autonomous_remediation',
    AVG(success_rate),
    AVG((performance_metrics->>'attempts')::DECIMAL) AS avg_activations
FROM
    it_governance.autonomous_remediation_bots

UNION ALL

SELECT
    'regulatory_prediction',
    AVG(CASE WHEN predicted_impact_date BETWEEN actual_impact_date - INTERVAL '14 days'
             AND actual_impact_date + INTERVAL '14 days' THEN 1 ELSE 0 END),
    AVG(confidence_score)
FROM
    it_governance.regulatory_signals
WITH DATA;



--blockchain enabled policy lifecycle management and versioning
--create a policy version chain
-- BUSINESS CASE: Eliminates policy disputes with cryptographic proof of versions
-- COMPLIANCE LINK: code of Federal Regulations (CFR) 21 Part 11 (Electronic Records) https://www.ecfr.gov/current/title-21/chapter-I/subchapter-A/part-11
CREATE TABLE it_governance.policy_blockchain (
    block_id UUID PRIMARY KEY,
    policy_id UUID NOT NULL REFERENCES it_governance.policies,
    version_hash BYTEA NOT NULL,  -- SHA-3-512 hash
    merkle_root BYTEA NOT NULL,
    previous_hash BYTEA NOT NULL,
    approvers TEXT[] NOT NULL,  -- DID (Decentralized Identifiers)
    validation_timestamp TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP,
    smart_contract_address TEXT  -- Ethereum/R3 Corda
) PARTITION BY RANGE (validation_timestamp);

CREATE INDEX idx_policy_chain ON it_governance.policy_blockchain(policy_id, validation_timestamp);

COMMENT ON TABLE it_governance.policy_blockchain IS 'Immutable policy version history using distributed ledger';

--policy smart contract sync
CREATE OR REPLACE PROCEDURE it_governance.sync_policy_to_blockchain()
LANGUAGE plpgsql
AS $$
DECLARE
    v_policy RECORD;
    v_previous_hash BYTEA;
BEGIN
    -- Get latest unblocked policy changes
    FOR v_policy IN
        SELECT p.policy_id, p.policy_text, p.version
        FROM it_governance.policies p
        LEFT JOIN it_governance.policy_blockchain b ON p.policy_id = b.policy_id
        WHERE b.block_id IS NULL OR p.version > (
            SELECT MAX(version)
            FROM it_governance.policy_versions
            WHERE policy_id = p.policy_id
            AND version_hash IN (
                SELECT version_hash
                FROM it_governance.policy_blockchain
                WHERE policy_id = p.policy_id)
        )
    LOOP
        -- Get previous block hash
        SELECT version_hash INTO v_previous_hash
        FROM it_governance.policy_blockchain
        WHERE policy_id = v_policy.policy_id
        ORDER BY validation_timestamp DESC
        LIMIT 1;

        -- Add to blockchain
        INSERT INTO it_governance.policy_blockchain
        VALUES (
            gen_random_uuid(),
            v_policy.policy_id,
            sha3_512(v_policy.policy_text),
            calculate_merkle_root(v_policy.policy_id),
            COALESCE(v_previous_hash, '\x'),
            ARRAY(SELECT did FROM it_governance.policy_approvers WHERE policy_id = v_policy.policy_id),
            CURRENT_TIMESTAMP,
            deploy_policy_smart_contract(v_policy.policy_id, v_policy.policy_text)
        );
    END LOOP;

    COMMIT;
END;
$$;


--predictive third party risk intelligence
-- to perform vendor threat surface analysis
-- BUSINESS CASE: Identifies % of third-party risks before contract renewal
CREATE TABLE it_governance.vendor_threat_surface (
    analysis_id UUID PRIMARY KEY,
    vendor_id UUID NOT NULL REFERENCES core.vendors,
    analysis_date TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP,
    digital_footprint JSONB NOT NULL,  -- {exposed_APIs, darkweb_mentions, credential_leaks}
    infrastructure_risks JSONB NOT NULL,  -- {misconfigurations, vuln_density}
    business_risks JSONB NOT NULL,  -- {financial_stress, legal_actions}
    composite_risk_score DECIMAL(5,2) GENERATED ALWAYS AS (
        (digital_footprint->>'score')::DECIMAL * 0.4 +
        (infrastructure_risks->>'score')::DECIMAL * 0.3 +
        (business_risks->>'score')::DECIMAL * 0.3
    ) STORED,
    prediction_horizon INTERVAL NOT NULL DEFAULT '90 days'
) PARTITION BY RANGE (analysis_date);

COMMENT ON TABLE it_governance.vendor_threat_surface IS '360-degree vendor risk assessment with predictive scoring';


--vendor risk prediction view
CREATE MATERIALIZED VIEW it_governance.vendor_risk_predictions
WITH (timescaledb.continuous) AS
SELECT
    v.vendor_name,
    v.industry,
    t.composite_risk_score AS current_risk,
    forecast_risk_score(t.vendor_id) AS predicted_90day_risk,
    t.digital_footprint->>'exposed_APIs' AS api_exposures,
    t.business_risks->>'legal_actions' AS legal_issues
FROM
    it_governance.vendor_threat_surface t
JOIN
    core.vendors v ON t.vendor_id = v.vendor_id
WHERE
    t.analysis_date > CURRENT_DATE - INTERVAL '7 days'
WITH DATA;

COMMENT ON MATERIALIZED VIEW it_governance.vendor_risk_predictions IS 'Real-time vendor risk assessment with forecasts';

--autonomous compliance certification
-- continuously certify
CREATE TABLE it_governance.auto_certification (
    certification_id UUID PRIMARY KEY,
    framework_id INTEGER NOT NULL REFERENCES it_governance.compliance_frameworks,
    certification_level VARCHAR(50) NOT NULL,
    start_date TIMESTAMPTZ NOT NULL,
    end_date TIMESTAMPTZ GENERATED ALWAYS AS (
        start_date + (SELECT validity_period
                     FROM it_governance.compliance_frameworks
                     WHERE framework_id = auto_certification.framework_id)
    ) STORED,
    current_score DECIMAL(5,2) NOT NULL,
    evidence_links JSONB NOT NULL,  -- {control_tests, policy_versions, audit_logs}
    auto_renewal BOOLEAN NOT NULL DEFAULT TRUE,
    CONSTRAINT valid_score CHECK (current_score BETWEEN 0 AND 100)
);

CREATE OR REPLACE PROCEDURE it_governance.maintain_certifications()
LANGUAGE plpgsql
AS $$
BEGIN
    -- Renew expiring certifications
    INSERT INTO it_governance.auto_certification
    SELECT
        gen_random_uuid(),
        c.framework_id,
        CASE
            WHEN calc_compliance_score(c.framework_id) >= 90 THEN 'FULL'
            WHEN calc_compliance_score(c.framework_id) >= 75 THEN 'PARTIAL'
            ELSE 'CONDITIONAL'
        END,
        CURRENT_TIMESTAMP,
        calc_compliance_score(c.framework_id),
        gather_evidence_links(c.framework_id),
        TRUE
    FROM
        it_governance.auto_certification c
    WHERE
        c.end_date BETWEEN CURRENT_DATE AND CURRENT_DATE + INTERVAL '30 days'
        AND c.auto_renewal = TRUE
    ON CONFLICT DO NOTHING;

    -- Update scores for active certifications
    UPDATE it_governance.auto_certification
    SET current_score = calc_compliance_score(framework_id)
    WHERE end_date > CURRENT_DATE;

    COMMIT;
END;
$$;

--AI Governance Dashboard -- Ethical AI Oversight Hub
CREATE TABLE it_governance.ai_governance_board (
    meeting_id UUID PRIMARY KEY,
    meeting_date TIMESTAMPTZ NOT NULL,
    agenda_items JSONB NOT NULL,  -- {model_reviews, incident_analysis, policy_updates}
    decisions JSONB NOT NULL,  -- {approved_models, restricted_use_cases}
    action_items JSONB NOT NULL,
    next_review_date TIMESTAMPTZ GENERATED ALWAYS AS (
        meeting_date + (SELECT review_cycle
                       FROM it_governance.ai_governance_settings)
    ) STORED
);

CREATE MATERIALIZED VIEW it_governance.ai_governance_overview
WITH (timescaledb.continuous) AS
SELECT
    m.meeting_date,
    COUNT(DISTINCT(m.decisions->>'approved_models')) AS models_approved,
    COUNT(DISTINCT(m.decisions->>'restricted_use_cases')) AS restrictions_issued,
    AVG((i.severity)::INTEGER) FILTER (WHERE i.report_date > m.meeting_date - INTERVAL '90 days') AS avg_incident_severity,
    (SELECT COUNT(*) FROM it_governance.ai_models
     WHERE approval_status = 'PENDING') AS pending_reviews
FROM
    it_governance.ai_governance_board m
LEFT JOIN
    it_governance.ai_incidents i ON i.report_date BETWEEN m.meeting_date - INTERVAL '90 days' AND m.meeting_date
GROUP BY
    m.meeting_date
WITH DATA;

-- completeness check for all operational components
-- Verify all components are operational
SELECT
    'Cognitive Controls' AS component,
    COUNT(*) AS configured,
    (SELECT COUNT(*) FROM it_governance.control_adaptation_logs
     WHERE adaptation_date > CURRENT_DATE - INTERVAL '30 days') AS monthly_adaptations
FROM
    it_governance.cognitive_controls

UNION ALL

SELECT
    'Blockchain Policies',
    COUNT(DISTINCT policy_id),
    (SELECT COUNT(*) FROM it_governance.policy_blockchain
     WHERE validation_timestamp > CURRENT_DATE - INTERVAL '30 days')
FROM
    it_governance.policies

UNION ALL

SELECT
    'Vendor Risk Predictions',
    COUNT(DISTINCT vendor_id),
    (SELECT COUNT(*) FROM it_governance.vendor_threat_surface
     WHERE analysis_date > CURRENT_DATE - INTERVAL '7 days')
FROM
    core.vendors;


-- to display GRC peformance metrics
CREATE MATERIALIZED VIEW it_governance.grc_metrics
WITH (timescaledb.continuous) AS
SELECT
    'Policy Adaptation' AS metric,
    AVG((p.performance_metrics->>'effectiveness')::DECIMAL) AS score,
    COUNT(*) FILTER (WHERE (p.performance_metrics->>'effectiveness')::DECIMAL > 0.9) AS high_performers
FROM
    it_governance.cognitive_controls p

UNION ALL

SELECT
    'Vendor Risk Reduction',
    AVG(100 - v.composite_risk_score),
    COUNT(*) FILTER (WHERE v.composite_risk_score < 30)
FROM
    it_governance.vendor_threat_surface v

UNION ALL

SELECT
    'Compliance Automation',
    AVG(c.current_score),
    COUNT(*) FILTER (WHERE c.current_score >= 90)
FROM
    it_governance.auto_certification c
WITH DATA;


-- self learning control -- workflow
-- mermaidjs diagram code
-- graph TD
--     A[Threat Intelligence] --> B[Cognitive Controls]
--     B --> C[Blockchain Policy]
--     C --> D[Auto Certification]
--     D --> E[AI Governance]
--     E --> A

-- KPI metrics -- % faster threat response through adaptive controls
-- KPI metrics -- % Audit proof policy records with blockchain
--KPI metrics -- % vendor risk visibility before contract decisions
--KPI metrics -- continuous compliance eliminating audit prep costs
-- AI adoption with governance safegaurds -- no of governance safegaurds


----------------------
-- Financial Risk Management
----------------------
CREATE SCHEMA financial_risk_management;
COMMENT ON SCHEMA financial_risk_management IS 'Schema for managing financial risks including market, credit, and liquidity risk with associated KPIs and metrics';

CREATE TABLE financial_risk_management.risk_categories (
    category_id SERIAL PRIMARY KEY,
    category_name VARCHAR(100) NOT NULL,
    description TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE financial_risk_management.risk_categories IS 'Master table for different types of financial risks';
COMMENT ON COLUMN financial_risk_management.risk_categories.category_name IS 'Name of the risk category (e.g., Market Risk, Credit Risk)';

CREATE TABLE financial_risk_management.risk_indicators (
    indicator_id SERIAL PRIMARY KEY,
    category_id INTEGER REFERENCES financial_risk_management.risk_categories(category_id),
    indicator_name VARCHAR(100) NOT NULL,
    description TEXT,
    measurement_unit VARCHAR(50),
    target_value NUMERIC,
    threshold_value NUMERIC,
    is_kpi BOOLEAN DEFAULT FALSE,
    is_krm BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE financial_risk_management.risk_indicators IS 'All risk indicators including KPIs and KRMs for financial risk management';
COMMENT ON COLUMN financial_risk_management.risk_indicators.is_kpi IS 'Flag indicating if this is a Key Performance Indicator';
COMMENT ON COLUMN financial_risk_management.risk_indicators.is_krm IS 'Flag indicating if this is a Key Risk Metric';

CREATE TABLE financial_risk_management.risk_assessments (
    assessment_id SERIAL PRIMARY KEY,
    assessment_name VARCHAR(100) NOT NULL,
    description TEXT,
    frequency VARCHAR(50) NOT NULL, -- Daily, Weekly, Monthly, Quarterly, etc.
    methodology TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE financial_risk_management.risk_assessments IS 'Types of risk assessments performed (VaR, stress testing, etc.)';

CREATE TABLE financial_risk_management.market_risk_data (
    market_risk_id SERIAL PRIMARY KEY,
    assessment_id INTEGER REFERENCES financial_risk_management.risk_assessments(assessment_id),
    portfolio_id INTEGER, -- Would reference a portfolio table in core schema
    calculation_date DATE NOT NULL,
    var_value NUMERIC,
    var_confidence_level NUMERIC,
    expected_shortfall NUMERIC,
    stress_test_result TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE financial_risk_management.market_risk_data IS 'Market risk calculations including Value at Risk (VaR) and stress test results';

CREATE TABLE financial_risk_management.market_risk_scenarios (
    scenario_id SERIAL PRIMARY KEY,
    scenario_name VARCHAR(100) NOT NULL,
    description TEXT,
    parameters JSONB,
    severity VARCHAR(50),
    likelihood VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE financial_risk_management.market_risk_scenarios IS 'Predefined market risk scenarios for stress testing';

CREATE TABLE financial_risk_management.counterparties (
    counterparty_id SERIAL PRIMARY KEY,
    counterparty_name VARCHAR(100) NOT NULL,
    counterparty_type VARCHAR(50) NOT NULL, -- Customer, Vendor, Bank, etc.
    credit_rating VARCHAR(20),
    rating_agency VARCHAR(50),
    rating_date DATE,
    internal_risk_score NUMERIC,
    next_review_date DATE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE financial_risk_management.counterparties IS 'Entities with credit exposure including customers and vendors';

CREATE TABLE financial_risk_management.credit_exposures (
    exposure_id SERIAL PRIMARY KEY,
    counterparty_id INTEGER REFERENCES financial_risk_management.counterparties(counterparty_id),
    exposure_type VARCHAR(50) NOT NULL, -- Loan, Derivative, Receivable, etc.
    exposure_amount NUMERIC NOT NULL,
    exposure_date DATE NOT NULL,
    maturity_date DATE,
    collateral_amount NUMERIC,
    net_exposure NUMERIC,
    probability_of_default NUMERIC,
    loss_given_default NUMERIC,
    expected_loss NUMERIC,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE financial_risk_management.credit_exposures IS 'Current credit exposures to counterparties with risk metrics';


CREATE TABLE financial_risk_management.liquidity_positions (
    position_id SERIAL PRIMARY KEY,
    calculation_date DATE NOT NULL,
    cash_position NUMERIC NOT NULL,
    liquid_assets NUMERIC NOT NULL,
    short_term_liabilities NUMERIC NOT NULL,
    liquidity_coverage_ratio NUMERIC,
    net_stable_funding_ratio NUMERIC,
    stress_test_result TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE financial_risk_management.liquidity_positions IS 'Daily liquidity position and key liquidity ratios';


CREATE TABLE financial_risk_management.cash_flow_forecasts (
    forecast_id SERIAL PRIMARY KEY,
    forecast_date DATE NOT NULL,
    time_horizon VARCHAR(50) NOT NULL, -- Daily, Weekly, Monthly
    forecast_period DATE NOT NULL,
    expected_inflows NUMERIC,
    expected_outflows NUMERIC,
    net_cash_flow NUMERIC,
    confidence_level VARCHAR(50),
    variance_from_actual NUMERIC,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE financial_risk_management.cash_flow_forecasts IS 'Cash flow forecasts for liquidity risk management';


CREATE TABLE financial_risk_management.operational_risk_events (
    event_id SERIAL PRIMARY KEY,
    event_date DATE NOT NULL,
    event_type VARCHAR(100) NOT NULL,
    description TEXT,
    impact_amount NUMERIC,
    business_line VARCHAR(100),
    root_cause TEXT,
    mitigation_actions TEXT,
    status VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE financial_risk_management.operational_risk_events IS 'Record of operational risk events with financial impact';

CREATE TABLE financial_risk_management.risk_metric_measurements (
    measurement_id SERIAL PRIMARY KEY,
    indicator_id INTEGER REFERENCES financial_risk_management.risk_indicators(indicator_id),
    measurement_date DATE NOT NULL,
    measured_value NUMERIC NOT NULL,
    target_value NUMERIC,
    status VARCHAR(50), -- Green, Yellow, Red
    comments TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE financial_risk_management.risk_metric_measurements IS 'Historical measurements of all risk indicators (KPIs and KRMs)';

CREATE OR REPLACE VIEW financial_risk_management.market_risk_dashboard AS
SELECT
    mrd.market_risk_id,
    mrd.calculation_date,
    mrd.var_value,
    mrd.var_confidence_level,
    mrd.expected_shortfall,
    ri.indicator_name,
    rmm.measured_value AS current_value,
    ri.target_value,
    CASE
        WHEN rmm.measured_value <= ri.threshold_value THEN 'Green'
        WHEN rmm.measured_value > ri.threshold_value AND rmm.measured_value <= ri.threshold_value * 1.2 THEN 'Yellow'
        ELSE 'Red'
    END AS status
FROM
    financial_risk_management.market_risk_data mrd
JOIN
    financial_risk_management.risk_indicators ri ON ri.indicator_name IN ('Value at Risk (VaR) Accuracy', 'Market Risk Exposure')
LEFT JOIN
    financial_risk_management.risk_metric_measurements rmm ON rmm.indicator_id = ri.indicator_id
    AND rmm.measurement_date = (SELECT MAX(measurement_date) FROM financial_risk_management.risk_metric_measurements WHERE indicator_id = ri.indicator_id);

COMMENT ON VIEW financial_risk_management.market_risk_dashboard IS 'Consolidated view of key market risk metrics for dashboard reporting';

CREATE OR REPLACE VIEW financial_risk_management.credit_risk_exposure AS
SELECT
    c.counterparty_id,
    c.counterparty_name,
    c.counterparty_type,
    c.credit_rating,
    c.internal_risk_score,
    SUM(ce.exposure_amount) AS total_exposure,
    SUM(ce.net_exposure) AS total_net_exposure,
    SUM(ce.expected_loss) AS total_expected_loss,
    COUNT(ce.exposure_id) AS number_of_exposures
FROM
    financial_risk_management.counterparties c
JOIN
    financial_risk_management.credit_exposures ce ON c.counterparty_id = ce.counterparty_id
GROUP BY
    c.counterparty_id, c.counterparty_name, c.counterparty_type, c.credit_rating, c.internal_risk_score;

COMMENT ON VIEW financial_risk_management.credit_risk_exposure IS 'Aggregated view of credit exposures by counterparty with risk metrics';

CREATE OR REPLACE VIEW financial_risk_management.liquidity_risk_status AS
SELECT
    lp.position_id,
    lp.calculation_date,
    lp.cash_position,
    lp.liquid_assets,
    lp.short_term_liabilities,
    lp.liquidity_coverage_ratio,
    CASE
        WHEN lp.liquidity_coverage_ratio >= 1.0 THEN 'Adequate'
        WHEN lp.liquidity_coverage_ratio >= 0.9 THEN 'Watch'
        ELSE 'Deficient'
    END AS lcr_status,
    (SELECT AVG(liquidity_coverage_ratio)
     FROM financial_risk_management.liquidity_positions
     WHERE calculation_date BETWEEN lp.calculation_date - INTERVAL '30 days' AND lp.calculation_date) AS thirty_day_avg_lcr
FROM
    financial_risk_management.liquidity_positions lp
ORDER BY
    lp.calculation_date DESC
LIMIT 30;

COMMENT ON VIEW financial_risk_management.liquidity_risk_status IS 'Current and historical liquidity coverage ratios with status indicators';

CREATE MATERIALIZED VIEW financial_risk_management.monthly_risk_metrics_summary AS
SELECT
    ri.category_id,
    rc.category_name,
    ri.indicator_id,
    ri.indicator_name,
    DATE_TRUNC('month', rmm.measurement_date) AS month,
    AVG(rmm.measured_value) AS avg_value,
    MIN(rmm.measured_value) AS min_value,
    MAX(rmm.measured_value) AS max_value,
    COUNT(rmm.measurement_id) AS measurement_count,
    SUM(CASE WHEN rmm.measured_value > ri.threshold_value THEN 1 ELSE 0 END) AS threshold_breach_count
FROM
    financial_risk_management.risk_metric_measurements rmm
JOIN
    financial_risk_management.risk_indicators ri ON rmm.indicator_id = ri.indicator_id
JOIN
    financial_risk_management.risk_categories rc ON ri.category_id = rc.category_id
GROUP BY
    ri.category_id, rc.category_name, ri.indicator_id, ri.indicator_name, DATE_TRUNC('month', rmm.measurement_date)
ORDER BY
    DATE_TRUNC('month', rmm.measurement_date) DESC, ri.category_id;

COMMENT ON MATERIALIZED VIEW financial_risk_management.monthly_risk_metrics_summary IS 'Pre-aggregated monthly summary of all risk metrics for reporting and trend analysis';


CREATE MATERIALIZED VIEW financial_risk_management.counterparty_risk_exposure_summary AS
SELECT
    c.counterparty_id,
    c.counterparty_name,
    c.counterparty_type,
    c.credit_rating,
    c.internal_risk_score,
    COUNT(ce.exposure_id) AS exposure_count,
    SUM(ce.exposure_amount) AS total_exposure,
    SUM(ce.net_exposure) AS total_net_exposure,
    SUM(ce.expected_loss) AS total_expected_loss,
    MAX(ce.exposure_date) AS latest_exposure_date,
    CASE
        WHEN SUM(ce.net_exposure) > 1000000 THEN 'High'
        WHEN SUM(ce.net_exposure) > 500000 THEN 'Medium'
        ELSE 'Low'
    END AS exposure_level
FROM
    financial_risk_management.counterparties c
LEFT JOIN
    financial_risk_management.credit_exposures ce ON c.counterparty_id = ce.counterparty_id
GROUP BY
    c.counterparty_id, c.counterparty_name, c.counterparty_type, c.credit_rating, c.internal_risk_score;

COMMENT ON MATERIALIZED VIEW financial_risk_management.counterparty_risk_exposure_summary IS 'Pre-computed summary of counterparty exposures for quick risk assessment';

CREATE MATERIALIZED VIEW financial_risk_management.counterparty_risk_exposure_summary AS
SELECT
    c.counterparty_id,
    c.counterparty_name,
    c.counterparty_type,
    c.credit_rating,
    c.internal_risk_score,
    COUNT(ce.exposure_id) AS exposure_count,
    SUM(ce.exposure_amount) AS total_exposure,
    SUM(ce.net_exposure) AS total_net_exposure,
    SUM(ce.expected_loss) AS total_expected_loss,
    MAX(ce.exposure_date) AS latest_exposure_date,
    CASE
        WHEN SUM(ce.net_exposure) > 1000000 THEN 'High'
        WHEN SUM(ce.net_exposure) > 500000 THEN 'Medium'
        ELSE 'Low'
    END AS exposure_level
FROM
    financial_risk_management.counterparties c
LEFT JOIN
    financial_risk_management.credit_exposures ce ON c.counterparty_id = ce.counterparty_id
GROUP BY
    c.counterparty_id, c.counterparty_name, c.counterparty_type, c.credit_rating, c.internal_risk_score;

COMMENT ON MATERIALIZED VIEW financial_risk_management.counterparty_risk_exposure_summary IS 'Pre-computed summary of counterparty exposures for quick risk assessment';

-- value at risk calculation -- historial simulation method by collecting historical returns for asset/portfolio
-- another approach variance and covariance parametric method
-- monte carlo simulation - use random simulations based on assumed or modeled distributions of returns

CREATE OR REPLACE PROCEDURE financial_risk_management.calculate_and_store_var(
    p_portfolio_id INTEGER,
    p_calculation_date DATE,
    p_confidence_level NUMERIC DEFAULT 0.95,
    p_time_horizon INTEGER DEFAULT 1
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_var_value NUMERIC;
    v_expected_shortfall NUMERIC;
BEGIN
    -- In a real implementation, this would call the actual VaR calculation logic
    -- For this example, we'll simulate the calculation

    -- Simulate VaR calculation (would be replaced with actual calculation)
    SELECT
        percentile_cont(1 - p_confidence_level) WITHIN GROUP (ORDER BY daily_return) * SQRT(p_time_horizon) * portfolio_value
    INTO
        v_var_value
    FROM
        core.portfolio_returns  -- Assuming this exists in core schema
    WHERE
        portfolio_id = p_portfolio_id
        AND return_date BETWEEN p_calculation_date - INTERVAL '1 year' AND p_calculation_date;

    -- Simulate Expected Shortfall calculation
    SELECT
        AVG(daily_return) * SQRT(p_time_horizon) * portfolio_value
    INTO
        v_expected_shortfall
    FROM
        core.portfolio_returns
    WHERE
        portfolio_id = p_portfolio_id
        AND return_date BETWEEN p_calculation_date - INTERVAL '1 year' AND p_calculation_date
        AND daily_return < (SELECT percentile_cont(1 - p_confidence_level) WITHIN GROUP (ORDER BY daily_return)
                           FROM core.portfolio_returns
                           WHERE portfolio_id = p_portfolio_id
                           AND return_date BETWEEN p_calculation_date - INTERVAL '1 year' AND p_calculation_date);

    -- Insert the results
    INSERT INTO financial_risk_management.market_risk_data (
        portfolio_id,
        calculation_date,
        var_value,
        var_confidence_level,
        expected_shortfall
    ) VALUES (
        p_portfolio_id,
        p_calculation_date,
        v_var_value,
        p_confidence_level,
        v_expected_shortfall
    );

    -- Update the risk metric measurements for VaR accuracy tracking
    INSERT INTO financial_risk_management.risk_metric_measurements (
        indicator_id,
        measurement_date,
        measured_value
    ) VALUES (
        (SELECT indicator_id FROM financial_risk_management.risk_indicators WHERE indicator_name = 'Value at Risk (VaR) Accuracy'),
        p_calculation_date,
        v_var_value
    );

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE financial_risk_management.calculate_and_store_var IS 'Calculates Value at Risk and Expected Shortfall for a portfolio and stores the results';


---update counterparty credit risk score
CREATE OR REPLACE PROCEDURE financial_risk_management.update_counterparty_credit_risk_score(
    p_counterparty_id INTEGER,
    p_review_date DATE DEFAULT CURRENT_DATE
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_total_exposure NUMERIC;
    v_past_due_amount NUMERIC;
    v_payment_history_score NUMERIC;
    v_financial_strength_score NUMERIC;
    v_internal_risk_score NUMERIC;
BEGIN
    -- Calculate total exposure
    SELECT COALESCE(SUM(exposure_amount), 0)
    INTO v_total_exposure
    FROM financial_risk_management.credit_exposures
    WHERE counterparty_id = p_counterparty_id;

    -- Calculate past due amount
    SELECT COALESCE(SUM(CASE WHEN exposure_date < CURRENT_DATE - INTERVAL '90 days' THEN exposure_amount ELSE 0 END), 0)
    INTO v_past_due_amount
    FROM financial_risk_management.credit_exposures
    WHERE counterparty_id = p_counterparty_id;

    -- Calculate payment history score (simplified example)
    SELECT
        CASE
            WHEN v_past_due_amount = 0 THEN 1.0
            WHEN v_past_due_amount < v_total_exposure * 0.05 THEN 0.8
            WHEN v_past_due_amount < v_total_exposure * 0.1 THEN 0.5
            ELSE 0.2
        END
    INTO v_payment_history_score;

    -- Get financial strength score (would come from external data in real implementation)
    SELECT financial_strength_score
    INTO v_financial_strength_score
    FROM core.external_credit_data  -- Assuming this exists in core schema
    WHERE counterparty_id = p_counterparty_id;

    -- Calculate internal risk score (weighted average of factors)
    v_internal_risk_score := (v_payment_history_score * 0.6 + COALESCE(v_financial_strength_score, 0.5) * 0.4) * 100;

    -- Update counterparty record
    UPDATE financial_risk_management.counterparties
    SET
        internal_risk_score = v_internal_risk_score,
        next_review_date = p_review_date + INTERVAL '90 days',
        updated_at = CURRENT_TIMESTAMP
    WHERE
        counterparty_id = p_counterparty_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE financial_risk_management.update_counterparty_credit_risk_score IS 'Recalculates and updates the internal credit risk score for a counterparty based on exposure and payment history';


-- generate liquidity risk report
CREATE OR REPLACE PROCEDURE financial_risk_management.generate_liquidity_risk_report(
    p_report_date DATE DEFAULT CURRENT_DATE,
    p_time_horizon VARCHAR DEFAULT '30 days'
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_report_id INTEGER;
BEGIN
    -- Create report header
    INSERT INTO financial_reporting.reports (report_type, report_date, status)
    VALUES ('Liquidity Risk Report', p_report_date, 'Generated')
    RETURNING report_id INTO v_report_id;

    -- Add liquidity position data
    INSERT INTO financial_reporting.report_data (report_id, data_type, data_json)
    SELECT
        v_report_id,
        'Liquidity Position',
        jsonb_build_object(
            'calculation_date', lp.calculation_date,
            'cash_position', lp.cash_position,
            'liquid_assets', lp.liquid_assets,
            'short_term_liabilities', lp.short_term_liabilities,
            'liquidity_coverage_ratio', lp.liquidity_coverage_ratio,
            'status', CASE
                WHEN lp.liquidity_coverage_ratio >= 1.0 THEN 'Adequate'
                WHEN lp.liquidity_coverage_ratio >= 0.9 THEN 'Watch'
                ELSE 'Deficient'
            END
        )
    FROM
        financial_risk_management.liquidity_positions lp
    WHERE
        lp.calculation_date = p_report_date;

    -- Add cash flow forecast data
    INSERT INTO financial_reporting.report_data (report_id, data_type, data_json)
    SELECT
        v_report_id,
        'Cash Flow Forecast',
        jsonb_build_object(
            'forecast_period', cff.forecast_period,
            'expected_inflows', cff.expected_inflows,
            'expected_outflows', cff.expected_outflows,
            'net_cash_flow', cff.net_cash_flow,
            'confidence_level', cff.confidence_level,
            'time_horizon', p_time_horizon
        )
    FROM
        financial_risk_management.cash_flow_forecasts cff
    WHERE
        cff.forecast_date = p_report_date
        AND cff.time_horizon = p_time_horizon;

    -- Add historical LCR trend
    INSERT INTO financial_reporting.report_data (report_id, data_type, data_json)
    SELECT
        v_report_id,
        'LCR Trend',
        jsonb_build_object(
            'time_period', p_time_horizon,
            'data', jsonb_agg(
                jsonb_build_object(
                    'date', lp.calculation_date,
                    'lcr', lp.liquidity_coverage_ratio
                )
            )
        )
    FROM
        financial_risk_management.liquidity_positions lp
    WHERE
        lp.calculation_date BETWEEN p_report_date - (p_time_horizon::interval) AND p_report_date;

    -- Update report status
    UPDATE financial_reporting.reports
    SET status = 'Completed'
    WHERE report_id = v_report_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE financial_risk_management.generate_liquidity_risk_report IS 'Generates a comprehensive liquidity risk report with current position, forecasts, and trends';

-- indexes for performance
-- Indexes for market risk data
CREATE INDEX idx_market_risk_data_date ON financial_risk_management.market_risk_data(calculation_date);
CREATE INDEX idx_market_risk_data_portfolio ON financial_risk_management.market_risk_data(portfolio_id);

-- Indexes for credit exposures
CREATE INDEX idx_credit_exposures_counterparty ON financial_risk_management.credit_exposures(counterparty_id);
CREATE INDEX idx_credit_exposures_date ON financial_risk_management.credit_exposures(exposure_date);

-- Indexes for liquidity positions
CREATE INDEX idx_liquidity_positions_date ON financial_risk_management.liquidity_positions(calculation_date);

-- Indexes for risk metric measurements
CREATE INDEX idx_risk_metric_measurements_indicator ON financial_risk_management.risk_metric_measurements(indicator_id);
CREATE INDEX idx_risk_metric_measurements_date ON financial_risk_management.risk_metric_measurements(measurement_date);

--todo -- integrated risk view -- consolidate market, credit, liquidity and operatioanl risk data
-- regulatory compliacne - track all required metrics for BASEL III, SOX and other regulatory requirements
-- Proactive RISK FRAMEWORK - for early indentification of risk threshold breaches through automated monitoring
-- Data driven decision making -- to provide executives with dashboards and reports absed on accurate, timely risk data
-- Efficiency gains -- Automates risk calculations and reporting that were previously manual processes



-- daily market risk monitoring --value at risk calculations for all portfolios
-- Run VaR calculation for all portfolios
DO $$
DECLARE
    v_portfolio_id INTEGER;
BEGIN
    FOR v_portfolio_id IN SELECT portfolio_id FROM core.portfolios WHERE is_active = true LOOP
        CALL financial_risk_management.calculate_and_store_var(v_portfolio_id, CURRENT_DATE);
    END LOOP;
END $$;

-- Check results
SELECT * FROM financial_risk_management.market_risk_dashboard
WHERE calculation_date = CURRENT_DATE;


-- monthly credit risk review
-- Update all counterparty risk scores
DO $$
DECLARE
    v_counterparty_id INTEGER;
BEGIN
    FOR v_counterparty_id IN SELECT counterparty_id FROM financial_risk_management.counterparties LOOP
        CALL financial_risk_management.update_counterparty_credit_risk_score(v_counterparty_id);
    END LOOP;
END $$;

-- Identify high-risk counterparties
SELECT * FROM financial_risk_management.counterparty_risk_exposure_summary
WHERE exposure_level = 'High'
ORDER BY total_net_exposure DESC;

--liquidity risk reporting
-- Generate weekly liquidity report
CALL financial_risk_management.generate_liquidity_risk_report(CURRENT_DATE, '7 days');

-- View the report
SELECT rd.data_type, rd.data_json
FROM financial_reporting.report_data rd
JOIN financial_reporting.reports r ON rd.report_id = r.report_id
WHERE r.report_type = 'Liquidity Risk Report'
ORDER BY r.report_date DESC
LIMIT 1;

--regulatory compliance monitoring
-- Check all KPIs/KRMs against targets
SELECT
    ri.indicator_name,
    rc.category_name,
    rmm.measured_value,
    ri.target_value,
    ri.threshold_value,
    CASE
        WHEN rmm.measured_value <= ri.threshold_value THEN 'Green'
        WHEN rmm.measured_value > ri.threshold_value AND rmm.measured_value <= ri.threshold_value * 1.2 THEN 'Yellow'
        ELSE 'Red'
    END AS status
FROM
    financial_risk_management.risk_metric_measurements rmm
JOIN
    financial_risk_management.risk_indicators ri ON rmm.indicator_id = ri.indicator_id
JOIN
    financial_risk_management.risk_categories rc ON ri.category_id = rc.category_id
WHERE
    rmm.measurement_date = (SELECT MAX(measurement_date) FROM financial_risk_management.risk_metric_measurements)
ORDER BY
    status, rc.category_name, ri.indicator_name;


--todo stress testing scenarios
--standardizes stress testing across the organization with predefined scenarios that can be consistently applied for regulatory compliance (CCAR, BASEL III) and internal risk assessment
-- Business Case: Provides auditable record of stress test executions and their impacts, supporting regulatory requirements and internal capital adequacy assessments.
CREATE TABLE financial_risk_management.stress_test_scenarios (
    scenario_id SERIAL PRIMARY KEY,
    scenario_name VARCHAR(100) NOT NULL,
    scenario_type VARCHAR(50) NOT NULL, -- Market, Credit, Liquidity, Operational
    description TEXT,
    severity VARCHAR(20) NOT NULL, -- Mild, Moderate, Severe, Extreme
    likelihood VARCHAR(20), -- Remote, Unlikely, Possible, Likely, Certain
    parameters JSONB NOT NULL, -- JSON structure with scenario parameters
    approval_status VARCHAR(20) DEFAULT 'Draft',
    approved_by VARCHAR(100),
    approved_date TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT chk_severity CHECK (severity IN ('Mild', 'Moderate', 'Severe', 'Extreme')),
    CONSTRAINT chk_likelihood CHECK (likelihood IN ('Remote', 'Unlikely', 'Possible', 'Likely', 'Certain'))
);

COMMENT ON TABLE financial_risk_management.stress_test_scenarios IS 'Predefined stress testing scenarios for various risk types with severity and likelihood ratings';
COMMENT ON COLUMN financial_risk_management.stress_test_scenarios.parameters IS 'JSON structure containing all parameters needed to execute the stress scenario';


--Business Case: Formalizes the organization's risk appetite by defining specific limits for risk-taking activities with clear escalation procedures for breaches
CREATE TABLE financial_risk_management.risk_limits (
    limit_id SERIAL PRIMARY KEY,
    limit_name VARCHAR(100) NOT NULL,
    description TEXT,
    risk_type VARCHAR(50) NOT NULL,
    metric_id INTEGER REFERENCES financial_risk_management.risk_indicators(indicator_id),
    limit_value NUMERIC NOT NULL,
    soft_limit_value NUMERIC,
    limit_breach_action TEXT,
    effective_date DATE NOT NULL,
    expiration_date DATE,
    approval_status VARCHAR(20) DEFAULT 'Active',
    approved_by VARCHAR(100),
    approved_date TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE financial_risk_management.risk_limits IS 'Risk appetite limits for various risk metrics with hard and soft limits';
COMMENT ON COLUMN financial_risk_management.risk_limits.soft_limit_value IS 'Warning threshold before actual limit is breached';

-- Limit Breaches --
--Business Case: Tracks and manages risk limit breaches with full audit trail, supporting prompt corrective actions and regulatory reporting.

CREATE TABLE financial_risk_management.limit_breaches (
    breach_id SERIAL PRIMARY KEY,
    limit_id INTEGER REFERENCES financial_risk_management.risk_limits(limit_id),
    breach_date TIMESTAMP NOT NULL,
    metric_value NUMERIC NOT NULL,
    breach_type VARCHAR(20) NOT NULL, -- Soft, Hard
    duration_hours NUMERIC,
    resolved_date TIMESTAMP,
    resolution_action TEXT,
    escalation_path TEXT,
    status VARCHAR(20) DEFAULT 'Open',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE financial_risk_management.limit_breaches IS 'Record of all risk limit breaches with tracking of resolution';
COMMENT ON COLUMN financial_risk_management.limit_breaches.duration_hours IS 'Duration of breach in hours until resolution';

--Risk Mitigation Actions
-- Business Case: Ensures identified risks have clear mitigation plans with accountability and tracking through to completion.
CREATE TABLE financial_risk_management.risk_mitigation_actions (
    action_id SERIAL PRIMARY KEY,
    risk_event_id INTEGER REFERENCES financial_risk_management.operational_risk_events(event_id),
    description TEXT NOT NULL,
    action_type VARCHAR(50) NOT NULL, -- Preventative, Corrective, Compensating
    assigned_to VARCHAR(100) NOT NULL,
    due_date DATE NOT NULL,
    completion_date DATE,
    status VARCHAR(20) DEFAULT 'Pending',
    effectiveness_rating INTEGER, -- 1-5 scale
    review_date DATE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE financial_risk_management.risk_mitigation_actions IS 'Tracking of actions taken to mitigate identified risks';
COMMENT ON COLUMN financial_risk_management.risk_mitigation_actions.effectiveness_rating IS 'Post-implementation rating of action effectiveness (1=Low to 5=High)';


-- Comprehensive Risk Dashboard
--Business Case: Provides executives with a single pane of glass view into the organization's risk profile across all risk types with drill-down capabilities.
CREATE OR REPLACE VIEW financial_risk_management.comprehensive_risk_dashboard AS
WITH latest_metrics AS (
    SELECT
        rmm.indicator_id,
        ri.indicator_name,
        rc.category_name,
        rmm.measured_value,
        rmm.measurement_date,
        ri.target_value,
        ri.threshold_value,
        ROW_NUMBER() OVER (PARTITION BY rmm.indicator_id ORDER BY rmm.measurement_date DESC) as rn
    FROM
        financial_risk_management.risk_metric_measurements rmm
    JOIN
        financial_risk_management.risk_indicators ri ON rmm.indicator_id = ri.indicator_id
    JOIN
        financial_risk_management.risk_categories rc ON ri.category_id = rc.category_id
)
SELECT
    lm.indicator_id,
    lm.indicator_name,
    lm.category_name,
    lm.measured_value,
    lm.measurement_date,
    lm.target_value,
    lm.threshold_value,
    CASE
        WHEN lm.measured_value <= lm.threshold_value THEN 'Green'
        WHEN lm.measured_value <= lm.threshold_value * 1.2 THEN 'Yellow'
        ELSE 'Red'
    END AS status,
    (SELECT COUNT(*) FROM financial_risk_management.limit_breaches lb
     JOIN financial_risk_management.risk_limits rl ON lb.limit_id = rl.limit_id
     WHERE rl.metric_id = lm.indicator_id AND lb.breach_date >= CURRENT_DATE - INTERVAL '30 days') AS breaches_last_30_days,
    (SELECT STRING_AGG(ss.scenario_name, ', ' ORDER BY ss.severity DESC)
     FROM financial_risk_management.stress_test_results str
     JOIN financial_risk_management.stress_test_scenarios ss ON str.scenario_id = ss.scenario_id
     WHERE str.risk_type = lm.category_name AND str.execution_date >= CURRENT_DATE - INTERVAL '90 days') AS recent_scenarios
FROM
    latest_metrics lm
WHERE
    lm.rn = 1
ORDER BY
    lm.category_name,
    CASE
        WHEN lm.measured_value <= lm.threshold_value THEN 1
        WHEN lm.measured_value <= lm.threshold_value * 1.2 THEN 2
        ELSE 3
    END,
    lm.indicator_name;

COMMENT ON VIEW financial_risk_management.comprehensive_risk_dashboard IS 'Consolidated view of all key risk metrics with status indicators, recent breaches, and stress test history';


--Stress Test Analysis view
--Business Case: Enables risk analysts to compare current stress test results with historical data and identify correlations with actual limit breaches.
CREATE OR REPLACE VIEW financial_risk_management.stress_test_analysis AS
SELECT
    str.result_id,
    sts.scenario_id,
    sts.scenario_name,
    sts.scenario_type,
    sts.severity,
    sts.likelihood,
    str.execution_date,
    str.risk_type,
    str.baseline_value,
    str.stressed_value,
    str.impact,
    str.impact_pct,
    (SELECT COUNT(*) FROM financial_risk_management.limit_breaches lb
     JOIN financial_risk_management.risk_limits rl ON lb.limit_id = rl.limit_id
     WHERE rl.metric_id IN (SELECT indicator_id FROM financial_risk_management.risk_indicators
                           WHERE category_id = (SELECT category_id FROM financial_risk_management.risk_categories
                                              WHERE category_name = str.risk_type))
     AND lb.breach_date BETWEEN str.execution_date - INTERVAL '7 days' AND str.execution_date + INTERVAL '7 days') AS related_breaches,
    (SELECT AVG(impact_pct) FROM financial_risk_management.stress_test_results
     WHERE scenario_id = sts.scenario_id AND execution_date >= CURRENT_DATE - INTERVAL '1 year') AS avg_historical_impact
FROM
    financial_risk_management.stress_test_results str
JOIN
    financial_risk_management.stress_test_scenarios sts ON str.scenario_id = sts.scenario_id
ORDER BY
    str.execution_date DESC,
    sts.severity DESC;

COMMENT ON VIEW financial_risk_management.stress_test_analysis IS 'Detailed analysis of stress test results including historical comparisons and related limit breaches';


-- risk limit monitoring summary
-- Business Case: Provides real-time monitoring of all risk limits with performance metrics on limit adherence, refreshed daily for risk committees.
CREATE MATERIALIZED VIEW financial_risk_management.risk_limit_monitoring_summary AS
SELECT
    rl.limit_id,
    rl.limit_name,
    rl.risk_type,
    ri.indicator_name,
    rl.limit_value,
    rl.soft_limit_value,
    (SELECT measured_value
     FROM financial_risk_management.risk_metric_measurements rmm
     WHERE rmm.indicator_id = rl.metric_id
     ORDER BY measurement_date DESC LIMIT 1) AS current_value,
    (SELECT COUNT(*)
     FROM financial_risk_management.limit_breaches lb
     WHERE lb.limit_id = rl.limit_id
     AND lb.breach_date >= CURRENT_DATE - INTERVAL '30 days') AS breaches_last_30_days,
    (SELECT COUNT(*)
     FROM financial_risk_management.limit_breaches lb
     WHERE lb.limit_id = rl.limit_id
     AND lb.breach_date >= CURRENT_DATE - INTERVAL '90 days') AS breaches_last_90_days,
    (SELECT AVG(duration_hours)
     FROM financial_risk_management.limit_breaches lb
     WHERE lb.limit_id = rl.limit_id
     AND lb.breach_date >= CURRENT_DATE - INTERVAL '90 days') AS avg_breach_duration_hours
FROM
    financial_risk_management.risk_limits rl
JOIN
    financial_risk_management.risk_indicators ri ON rl.metric_id = ri.indicator_id
WHERE
    rl.approval_status = 'Active'
    AND (rl.expiration_date IS NULL OR rl.expiration_date >= CURRENT_DATE)
ORDER BY
    rl.risk_type,
    ri.indicator_name;

COMMENT ON MATERIALIZED VIEW financial_risk_management.risk_limit_monitoring_summary IS 'Pre-computed summary of all active risk limits with current values and recent breach statistics';


--Risk Mitigation Effectiveness
-- Business Case: Measures the actual effectiveness of risk mitigation actions by comparing incident rates before and after implementation, supporting continuous improvement.
CREATE MATERIALIZED VIEW financial_risk_management.risk_mitigation_effectiveness AS
SELECT
    rc.category_name AS risk_category,
    EXTRACT(YEAR FROM rma.due_date) AS year,
    EXTRACT(QUARTER FROM rma.due_date) AS quarter,
    rma.action_type,
    COUNT(*) AS total_actions,
    SUM(CASE WHEN rma.status = 'Completed' THEN 1 ELSE 0 END) AS completed_actions,
    AVG(rma.effectiveness_rating) FILTER (WHERE rma.effectiveness_rating IS NOT NULL) AS avg_effectiveness,
    (SELECT COUNT(*)
     FROM financial_risk_management.operational_risk_events ore
     WHERE ore.event_type = rc.category_name
     AND ore.event_date BETWEEN DATE_TRUNC('quarter', rma.due_date) - INTERVAL '1 year'
                            AND DATE_TRUNC('quarter', rma.due_date) - INTERVAL '1 day') AS incidents_year_before,
    (SELECT COUNT(*)
     FROM financial_risk_management.operational_risk_events ore
     WHERE ore.event_type = rc.category_name
     AND ore.event_date BETWEEN DATE_TRUNC('quarter', rma.due_date)
                            AND DATE_TRUNC('quarter', rma.due_date) + INTERVAL '1 year' - INTERVAL '1 day') AS incidents_year_after
FROM
    financial_risk_management.risk_mitigation_actions rma
JOIN
    financial_risk_management.operational_risk_events ore ON rma.risk_event_id = ore.event_id
JOIN
    financial_risk_management.risk_categories rc ON ore.event_type = rc.category_name
WHERE
    rma.due_date >= CURRENT_DATE - INTERVAL '3 years'
GROUP BY
    rc.category_name,
    EXTRACT(YEAR FROM rma.due_date),
    EXTRACT(QUARTER FROM rma.due_date),
    rma.action_type
ORDER BY
    rc.category_name,
    year DESC,
    quarter DESC;

COMMENT ON MATERIALIZED VIEW financial_risk_management.risk_mitigation_effectiveness IS 'Analysis of risk mitigation action effectiveness by comparing incident rates before and after implementation';

--stored procedures
-- execute stress test scenario
--Business Case: Standardizes stress testing execution across the organization with automatic recording of results and limit breach detection.
CREATE OR REPLACE PROCEDURE financial_risk_management.execute_stress_test_scenario(
    p_scenario_id INTEGER,
    p_execution_user VARCHAR(100),
    p_portfolio_id INTEGER DEFAULT NULL
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_scenario_record RECORD;
    v_baseline_value NUMERIC;
    v_stressed_value NUMERIC;
    v_risk_type VARCHAR(50);
    v_impact NUMERIC;
BEGIN
    -- Get scenario details
    SELECT * INTO v_scenario_record
    FROM financial_risk_management.stress_test_scenarios
    WHERE scenario_id = p_scenario_id;

    IF NOT FOUND THEN
        RAISE EXCEPTION 'Scenario with ID % not found', p_scenario_id;
    END IF;

    -- Determine risk type from scenario
    v_risk_type := v_scenario_record.scenario_type;

    -- Execute appropriate stress test based on risk type
    CASE v_risk_type
        WHEN 'Market' THEN
            -- Market risk stress test (simplified example)
            SELECT var_value INTO v_baseline_value
            FROM financial_risk_management.market_risk_data
            WHERE portfolio_id = p_portfolio_id
            ORDER BY calculation_date DESC LIMIT 1;

            -- Apply stress factors (in reality would use complex models)
            v_stressed_value := v_baseline_value *
                (1 + (v_scenario_record.parameters->>'equity_shock')::NUMERIC *
                (1 + (v_scenario_record.parameters->>'interest_rate_shock')::NUMERIC);

        WHEN 'Credit' THEN
            -- Credit risk stress test
            SELECT SUM(expected_loss) INTO v_baseline_value
            FROM financial_risk_management.credit_exposures;

            -- Apply PD and LGD shocks
            v_stressed_value := v_baseline_value *
                (1 + (v_scenario_record.parameters->>'pd_shock')::NUMERIC) *
                (1 + (v_scenario_record.parameters->>'lgd_shock')::NUMERIC);

        WHEN 'Liquidity' THEN
            -- Liquidity risk stress test
            SELECT liquidity_coverage_ratio INTO v_baseline_value
            FROM financial_risk_management.liquidity_positions
            ORDER BY calculation_date DESC LIMIT 1;

            -- Apply outflow shocks
            v_stressed_value := v_baseline_value /
                (1 + (v_scenario_record.parameters->>'outflow_shock')::NUMERIC);

        ELSE
            RAISE EXCEPTION 'Unsupported risk type % for stress testing', v_risk_type;
    END CASE;

    -- Record the stress test results
    INSERT INTO financial_risk_management.stress_test_results (
        scenario_id,
        execution_date,
        execution_user,
        risk_type,
        portfolio_id,
        baseline_value,
        stressed_value,
        status
    ) VALUES (
        p_scenario_id,
        CURRENT_TIMESTAMP,
        p_execution_user,
        v_risk_type,
        p_portfolio_id,
        v_baseline_value,
        v_stressed_value,
        'Completed'
    );

    -- Check for limit breaches caused by this stress scenario
    PERFORM financial_risk_management.check_stress_impact_limits(
        p_scenario_id,
        v_risk_type,
        v_baseline_value,
        v_stressed_value);

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE financial_risk_management.execute_stress_test_scenario IS 'Executes a predefined stress test scenario and records results, including automatic limit breach checking';

-- check stress impact limits
--Business Case: Automates limit breach detection during stress testing, ensuring prompt identification of potential risk appetite violations.
CREATE OR REPLACE PROCEDURE financial_risk_management.check_stress_impact_limits(
    p_scenario_id INTEGER,
    p_risk_type VARCHAR(50),
    p_baseline_value NUMERIC,
    p_stressed_value NUMERIC
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_limit_record RECORD;
    v_impact_pct NUMERIC;
    v_breach_found BOOLEAN := FALSE;
BEGIN
    -- Calculate percentage impact
    v_impact_pct := (p_stressed_value - p_baseline_value) / NULLIF(p_baseline_value, 0) * 100;

    -- Check all relevant limits for this risk type
    FOR v_limit_record IN
        SELECT rl.*, ri.indicator_name
        FROM financial_risk_management.risk_limits rl
        JOIN financial_risk_management.risk_indicators ri ON rl.metric_id = ri.indicator_id
        JOIN financial_risk_management.risk_categories rc ON ri.category_id = rc.category_id
        WHERE rc.category_name = p_risk_type
        AND rl.approval_status = 'Active'
        AND (rl.expiration_date IS NULL OR rl.expiration_date >= CURRENT_DATE)
    LOOP
        -- Check if stressed value breaches the limit
        IF (v_limit_record.limit_value > 0 AND p_stressed_value > v_limit_record.limit_value) OR
           (v_limit_record.limit_value < 0 AND p_stressed_value < v_limit_record.limit_value) THEN

            -- Record the breach
            INSERT INTO financial_risk_management.limit_breaches (
                limit_id,
                breach_date,
                metric_value,
                breach_type,
                status
            ) VALUES (
                v_limit_record.limit_id,
                CURRENT_TIMESTAMP,
                p_stressed_value,
                'Hard',
                'Open'
            );

            v_breach_found := TRUE;
        END IF;
    END LOOP;

    -- If any breaches found, log scenario impact
    IF v_breach_found THEN
        UPDATE financial_risk_management.stress_test_results
        SET notes = COALESCE(notes, '') || '\nResulted in limit breaches'
        WHERE scenario_id = p_scenario_id
        AND execution_date = (SELECT MAX(execution_date)
                             FROM financial_risk_management.stress_test_results
                             WHERE scenario_id = p_scenario_id);
    END IF;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE financial_risk_management.check_stress_impact_limits IS 'Checks if stress test results would breach any defined risk limits and records breaches if found';


-- generate regulatory risk report
-- Business Case: Automates creation of regulatory reports with consistent formatting and data, reducing manual effort and improving compliance.
CREATE OR REPLACE PROCEDURE financial_risk_management.generate_regulatory_risk_report(
    p_report_type VARCHAR(50),
    p_as_of_date DATE DEFAULT CURRENT_DATE,
    p_regulator VARCHAR(50) DEFAULT NULL
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_report_id INTEGER;
    v_report_title VARCHAR(100);
    v_report_period VARCHAR(100);
BEGIN
    -- Determine report title and period based on type
    CASE p_report_type
        WHEN 'Basel III' THEN
            v_report_title := 'Basel III Risk Metrics Report';
            v_report_period := 'Quarterly ' || EXTRACT(QUARTER FROM p_as_of_date) || 'Q ' || EXTRACT(YEAR FROM p_as_of_date);

            -- Create report header
            INSERT INTO financial_reporting.reports (report_type, report_date, report_period, status)
            VALUES (v_report_title, p_as_of_date, v_report_period, 'In Progress')
            RETURNING report_id INTO v_report_id;

            -- Add Basel III liquidity metrics
            INSERT INTO financial_reporting.report_data (report_id, data_type, data_json)
            SELECT
                v_report_id,
                'Liquidity Coverage Ratio',
                jsonb_build_object(
                    'as_of_date', calculation_date,
                    'lcr_value', liquidity_coverage_ratio,
                    'minimum_requirement', 1.0,
                    'status', CASE WHEN liquidity_coverage_ratio >= 1.0 THEN 'Compliant' ELSE 'Deficient' END
                )
            FROM
                financial_risk_management.liquidity_positions
            WHERE
                calculation_date = p_as_of_date;

            -- Add capital adequacy metrics (simplified)
            INSERT INTO financial_reporting.report_data (report_id, data_type, data_json)
            VALUES (
                v_report_id,
                'Capital Adequacy',
                jsonb_build_object(
                    'as_of_date', p_as_of_date,
                    'tier1_ratio', 0.15, -- Would come from actual calculations
                    'total_capital_ratio', 0.18, -- Would come from actual calculations
                    'minimum_requirements', jsonb_build_object(
                        'tier1_minimum', 0.06,
                        'total_capital_minimum', 0.08
                    ),
                    'status', 'Compliant'
                )
            );

        WHEN 'CCAR' THEN
            v_report_title := 'Comprehensive Capital Analysis and Review';
            v_report_period := 'Annual ' || EXTRACT(YEAR FROM p_as_of_date);

            -- Create report header
            INSERT INTO financial_reporting.reports (report_type, report_date, report_period, status)
            VALUES (v_report_title, p_as_of_date, v_report_period, 'In Progress')
            RETURNING report_id INTO v_report_id;

            -- Add stress test results
            INSERT INTO financial_reporting.report_data (report_id, data_type, data_json)
            SELECT
                v_report_id,
                'Stress Test Results',
                jsonb_build_object(
                    'scenario_name', sts.scenario_name,
                    'severity', sts.severity,
                    'baseline_value', str.baseline_value,
                    'stressed_value', str.stressed_value,
                    'impact', str.impact,
                    'impact_pct', str.impact_pct,
                    'execution_date', str.execution_date
                )
            FROM
                financial_risk_management.stress_test_results str
            JOIN
                financial_risk_management.stress_test_scenarios sts ON str.scenario_id = sts.scenario_id
            WHERE
                str.execution_date BETWEEN p_as_of_date - INTERVAL '90 days' AND p_as_of_date
                AND sts.severity IN ('Severe', 'Extreme');

        ELSE
            RAISE EXCEPTION 'Unsupported regulatory report type: %', p_report_type;
    END CASE;

    -- Finalize report
    UPDATE financial_reporting.reports
    SET status = 'Completed',
        completion_date = CURRENT_TIMESTAMP
    WHERE report_id = v_report_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE financial_risk_management.generate_regulatory_risk_report IS 'Generates standardized risk reports for regulatory requirements like Basel III and CCAR';


-- risk appetite framework tables
-- Business Case: Formalizes the organization's risk appetite with version-controlled statements and specific measurable metrics.

CREATE TABLE financial_risk_management.risk_appetite_statements (
    statement_id SERIAL PRIMARY KEY,
    version VARCHAR(20) NOT NULL,
    effective_date DATE NOT NULL,
    expiration_date DATE,
    statement_text TEXT NOT NULL,
    approval_status VARCHAR(20) DEFAULT 'Draft',
    approved_by VARCHAR(100),
    approved_date TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE financial_risk_management.risk_appetite_metrics (
    metric_id SERIAL PRIMARY KEY,
    statement_id INTEGER REFERENCES financial_risk_management.risk_appetite_statements(statement_id),
    metric_name VARCHAR(100) NOT NULL,
    metric_description TEXT,
    target_value NUMERIC,
    tolerance_range NUMERIC,
    measurement_frequency VARCHAR(50) NOT NULL,
    responsible_party VARCHAR(100) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);


--Risk culture assessment table
--Business Case: Measures and tracks risk culture through regular surveys, identifying areas for improvement in the organization's risk mindset.
CREATE TABLE financial_risk_management.risk_culture_surveys (
    survey_id SERIAL PRIMARY KEY,
    survey_name VARCHAR(100) NOT NULL,
    survey_period VARCHAR(50) NOT NULL, -- Q1 2023, Annual 2022, etc.
    start_date DATE NOT NULL,
    end_date DATE NOT NULL,
    target_audience VARCHAR(100) NOT NULL, -- All Employees, Senior Management, Front Office, etc.
    participation_rate NUMERIC,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE financial_risk_management.risk_culture_questions (
    question_id SERIAL PRIMARY KEY,
    survey_id INTEGER REFERENCES financial_risk_management.risk_culture_surveys(survey_id),
    question_text TEXT NOT NULL,
    question_category VARCHAR(50) NOT NULL, -- Tone at the Top, Accountability, etc.
    response_type VARCHAR(20) NOT NULL, -- Likert 1-5, Yes/No, Free Text
    benchmark_value NUMERIC,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE financial_risk_management.risk_culture_responses (
    response_id SERIAL PRIMARY KEY,
    question_id INTEGER REFERENCES financial_risk_management.risk_culture_questions(question_id),
    respondent_id INTEGER, -- Would reference HR system
    response_value NUMERIC,
    response_text TEXT,
    response_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

--predictive risk models
-- Business Case: Centralizes management of ML models used for risk prediction with full audit trail, performance tracking, and compliance documentation for model risk management (SR 11-7).https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm
CREATE TABLE financial_risk_management.predictive_models (
    model_id SERIAL PRIMARY KEY,
    model_name VARCHAR(100) NOT NULL,
    model_type VARCHAR(50) NOT NULL, -- 'Market Risk', 'Credit Scoring', 'Fraud Detection'
    model_version VARCHAR(50) NOT NULL,
    model_owner VARCHAR(100) NOT NULL,
    deployment_date DATE NOT NULL,
    last_refresh_date DATE,
    performance_metrics JSONB, -- Accuracy, Precision, Recall, AUC-ROC
    input_features JSONB NOT NULL,
    output_definition JSONB NOT NULL,
    model_artifact BYTEA, -- Serialized model file
    approval_status VARCHAR(20) DEFAULT 'Development',
    explanation_report TEXT, -- Model interpretability documentation
    bias_metrics JSONB, -- Fairness metrics
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT chk_model_type CHECK (model_type IN ('Market Risk', 'Credit Scoring', 'Fraud Detection', 'Liquidity Forecasting', 'Operational Risk'))
);

COMMENT ON TABLE financial_risk_management.predictive_models IS 'Registry of all machine learning models used for risk prediction with version control and performance tracking';
COMMENT ON COLUMN financial_risk_management.predictive_models.explanation_report IS 'Model interpretability documentation meeting regulatory requirements for explainable AI';


-- real-time risk alerts
-- Business Case: Enables real-time monitoring and alerting of risk thresholds with integrated workflow for resolution and root cause analysis.
CREATE TABLE financial_risk_management.real_time_alerts (
    alert_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    alert_type VARCHAR(50) NOT NULL, -- 'Limit Breach', 'Anomaly', 'Threshold'
    severity VARCHAR(20) NOT NULL, -- 'Critical', 'High', 'Medium', 'Low'
    trigger_value NUMERIC NOT NULL,
    trigger_condition TEXT NOT NULL,
    alert_timestamp TIMESTAMPTZ NOT NULL,
    resolved_timestamp TIMESTAMPTZ,
    status VARCHAR(20) DEFAULT 'Active',
    assigned_to VARCHAR(100),
    escalation_path JSONB, -- JSON array of escalation points
    related_entities JSONB, -- Affected portfolios, counterparties
    root_cause_analysis TEXT,
    mitigation_action TEXT,
    model_id INTEGER REFERENCES financial_risk_management.predictive_models(model_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT chk_severity CHECK (severity IN ('Critical', 'High', 'Medium', 'Low'))
);

CREATE INDEX idx_real_time_alerts_status ON financial_risk_management.real_time_alerts(status, alert_timestamp);
CREATE INDEX idx_real_time_alerts_type ON financial_risk_management.real_time_alerts(alert_type);


-- ESG Risk Factors
-- Business Case: Integrates ESG risk factors into financial risk assessment with detailed scoring for sustainable finance initiatives and regulatory compliance (EU Taxonomy, SFDR).
CREATE TABLE financial_risk_management.esg_risk_factors (
    factor_id SERIAL PRIMARY KEY,
    factor_name VARCHAR(100) NOT NULL,
    factor_category VARCHAR(50) NOT NULL, -- 'Environmental', 'Social', 'Governance'
    measurement_unit VARCHAR(50) NOT NULL,
    data_source VARCHAR(100) NOT NULL,
    refresh_frequency VARCHAR(50) NOT NULL,
    weighting_factor NUMERIC, -- For composite scoring
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE financial_risk_management.esg_risk_assessments (
    assessment_id SERIAL PRIMARY KEY,
    entity_id INTEGER NOT NULL, -- Counterparty, Investment, etc.
    entity_type VARCHAR(50) NOT NULL,
    assessment_date DATE NOT NULL,
    overall_score NUMERIC NOT NULL,
    environmental_score NUMERIC,
    social_score NUMERIC,
    governance_score NUMERIC,
    percentile_ranking NUMERIC,
    trend_direction VARCHAR(20), -- 'Improving', 'Stable', 'Deteriorating'
    next_assessment_date DATE,
    approved_by VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE financial_risk_management.esg_risk_details (
    detail_id SERIAL PRIMARY KEY,
    assessment_id INTEGER REFERENCES financial_risk_management.esg_risk_assessments(assessment_id),
    factor_id INTEGER REFERENCES financial_risk_management.esg_risk_factors(factor_id),
    factor_value NUMERIC NOT NULL,
    factor_score NUMERIC NOT NULL,
    contributor_weight NUMERIC,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- predictive risk dashbaord
-- Business Case: Provides model risk oversight with performance monitoring and business impact tracking for all AI/ML models in production.
CREATE OR REPLACE VIEW financial_risk_management.predictive_risk_dashboard AS
WITH model_performance AS (
    SELECT
        model_id,
        model_name,
        model_type,
        (performance_metrics->>'accuracy')::NUMERIC AS accuracy,
        (performance_metrics->>'precision')::NUMERIC AS precision,
        (performance_metrics->>'recall')::NUMERIC AS recall,
        (performance_metrics->>'auc_roc')::NUMERIC AS auc_roc,
        last_refresh_date,
        ROW_NUMBER() OVER (PARTITION BY model_type ORDER BY (performance_metrics->>'accuracy')::NUMERIC DESC) AS performance_rank
    FROM
        financial_risk_management.predictive_models
    WHERE
        approval_status = 'Production'
)
SELECT
    mp.model_id,
    mp.model_name,
    mp.model_type,
    mp.accuracy,
    mp.precision,
    mp.recall,
    mp.auc_roc,
    mp.performance_rank,
    COUNT(ra.alert_id) FILTER (WHERE ra.alert_timestamp >= CURRENT_DATE - INTERVAL '30 days') AS alerts_last_30_days,
    COUNT(ra.alert_id) FILTER (WHERE ra.status = 'Active') AS active_alerts,
    (SELECT STRING_AGG(feature, ', ' ORDER BY feature)
     FROM JSONB_ARRAY_ELEMENTS_TEXT(mp.input_features::JSONB) AS key_features
FROM
    model_performance mp
LEFT JOIN
    financial_risk_management.real_time_alerts ra ON mp.model_id = ra.model_id
GROUP BY
    mp.model_id, mp.model_name, mp.model_type, mp.accuracy, mp.precision, mp.recall,
    mp.auc_roc, mp.performance_rank, mp.input_features, mp.last_refresh_date
ORDER BY
    mp.model_type, mp.performance_rank;

COMMENT ON VIEW financial_risk_management.predictive_risk_dashboard IS 'Consolidated view of all predictive risk models with performance metrics and alert generation statistics';


--ESG Risk Exposure View
-- Business case:Enables portfolio managers to assess ESG risk concentrations and identify entities with significant sustainability risks.
CREATE OR REPLACE VIEW financial_risk_management.esg_risk_exposure AS
SELECT
    era.entity_id,
    era.entity_type,
    era.assessment_date,
    era.overall_score,
    era.percentile_ranking,
    era.trend_direction,
    (SELECT COUNT(*)
     FROM financial_risk_management.esg_risk_details erd
     WHERE erd.assessment_id = era.assessment_id
     AND erd.factor_score < 0.3) AS high_risk_factors,
    (SELECT STRING_AGG(erf.factor_name, ', ')
     FROM financial_risk_management.esg_risk_details erd
     JOIN financial_risk_management.esg_risk_factors erf ON erd.factor_id = erf.factor_id
     WHERE erd.assessment_id = era.assessment_id
     AND erd.factor_score < 0.3) AS high_risk_factor_names,
    (SELECT AVG(erd.factor_score)
     FROM financial_risk_management.esg_risk_details erd
     WHERE erd.assessment_id = era.assessment_id
     AND erd.factor_id IN (SELECT factor_id FROM financial_risk_management.esg_risk_factors WHERE factor_category = 'Environmental')) AS env_score,
    (SELECT AVG(erd.factor_score)
     FROM financial_risk_management.esg_risk_details erd
     WHERE erd.assessment_id = era.assessment_id
     AND erd.factor_id IN (SELECT factor_id FROM financial_risk_management.esg_risk_factors WHERE factor_category = 'Social')) AS social_score,
    (SELECT AVG(erd.factor_score)
     FROM financial_risk_management.esg_risk_details erd
     WHERE erd.assessment_id = era.assessment_id
     AND erd.factor_id IN (SELECT factor_id FROM financial_risk_management.esg_risk_factors WHERE factor_category = 'Governance')) AS gov_score
FROM
    financial_risk_management.esg_risk_assessments era
ORDER BY
    era.overall_score, era.assessment_date DESC;

COMMENT ON VIEW financial_risk_management.esg_risk_exposure IS 'Detailed ESG risk assessment with component scores and high-risk factor identification';

--Anomaly Detection Patterns
--Business Case: Identifies temporal patterns and concentrations in risk alerts to improve model calibration and detect potential systemic risks.
CREATE MATERIALIZED VIEW financial_risk_management.anomaly_detection_patterns AS
SELECT
    ra.alert_type,
    DATE_TRUNC('hour', ra.alert_timestamp) AS alert_hour,
    COUNT(*) AS alert_count,
    AVG(ra.trigger_value) AS avg_trigger_value,
    MIN(ra.trigger_value) AS min_trigger_value,
    MAX(ra.trigger_value) AS max_trigger_value,
    (SELECT STRING_AGG(DISTINCT pm.model_name, ', ')
     FROM financial_risk_management.predictive_models pm
     WHERE pm.model_id = ra.model_id) AS source_models,
    (SELECT STRING_AGG(DISTINCT jsonb_object_keys(ra.related_entities), ', ')
     FROM (SELECT jsonb_object_keys(ra.related_entities) AS entity_type) AS subq) AS affected_entity_types
FROM
    financial_risk_management.real_time_alerts ra
WHERE
    ra.alert_timestamp >= CURRENT_DATE - INTERVAL '90 days'
GROUP BY
    ra.alert_type, DATE_TRUNC('hour', ra.alert_timestamp)
ORDER BY
    alert_count DESC;

COMMENT ON MATERIALIZED VIEW financial_risk_management.anomaly_detection_patterns IS 'Aggregated patterns in anomaly detection alerts for identifying systemic issues and improving model thresholds';

--Risk Adjusted Performance
--Business Case: Enables performance comparison across business lines and portfolios with full consideration of underlying risk exposures.
CREATE MATERIALIZED VIEW financial_risk_management.risk_adjusted_performance AS
WITH portfolio_metrics AS (
    SELECT
        p.portfolio_id,
        p.portfolio_name,
        SUM(pe.market_value) AS total_value,
        AVG(mrd.var_value) AS avg_var,
        MAX(mrd.var_value) AS max_var,
        (SELECT lcr.liquidity_coverage_ratio
         FROM financial_risk_management.liquidity_positions lcr
         ORDER BY lcr.calculation_date DESC LIMIT 1) AS current_lcr,
        (SELECT SUM(ce.expected_loss)
         FROM financial_risk_management.credit_exposures ce
         WHERE ce.portfolio_id = p.portfolio_id) AS credit_risk
    FROM
        core.portfolios p
    LEFT JOIN
        core.portfolio_entries pe ON p.portfolio_id = pe.portfolio_id
    LEFT JOIN
        financial_risk_management.market_risk_data mrd ON p.portfolio_id = mrd.portfolio_id
    GROUP BY
        p.portfolio_id, p.portfolio_name
)
SELECT
    pm.portfolio_id,
    pm.portfolio_name,
    pm.total_value,
    pm.avg_var,
    pm.max_var,
    pm.current_lcr,
    pm.credit_risk,
    (SELECT SUM(t.return_amount)
     FROM core.transactions t
     WHERE t.portfolio_id = pm.portfolio_id
     AND t.transaction_date >= CURRENT_DATE - INTERVAL '1 year') AS annual_return,
    CASE
        WHEN pm.avg_var = 0 THEN NULL
        ELSE (SELECT SUM(t.return_amount)
              FROM core.transactions t
              WHERE t.portfolio_id = pm.portfolio_id
              AND t.transaction_date >= CURRENT_DATE - INTERVAL '1 year') / pm.avg_var
    END AS return_per_unit_var,
    (pm.current_lcr * 0.3 + (1 - (pm.credit_risk/pm.total_value)) * 0.7) AS liquidity_credit_score
FROM
    portfolio_metrics pm
ORDER BY
    return_per_unit_var DESC NULLS LAST;

COMMENT ON MATERIALIZED VIEW financial_risk_management.risk_adjusted_performance IS 'Calculates risk-adjusted performance metrics across all portfolios incorporating market, credit and liquidity risk factors';

-- to predict credit risk using machine learning interference
-- Business Case: Automates credit risk assessment using machine learning with full audit trail and explainable AI for regulatory compliance.
CREATE OR REPLACE PROCEDURE financial_risk_management.predict_credit_risk(
    p_counterparty_id INTEGER,
    p_model_version VARCHAR(50) DEFAULT 'latest'
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_model_id INTEGER;
    v_model_input JSONB;
    v_prediction_result JSONB;
    v_new_risk_score NUMERIC;
    v_explanation TEXT;
BEGIN
    -- Get the latest production model for credit risk
    SELECT model_id INTO v_model_id
    FROM financial_risk_management.predictive_models
    WHERE model_type = 'Credit Scoring'
    AND approval_status = 'Production'
    AND (p_model_version = 'latest' OR model_version = p_model_version)
    ORDER BY deployment_date DESC
    LIMIT 1;

    IF NOT FOUND THEN
        RAISE EXCEPTION 'No production credit scoring model found';
    END IF;

    -- Prepare input features from counterparty data
    SELECT jsonb_build_object(
        'credit_rating', c.credit_rating,
        'exposure_amount', COALESCE(SUM(ce.exposure_amount), 0),
        'past_due_amount', COALESCE(SUM(CASE WHEN ce.exposure_date < CURRENT_DATE - INTERVAL '90 days' THEN ce.exposure_amount ELSE 0 END), 0),
        'industry_sector', cp.industry_sector, -- From core.counterparties
        'financial_metrics', cp.financial_metrics -- From core.counterparties
    ) INTO v_model_input
    FROM financial_risk_management.counterparties c
    LEFT JOIN financial_risk_management.credit_exposures ce ON c.counterparty_id = ce.counterparty_id
    LEFT JOIN core.counterparties cp ON c.counterparty_id = cp.counterparty_id
    WHERE c.counterparty_id = p_counterparty_id
    GROUP BY c.counterparty_id, cp.industry_sector, cp.financial_metrics;

    -- Call ML inference function (implementation would use PL/Python or external service call)
    -- This is a simplified placeholder for the actual ML inference
    v_prediction_result := jsonb_build_object(
        'risk_score', 0.75, -- Would come from actual model
        'confidence', 0.92,
        'key_factors', jsonb_build_array(
            jsonb_build_object('feature', 'past_due_amount', 'contribution', 0.45),
            jsonb_build_object('feature', 'industry_sector', 'contribution', 0.30)
        )
    );

    -- Extract prediction score (scale 0-1 to 0-100)
    v_new_risk_score := (v_prediction_result->>'risk_score')::NUMERIC * 100;
    v_explanation := 'Key factors: ' || (SELECT STRING_AGG(f->>'feature' || ' (' || (f->>'contribution')::NUMERIC * 100 || '%)', ', ')
                                         FROM jsonb_array_elements(v_prediction_result->'key_factors') f);

    -- Update counterparty record with new prediction
    UPDATE financial_risk_management.counterparties
    SET
        internal_risk_score = v_new_risk_score,
        risk_score_explanation = v_explanation,
        risk_score_model_id = v_model_id,
        risk_score_date = CURRENT_TIMESTAMP,
        updated_at = CURRENT_TIMESTAMP
    WHERE
        counterparty_id = p_counterparty_id;

    -- Log prediction event
    INSERT INTO financial_risk_management.model_predictions (
        model_id,
        entity_id,
        entity_type,
        prediction_value,
        prediction_confidence,
        prediction_explanation,
        input_features
    ) VALUES (
        v_model_id,
        p_counterparty_id,
        'Counterparty',
        v_new_risk_score,
        (v_prediction_result->>'confidence')::NUMERIC,
        v_explanation,
        v_model_input
    );

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE financial_risk_management.predict_credit_risk IS 'Executes ML model to predict credit risk for a counterparty and updates records with explainable AI results';


--adaptive risk limit adjustment
--Business Case: Enables dynamic adjustment of risk limits in response to changing market conditions or risk appetite with proper governance controls.
CREATE OR REPLACE PROCEDURE financial_risk_management.adaptive_limit_adjustment(
    p_limit_id INTEGER,
    p_adjustment_factor NUMERIC,
    p_reason TEXT,
    p_approved_by VARCHAR(100)
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_current_limit RECORD;
    v_new_limit_value NUMERIC;
    v_new_soft_limit NUMERIC;
    v_historical_breaches INTEGER;
BEGIN
    -- Get current limit details
    SELECT * INTO v_current_limit
    FROM financial_risk_management.risk_limits
    WHERE limit_id = p_limit_id;

    IF NOT FOUND THEN
        RAISE EXCEPTION 'Limit ID % not found', p_limit_id;
    END IF;

    -- Calculate historical breaches
    SELECT COUNT(*) INTO v_historical_breaches
    FROM financial_risk_management.limit_breaches
    WHERE limit_id = p_limit_id
    AND breach_date >= CURRENT_DATE - INTERVAL '1 year';

    -- Calculate new limit values
    v_new_limit_value := v_current_limit.limit_value * p_adjustment_factor;
    v_new_soft_limit := CASE
        WHEN v_current_limit.soft_limit_value IS NOT NULL
        THEN v_current_limit.soft_limit_value * p_adjustment_factor
        ELSE NULL
    END;

    -- Create new version of the limit (maintain history)
    INSERT INTO financial_risk_management.risk_limits (
        limit_name,
        description,
        risk_type,
        metric_id,
        limit_value,
        soft_limit_value,
        limit_breach_action,
        effective_date,
        expiration_date,
        approval_status,
        approved_by,
        approved_date,
        parent_limit_id,
        adjustment_reason,
        historical_breaches
    ) VALUES (
        v_current_limit.limit_name,
        v_current_limit.description,
        v_current_limit.risk_type,
        v_current_limit.metric_id,
        v_new_limit_value,
        v_new_soft_limit,
        v_current_limit.limit_breach_action,
        CURRENT_DATE,
        NULL,
        'Pending',
        p_approved_by,
        NULL,
        p_limit_id,
        p_reason,
        v_historical_breaches
    );

    -- Expire the old limit
    UPDATE financial_risk_management.risk_limits
    SET
        expiration_date = CURRENT_DATE - INTERVAL '1 day',
        updated_at = CURRENT_TIMESTAMP
    WHERE
        limit_id = p_limit_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE financial_risk_management.adaptive_limit_adjustment IS 'Implements dynamic risk limit adjustment based on changing risk profiles with full audit trail';

-- Real-time liquidity monitoring
--Business Case: Provides real-time liquidity monitoring with automated escalation and contingency plan activation to prevent liquidity crises.
CREATE OR REPLACE PROCEDURE financial_risk_management.real_time_liquidity_monitoring()
LANGUAGE plpgsql
AS $$
DECLARE
    v_current_lcr NUMERIC;
    v_required_lcr NUMERIC := 1.0; -- Regulatory minimum
    v_stressed_lcr NUMERIC;
    v_liquidity_alert BOOLEAN := FALSE;
    v_message TEXT;
BEGIN
    -- Get current LCR from real-time data feed (simplified)
    SELECT liquidity_coverage_ratio INTO v_current_lcr
    FROM financial_risk_management.liquidity_positions
    ORDER BY calculation_date DESC
    LIMIT 1;

    -- Calculate stressed LCR (simplified scenario)
    v_stressed_lcr := v_current_lcr * 0.7; -- Assume 30% outflow shock

    -- Check for alert conditions
    IF v_current_lcr < v_required_lcr THEN
        v_liquidity_alert := TRUE;
        v_message := 'CRITICAL: LCR ' || v_current_lcr || ' below regulatory requirement ' || v_required_lcr;
    ELSIF v_stressed_lcr < v_required_lcr THEN
        v_liquidity_alert := TRUE;
        v_message := 'WARNING: Stressed LCR ' || v_stressed_lcr || ' would breach requirement ' || v_required_lcr;
    ELSIF v_current_lcr < 1.1 THEN
        v_liquidity_alert := TRUE;
        v_message := 'Watch: LCR ' || v_current_lcr || ' approaching regulatory requirement';
    END IF;

    -- Create alert if needed
    IF v_liquidity_alert THEN
        INSERT INTO financial_risk_management.real_time_alerts (
            alert_type,
            severity,
            trigger_value,
            trigger_condition,
            alert_timestamp,
            status,
            escalation_path,
            related_entities
        ) VALUES (
            'Liquidity',
            CASE
                WHEN v_current_lcr < v_required_lcr THEN 'Critical'
                WHEN v_stressed_lcr < v_required_lcr THEN 'High'
                ELSE 'Medium'
            END,
            v_current_lcr,
            v_message,
            CURRENT_TIMESTAMP,
            'Active',
            jsonb_build_array('Treasury', 'CFO', 'Risk Committee'),
            jsonb_build_object('portfolio', 'All', 'currency', 'USD')
        );

        -- Trigger contingency funding plan if critical
        IF v_current_lcr < v_required_lcr THEN
            PERFORM financial_risk_management.activate_contingency_funding();
        END IF;
    END IF;

    -- Store monitoring result
    INSERT INTO financial_risk_management.liquidity_monitoring_log (
        check_time,
        current_lcr,
        stressed_lcr,
        alert_generated,
        alert_message
    ) VALUES (
        CURRENT_TIMESTAMP,
        v_current_lcr,
        v_stressed_lcr,
        v_liquidity_alert,
        v_message
    );

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE financial_risk_management.real_time_liquidity_monitoring IS 'Continuous liquidity monitoring with automated alerts and contingency plan activation';


-- nonlinear risk interactions
--Business Case: Models complex risk factor interactions that traditional correlation matrices miss, particularly during market stress periods.
CREATE TABLE financial_risk_management.nonlinear_interactions (
    interaction_id SERIAL PRIMARY KEY,
    risk_factor_1 VARCHAR(50) NOT NULL,
    risk_factor_2 VARCHAR(50) NOT NULL,
    correlation_normal NUMERIC,
    correlation_stressed NUMERIC,
    tail_dependence NUMERIC,
    copula_model VARCHAR(50),
    last_calibrated TIMESTAMP,
    change_detection_score NUMERIC, -- 0-1 measure of recent structural change
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_interaction UNIQUE (risk_factor_1, risk_factor_2)
);

CREATE INDEX idx_nonlinear_interactions ON financial_risk_management.nonlinear_interactions
(GREATEST(change_detection_score, 0.7));

COMMENT ON TABLE financial_risk_management.nonlinear_interactions IS 'Captures nonlinear and regime-dependent relationships between risk factors';
COMMENT ON COLUMN financial_risk_management.nonlinear_interactions.tail_dependence IS 'Measure of co-movement during extreme events (0-1 scale)';

--trader behaviour analytics
-- Business Case: Mitigates human factor risks by detecting dangerous behavioral patterns and recommending targeted interventions before losses occur.
CREATE TABLE financial_risk_management.trader_behavior (
    behavior_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    trader_id INTEGER NOT NULL, -- References HR system
    analysis_date DATE NOT NULL,
    risk_appetite_score NUMERIC, -- 0-100
    compliance_adherence NUMERIC, -- 0-1
    pattern_consistency NUMERIC, -- 0-1
    anomaly_count INTEGER,
    stress_response_profile JSONB, -- Behavioral patterns under market stress
    ai_insights TEXT, -- NLP summary of behavioral patterns
    intervention_recommendation VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE financial_risk_management.behavioral_risk_events (
    event_id SERIAL PRIMARY KEY,
    trader_id INTEGER NOT NULL,
    event_timestamp TIMESTAMPTZ NOT NULL,
    event_type VARCHAR(50) NOT NULL, -- 'Limit Override', 'Unusual Timing', 'Concentration'
    risk_score NUMERIC NOT NULL,
    context_data JSONB NOT NULL,
    review_status VARCHAR(20) DEFAULT 'Pending',
    reviewer_comments TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE financial_risk_management.trader_behavior IS 'Behavioral risk profiles for front office staff with AI-driven insights';
COMMENT ON COLUMN financial_risk_management.trader_behavior.stress_response_profile IS 'JSON structure documenting behavioral changes during volatile periods';

--climate risk assesments phsical impact
-- Business Case: Quantifies direct climate-related physical risks to assets under different warming scenarios for TCFD reporting and resilience planning.
CREATE TABLE financial_risk_management.physical_climate_risks (
    assessment_id SERIAL PRIMARY KEY,
    asset_id INTEGER NOT NULL, -- Real estate, facilities, etc.
    asset_type VARCHAR(50) NOT NULL,
    location_geography GEOGRAPHY NOT NULL,
    hazard_type VARCHAR(50) NOT NULL, -- 'Flood', 'Wildfire', 'Sea Level'
    rcp_scenario VARCHAR(10) NOT NULL, -- 'RCP4.5', 'RCP8.5'
    time_horizon VARCHAR(20) NOT NULL, -- '2030', '2050', '2100'
    probability_score NUMERIC NOT NULL, -- 0-1
    impact_severity NUMERIC NOT NULL, -- 0-100
    financial_impact NUMERIC, -- USD estimate
    adaptation_costs NUMERIC,
    data_provider VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_climate_risk_geo ON financial_risk_management.physical_climate_risks USING GIST(location_geography);

COMMENT ON TABLE financial_risk_management.physical_climate_risks IS 'Physical climate risk assessments for all organizational assets across multiple scenarios';
COMMENT ON COLUMN financial_risk_management.physical_climate_risks.rcp_scenario IS 'Representative Concentration Pathway climate scenario';

-- Decentralized finance risk metrics
-- Business Case: Manages emerging risks from decentralized finance exposures with protocol-level risk scoring and wallet-level monitoring.
CREATE TABLE financial_risk_management.defi_risk_metrics (
    metric_id SERIAL PRIMARY KEY,
    protocol_id INTEGER NOT NULL, -- References DeFi protocol registry
    metric_timestamp TIMESTAMPTZ NOT NULL,
    tvl_usd NUMERIC, -- Total Value Locked
    smart_contract_risk NUMERIC, -- 0-100
    oracle_risk NUMERIC, -- 0-100
    governance_risk NUMERIC, -- 0-100
    liquidity_risk NUMERIC, -- 0-100
    counterparty_risk NUMERIC, -- 0-100
    composite_risk_score NUMERIC GENERATED ALWAYS AS (
        (smart_contract_risk * 0.3 + oracle_risk * 0.2 + governance_risk * 0.2 +
         liquidity_risk * 0.2 + counterparty_risk * 0.1)
    ) STORED,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE financial_risk_management.defi_exposures (
    exposure_id SERIAL PRIMARY KEY,
    wallet_address VARCHAR(42) NOT NULL, -- Blockchain address
    protocol_id INTEGER NOT NULL,
    position_type VARCHAR(50) NOT NULL, -- 'Liquidity Provider', 'Borrower', 'Staker'
    exposure_amount_usd NUMERIC NOT NULL,
    collateralization_ratio NUMERIC,
    liquidation_risk NUMERIC, -- 0-100
    last_updated TIMESTAMPTZ NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE financial_risk_management.defi_risk_metrics IS 'Risk metrics for decentralized finance protocols tracking smart contract, oracle, and governance risks';
COMMENT ON COLUMN financial_risk_management.defi_exposures.wallet_address IS 'Ethereum-style 0x-prefixed hexadecimal wallet address';



-- compliance officers require a single view of behavioral risk indicators across trading desks to proactively manage conduct risk
-- Business Case: Provides compliance officers with a single view of behavioral risk indicators across trading desks to proactively manage conduct risk.
CREATE OR REPLACE VIEW financial_risk_management.behavioral_risk_dashboard AS
WITH trader_stats AS (
    SELECT
        trader_id,
        COUNT(*) FILTER (WHERE event_type = 'Limit Override') AS limit_overrides,
        COUNT(*) FILTER (WHERE event_type = 'Unusual Timing') AS timing_anomalies,
        MAX(risk_score) AS max_risk_score,
        AVG(risk_score) AS avg_risk_score
    FROM
        financial_risk_management.behavioral_risk_events
    WHERE
        event_timestamp >= CURRENT_DATE - INTERVAL '90 days'
    GROUP BY
        trader_id
)
SELECT
    t.trader_id,
    e.employee_name,
    e.department,
    tb.risk_appetite_score,
    ts.limit_overrides,
    ts.timing_anomalies,
    ts.max_risk_score,
    ts.avg_risk_score,
    tb.intervention_recommendation,
    (SELECT STRING_AGG(event_type, ', ' ORDER BY event_timestamp DESC)
    FROM financial_risk_management.behavioral_risk_events bre
    WHERE bre.trader_id = t.trader_id
    AND bre.event_timestamp >= CURRENT_DATE - INTERVAL '7 days') AS recent_events
FROM
    trader_stats ts
JOIN
    financial_risk_management.trader_behavior tb ON ts.trader_id = tb.trader_id
JOIN
    hr.employees e ON ts.trader_id = e.employee_id
WHERE
    tb.analysis_date >= CURRENT_DATE - INTERVAL '30 days'
ORDER BY
    ts.max_risk_score DESC NULLS LAST;

COMMENT ON VIEW financial_risk_management.behavioral_risk_dashboard IS 'Consolidated view of behavioral risk indicators for front office staff with intervention tracking';


-- Climate Risk Exposure Matris
-- Business Case: Enables climate risk scenario analysis by aggregating physical risks across the organization's asset portfolio under different climate pathways.
CREATE MATERIALIZED VIEW financial_risk_management.climate_risk_exposure_matrix AS
SELECT
    asset_type,
    hazard_type,
    rcp_scenario,
    time_horizon,
    COUNT(*) AS asset_count,
    AVG(probability_score) AS avg_probability,
    AVG(impact_severity) AS avg_impact,
    SUM(financial_impact) AS total_financial_impact,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY impact_severity) AS worst_case_impact,
    (SELECT STRING_AGG(DISTINCT SUBSTRING(location_geography::TEXT, 1, 30), ' | ')
     FROM financial_risk_management.physical_climate_risks pcr2
     WHERE pcr2.asset_type = pcr1.asset_type
     AND pcr2.hazard_type = pcr1.hazard_type
     AND pcr2.rcp_scenario = pcr1.rcp_scenario
     AND pcr2.time_horizon = pcr1.time_horizon
     AND pcr2.impact_severity >= 70) AS high_risk_locations
FROM
    financial_risk_management.physical_climate_risks pcr1
GROUP BY
    asset_type, hazard_type, rcp_scenario, time_horizon
ORDER BY
    total_financial_impact DESC;

COMMENT ON MATERIALIZED VIEW financial_risk_management.climate_risk_exposure_matrix IS 'Pre-computed climate risk exposure across asset types, hazards, and scenarios for strategic planning';


--Business Case: Monitors growing DeFi exposures with risk-adjusted metrics to prevent overconcentration in high-risk protocols.
CREATE MATERIALIZED VIEW financial_risk_management.defi_risk_concentration AS
WITH protocol_exposures AS (
    SELECT
        d.protocol_id,
        p.protocol_name,
        p.protocol_category,
        COUNT(DISTINCT de.wallet_address) AS unique_wallets,
        SUM(de.exposure_amount_usd) AS total_exposure,
        AVG(d.composite_risk_score) AS avg_risk_score,
        MAX(d.composite_risk_score) AS max_risk_score
    FROM
        financial_risk_management.defi_exposures de
    JOIN
        financial_risk_management.defi_risk_metrics d ON de.protocol_id = d.protocol_id
    JOIN
        financial_risk_management.defi_protocols p ON de.protocol_id = p.protocol_id
    WHERE
        d.metric_timestamp >= CURRENT_TIMESTAMP - INTERVAL '1 day'
    GROUP BY
        d.protocol_id, p.protocol_name, p.protocol_category
)
SELECT
    protocol_id,
    protocol_name,
    protocol_category,
    unique_wallets,
    total_exposure,
    avg_risk_score,
    max_risk_score,
    total_exposure * avg_risk_score / 100 AS risk_adjusted_exposure,
    (SELECT COUNT(*)
     FROM financial_risk_management.defi_exposures de2
     WHERE de2.protocol_id = pe.protocol_id
     AND de2.liquidation_risk > 70) AS high_risk_positions,
    (SELECT STRING_AGG(wallet_address, ', ')
     FROM (SELECT wallet_address
           FROM financial_risk_management.defi_exposures de2
           WHERE de2.protocol_id = pe.protocol_id
           ORDER BY exposure_amount_usd DESC
           LIMIT 3) AS top_wallets) AS largest_exposure_wallets
FROM
    protocol_exposures pe
ORDER BY
    risk_adjusted_exposure DESC;

COMMENT ON MATERIALIZED VIEW financial_risk_management.defi_risk_concentration IS 'Identifies concentration risks in decentralized finance protocols with wallet-level exposure details';

--Adaptive behavioral risk scoring
-- Business Case: Continuously monitors and scores behavioral risk factors for trading staff with adaptive interventions to prevent misconduct events.
CREATE OR REPLACE PROCEDURE financial_risk_management.update_behavioral_scores()
LANGUAGE plpgsql
AS $$
DECLARE
    v_trader RECORD;
    v_new_score NUMERIC;
    v_anomaly_count INTEGER;
    v_insights TEXT;
BEGIN
    FOR v_trader IN SELECT DISTINCT trader_id FROM hr.employees WHERE department IN ('Trading', 'Investments') LOOP
        -- Get recent anomaly count
        SELECT COUNT(*) INTO v_anomaly_count
        FROM financial_risk_management.behavioral_risk_events
        WHERE trader_id = v_trader.trader_id
        AND event_timestamp >= CURRENT_DATE - INTERVAL '30 days';

        -- Generate AI insights (simplified example)
        SELECT 'Trader shows ' ||
               CASE
                   WHEN v_anomaly_count > 5 THEN 'elevated pattern of limit-testing behavior'
                   WHEN v_anomaly_count > 2 THEN 'occasional deviations from typical patterns'
                   ELSE 'consistent adherence to risk framework'
               END ||
               ' with ' || v_anomaly_count || ' recorded anomalies in past 30 days.'
        INTO v_insights;

        -- Calculate new risk score (simplified algorithm)
        v_new_score := LEAST(100, GREATEST(0,
            50 + (v_anomaly_count * 5) +
            CASE
                WHEN (SELECT COUNT(*) FROM financial_risk_management.behavioral_risk_events
                      WHERE trader_id = v_trader.trader_id AND risk_score > 80) > 0 THEN 15
                ELSE 0
            END));

        -- Determine intervention recommendation
        DECLARE
            v_recommendation VARCHAR(100);
        BEGIN
            IF v_new_score > 80 THEN
                v_recommendation := 'Immediate supervisor review + trading limits reduction';
            ELSIF v_new_score > 65 THEN
                v_recommendation := 'Enhanced monitoring + mandatory training';
            ELSIF v_new_score > 50 THEN
                v_recommendation := 'Monthly coaching session';
            ELSE
                v_recommendation := 'Routine monitoring';
            END IF;

            -- Update or insert behavioral record
            INSERT INTO financial_risk_management.trader_behavior (
                trader_id,
                analysis_date,
                risk_appetite_score,
                compliance_adherence,
                pattern_consistency,
                anomaly_count,
                ai_insights,
                intervention_recommendation
            ) VALUES (
                v_trader.trader_id,
                CURRENT_DATE,
                v_new_score,
                1 - (v_anomaly_count * 0.05),
                1 - (v_anomaly_count * 0.03),
                v_anomaly_count,
                v_insights,
                v_recommendation
            )
            ON CONFLICT (trader_id)
            DO UPDATE SET
                analysis_date = CURRENT_DATE,
                risk_appetite_score = v_new_score,
                compliance_adherence = 1 - (v_anomaly_count * 0.05),
                pattern_consistency = 1 - (v_anomaly_count * 0.03),
                anomaly_count = v_anomaly_count,
                ai_insights = v_insights,
                intervention_recommendation = v_recommendation,
                updated_at = CURRENT_TIMESTAMP;
        END;
    END LOOP;

    -- Refresh materialized views
    REFRESH MATERIALIZED VIEW financial_risk_management.behavioral_risk_dashboard;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE financial_risk_management.update_behavioral_scores IS 'Automatically updates trader behavior risk scores using anomaly detection patterns and AI analysis';


--climate value at risk (CVaR ) Calculations
-- Business Case: Quantifies potential financial impacts from both physical and transition climate risks under different warming scenarios to inform long-term strategy.
CREATE OR REPLACE PROCEDURE financial_risk_management.calculate_climate_var(
    p_scenario VARCHAR(10) DEFAULT 'RCP4.5',
    p_time_horizon VARCHAR(10) DEFAULT '2030'
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_total_assets INTEGER;
    v_physical_risk NUMERIC;
    v_transition_risk NUMERIC;
    v_cvar NUMERIC;
    v_high_risk_assets TEXT;
BEGIN
    -- Get physical risk exposure
    SELECT
        SUM(financial_impact),
        STRING_AGG(asset_id::TEXT, ', ' ORDER BY financial_impact DESC LIMIT 5)
    INTO
        v_physical_risk,
        v_high_risk_assets
    FROM
        financial_risk_management.physical_climate_risks
    WHERE
        rcp_scenario = p_scenario
        AND time_horizon = p_time_horizon
        AND probability_score > 0.3;

    -- Get transition risk (from separate analysis)
    SELECT transition_risk INTO v_transition_risk
    FROM financial_risk_management.climate_transition_risks
    WHERE scenario = p_scenario
    AND year = p_time_horizon;

    -- Calculate composite Climate VaR
    v_cvar := COALESCE(v_physical_risk, 0) + COALESCE(v_transition_risk, 0);

    -- Store results
    INSERT INTO financial_risk_management.climate_var_results (
        scenario,
        time_horizon,
        physical_risk_component,
        transition_risk_component,
        total_cvar,
        high_risk_assets,
        calculation_date
    ) VALUES (
        p_scenario,
        p_time_horizon,
        v_physical_risk,
        v_transition_risk,
        v_cvar,
        v_high_risk_assets,
        CURRENT_DATE
    );

    -- Generate alert if CVaR exceeds threshold
    IF v_cvar > (SELECT SUM(financial_impact) * 0.1
                 FROM financial_risk_management.physical_climate_risks) THEN
        INSERT INTO financial_risk_management.real_time_alerts (
            alert_type,
            severity,
            trigger_value,
            trigger_condition,
            alert_timestamp,
            status,
            escalation_path,
            related_entities
        ) VALUES (
            'Climate Risk',
            'High',
            v_cvar,
            'Climate VaR exceeds 10% of asset base under ' || p_scenario || ' scenario',
            CURRENT_TIMESTAMP,
            'Active',
            jsonb_build_array('CFO', 'Sustainability Committee', 'Board Risk Committee'),
            jsonb_build_object('scenario', p_scenario, 'time_horizon', p_time_horizon)
        );
    END IF;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE financial_risk_management.calculate_climate_var IS 'Calculates Climate Value at Risk (CVaR) under different warming scenarios for TCFD reporting and strategic planning';
----------------------
--Audit Management
----------------------
CREATE SCHEMA IF NOT EXISTS audit_management;
CREATE SCHEMA IF NOT EXISTS core;

-- Core tables that might be shared across modules
CREATE TABLE core.organizational_units (
    unit_id SERIAL PRIMARY KEY,
    unit_name VARCHAR(100) NOT NULL,
    parent_unit_id INTEGER REFERENCES core.organizational_units(unit_id),
    description TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE core.employees (
    employee_id SERIAL PRIMARY KEY,
    first_name VARCHAR(50) NOT NULL,
    last_name VARCHAR(50) NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    department_id INTEGER REFERENCES core.organizational_units(unit_id),
    position VARCHAR(100),
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE core.risk_categories (
    category_id SERIAL PRIMARY KEY,
    category_name VARCHAR(100) NOT NULL,
    description TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);


-- Audit universe containing all auditable entities
CREATE TABLE audit_management.audit_universe (
    audit_entity_id SERIAL PRIMARY KEY,
    entity_name VARCHAR(100) NOT NULL,
    entity_description TEXT,
    unit_id INTEGER REFERENCES core.organizational_units(unit_id),
    risk_category_id INTEGER REFERENCES core.risk_categories(category_id),
    inherent_risk_level VARCHAR(20),
    residual_risk_level VARCHAR(20),
    last_audit_date DATE,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Annual audit plan
CREATE TABLE audit_management.annual_audit_plan (
    plan_id SERIAL PRIMARY KEY,
    plan_year INTEGER NOT NULL,
    plan_name VARCHAR(100) NOT NULL,
    description TEXT,
    status VARCHAR(20) CHECK (status IN ('Draft', 'Approved', 'In Progress', 'Completed')),
    approved_by INTEGER REFERENCES core.employees(employee_id),
    approved_date DATE,
    created_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Individual audits within the annual plan
CREATE TABLE audit_management.audit_activities (
    audit_id SERIAL PRIMARY KEY,
    plan_id INTEGER REFERENCES audit_management.annual_audit_plan(plan_id) NOT NULL,
    audit_entity_id INTEGER REFERENCES audit_management.audit_universe(audit_entity_id) NOT NULL,
    audit_title VARCHAR(100) NOT NULL,
    audit_description TEXT,
    audit_type VARCHAR(50) CHECK (audit_type IN ('Financial', 'Operational', 'Compliance', 'IT', 'Special Investigation')),
    risk_level VARCHAR(20) CHECK (risk_level IN ('Low', 'Medium', 'High', 'Critical')),
    planned_start_date DATE NOT NULL,
    planned_end_date DATE NOT NULL,
    actual_start_date DATE,
    actual_end_date DATE,
    budget_estimated NUMERIC(12,2),
    budget_actual NUMERIC(12,2),
    status VARCHAR(20) CHECK (status IN ('Planned', 'In Progress', 'Fieldwork Completed', 'Reporting', 'Completed', 'Cancelled')),
    lead_auditor_id INTEGER REFERENCES core.employees(employee_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT valid_dates CHECK (planned_end_date >= planned_start_date)
);

-- Audit team assignments
CREATE TABLE audit_management.audit_team_members (
    team_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id) NOT NULL,
    employee_id INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    role VARCHAR(50) CHECK (role IN ('Lead Auditor', 'Auditor', 'Reviewer', 'Subject Matter Expert')),
    hours_allocated INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (audit_id, employee_id)
);

-- Audit schedule calendar
CREATE TABLE audit_management.audit_calendar (
    calendar_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id) NOT NULL,
    scheduled_date DATE NOT NULL,
    scheduled_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    status VARCHAR(20) CHECK (status IN ('Scheduled', 'Rescheduled', 'Completed', 'Cancelled')),
    notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Audit workpapers
CREATE TABLE audit_management.workpapers (
    workpaper_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id) NOT NULL,
    title VARCHAR(100) NOT NULL,
    description TEXT,
    workpaper_type VARCHAR(50) CHECK (workpaper_type IN ('Planning', 'Testing', 'Analysis', 'Summary')),
    status VARCHAR(20) CHECK (status IN ('Draft', 'In Review', 'Approved', 'Finalized')),
    created_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_by INTEGER REFERENCES core.employees(employee_id)
);

-- Workpaper versions
CREATE TABLE audit_management.workpaper_versions (
    version_id SERIAL PRIMARY KEY,
    workpaper_id INTEGER REFERENCES audit_management.workpapers(workpaper_id) NOT NULL,
    version_number INTEGER NOT NULL,
    content TEXT NOT NULL,
    change_description TEXT,
    created_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (workpaper_id, version_number)
);

-- Audit evidence
CREATE TABLE audit_management.audit_evidence (
    evidence_id SERIAL PRIMARY KEY,
    workpaper_id INTEGER REFERENCES audit_management.workpapers(workpaper_id) NOT NULL,
    evidence_type VARCHAR(50) CHECK (evidence_type IN ('Document', 'Email', 'Screenshot', 'Data Extract', 'Interview')),
    description TEXT NOT NULL,
    file_path VARCHAR(255),
    file_type VARCHAR(50),
    file_size INTEGER,
    collected_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    collected_date DATE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Audit testing procedures
CREATE TABLE audit_management.testing_procedures (
    test_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id) NOT NULL,
    test_name VARCHAR(100) NOT NULL,
    test_objective TEXT NOT NULL,
    test_method VARCHAR(100),
    sample_size INTEGER,
    population_size INTEGER,
    expected_result TEXT,
    actual_result TEXT,
    conclusion VARCHAR(100),
    performed_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    performed_date DATE NOT NULL,
    reviewed_by INTEGER REFERENCES core.employees(employee_id),
    review_date DATE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Audit findings
CREATE TABLE audit_management.audit_findings (
    finding_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id) NOT NULL,
    finding_title VARCHAR(100) NOT NULL,
    finding_description TEXT NOT NULL,
    finding_category VARCHAR(50) CHECK (finding_category IN ('Compliance', 'Operational', 'Financial', 'IT Security', 'Other')),
    severity VARCHAR(20) CHECK (severity IN ('Low', 'Medium', 'High', 'Critical')),
    root_cause TEXT,
    recommendation TEXT,
    reported_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    reported_date DATE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Finding evidence mapping
CREATE TABLE audit_management.finding_evidence (
    mapping_id SERIAL PRIMARY KEY,
    finding_id INTEGER REFERENCES audit_management.audit_findings(finding_id) NOT NULL,
    evidence_id INTEGER REFERENCES audit_management.audit_evidence(evidence_id) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (finding_id, evidence_id)
);

-- Issue tracking
CREATE TABLE audit_management.issue_tracking (
    issue_id SERIAL PRIMARY KEY,
    finding_id INTEGER REFERENCES audit_management.audit_findings(finding_id),
    issue_title VARCHAR(100) NOT NULL,
    issue_description TEXT NOT NULL,
    status VARCHAR(20) CHECK (status IN ('Open', 'In Progress', 'Resolved', 'Closed', 'Reopened')),
    priority VARCHAR(20) CHECK (priority IN ('Low', 'Medium', 'High', 'Critical')),
    assigned_to INTEGER REFERENCES core.employees(employee_id),
    assigned_date DATE,
    target_resolution_date DATE,
    actual_resolution_date DATE,
    created_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Corrective action plans
CREATE TABLE audit_management.corrective_actions (
    action_id SERIAL PRIMARY KEY,
    issue_id INTEGER REFERENCES audit_management.issue_tracking(issue_id) NOT NULL,
    action_description TEXT NOT NULL,
    responsible_party INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    due_date DATE NOT NULL,
    status VARCHAR(20) CHECK (status IN ('Not Started', 'In Progress', 'Completed', 'Verified')),
    completion_notes TEXT,
    completed_date DATE,
    verified_by INTEGER REFERENCES core.employees(employee_id),
    verification_date DATE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Audit reports
CREATE TABLE audit_management.audit_reports (
    report_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id) NOT NULL,
    report_title VARCHAR(100) NOT NULL,
    report_type VARCHAR(50) CHECK (report_type IN ('Draft', 'Interim', 'Final', 'Executive Summary')),
    report_status VARCHAR(20) CHECK (report_status IN ('Draft', 'In Review', 'Approved', 'Published')),
    report_content TEXT,
    published_date DATE,
    published_by INTEGER REFERENCES core.employees(employee_id),
    created_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Dashboard configurations
CREATE TABLE audit_management.dashboard_configs (
    dashboard_id SERIAL PRIMARY KEY,
    dashboard_name VARCHAR(100) NOT NULL,
    description TEXT,
    owner_id INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    is_shared BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Dashboard widgets
CREATE TABLE audit_management.dashboard_widgets (
    widget_id SERIAL PRIMARY KEY,
    dashboard_id INTEGER REFERENCES audit_management.dashboard_configs(dashboard_id) NOT NULL,
    widget_type VARCHAR(50) NOT NULL,
    widget_title VARCHAR(100) NOT NULL,
    widget_config JSONB NOT NULL,
    display_order INTEGER NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Audit reports
CREATE TABLE audit_management.audit_reports (
    report_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id) NOT NULL,
    report_title VARCHAR(100) NOT NULL,
    report_type VARCHAR(50) CHECK (report_type IN ('Draft', 'Interim', 'Final', 'Executive Summary')),
    report_status VARCHAR(20) CHECK (report_status IN ('Draft', 'In Review', 'Approved', 'Published')),
    report_content TEXT,
    published_date DATE,
    published_by INTEGER REFERENCES core.employees(employee_id),
    created_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Dashboard configurations
CREATE TABLE audit_management.dashboard_configs (
    dashboard_id SERIAL PRIMARY KEY,
    dashboard_name VARCHAR(100) NOT NULL,
    description TEXT,
    owner_id INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    is_shared BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Dashboard widgets
CREATE TABLE audit_management.dashboard_widgets (
    widget_id SERIAL PRIMARY KEY,
    dashboard_id INTEGER REFERENCES audit_management.dashboard_configs(dashboard_id) NOT NULL,
    widget_type VARCHAR(50) NOT NULL,
    widget_title VARCHAR(100) NOT NULL,
    widget_config JSONB NOT NULL,
    display_order INTEGER NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- KPI definitions
CREATE TABLE audit_management.kpi_definitions (
    kpi_id SERIAL PRIMARY KEY,
    kpi_name VARCHAR(100) NOT NULL,
    kpi_description TEXT,
    category VARCHAR(50) NOT NULL,
    measurement_unit VARCHAR(50) NOT NULL,
    target_value NUMERIC(12,2),
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- KPI measurements
CREATE TABLE audit_management.kpi_measurements (
    measurement_id SERIAL PRIMARY KEY,
    kpi_id INTEGER REFERENCES audit_management.kpi_definitions(kpi_id) NOT NULL,
    measurement_period DATE NOT NULL,
    measured_value NUMERIC(12,2) NOT NULL,
    notes TEXT,
    measured_by INTEGER REFERENCES core.employees(employee_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (kpi_id, measurement_period)
);

-- KRM definitions
CREATE TABLE audit_management.krm_definitions (
    krm_id SERIAL PRIMARY KEY,
    krm_name VARCHAR(100) NOT NULL,
    krm_description TEXT,
    category VARCHAR(50) NOT NULL,
    measurement_unit VARCHAR(50) NOT NULL,
    target_value NUMERIC(12,2),
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- KRM measurements
CREATE TABLE audit_management.krm_measurements (
    measurement_id SERIAL PRIMARY KEY,
    krm_id INTEGER REFERENCES audit_management.krm_definitions(krm_id) NOT NULL,
    measurement_period DATE NOT NULL,
    measured_value NUMERIC(12,2) NOT NULL,
    notes TEXT,
    measured_by INTEGER REFERENCES core.employees(employee_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (krm_id, measurement_period)
);


-- View for audit plan completion status
CREATE OR REPLACE VIEW audit_management.vw_audit_plan_completion AS
SELECT
    ap.plan_id,
    ap.plan_year,
    ap.plan_name,
    COUNT(aa.audit_id) AS total_audits,
    COUNT(CASE WHEN aa.status = 'Completed' THEN 1 END) AS completed_audits,
    COUNT(CASE WHEN aa.status = 'In Progress' THEN 1 END) AS in_progress_audits,
    COUNT(CASE WHEN aa.status = 'Planned' THEN 1 END) AS planned_audits,
    ROUND(COUNT(CASE WHEN aa.status = 'Completed' THEN 1 END) * 100.0 / COUNT(aa.audit_id), 2) AS completion_percentage
FROM
    audit_management.annual_audit_plan ap
LEFT JOIN
    audit_management.audit_activities aa ON ap.plan_id = aa.plan_id
GROUP BY
    ap.plan_id, ap.plan_year, ap.plan_name;

-- View for audit findings by severity
CREATE OR REPLACE VIEW audit_management.vw_audit_findings_severity AS
SELECT
    a.audit_id,
    a.audit_title,
    au.entity_name AS audited_entity,
    COUNT(f.finding_id) AS total_findings,
    COUNT(CASE WHEN f.severity = 'Critical' THEN 1 END) AS critical_findings,
    COUNT(CASE WHEN f.severity = 'High' THEN 1 END) AS high_findings,
    COUNT(CASE WHEN f.severity = 'Medium' THEN 1 END) AS medium_findings,
    COUNT(CASE WHEN f.severity = 'Low' THEN 1 END) AS low_findings
FROM
    audit_management.audit_activities a
JOIN
    audit_management.audit_universe au ON a.audit_entity_id = au.audit_entity_id
LEFT JOIN
    audit_management.audit_findings f ON a.audit_id = f.audit_id
GROUP BY
    a.audit_id, a.audit_title, au.entity_name;

-- View for issue remediation status
CREATE OR REPLACE VIEW audit_management.vw_issue_remediation_status AS
SELECT
    it.issue_id,
    it.issue_title,
    it.status AS issue_status,
    it.priority,
    it.assigned_to,
    e.first_name || ' ' || e.last_name AS assigned_to_name,
    it.target_resolution_date,
    it.actual_resolution_date,
    CASE
        WHEN it.actual_resolution_date IS NULL AND it.target_resolution_date < CURRENT_DATE THEN 'Overdue'
        WHEN it.actual_resolution_date IS NULL THEN 'Open'
        WHEN it.actual_resolution_date <= it.target_resolution_date THEN 'On Time'
        ELSE 'Late'
    END AS resolution_status,
    COUNT(ca.action_id) AS total_actions,
    COUNT(CASE WHEN ca.status = 'Completed' THEN 1 END) AS completed_actions,
    COUNT(CASE WHEN ca.status = 'In Progress' THEN 1 END) AS in_progress_actions
FROM
    audit_management.issue_tracking it
LEFT JOIN
    core.employees e ON it.assigned_to = e.employee_id
LEFT JOIN
    audit_management.corrective_actions ca ON it.issue_id = ca.issue_id
GROUP BY
    it.issue_id, it.issue_title, it.status, it.priority, it.assigned_to, e.first_name, e.last_name,
    it.target_resolution_date, it.actual_resolution_date;

-- View for KPI tracking
CREATE OR REPLACE VIEW audit_management.vw_kpi_tracking AS
SELECT
    kd.kpi_id,
    kd.kpi_name,
    kd.category,
    kd.measurement_unit,
    kd.target_value,
    km.measurement_period,
    km.measured_value,
    CASE
        WHEN kd.target_value IS NULL THEN NULL
        WHEN km.measured_value >= kd.target_value THEN 'Met'
        ELSE 'Not Met'
    END AS target_status,
    km.notes
FROM
    audit_management.kpi_definitions kd
JOIN
    audit_management.kpi_measurements km ON kd.kpi_id = km.kpi_id
ORDER BY
    kd.category, kd.kpi_name, km.measurement_period DESC;

    -- Materialized view for audit performance metrics (refreshed weekly)
CREATE MATERIALIZED VIEW audit_management.mv_audit_performance_metrics AS
SELECT
    DATE_TRUNC('month', aa.planned_start_date) AS month,
    COUNT(aa.audit_id) AS total_audits,
    AVG(EXTRACT(DAY FROM (aa.actual_end_date - aa.actual_start_date))) AS avg_duration_days,
    SUM(aa.budget_actual) AS total_audit_cost,
    SUM(CASE WHEN aa.actual_end_date <= aa.planned_end_date THEN 1 ELSE 0 END) * 100.0 / COUNT(aa.audit_id) AS on_time_completion_rate,
    COUNT(af.finding_id) AS total_findings,
    COUNT(CASE WHEN af.severity IN ('High', 'Critical') THEN 1 END) AS high_critical_findings
FROM
    audit_management.audit_activities aa
LEFT JOIN
    audit_management.audit_findings af ON aa.audit_id = af.audit_id
WHERE
    aa.status = 'Completed'
GROUP BY
    DATE_TRUNC('month', aa.planned_start_date)
ORDER BY
    month DESC;

-- Materialized view for risk-based audit coverage (refreshed monthly)
CREATE MATERIALIZED VIEW audit_management.mv_risk_based_audit_coverage AS
SELECT
    rc.category_name AS risk_category,
    COUNT(au.audit_entity_id) AS total_entities,
    COUNT(DISTINCT aa.audit_id) AS audited_entities,
    COUNT(DISTINCT aa.audit_id) * 100.0 / COUNT(au.audit_entity_id) AS coverage_percentage,
    AVG(CASE WHEN au.inherent_risk_level = 'High' THEN 1 ELSE 0 END) * 100 AS high_risk_percentage,
    AVG(CASE WHEN au.inherent_risk_level = 'Medium' THEN 1 ELSE 0 END) * 100 AS medium_risk_percentage,
    AVG(CASE WHEN au.inherent_risk_level = 'Low' THEN 1 ELSE 0 END) * 100 AS low_risk_percentage
FROM
    core.risk_categories rc
LEFT JOIN
    audit_management.audit_universe au ON rc.category_id = au.risk_category_id
LEFT JOIN
    audit_management.audit_activities aa ON au.audit_entity_id = aa.audit_entity_id AND aa.status = 'Completed'
GROUP BY
    rc.category_name
ORDER BY
    coverage_percentage DESC;

-- Materialized view for issue aging analysis (refreshed daily)
CREATE MATERIALIZED VIEW audit_management.mv_issue_aging_analysis AS
SELECT
    it.assigned_to,
    e.first_name || ' ' || e.last_name AS assigned_to_name,
    COUNT(it.issue_id) AS total_issues,
    COUNT(CASE WHEN it.status = 'Open' THEN 1 END) AS open_issues,
    COUNT(CASE WHEN it.status = 'In Progress' THEN 1 END) AS in_progress_issues,
    COUNT(CASE WHEN it.status = 'Resolved' THEN 1 END) AS resolved_issues,
    AVG(CASE WHEN it.status IN ('Open', 'In Progress') THEN CURRENT_DATE - it.assigned_date END) AS avg_days_open,
    MAX(CASE WHEN it.status IN ('Open', 'In Progress') THEN CURRENT_DATE - it.assigned_date END) AS max_days_open,
    COUNT(CASE WHEN it.status IN ('Open', 'In Progress') AND CURRENT_DATE - it.assigned_date > 30 THEN 1 END) AS issues_over_30_days
FROM
    audit_management.issue_tracking it
LEFT JOIN
    core.employees e ON it.assigned_to = e.employee_id
WHERE
    it.status IN ('Open', 'In Progress')
GROUP BY
    it.assigned_to, e.first_name, e.last_name
ORDER BY
    avg_days_open DESC;


    -- Procedure to create a new audit activity
    CREATE OR REPLACE PROCEDURE audit_management.sp_create_audit_activity(
        p_plan_id INTEGER,
        p_audit_entity_id INTEGER,
        p_audit_title VARCHAR(100),
        p_audit_description TEXT,
        p_audit_type VARCHAR(50),
        p_risk_level VARCHAR(20),
        p_planned_start_date DATE,
        p_planned_end_date DATE,
        p_budget_estimated NUMERIC(12,2),
        p_lead_auditor_id INTEGER,
        OUT p_audit_id INTEGER
    )
    LANGUAGE plpgsql
    AS $$
    BEGIN
        -- Validate dates
        IF p_planned_end_date < p_planned_start_date THEN
            RAISE EXCEPTION 'Planned end date must be after planned start date';
        END IF;

        -- Insert the new audit activity
        INSERT INTO audit_management.audit_activities (
            plan_id,
            audit_entity_id,
            audit_title,
            audit_description,
            audit_type,
            risk_level,
            planned_start_date,
            planned_end_date,
            budget_estimated,
            status,
            lead_auditor_id,
            created_at,
            updated_at
        ) VALUES (
            p_plan_id,
            p_audit_entity_id,
            p_audit_title,
            p_audit_description,
            p_audit_type,
            p_risk_level,
            p_planned_start_date,
            p_planned_end_date,
            p_budget_estimated,
            'Planned',
            p_lead_auditor_id,
            CURRENT_TIMESTAMP,
            CURRENT_TIMESTAMP
        ) RETURNING audit_id INTO p_audit_id;

        -- Add lead auditor to the audit team
        INSERT INTO audit_management.audit_team_members (
            audit_id,
            employee_id,
            role,
            created_at,
            updated_at
        ) VALUES (
            p_audit_id,
            p_lead_auditor_id,
            'Lead Auditor',
            CURRENT_TIMESTAMP,
            CURRENT_TIMESTAMP
        );

        -- Log the creation
        RAISE NOTICE 'Created new audit activity with ID: %', p_audit_id;
    END;
    $$;

    -- Procedure to add a finding to an audit
    CREATE OR REPLACE PROCEDURE audit_management.sp_add_audit_finding(
        p_audit_id INTEGER,
        p_finding_title VARCHAR(100),
        p_finding_description TEXT,
        p_finding_category VARCHAR(50),
        p_severity VARCHAR(20),
        p_root_cause TEXT,
        p_recommendation TEXT,
        p_reported_by INTEGER,
        OUT p_finding_id INTEGER
    )
    LANGUAGE plpgsql
    AS $$
    BEGIN
        -- Validate audit exists and is in progress
        IF NOT EXISTS (SELECT 1 FROM audit_management.audit_activities WHERE audit_id = p_audit_id AND status IN ('In Progress', 'Fieldwork Completed')) THEN
            RAISE EXCEPTION 'Audit must be in progress to add findings';
        END IF;

        -- Insert the finding
        INSERT INTO audit_management.audit_findings (
            audit_id,
            finding_title,
            finding_description,
            finding_category,
            severity,
            root_cause,
            recommendation,
            reported_by,
            reported_date,
            created_at,
            updated_at
        ) VALUES (
            p_audit_id,
            p_finding_title,
            p_finding_description,
            p_finding_category,
            p_severity,
            p_root_cause,
            p_recommendation,
            p_reported_by,
            CURRENT_DATE,
            CURRENT_TIMESTAMP,
            CURRENT_TIMESTAMP
        ) RETURNING finding_id INTO p_finding_id;

        -- Create corresponding issue
        INSERT INTO audit_management.issue_tracking (
            finding_id,
            issue_title,
            issue_description,
            status,
            priority,
            created_by,
            created_at,
            updated_at
        ) VALUES (
            p_finding_id,
            p_finding_title,
            p_finding_description,
            'Open',
            p_severity,
            p_reported_by,
            CURRENT_TIMESTAMP,
            CURRENT_TIMESTAMP
        );

        -- Log the creation
        RAISE NOTICE 'Added new finding with ID: % to audit ID: %', p_finding_id, p_audit_id;
    END;
    $$;

    -- Procedure to update issue status
    CREATE OR REPLACE PROCEDURE audit_management.sp_update_issue_status(
        p_issue_id INTEGER,
        p_new_status VARCHAR(20),
        p_updated_by INTEGER,
        p_resolution_date DATE DEFAULT NULL,
        p_completion_notes TEXT DEFAULT NULL
    )
    LANGUAGE plpgsql
    AS $$
    DECLARE
        v_current_status VARCHAR(20);
        v_finding_id INTEGER;
    BEGIN
        -- Get current status
        SELECT status, finding_id INTO v_current_status, v_finding_id
        FROM audit_management.issue_tracking
        WHERE issue_id = p_issue_id;

        -- Validate transition
        IF v_current_status = 'Closed' AND p_new_status != 'Reopened' THEN
            RAISE EXCEPTION 'Closed issues can only be reopened';
        END IF;

        IF p_new_status = 'Resolved' AND p_resolution_date IS NULL THEN
            RAISE EXCEPTION 'Resolution date is required when resolving an issue';
        END IF;

        -- Update issue status
        UPDATE audit_management.issue_tracking
        SET
            status = p_new_status,
            actual_resolution_date = CASE WHEN p_new_status = 'Resolved' THEN p_resolution_date ELSE NULL END,
            updated_at = CURRENT_TIMESTAMP
        WHERE issue_id = p_issue_id;

        -- Log the update
        RAISE NOTICE 'Updated issue ID: % from status: % to status: %', p_issue_id, v_current_status, p_new_status;

        -- If resolving, update any open corrective actions
        IF p_new_status = 'Resolved' THEN
            UPDATE audit_management.corrective_actions
            SET
                status = 'Completed',
                completion_notes = p_completion_notes,
                completed_date = p_resolution_date,
                updated_at = CURRENT_TIMESTAMP
            WHERE
                issue_id = p_issue_id
                AND status IN ('Not Started', 'In Progress');

            RAISE NOTICE 'Marked all corrective actions for issue ID: % as completed', p_issue_id;
        END IF;
    END;
    $$;

    -- Procedure to calculate and store KPIs
    CREATE OR REPLACE PROCEDURE audit_management.sp_calculate_kpis(
        p_measurement_period DATE DEFAULT CURRENT_DATE
    )
    LANGUAGE plpgsql
    AS $$
    DECLARE
        v_audit_plan_completion NUMERIC(12,2);
        v_avg_remediation_time NUMERIC(12,2);
        v_audit_cycle_reduction NUMERIC(12,2);
        v_prev_year_avg_cycle NUMERIC(12,2);
        v_current_year_avg_cycle NUMERIC(12,2);
    BEGIN
        -- Calculate Audit Plan Completion Rate
        SELECT
            COUNT(CASE WHEN status = 'Completed' THEN 1 END) * 100.0 / COUNT(audit_id)
        INTO v_audit_plan_completion
        FROM audit_management.audit_activities
        WHERE planned_start_date >= DATE_TRUNC('year', p_measurement_period)
          AND planned_start_date < DATE_TRUNC('year', p_measurement_period) + INTERVAL '1 year';

        -- Store KPI
        INSERT INTO audit_management.kpi_measurements (
            kpi_id,
            measurement_period,
            measured_value,
            measured_by,
            created_at,
            updated_at
        )
        SELECT
            kpi_id,
            p_measurement_period,
            v_audit_plan_completion,
            1, -- System user
            CURRENT_TIMESTAMP,
            CURRENT_TIMESTAMP
        FROM
            audit_management.kpi_definitions
        WHERE
            kpi_name = 'Audit Plan Completion Rate'
        ON CONFLICT (kpi_id, measurement_period)
        DO UPDATE SET
            measured_value = v_audit_plan_completion,
            updated_at = CURRENT_TIMESTAMP;

        -- Calculate Average Remediation Time for Critical Findings
        SELECT
            AVG(EXTRACT(DAY FROM (actual_resolution_date - assigned_date))
        INTO v_avg_remediation_time
        FROM audit_management.issue_tracking
        WHERE priority = 'Critical'
          AND status = 'Resolved'
          AND actual_resolution_date >= DATE_TRUNC('year', p_measurement_period)
          AND actual_resolution_date < DATE_TRUNC('year', p_measurement_period) + INTERVAL '1 year';

        -- Store KPI
        INSERT INTO audit_management.kpi_measurements (
            kpi_id,
            measurement_period,
            measured_value,
            measured_by,
            created_at,
            updated_at
        )
        SELECT
            kpi_id,
            p_measurement_period,
            v_avg_remediation_time,
            1, -- System user
            CURRENT_TIMESTAMP,
            CURRENT_TIMESTAMP
        FROM
            audit_management.kpi_definitions
        WHERE
            kpi_name = 'Audit Finding Remediation Time'
        ON CONFLICT (kpi_id, measurement_period)
        DO UPDATE SET
            measured_value = v_avg_remediation_time,
            updated_at = CURRENT_TIMESTAMP;

        -- Calculate Audit Efficiency (Reduction in Audit Cycle Time)
        -- Get previous year average
        SELECT
            AVG(EXTRACT(DAY FROM (actual_end_date - actual_start_date)))
        INTO v_prev_year_avg_cycle
        FROM audit_management.audit_activities
        WHERE status = 'Completed'
          AND actual_end_date >= DATE_TRUNC('year', p_measurement_period) - INTERVAL '1 year'
          AND actual_end_date < DATE_TRUNC('year', p_measurement_period);

        -- Get current year average
        SELECT
            AVG(EXTRACT(DAY FROM (actual_end_date - actual_start_date)))
        INTO v_current_year_avg_cycle
        FROM audit_management.audit_activities
        WHERE status = 'Completed'
          AND actual_end_date >= DATE_TRUNC('year', p_measurement_period)
          AND actual_end_date < DATE_TRUNC('year', p_measurement_period) + INTERVAL '1 year';

        -- Calculate percentage reduction
        IF v_prev_year_avg_cycle > 0 THEN
            v_audit_cycle_reduction := (1 - (v_current_year_avg_cycle / v_prev_year_avg_cycle)) * 100;
        ELSE
            v_audit_cycle_reduction := 0;
        END IF;

        -- Store KPI
        INSERT INTO audit_management.kpi_measurements (
            kpi_id,
            measurement_period,
            measured_value,
            measured_by,
            created_at,
            updated_at
        )
        SELECT
            kpi_id,
            p_measurement_period,
            v_audit_cycle_reduction,
            1, -- System user
            CURRENT_TIMESTAMP,
            CURRENT_TIMESTAMP
        FROM
            audit_management.kpi_definitions
        WHERE
            kpi_name = 'Audit Efficiency'
        ON CONFLICT (kpi_id, measurement_period)
        DO UPDATE SET
            measured_value = v_audit_cycle_reduction,
            updated_at = CURRENT_TIMESTAMP;

        -- Log completion
        RAISE NOTICE 'Calculated and stored KPIs for measurement period: %', p_measurement_period;
    END;
    $$;

-- create a new audit
CALL audit_management.sp_create_audit_activity(
    1, -- plan_id
    5, -- audit_entity_id
    'Financial Controls Audit Q2', -- audit_title
    'Audit of financial controls for Q2 2023', -- description
    'Financial', -- audit_type
    'High', -- risk_level
    '2023-04-01', -- planned_start_date
    '2023-04-15', -- planned_end_date
    5000.00, -- budget_estimated
    101, -- lead_auditor_id
    NULL -- output parameter
);

-- adding a finding to an audit
CALL audit_management.sp_add_audit_finding(
    42, -- audit_id
    'Missing approval for large payments', -- finding_title
    'Payments over $10,000 were processed without required approval', -- description
    'Financial', -- category
    'High', -- severity
    'Lack of training on approval thresholds', -- root_cause
    'Implement training on payment approval requirements', -- recommendation
    102, -- reported_by
    NULL -- output parameter
);

--updating an issue status
CALL audit_management.sp_update_issue_status(
    15, -- issue_id
    'Resolved', -- new_status
    103, -- updated_by
    '2023-05-15', -- resolution_date
    'Training completed for all finance staff' -- completion_notes
);

CALL audit_management.sp_calculate_kpis();

-- Enhanced Audit Management Schema with additional tables, views, and procedures
-- All new objects include inline documentation with business case explanations

--------------------------------------------------
-- NEW TABLES FOR ENHANCED FUNCTIONALITY
--------------------------------------------------

/*
 * Table: audit_management.risk_assessment_snapshots
 * Purpose: Captures periodic risk assessments to track risk profile changes over time
 * Business Case: Enables trend analysis of risk exposure and supports risk-based audit planning
 * by maintaining historical risk data for comparison.
 */
CREATE TABLE audit_management.risk_assessment_snapshots (
    snapshot_id SERIAL PRIMARY KEY,
    assessment_date DATE NOT NULL,
    assessed_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    assessment_method VARCHAR(100) NOT NULL,
    total_high_risk_entities INTEGER NOT NULL,
    total_medium_risk_entities INTEGER NOT NULL,
    total_low_risk_entities INTEGER NOT NULL,
    notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

/*
 * Table: audit_management.compliance_requirements
 * Purpose: Tracks regulatory and policy requirements that audits must assess
 * Business Case: Centralizes compliance obligations to ensure audit coverage
 * of all critical regulations and internal policies.
 */
CREATE TABLE audit_management.compliance_requirements (
    requirement_id SERIAL PRIMARY KEY,
    requirement_name VARCHAR(100) NOT NULL,
    description TEXT,
    regulation_standard VARCHAR(100) NOT NULL,
    applicable_to VARCHAR(100) NOT NULL, -- Departments, processes, etc.
    effective_date DATE NOT NULL,
    review_frequency VARCHAR(50) NOT NULL, -- Annual, Quarterly, etc.
    owner_id INTEGER REFERENCES core.employees(employee_id),
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

/*
 * Table: audit_management.audit_compliance_mapping
 * Purpose: Maps audits to compliance requirements they address
 * Business Case: Demonstrates coverage of compliance obligations through
 * audit activities and identifies gaps in compliance verification.
 */
CREATE TABLE audit_management.audit_compliance_mapping (
    mapping_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id) NOT NULL,
    requirement_id INTEGER REFERENCES audit_management.compliance_requirements(requirement_id) NOT NULL,
    coverage_percentage INTEGER CHECK (coverage_percentage BETWEEN 0 AND 100),
    notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (audit_id, requirement_id)
);

/*
 * Table: audit_management.stakeholder_communications
 * Purpose: Logs all communications with audit stakeholders
 * Business Case: Provides audit trail of stakeholder engagement
 * and ensures transparency in audit communications.
 */
CREATE TABLE audit_management.stakeholder_communications (
    communication_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id),
    communication_type VARCHAR(50) NOT NULL CHECK (communication_type IN ('Email', 'Meeting', 'Report', 'Presentation', 'Other')),
    subject VARCHAR(200) NOT NULL,
    content TEXT,
    sent_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    sent_to INTEGER REFERENCES core.employees(employee_id),
    sent_date TIMESTAMP NOT NULL,
    acknowledgement_required BOOLEAN DEFAULT FALSE,
    acknowledgement_received BOOLEAN DEFAULT FALSE,
    acknowledgement_date TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

/*
 * Table: audit_management.continuous_monitoring
 * Purpose: Stores data from continuous monitoring activities
 * Business Case: Supports integration of continuous auditing/monitoring
 * with traditional audit activities for comprehensive coverage.
 */
CREATE TABLE audit_management.continuous_monitoring (
    monitoring_id SERIAL PRIMARY KEY,
    monitor_name VARCHAR(100) NOT NULL,
    description TEXT,
    frequency VARCHAR(50) NOT NULL, -- Daily, Weekly, Monthly, etc.
    risk_category_id INTEGER REFERENCES core.risk_categories(category_id),
    control_being_monitored VARCHAR(200),
    alert_threshold NUMERIC(12,2),
    alert_message TEXT,
    is_active BOOLEAN DEFAULT TRUE,
    created_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

/*
 * Table: audit_management.monitoring_alerts
 * Purpose: Records alerts generated by continuous monitoring
 * Business Case: Provides audit trail of automated monitoring
 * results and facilitates timely follow-up on anomalies.
 */
CREATE TABLE audit_management.monitoring_alerts (
    alert_id SERIAL PRIMARY KEY,
    monitoring_id INTEGER REFERENCES audit_management.continuous_monitoring(monitoring_id) NOT NULL,
    alert_date TIMESTAMP NOT NULL,
    alert_value NUMERIC(12,2) NOT NULL,
    severity VARCHAR(20) CHECK (severity IN ('Low', 'Medium', 'High', 'Critical')),
    status VARCHAR(20) CHECK (status IN ('Open', 'Investigating', 'Resolved', 'False Positive')),
    assigned_to INTEGER REFERENCES core.employees(employee_id),
    resolution_notes TEXT,
    resolved_date TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

/*
 * Table: audit_management.business_continuity_audits
 * Purpose: Specialized audits for business continuity and disaster recovery
 * Business Case: Tracks resilience-specific audit activities separately
 * to ensure proper coverage of critical continuity controls.
 */
CREATE TABLE audit_management.business_continuity_audits (
    bca_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id) NOT NULL,
    bc_plan_version VARCHAR(50) NOT NULL,
    last_test_date DATE,
    test_coverage_percentage INTEGER CHECK (test_coverage_percentage BETWEEN 0 AND 100),
    rto_achieved NUMERIC(10,2), -- Recovery Time Objective in hours
    rpo_achieved NUMERIC(10,2), -- Recovery Point Objective in hours
    critical_processes_covered INTEGER,
    gaps_identified INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

--------------------------------------------------
-- ENHANCED VIEWS WITH BUSINESS CASE DOCUMENTATION
--------------------------------------------------

/*
 * View: audit_management.vw_compliance_coverage
 * Purpose: Shows coverage of compliance requirements by audit activities
 * Business Case: Helps audit management ensure all regulatory requirements
 * are being assessed and identifies gaps in compliance verification.
 */
CREATE OR REPLACE VIEW audit_management.vw_compliance_coverage AS
SELECT
    cr.requirement_id,
    cr.requirement_name,
    cr.regulation_standard,
    cr.applicable_to,
    COUNT(DISTINCT acm.audit_id) AS audits_covering,
    MAX(aa.actual_end_date) AS last_audit_date,
    CASE
        WHEN MAX(aa.actual_end_date) IS NULL THEN 'Never Audited'
        WHEN MAX(aa.actual_end_date) < CURRENT_DATE - INTERVAL '1 year' THEN 'Overdue'
        ELSE 'Current'
    END AS audit_status,
    STRING_AGG(DISTINCT aa.audit_title, ', ') AS audit_titles
FROM
    audit_management.compliance_requirements cr
LEFT JOIN
    audit_management.audit_compliance_mapping acm ON cr.requirement_id = acm.requirement_id
LEFT JOIN
    audit_management.audit_activities aa ON acm.audit_id = aa.audit_id
WHERE
    cr.is_active = TRUE
GROUP BY
    cr.requirement_id, cr.requirement_name, cr.regulation_standard, cr.applicable_to
ORDER BY
    audit_status, cr.regulation_standard, cr.requirement_name;

/*
 * View: audit_management.vw_continuous_monitoring_summary
 * Purpose: Provides overview of continuous monitoring program effectiveness
 * Business Case: Demonstrates value of continuous monitoring by showing
 * alert volumes and resolution rates to justify investment in automation.
 */
CREATE OR REPLACE VIEW audit_management.vw_continuous_monitoring_summary AS
SELECT
    cm.monitor_name,
    cm.description,
    cm.frequency,
    rc.category_name AS risk_category,
    COUNT(ma.alert_id) AS total_alerts,
    COUNT(CASE WHEN ma.severity = 'Critical' THEN 1 END) AS critical_alerts,
    COUNT(CASE WHEN ma.severity = 'High' THEN 1 END) AS high_alerts,
    COUNT(CASE WHEN ma.status = 'Open' THEN 1 END) AS open_alerts,
    COUNT(CASE WHEN ma.status = 'Resolved' THEN 1 END) AS resolved_alerts,
    ROUND(COUNT(CASE WHEN ma.status = 'Resolved' THEN 1 END) * 100.0 /
          NULLIF(COUNT(ma.alert_id), 0), 2) AS resolution_rate,
    MAX(ma.alert_date) AS last_alert_date
FROM
    audit_management.continuous_monitoring cm
LEFT JOIN
    core.risk_categories rc ON cm.risk_category_id = rc.category_id
LEFT JOIN
    audit_management.monitoring_alerts ma ON cm.monitoring_id = ma.monitoring_id
WHERE
    cm.is_active = TRUE
GROUP BY
    cm.monitoring_id, cm.monitor_name, cm.description, cm.frequency, rc.category_name
ORDER BY
    total_alerts DESC;

/*
 * View: audit_management.vw_risk_trend_analysis
 * Purpose: Analyzes risk trends over time using snapshot data
 * Business Case: Enables identification of increasing risk areas that may
 * require additional audit focus or mitigation efforts.
 */
CREATE OR REPLACE VIEW audit_management.vw_risk_trend_analysis AS
WITH snapshot_data AS (
    SELECT
        assessment_date,
        total_high_risk_entities,
        total_medium_risk_entities,
        total_low_risk_entities,
        total_high_risk_entities + total_medium_risk_entities + total_low_risk_entities AS total_entities,
        LAG(total_high_risk_entities) OVER (ORDER BY assessment_date) AS prev_high,
        LAG(total_medium_risk_entities) OVER (ORDER BY assessment_date) AS prev_medium,
        LAG(total_low_risk_entities) OVER (ORDER BY assessment_date) AS prev_low
    FROM
        audit_management.risk_assessment_snapshots
)
SELECT
    assessment_date,
    total_high_risk_entities,
    total_medium_risk_entities,
    total_low_risk_entities,
    total_entities,
    ROUND(total_high_risk_entities * 100.0 / total_entities, 2) AS pct_high_risk,
    ROUND(total_medium_risk_entities * 100.0 / total_entities, 2) AS pct_medium_risk,
    ROUND(total_low_risk_entities * 100.0 / total_entities, 2) AS pct_low_risk,
    CASE
        WHEN prev_high IS NULL THEN NULL
        ELSE total_high_risk_entities - prev_high
    END AS high_risk_change,
    CASE
        WHEN prev_medium IS NULL THEN NULL
        ELSE total_medium_risk_entities - prev_medium
    END AS medium_risk_change,
    CASE
        WHEN prev_low IS NULL THEN NULL
        ELSE total_low_risk_entities - prev_low
    END AS low_risk_change
FROM
    snapshot_data
ORDER BY
    assessment_date DESC;

--------------------------------------------------
-- NEW MATERIALIZED VIEWS WITH BUSINESS CASE
--------------------------------------------------

/*
 * Materialized View: audit_management.mv_audit_finding_trends
 * Purpose: Tracks finding trends by category and severity over time
 * Business Case: Identifies recurring issues and areas with worsening
 * control environments to focus remediation efforts (refreshed monthly).
 */
CREATE MATERIALIZED VIEW audit_management.mv_audit_finding_trends AS
SELECT
    DATE_TRUNC('month', af.created_at) AS month,
    af.finding_category,
    af.severity,
    COUNT(af.finding_id) AS finding_count,
    COUNT(DISTINCT af.audit_id) AS audits_with_findings,
    COUNT(CASE WHEN it.status = 'Open' THEN 1 END) AS open_issues,
    COUNT(CASE WHEN it.status = 'Resolved' THEN 1 END) AS resolved_issues,
    AVG(EXTRACT(DAY FROM (it.actual_resolution_date - it.assigned_date))) AS avg_resolution_days
FROM
    audit_management.audit_findings af
LEFT JOIN
    audit_management.issue_tracking it ON af.finding_id = it.finding_id
GROUP BY
    DATE_TRUNC('month', af.created_at), af.finding_category, af.severity
ORDER BY
    month DESC, finding_count DESC;

/*
 * Materialized View: audit_management.mv_business_continuity_coverage
 * Purpose: Tracks audit coverage of business continuity plans
 * Business Case: Ensures critical business continuity controls
 * are regularly tested and demonstrates compliance with resilience
 * requirements (refreshed quarterly).
 */
CREATE MATERIALIZED VIEW audit_management.mv_business_continuity_coverage AS
SELECT
    ou.unit_name AS business_unit,
    COUNT(DISTINCT bca.bca_id) AS bc_audits_completed,
    MAX(bca.last_test_date) AS last_test_date,
    ROUND(AVG(bca.test_coverage_percentage), 2) AS avg_test_coverage,
    ROUND(AVG(bca.rto_achieved), 2) AS avg_rto_achieved,
    ROUND(AVG(bca.rpo_achieved), 2) AS avg_rpo_achieved,
    SUM(bca.gaps_identified) AS total_gaps_identified,
    COUNT(DISTINCT CASE WHEN bca.last_test_date < CURRENT_DATE - INTERVAL '1 year' THEN bca.bca_id END) AS overdue_for_testing
FROM
    audit_management.business_continuity_audits bca
JOIN
    audit_management.audit_activities aa ON bca.audit_id = aa.audit_id
JOIN
    audit_management.audit_universe au ON aa.audit_entity_id = au.audit_entity_id
JOIN
    core.organizational_units ou ON au.unit_id = ou.unit_id
GROUP BY
    ou.unit_name
ORDER BY
    overdue_for_testing DESC, avg_test_coverage;

--------------------------------------------------
-- ENHANCED STORED PROCEDURES WITH BUSINESS CASE
--------------------------------------------------

/*
 * Procedure: audit_management.sp_schedule_risk_based_audits
 * Purpose: Automates audit scheduling based on risk assessments
 * Business Case: Improves audit planning efficiency by automatically
 * scheduling audits for high-risk areas while considering resource constraints.
 */
CREATE OR REPLACE PROCEDURE audit_management.sp_schedule_risk_based_audits(
    p_plan_id INTEGER,
    p_max_audits INTEGER,
    p_start_date DATE,
    p_end_date DATE,
    p_lead_auditor_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_audit_count INTEGER := 0;
    v_entity RECORD;
BEGIN
    -- Validate plan exists
    IF NOT EXISTS (SELECT 1 FROM audit_management.annual_audit_plan WHERE plan_id = p_plan_id) THEN
        RAISE EXCEPTION 'Audit plan % does not exist', p_plan_id;
    END IF;

    -- Clear existing scheduled audits for this plan that are still in planned status
    DELETE FROM audit_management.audit_calendar
    WHERE audit_id IN (
        SELECT audit_id FROM audit_management.audit_activities
        WHERE plan_id = p_plan_id AND status = 'Planned'
    );

    DELETE FROM audit_management.audit_activities
    WHERE plan_id = p_plan_id AND status = 'Planned';

    -- Schedule audits for high risk entities first, then medium
    FOR v_entity IN (
        SELECT
            au.audit_entity_id,
            au.entity_name,
            au.inherent_risk_level,
            rs.snapshot_id,
            DENSE_RANK() OVER (ORDER BY
                CASE au.inherent_risk_level
                    WHEN 'High' THEN 1
                    WHEN 'Medium' THEN 2
                    WHEN 'Low' THEN 3
                END,
                rs.total_high_risk_entities DESC) AS risk_rank
        FROM
            audit_management.audit_universe au
        JOIN
            audit_management.risk_assessment_snapshots rs ON au.risk_category_id = rs.snapshot_id
        WHERE
            au.is_active = TRUE
            AND (au.last_audit_date IS NULL OR au.last_audit_date < CURRENT_DATE - INTERVAL '18 months')
        ORDER BY
            risk_rank
        LIMIT
            p_max_audits
    ) LOOP
        -- Create audit activity
        INSERT INTO audit_management.audit_activities (
            plan_id,
            audit_entity_id,
            audit_title,
            audit_description,
            audit_type,
            risk_level,
            planned_start_date,
            planned_end_date,
            status,
            lead_auditor_id,
            created_at,
            updated_at
        ) VALUES (
            p_plan_id,
            v_entity.audit_entity_id,
            'Risk-Based Audit: ' || v_entity.entity_name,
            'Audit of ' || v_entity.entity_name || ' based on ' || v_entity.inherent_risk_level || ' risk rating',
            'Operational',
            v_entity.inherent_risk_level,
            p_start_date + (v_audit_count * INTERVAL '2 weeks'),
            p_start_date + (v_audit_count * INTERVAL '2 weeks') + INTERVAL '2 weeks',
            'Planned',
            p_lead_auditor_id,
            CURRENT_TIMESTAMP,
            CURRENT_TIMESTAMP
        );

        -- Add to calendar
        INSERT INTO audit_management.audit_calendar (
            audit_id,
            scheduled_date,
            scheduled_by,
            status,
            created_at,
            updated_at
        ) VALUES (
            CURRVAL('audit_management.audit_activities_audit_id_seq'),
            p_start_date + (v_audit_count * INTERVAL '2 weeks'),
            p_lead_auditor_id,
            'Scheduled',
            CURRENT_TIMESTAMP,
            CURRENT_TIMESTAMP
        );

        v_audit_count := v_audit_count + 1;

        -- Exit if we've reached max audits
        EXIT WHEN v_audit_count >= p_max_audits;
    END LOOP;

    RAISE NOTICE 'Scheduled % risk-based audits for plan %', v_audit_count, p_plan_id;
END;
$$;

/*
 * Procedure: audit_management.sp_generate_audit_report
 * Purpose: Generates comprehensive audit reports with findings and recommendations
 * Business Case: Standardizes audit reporting format and ensures all critical
 * elements are included while saving auditor time on report preparation.
 */
CREATE OR REPLACE PROCEDURE audit_management.sp_generate_audit_report(
    p_audit_id INTEGER,
    p_report_type VARCHAR(50),
    p_published_by INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_audit RECORD;
    v_report_content TEXT := '';
    v_finding_count INTEGER := 0;
    v_critical_findings INTEGER := 0;
    v_high_findings INTEGER := 0;
BEGIN
    -- Get audit details
    SELECT
        aa.audit_id, aa.audit_title, aa.audit_description,
        aa.planned_start_date, aa.actual_start_date,
        aa.planned_end_date, aa.actual_end_date,
        au.entity_name AS audited_entity,
        e.first_name || ' ' || e.last_name AS lead_auditor
    INTO v_audit
    FROM
        audit_management.audit_activities aa
    JOIN
        audit_management.audit_universe au ON aa.audit_entity_id = au.audit_entity_id
    JOIN
        core.employees e ON aa.lead_auditor_id = e.employee_id
    WHERE
        aa.audit_id = p_audit_id;

    IF v_audit.audit_id IS NULL THEN
        RAISE EXCEPTION 'Audit ID % not found', p_audit_id;
    END IF;

    -- Start building report content
    v_report_content := 'AUDIT REPORT' || E'\n' ||
                        '=============' || E'\n\n' ||
                        'Audit Title: ' || v_audit.audit_title || E'\n' ||
                        'Audited Entity: ' || v_audit.audited_entity || E'\n' ||
                        'Lead Auditor: ' || v_audit.lead_auditor || E'\n' ||
                        'Planned Period: ' || v_audit.planned_start_date || ' to ' || v_audit.planned_end_date || E'\n' ||
                        'Actual Period: ' || COALESCE(v_audit.actual_start_date::TEXT, 'N/A') || ' to ' ||
                        COALESCE(v_audit.actual_end_date::TEXT, 'N/A') || E'\n\n';

    -- Add objectives and scope
    v_report_content := v_report_content ||
                        'OBJECTIVES AND SCOPE' || E'\n' ||
                        '--------------------' || E'\n' ||
                        v_audit.audit_description || E'\n\n';

    -- Add executive summary
    v_report_content := v_report_content ||
                        'EXECUTIVE SUMMARY' || E'\n' ||
                        '----------------' || E'\n';

    -- Count findings by severity
    SELECT
        COUNT(finding_id),
        COUNT(CASE WHEN severity = 'Critical' THEN 1 END),
        COUNT(CASE WHEN severity = 'High' THEN 1 END)
    INTO
        v_finding_count, v_critical_findings, v_high_findings
    FROM
        audit_management.audit_findings
    WHERE
        audit_id = p_audit_id;

    IF v_finding_count = 0 THEN
        v_report_content := v_report_content ||
                            'No significant findings were identified during this audit.' || E'\n';
    ELSE
        v_report_content := v_report_content ||
                            'The audit identified ' || v_finding_count || ' findings, including:' || E'\n' ||
                            '- ' || v_critical_findings || ' Critical findings' || E'\n' ||
                            '- ' || v_high_findings || ' High findings' || E'\n\n' ||
                            'Key areas requiring management attention are detailed in the Findings section.' || E'\n';
    END IF;

    -- Add detailed findings if this is a full report
    IF p_report_type = 'Final' THEN
        v_report_content := v_report_content || E'\n' ||
                            'DETAILED FINDINGS' || E'\n' ||
                            '----------------' || E'\n';

        -- Add each finding with details
        FOR f IN (
            SELECT
                finding_title, finding_description, severity,
                root_cause, recommendation
            FROM
                audit_management.audit_findings
            WHERE
                audit_id = p_audit_id
            ORDER BY
                CASE severity
                    WHEN 'Critical' THEN 1
                    WHEN 'High' THEN 2
                    WHEN 'Medium' THEN 3
                    WHEN 'Low' THEN 4
                END
        ) LOOP
            v_report_content := v_report_content ||
                                'Finding: ' || f.finding_title || E'\n' ||
                                'Severity: ' || f.severity || E'\n' ||
                                'Description: ' || f.finding_description || E'\n' ||
                                'Root Cause: ' || COALESCE(f.root_cause, 'Not specified') || E'\n' ||
                                'Recommendation: ' || COALESCE(f.recommendation, 'Pending') || E'\n\n';
        END LOOP;
    END IF;

    -- Add conclusion
    v_report_content := v_report_content ||
                        'CONCLUSION' || E'\n' ||
                        '----------' || E'\n' ||
                        CASE
                            WHEN v_critical_findings > 0 THEN
                                'Urgent action required to address critical control deficiencies.'
                            WHEN v_high_findings > 0 THEN
                                'Significant improvements needed to mitigate high-risk findings.'
                            WHEN v_finding_count > 0 THEN
                                'Moderate improvements recommended to strengthen controls.'
                            ELSE
                                'Controls are generally effective with no major deficiencies identified.'
                        END || E'\n';

    -- Insert the report
    INSERT INTO audit_management.audit_reports (
        audit_id,
        report_title,
        report_type,
        report_status,
        report_content,
        published_by,
        published_date,
        created_by,
        created_at,
        updated_at
    ) VALUES (
        p_audit_id,
        v_audit.audit_title || ' - ' || INITCAP(p_report_type) || ' Report',
        p_report_type,
        'Published',
        v_report_content,
        p_published_by,
        CURRENT_DATE,
        p_published_by,
        CURRENT_TIMESTAMP,
        CURRENT_TIMESTAMP
    );

    -- Update audit status if this is the final report
    IF p_report_type = 'Final' THEN
        UPDATE audit_management.audit_activities
        SET status = 'Completed',
            updated_at = CURRENT_TIMESTAMP
        WHERE audit_id = p_audit_id;
    END IF;

    RAISE NOTICE 'Generated % report for audit %', p_report_type, p_audit_id;
END;
$$;

/*
 * Procedure: audit_management.sp_monitor_regulatory_changes
 * Purpose: Checks for new regulations and updates compliance requirements
 * Business Case: Automates tracking of regulatory changes to ensure audit
 * plans remain current with compliance obligations.
 */
CREATE OR REPLACE PROCEDURE audit_management.sp_monitor_regulatory_changes(
    p_effective_date DATE DEFAULT CURRENT_DATE
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_new_reg_count INTEGER := 0;
    v_updated_reg_count INTEGER := 0;
BEGIN
    -- In a real implementation, this would connect to a regulatory feed API
    -- For this example, we'll simulate finding new regulations

    -- Simulate finding new regulations (normally would be API call)
    -- Insert new compliance requirements for demonstration
    INSERT INTO audit_management.compliance_requirements (
        requirement_name,
        description,
        regulation_standard,
        applicable_to,
        effective_date,
        review_frequency,
        owner_id,
        is_active,
        created_at,
        updated_at
    )
    SELECT
        'Data Privacy - ' || r.region_name,
        'Compliance with ' || r.region_name || ' data privacy regulations',
        r.region_code || '-PRIV-2023',
        'All business units processing ' || r.region_name || ' citizen data',
        p_effective_date,
        'Annual',
        1, -- Default owner (would normally look up)
        TRUE,
        CURRENT_TIMESTAMP,
        CURRENT_TIMESTAMP
    FROM
        (VALUES ('EU'), ('California'), ('Brazil'), ('Japan')) AS r(region_name, region_code)
    WHERE NOT EXISTS (
        SELECT 1 FROM audit_management.compliance_requirements
        WHERE regulation_standard = r.region_code || '-PRIV-2023'
    )
    RETURNING 1 INTO v_new_reg_count;

    -- Log results
    IF v_new_reg_count > 0 THEN
        RAISE NOTICE 'Added % new regulatory requirements effective %', v_new_reg_count, p_effective_date;
    ELSE
        RAISE NOTICE 'No new regulatory requirements found effective %', p_effective_date;
    END IF;

    -- In a real implementation, would also check for updates to existing regulations
END;
$$;

CREATE TABLE audit_management.quality_assurance_reviews (
    review_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id) NOT NULL,
    review_type VARCHAR(50) CHECK (review_type IN ('Peer Review', 'Supervisory Review', 'External Review')),
    review_date DATE NOT NULL,
    reviewer_id INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    overall_rating VARCHAR(20) CHECK (overall_rating IN ('Unsatisfactory', 'Needs Improvement', 'Satisfactory', 'Excellent')),
    compliance_score INTEGER CHECK (compliance_score BETWEEN 1 AND 5),
    documentation_score INTEGER CHECK (documentation_score BETWEEN 1 AND 5),
    methodology_score INTEGER CHECK (methodology_score BETWEEN 1 AND 5),
    findings_score INTEGER CHECK (findings_score BETWEEN 1 AND 5),
    comments TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE audit_management.qa_findings (
    qa_finding_id SERIAL PRIMARY KEY,
    review_id INTEGER REFERENCES audit_management.quality_assurance_reviews(review_id) NOT NULL,
    finding_description TEXT NOT NULL,
    severity VARCHAR(20) CHECK (severity IN ('Minor', 'Moderate', 'Major', 'Critical')),
    corrective_action TEXT,
    status VARCHAR(20) CHECK (status IN ('Open', 'In Progress', 'Resolved', 'Closed')),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);


-- Busines case: to schedule up to 10 risk-based audits for Q2 2025
CALL audit_management.sp_schedule_risk_based_audits(
    5,  -- plan_id
    10, -- max_audits
    '2025-04-01', -- start_date
    '2025-06-30', -- end_date
    101 -- lead_auditor_id
);

-- Business case: to generate a final reqport for audit id 42
-- Generate a final report for audit ID 42
CALL audit_management.sp_generate_audit_report(
    42,           -- audit_id
    'Final',      -- report_type
    102           -- published_by (employee_id)
);

-- Check for new regulations effective July 1, 2025
CALL audit_management.sp_monitor_regulatory_changes('2025-07-01');

CREATE TABLE audit_management.continuous_monitoring_rules (
    rule_id SERIAL PRIMARY KEY,
    rule_name VARCHAR(100) NOT NULL,
    rule_description TEXT,
    risk_category_id INTEGER REFERENCES core.risk_categories(category_id),
    data_source VARCHAR(100) NOT NULL,
    query_or_condition TEXT NOT NULL,
    frequency VARCHAR(50) CHECK (frequency IN ('Daily', 'Weekly', 'Monthly', 'Quarterly')),
    severity VARCHAR(20) CHECK (severity IN ('Low', 'Medium', 'High', 'Critical')),
    is_active BOOLEAN DEFAULT TRUE,
    created_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE audit_management.monitoring_alerts (
    alert_id SERIAL PRIMARY KEY,
    rule_id INTEGER REFERENCES audit_management.continuous_monitoring_rules(rule_id) NOT NULL,
    alert_timestamp TIMESTAMP NOT NULL,
    alert_details JSONB NOT NULL,
    status VARCHAR(20) CHECK (status IN ('New', 'In Review', 'Escalated', 'Resolved')),
    assigned_to INTEGER REFERENCES core.employees(employee_id),
    resolution_notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
CREATE TABLE audit_management.audit_methodologies (
    methodology_id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    description TEXT,
    category VARCHAR(50) NOT NULL,
    content TEXT NOT NULL,
    version VARCHAR(20) NOT NULL,
    is_current BOOLEAN DEFAULT TRUE,
    created_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE audit_management.audit_templates (
    template_id SERIAL PRIMARY KEY,
    template_name VARCHAR(100) NOT NULL,
    template_type VARCHAR(50) CHECK (template_type IN ('Workpaper', 'Report', 'Checklist', 'Questionnaire')),
    audit_type VARCHAR(50),
    risk_level VARCHAR(20),
    content TEXT NOT NULL,
    version VARCHAR(20) NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    created_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);


--audit quality metrics
CREATE OR REPLACE VIEW audit_management.vw_audit_quality_metrics AS
SELECT
    a.audit_id,
    a.audit_title,
    q.review_date,
    q.overall_rating,
    (q.compliance_score + q.documentation_score + q.methodology_score + q.findings_score) / 4.0 AS average_score,
    COUNT(qf.qa_finding_id) AS qa_findings_count,
    COUNT(CASE WHEN qf.severity IN ('Major', 'Critical') THEN 1 END) AS major_qa_findings
FROM
    audit_management.audit_activities a
LEFT JOIN
    audit_management.quality_assurance_reviews q ON a.audit_id = q.audit_id
LEFT JOIN
    audit_management.qa_findings qf ON q.review_id = qf.review_id
GROUP BY
    a.audit_id, a.audit_title, q.review_date, q.overall_rating,
    q.compliance_score, q.documentation_score, q.methodology_score, q.findings_score;

    --continuous monitoring effectiveness
    CREATE OR REPLACE VIEW audit_management.vw_monitoring_effectiveness AS
    SELECT
        r.rule_id,
        r.rule_name,
        r.risk_category_id,
        rc.category_name AS risk_category,
        r.severity AS rule_severity,
        COUNT(a.alert_id) AS total_alerts,
        COUNT(CASE WHEN a.status = 'Resolved' THEN 1 END) AS resolved_alerts,
        COUNT(CASE WHEN a.status = 'New' THEN 1 END) AS new_alerts,
        COUNT(CASE WHEN a.status = 'Escalated' THEN 1 END) AS escalated_alerts,
        MIN(a.alert_timestamp) AS first_alert,
        MAX(a.alert_timestamp) AS last_alert
    FROM
        audit_management.continuous_monitoring_rules r
    LEFT JOIN
        core.risk_categories rc ON r.risk_category_id = rc.category_id
    LEFT JOIN
        audit_management.monitoring_alerts a ON r.rule_id = a.rule_id
    GROUP BY
        r.rule_id, r.rule_name, r.risk_category_id, rc.category_name, r.severity;


--audit methodology usage
CREATE OR REPLACE VIEW audit_management.vw_methodology_usage AS
SELECT
    m.methodology_id,
    m.name,
    m.category,
    m.version,
    COUNT(w.workpaper_id) AS usage_count,
    COUNT(DISTINCT a.audit_id) AS audit_count,
    MAX(a.actual_end_date) AS last_used_date
FROM
    audit_management.audit_methodologies m
LEFT JOIN
    audit_management.workpapers w ON w.description LIKE '%' || m.name || '%'
LEFT JOIN
    audit_management.audit_activities a ON w.audit_id = a.audit_id
WHERE
    m.is_current = TRUE
GROUP BY
    m.methodology_id, m.name, m.category, m.version;

--additional materialized view for audit risk coverage trends for monthly refresh
CREATE MATERIALIZED VIEW audit_management.mv_audit_risk_coverage_trends AS
SELECT
    DATE_TRUNC('month', a.actual_end_date) AS month,
    au.risk_category_id,
    rc.category_name AS risk_category,
    COUNT(DISTINCT a.audit_id) AS audits_completed,
    COUNT(f.finding_id) AS findings_identified,
    COUNT(CASE WHEN f.severity = 'Critical' THEN 1 END) AS critical_findings,
    COUNT(CASE WHEN f.severity = 'High' THEN 1 END) AS high_findings,
    AVG(EXTRACT(DAY FROM (a.actual_end_date - a.actual_start_date))) AS avg_duration_days
FROM
    audit_management.audit_activities a
JOIN
    audit_management.audit_universe au ON a.audit_entity_id = au.audit_entity_id
JOIN
    core.risk_categories rc ON au.risk_category_id = rc.category_id
LEFT JOIN
    audit_management.audit_findings f ON a.audit_id = f.audit_id
WHERE
    a.status = 'Completed'
GROUP BY
    DATE_TRUNC('month', a.actual_end_date), au.risk_category_id, rc.category_name
ORDER BY
    month, risk_category;


--continuous monitoring alert trends -- weekly refresh
CREATE MATERIALIZED VIEW audit_management.mv_monitoring_alert_trends AS
SELECT
    DATE_TRUNC('week', a.alert_timestamp) AS week,
    r.rule_id,
    r.rule_name,
    r.risk_category_id,
    rc.category_name AS risk_category,
    r.severity AS rule_severity,
    COUNT(a.alert_id) AS alert_count,
    COUNT(CASE WHEN a.status = 'Resolved' THEN 1 END) AS resolved_count,
    COUNT(CASE WHEN a.status = 'Escalated' THEN 1 END) AS escalated_count,
    AVG(EXTRACT(DAY FROM (COALESCE(a.updated_at, CURRENT_TIMESTAMP) - a.alert_timestamp))) AS avg_resolution_days
FROM
    audit_management.monitoring_alerts a
JOIN
    audit_management.continuous_monitoring_rules r ON a.rule_id = r.rule_id
LEFT JOIN
    core.risk_categories rc ON r.risk_category_id = rc.category_id
WHERE
    a.alert_timestamp >= CURRENT_DATE - INTERVAL '6 months'
GROUP BY
    DATE_TRUNC('week', a.alert_timestamp), r.rule_id, r.rule_name, r.risk_category_id, rc.category_name, r.severity
ORDER BY
    week, alert_count DESC;

--procedure for scheduling continuous monitoring rules
CREATE OR REPLACE PROCEDURE audit_management.sp_execute_monitoring_rules()
LANGUAGE plpgsql
AS $$
DECLARE
    v_rule RECORD;
    v_result_count INTEGER;
    v_alert_details JSONB;
BEGIN
    -- Process daily rules
    FOR v_rule IN
        SELECT * FROM audit_management.continuous_monitoring_rules
        WHERE frequency = 'Daily' AND is_active = TRUE
    LOOP
        -- Execute the rule query (simplified example)
        -- In practice, this would use dynamic SQL or call external procedures
        EXECUTE 'SELECT COUNT(*) FROM ' || v_rule.data_source || ' WHERE ' || v_rule.query_or_condition
        INTO v_result_count;

        -- If results found, create alert
        IF v_result_count > 0 THEN
            v_alert_details := jsonb_build_object(
                'rule_id', v_rule.rule_id,
                'rule_name', v_rule.rule_name,
                'result_count', v_result_count,
                'execution_time', CURRENT_TIMESTAMP
            );

            INSERT INTO audit_management.monitoring_alerts (
                rule_id,
                alert_timestamp,
                alert_details,
                status,
                created_at,
                updated_at
            ) VALUES (
                v_rule.rule_id,
                CURRENT_TIMESTAMP,
                v_alert_details,
                'New',
                CURRENT_TIMESTAMP,
                CURRENT_TIMESTAMP
            );

            RAISE NOTICE 'Created alert for rule %: % findings', v_rule.rule_name, v_result_count;
        END IF;
    END LOOP;

    -- Log completion
    RAISE NOTICE 'Completed execution of daily monitoring rules';
END;
$$;

-- procedure for audit quality scoring
CREATE OR REPLACE PROCEDURE audit_management.sp_calculate_audit_quality_score(
    p_audit_id INTEGER,
    p_reviewer_id INTEGER,
    OUT p_score NUMERIC(5,2),
    OUT p_rating VARCHAR(20)
LANGUAGE plpgsql
AS $$
DECLARE
    v_compliance_score INTEGER;
    v_documentation_score INTEGER;
    v_methodology_score INTEGER;
    v_findings_score INTEGER;
    v_total_score NUMERIC(5,2);
BEGIN
    -- Calculate scores based on various factors (simplified example)

    -- Compliance with standards (check if all required procedures were followed)
    SELECT COUNT(*) INTO v_compliance_score
    FROM audit_management.workpapers w
    JOIN audit_management.audit_activities a ON w.audit_id = a.audit_id
    WHERE a.audit_id = p_audit_id
    AND w.status = 'Finalized';

    -- Documentation quality (check completeness of workpapers)
    SELECT AVG(LENGTH(content)) INTO v_documentation_score
    FROM audit_management.workpaper_versions v
    JOIN audit_management.workpapers w ON v.workpaper_id = w.workpaper_id
    WHERE w.audit_id = p_audit_id;

    -- Methodology appropriateness (check if right methods were used)
    SELECT COUNT(*) INTO v_methodology_score
    FROM audit_management.testing_procedures
    WHERE audit_id = p_audit_id
    AND conclusion = 'Effective';

    -- Findings quality (check severity and completeness of findings)
    SELECT COUNT(*) INTO v_findings_score
    FROM audit_management.audit_findings
    WHERE audit_id = p_audit_id
    AND severity IN ('High', 'Critical');

    -- Normalize scores to 1-5 scale (simplified)
    v_compliance_score := LEAST(5, GREATEST(1, v_compliance_score / 5));
    v_documentation_score := LEAST(5, GREATEST(1, v_documentation_score / 500));
    v_methodology_score := LEAST(5, GREATEST(1, v_methodology_score / 2));
    v_findings_score := LEAST(5, GREATEST(1, v_findings_score));

    -- Calculate total score
    v_total_score := (v_compliance_score + v_documentation_score + v_methodology_score + v_findings_score) / 4.0;
    p_score := v_total_score;

    -- Determine rating
    IF v_total_score >= 4.5 THEN
        p_rating := 'Excellent';
    ELSIF v_total_score >= 3.5 THEN
        p_rating := 'Satisfactory';
    ELSIF v_total_score >= 2.5 THEN
        p_rating := 'Needs Improvement';
    ELSE
        p_rating := 'Unsatisfactory';
    END IF;

    -- Store the quality review
    INSERT INTO audit_management.quality_assurance_reviews (
        audit_id,
        review_type,
        review_date,
        reviewer_id,
        overall_rating,
        compliance_score,
        documentation_score,
        methodology_score,
        findings_score,
        created_at,
        updated_at
    ) VALUES (
        p_audit_id,
        'Supervisory Review',
        CURRENT_DATE,
        p_reviewer_id,
        p_rating,
        v_compliance_score,
        v_documentation_score,
        v_methodology_score,
        v_findings_score,
        CURRENT_TIMESTAMP,
        CURRENT_TIMESTAMP
    );

    RAISE NOTICE 'Calculated quality score % (%) for audit %', p_rating, p_score, p_audit_id;
END;
$$;

--procedure for audit knowledge base maintenance
CREATE OR REPLACE PROCEDURE audit_management.sp_update_audit_knowledge_base(
    p_methodology_id INTEGER DEFAULT NULL,
    p_template_id INTEGER DEFAULT NULL
)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Update methodology versions
    IF p_methodology_id IS NOT NULL THEN
        -- Mark all other versions of this methodology as not current
        UPDATE audit_management.audit_methodologies
        SET is_current = FALSE,
            updated_at = CURRENT_TIMESTAMP
        WHERE name = (SELECT name FROM audit_management.audit_methodologies WHERE methodology_id = p_methodology_id)
        AND methodology_id != p_methodology_id;

        -- Mark this version as current
        UPDATE audit_management.audit_methodologies
        SET is_current = TRUE,
            updated_at = CURRENT_TIMESTAMP
        WHERE methodology_id = p_methodology_id;

        RAISE NOTICE 'Updated current version for methodology ID: %', p_methodology_id;
    END IF;

    -- Update template versions
    IF p_template_id IS NOT NULL THEN
        -- Mark all other versions of this template as not active
        UPDATE audit_management.audit_templates
        SET is_active = FALSE,
            updated_at = CURRENT_TIMESTAMP
        WHERE template_name = (SELECT template_name FROM audit_management.audit_templates WHERE template_id = p_template_id)
        AND template_id != p_template_id;

        -- Mark this version as active
        UPDATE audit_management.audit_templates
        SET is_active = TRUE,
            updated_at = CURRENT_TIMESTAMP
        WHERE template_id = p_template_id;

        RAISE NOTICE 'Updated active version for template ID: %', p_template_id;
    END IF;

    -- If nothing specified, check for outdated methodologies
    IF p_methodology_id IS NULL AND p_template_id IS NULL THEN
        -- Archive methodologies older than 2 years
        UPDATE audit_management.audit_methodologies
        SET is_current = FALSE,
            updated_at = CURRENT_TIMESTAMP
        WHERE is_current = TRUE
        AND created_at < CURRENT_DATE - INTERVAL '2 years';

        RAISE NOTICE 'Archived outdated methodologies';
    END IF;
END;
$$;

-- craeting a new monitoring rule
-- Create a new monitoring rule
INSERT INTO audit_management.continuous_monitoring_rules (
    rule_name, rule_description, risk_category_id, data_source,
    query_or_condition, frequency, severity, is_active, created_by
) VALUES (
    'Unauthorized System Access',
    'Detects unauthorized access to critical systems',
    3, -- IT Security risk category
    'system_access_logs',
    'access_level > user_level AND approval_status IS NULL',
    'Daily',
    'High',
    TRUE,
    101
);

-- Execute monitoring rules (typically scheduled)
CALL audit_management.sp_execute_monitoring_rules();

--adding a new methodology version
-- Add a new methodology version
INSERT INTO audit_management.audit_methodologies (
    name, description, category, content, version, is_current, created_by
) VALUES (
    'IT General Controls Audit',
    'Standard methodology for ITGC audits',
    'IT',
    'Detailed methodology content...',
    '2.1',
    FALSE, -- Will be set to TRUE by the procedure
    101
);

-- Set the new version as current
CALL audit_management.sp_update_audit_knowledge_base(
    p_methodology_id => currval('audit_management.audit_methodologies_methodology_id_seq')
);

-- recommended index for performance
-- Quality assurance indexes
CREATE INDEX idx_qa_reviews_audit_id ON audit_management.quality_assurance_reviews(audit_id);
CREATE INDEX idx_qa_findings_review_id ON audit_management.qa_findings(review_id);

-- Continuous monitoring indexes
CREATE INDEX idx_monitoring_alerts_rule_id ON audit_management.monitoring_alerts(rule_id);
CREATE INDEX idx_monitoring_alerts_status ON audit_management.monitoring_alerts(status);
CREATE INDEX idx_monitoring_alerts_timestamp ON audit_management.monitoring_alerts(alert_timestamp);

-- Knowledge base indexes
CREATE INDEX idx_methodologies_name ON audit_management.audit_methodologies(name, is_current);
CREATE INDEX idx_templates_type ON audit_management.audit_templates(template_type, is_active);

/**
* TABLE: audit_strategic_objectives
* BUSINESS CASE: Links audit activities to organizational strategic objectives to demonstrate
* how audit work supports broader business goals. Essential for audit value reporting to executives.
*/
CREATE TABLE audit_management.audit_strategic_objectives (
    objective_id SERIAL PRIMARY KEY,
    objective_name VARCHAR(100) NOT NULL,
    objective_description TEXT,
    strategic_theme VARCHAR(50) NOT NULL, -- e.g., "Growth", "Compliance", "Efficiency"
    weight DECIMAL(5,2) CHECK (weight BETWEEN 0 AND 1), -- Relative importance
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

/**
* TABLE: audit_objective_mapping
* BUSINESS CASE: Provides traceability between individual audits and strategic objectives,
* enabling measurement of audit contribution to organizational goals.
*/
CREATE TABLE audit_management.audit_objective_mapping (
    mapping_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id) NOT NULL,
    objective_id INTEGER REFERENCES audit_management.audit_strategic_objectives(objective_id) NOT NULL,
    alignment_strength VARCHAR(20) CHECK (alignment_strength IN ('Primary', 'Secondary', 'Tertiary')),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (audit_id, objective_id)
);

/**
* TABLE: agile_audit_sprints
* BUSINESS CASE: Supports agile audit methodologies by breaking audits into sprints
* with defined deliverables, enabling iterative value delivery.
*/
CREATE TABLE audit_management.agile_audit_sprints (
    sprint_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id) NOT NULL,
    sprint_name VARCHAR(100) NOT NULL,
    start_date DATE NOT NULL,
    end_date DATE NOT NULL,
    goal TEXT NOT NULL,
    status VARCHAR(20) CHECK (status IN ('Planning', 'In Progress', 'Completed', 'Review')),
    velocity_score INTEGER, -- Team capacity measurement
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT valid_sprint_dates CHECK (end_date >= start_date)
);

/**
* TABLE: sprint_deliverables
* BUSINESS CASE: Tracks specific outputs from each audit sprint, ensuring
* continuous stakeholder engagement and value demonstration.
*/
CREATE TABLE audit_management.sprint_deliverables (
    deliverable_id SERIAL PRIMARY KEY,
    sprint_id INTEGER REFERENCES audit_management.agile_audit_sprints(sprint_id) NOT NULL,
    deliverable_name VARCHAR(100) NOT NULL,
    description TEXT,
    type VARCHAR(50) CHECK (type IN ('Finding', 'Recommendation', 'Report', 'Dashboard', 'Other')),
    status VARCHAR(20) CHECK (status IN ('Pending', 'In Progress', 'Completed', 'Accepted')),
    stakeholder_feedback TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

/**
* TABLE: audit_value_metrics
* BUSINESS CASE: Quantifies the business value generated by audit activities beyond
* traditional findings, including cost savings, process improvements, and risk reduction.
*/
CREATE TABLE audit_management.audit_value_metrics (
    metric_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id) NOT NULL,
    metric_type VARCHAR(50) NOT NULL CHECK (metric_type IN ('Cost Savings', 'Revenue Impact', 'Risk Reduction', 'Efficiency Gain')),
    description TEXT NOT NULL,
    amount NUMERIC(12,2) NOT NULL,
    calculation_methodology TEXT,
    validated_by INTEGER REFERENCES core.employees(employee_id),
    validation_date DATE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

/**
* TABLE: stakeholder_value_assessments
* BUSINESS CASE: Captures qualitative stakeholder assessments of audit value,
* providing balanced scorecard perspective beyond quantitative metrics.
*/
CREATE TABLE audit_management.stakeholder_value_assessments (
    assessment_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id) NOT NULL,
    stakeholder_id INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    assessment_date DATE NOT NULL,
    value_rating INTEGER CHECK (value_rating BETWEEN 1 AND 5), -- 1=Low, 5=High
    timeliness_rating INTEGER CHECK (timeliness_rating BETWEEN 1 AND 5),
    relevance_rating INTEGER CHECK (relevance_rating BETWEEN 1 AND 5),
    comments TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (audit_id, stakeholder_id)
);

/**
* VIEW: vw_audit_strategic_coverage
* BUSINESS CASE: Provides executives with visibility into how audit resources are allocated
* across strategic priorities, demonstrating audit's contribution to organizational goals.
*/
CREATE OR REPLACE VIEW audit_management.vw_audit_strategic_coverage AS
SELECT
    so.objective_id,
    so.objective_name,
    so.strategic_theme,
    so.weight AS objective_weight,
    COUNT(DISTINCT aom.audit_id) AS audit_count,
    SUM(a.budget_actual) AS total_audit_spend,
    COUNT(f.finding_id) AS findings_count,
    ROUND(COUNT(DISTINCT aom.audit_id) * 100.0 /
          (SELECT COUNT(*) FROM audit_management.audit_activities WHERE status = 'Completed'), 2) AS percentage_coverage
FROM
    audit_management.audit_strategic_objectives so
LEFT JOIN
    audit_management.audit_objective_mapping aom ON so.objective_id = aom.objective_id
LEFT JOIN
    audit_management.audit_activities a ON aom.audit_id = a.audit_id
LEFT JOIN
    audit_management.audit_findings f ON a.audit_id = f.audit_id
WHERE
    so.is_active = TRUE
GROUP BY
    so.objective_id, so.objective_name, so.strategic_theme, so.weight
ORDER BY
    so.weight DESC;


    /**
    * VIEW: vw_agile_audit_performance
    * BUSINESS CASE: Enables audit managers to monitor agile audit execution metrics,
    * including sprint velocity, deliverable completion rates, and stakeholder acceptance.
    */
    CREATE OR REPLACE VIEW audit_management.vw_agile_audit_performance AS
    SELECT
        a.audit_id,
        a.audit_title,
        COUNT(DISTINCT s.sprint_id) AS total_sprints,
        AVG(s.velocity_score) AS avg_velocity,
        COUNT(d.deliverable_id) AS total_deliverables,
        COUNT(CASE WHEN d.status = 'Accepted' THEN 1 END) AS accepted_deliverables,
        MIN(s.start_date) AS first_sprint_start,
        MAX(s.end_date) AS last_sprint_end,
        ROUND(COUNT(CASE WHEN d.status = 'Accepted' THEN 1 END) * 100.0 /
              NULLIF(COUNT(d.deliverable_id), 0), 2) AS acceptance_rate
    FROM
        audit_management.audit_activities a
    JOIN
        audit_management.agile_audit_sprints s ON a.audit_id = s.audit_id
    LEFT JOIN
        audit_management.sprint_deliverables d ON s.sprint_id = d.sprint_id
    WHERE
        a.status = 'Completed'
    GROUP BY
        a.audit_id, a.audit_title;


        /**
        * VIEW: vw_audit_value_realization
        * BUSINESS CASE: Demonstrates the quantitative and qualitative value generated by audits,
        * combining financial metrics with stakeholder satisfaction for comprehensive value reporting.
        */
        CREATE OR REPLACE VIEW audit_management.vw_audit_value_realization AS
        SELECT
            a.audit_id,
            a.audit_title,
            SUM(CASE WHEN vm.metric_type = 'Cost Savings' THEN vm.amount ELSE 0 END) AS cost_savings,
            SUM(CASE WHEN vm.metric_type = 'Revenue Impact' THEN vm.amount ELSE 0 END) AS revenue_impact,
            SUM(CASE WHEN vm.metric_type = 'Risk Reduction' THEN vm.amount ELSE 0 END) AS risk_reduction_value,
            SUM(CASE WHEN vm.metric_type = 'Efficiency Gain' THEN vm.amount ELSE 0 END) AS efficiency_gains,
            AVG(sva.value_rating) AS avg_value_rating,
            AVG(sva.timeliness_rating) AS avg_timeliness_rating,
            AVG(sva.relevance_rating) AS avg_relevance_rating,
            COUNT(DISTINCT sva.stakeholder_id) AS stakeholders_assessed
        FROM
            audit_management.audit_activities a
        LEFT JOIN
            audit_management.audit_value_metrics vm ON a.audit_id = vm.audit_id
        LEFT JOIN
            audit_management.stakeholder_value_assessments sva ON a.audit_id = sva.audit_id
        WHERE
            a.status = 'Completed'
        GROUP BY
            a.audit_id, a.audit_title;


            /**
            * MATERIALIZED VIEW: mv_strategic_audit_impact_trends
            * BUSINESS CASE: Tracks audit's contribution to strategic objectives over time,
            * enabling analysis of whether audit focus aligns with evolving organizational priorities.
            * Refreshed quarterly to support strategic planning cycles.
            */
            CREATE MATERIALIZED VIEW audit_management.mv_strategic_audit_impact_trends AS
            SELECT
                DATE_TRUNC('quarter', a.actual_end_date) AS quarter,
                so.strategic_theme,
                COUNT(DISTINCT a.audit_id) AS audit_count,
                SUM(a.budget_actual) AS total_investment,
                COUNT(f.finding_id) AS findings_count,
                COUNT(CASE WHEN f.severity IN ('High', 'Critical') THEN 1 END) AS major_findings,
                ROUND(AVG(sva.value_rating), 2) AS avg_value_rating
            FROM
                audit_management.audit_activities a
            JOIN
                audit_management.audit_objective_mapping aom ON a.audit_id = aom.audit_id
            JOIN
                audit_management.audit_strategic_objectives so ON aom.objective_id = so.objective_id
            LEFT JOIN
                audit_management.audit_findings f ON a.audit_id = f.audit_id
            LEFT JOIN
                audit_management.stakeholder_value_assessments sva ON a.audit_id = sva.audit_id
            WHERE
                a.status = 'Completed'
                AND so.is_active = TRUE
            GROUP BY
                DATE_TRUNC('quarter', a.actual_end_date), so.strategic_theme
            ORDER BY
                quarter, so.strategic_theme;

                /**
                * MATERIALIZED VIEW: mv_agile_audit_velocity
                * BUSINESS CASE: Monitors agile audit team performance metrics over time,
                * enabling capacity planning and continuous improvement of audit processes.
                * Refreshed weekly to support sprint planning and retrospectives.
                */
                CREATE MATERIALIZED VIEW audit_management.mv_agile_audit_velocity AS
                SELECT
                    DATE_TRUNC('week', s.start_date) AS week,
                    e.employee_id,
                    e.first_name || ' ' || e.last_name AS auditor_name,
                    COUNT(DISTINCT s.sprint_id) AS sprints_contributed,
                    AVG(s.velocity_score) AS avg_velocity,
                    COUNT(d.deliverable_id) AS deliverables_worked,
                    COUNT(CASE WHEN d.status = 'Accepted' THEN 1 END) AS accepted_deliverables,
                    ROUND(COUNT(CASE WHEN d.status = 'Accepted' THEN 1 END) * 100.0 /
                          NULLIF(COUNT(d.deliverable_id), 0), 2) AS acceptance_rate
                FROM
                    audit_management.agile_audit_sprints s
                JOIN
                    audit_management.audit_team_members atm ON s.audit_id = atm.audit_id
                JOIN
                    core.employees e ON atm.employee_id = e.employee_id
                LEFT JOIN
                    audit_management.sprint_deliverables d ON s.sprint_id = d.sprint_id
                WHERE
                    s.start_date >= CURRENT_DATE - INTERVAL '12 months'
                GROUP BY
                    DATE_TRUNC('week', s.start_date), e.employee_id, e.first_name, e.last_name
                ORDER BY
                    week, avg_velocity DESC;


                    /**
* PROCEDURE: sp_generate_strategic_audit_plan
* BUSINESS CASE: Automates generation of risk-based audit plans that align with
* organizational strategic objectives, optimizing audit resource allocation.
* Incorporates both risk factors and strategic importance in prioritization.
*/
CREATE OR REPLACE PROCEDURE audit_management.sp_generate_strategic_audit_plan(
    p_plan_year INTEGER,
    p_plan_owner INTEGER,
    OUT p_plan_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_total_risk_weight DECIMAL(5,2);
    v_total_strategic_weight DECIMAL(5,2);
    v_normalization_factor DECIMAL(5,2);
BEGIN
    -- Create new annual plan
    INSERT INTO audit_management.annual_audit_plan (
        plan_year,
        plan_name,
        description,
        status,
        created_by
    ) VALUES (
        p_plan_year,
        'Strategic Audit Plan ' || p_plan_year,
        'Automatically generated strategic audit plan',
        'Draft',
        p_plan_owner
    ) RETURNING plan_id INTO p_plan_id;

    -- Calculate total weights for normalization
    SELECT SUM(
        CASE
            WHEN au.inherent_risk_level = 'Critical' THEN 1.0
            WHEN au.inherent_risk_level = 'High' THEN 0.75
            WHEN au.inherent_risk_level = 'Medium' THEN 0.5
            WHEN au.inherent_risk_level = 'Low' THEN 0.25
            ELSE 0
        END)
    INTO v_total_risk_weight
    FROM audit_management.audit_universe au
    WHERE au.is_active = TRUE;

    SELECT SUM(so.weight)
    INTO v_total_strategic_weight
    FROM audit_management.audit_strategic_objectives so
    WHERE so.is_active = TRUE;

    -- Calculate normalization factor to balance risk and strategic importance
    v_normalization_factor := CASE
        WHEN v_total_risk_weight > 0 AND v_total_strategic_weight > 0 THEN
            v_total_risk_weight / v_total_strategic_weight
        ELSE 1
    END;

    -- Generate audit activities based on combined risk and strategic importance
    INSERT INTO audit_management.audit_activities (
        plan_id,
        audit_entity_id,
        audit_title,
        audit_description,
        audit_type,
        risk_level,
        planned_start_date,
        planned_end_date,
        status,
        created_at,
        updated_at
    )
    SELECT
        p_plan_id,
        au.audit_entity_id,
        'Audit of ' || au.entity_name,
        'Risk-based audit of ' || au.entity_name || ' aligned with strategic objectives',
        CASE
            WHEN au.risk_category_id IN (SELECT category_id FROM core.risk_categories WHERE category_name LIKE '%IT%') THEN 'IT'
            WHEN au.risk_category_id IN (SELECT category_id FROM core.risk_categories WHERE category_name LIKE '%Financial%') THEN 'Financial'
            ELSE 'Operational'
        END,
        au.inherent_risk_level,
        (DATE(p_plan_year || '-01-01') + (random() * 180)::INTEGER * INTERVAL '1 day')::DATE,
        (DATE(p_plan_year || '-01-01') + (random() * 180)::INTEGER * INTERVAL '1 day' + INTERVAL '2 weeks')::DATE,
        'Planned',
        CURRENT_TIMESTAMP,
        CURRENT_TIMESTAMP
    FROM
        audit_management.audit_universe au
    JOIN (
        -- Select top entities by combined risk and strategic importance
        SELECT
            au.audit_entity_id,
            ROW_NUMBER() OVER (ORDER BY
                (CASE
                    WHEN au.inherent_risk_level = 'Critical' THEN 1.0
                    WHEN au.inherent_risk_level = 'High' THEN 0.75
                    WHEN au.inherent_risk_level = 'Medium' THEN 0.5
                    WHEN au.inherent_risk_level = 'Low' THEN 0.25
                    ELSE 0
                END) +
                COALESCE((
                    SELECT SUM(so.weight) * v_normalization_factor
                    FROM audit_management.audit_strategic_objectives so
                    JOIN audit_management.audit_objective_mapping aom ON so.objective_id = aom.objective_id
                    WHERE aom.audit_id IN (
                        SELECT audit_id
                        FROM audit_management.audit_activities
                        WHERE audit_entity_id = au.audit_entity_id
                    )
                ), 0) DESC
            ) AS priority_rank
        FROM
            audit_management.audit_universe au
        WHERE
            au.is_active = TRUE
    ) ranked_entities ON au.audit_entity_id = ranked_entities.audit_entity_id
    WHERE
        ranked_entities.priority_rank <= 20 -- Top 20 prioritized audits
        AND au.is_active = TRUE;

    -- Map audits to strategic objectives based on historical alignment
    INSERT INTO audit_management.audit_objective_mapping (
        audit_id,
        objective_id,
        alignment_strength,
        created_at
    )
    SELECT
        a.audit_id,
        so.objective_id,
        CASE
            WHEN random() < 0.3 THEN 'Primary'
            WHEN random() < 0.6 THEN 'Secondary'
            ELSE 'Tertiary'
        END AS alignment_strength,
        CURRENT_TIMESTAMP
    FROM
        audit_management.audit_activities a
    CROSS JOIN
        audit_management.audit_strategic_objectives so
    WHERE
        a.plan_id = p_plan_id
        AND so.is_active = TRUE
        AND random() < 0.7; -- 70% chance of alignment

    RAISE NOTICE 'Generated strategic audit plan ID: % with % audits',
        p_plan_id,
        (SELECT COUNT(*) FROM audit_management.audit_activities WHERE plan_id = p_plan_id);
END;
$$;

---Agile audit sprint planning
/**
* PROCEDURE: sp_plan_agile_audit_sprints
* BUSINESS CASE: Automates sprint planning for agile audits based on audit complexity,
* team capacity, and historical velocity. Ensures realistic planning of audit work.
*/
CREATE OR REPLACE PROCEDURE audit_management.sp_plan_agile_audit_sprints(
    p_audit_id INTEGER,
    p_sprint_duration_days INTEGER DEFAULT 14,
    p_team_capacity_hours INTEGER DEFAULT 120
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_audit_record RECORD;
    v_team_size INTEGER;
    v_total_hours_required INTEGER;
    v_sprints_needed INTEGER;
    v_start_date DATE;
    v_end_date DATE;
    v_remaining_hours INTEGER;
    v_sprint_number INTEGER := 1;
    v_velocity INTEGER;
BEGIN
    -- Get audit details
    SELECT
        a.*,
        au.entity_name,
        au.inherent_risk_level
    INTO v_audit_record
    FROM
        audit_management.audit_activities a
    JOIN
        audit_management.audit_universe au ON a.audit_entity_id = au.audit_entity_id
    WHERE
        a.audit_id = p_audit_id;

    -- Validate audit status
    IF v_audit_record.status != 'Planned' THEN
        RAISE EXCEPTION 'Audit must be in Planned status to create sprints';
    END IF;

    -- Get team size
    SELECT COUNT(*) INTO v_team_size
    FROM audit_management.audit_team_members
    WHERE audit_id = p_audit_id;

    IF v_team_size = 0 THEN
        RAISE EXCEPTION 'No team members assigned to this audit';
    END IF;

    -- Estimate hours required based on risk level and historical data
    CASE v_audit_record.inherent_risk_level
        WHEN 'Critical' THEN v_total_hours_required := 200 + (random() * 100)::INTEGER;
        WHEN 'High' THEN v_total_hours_required := 120 + (random() * 80)::INTEGER;
        WHEN 'Medium' THEN v_total_hours_required := 80 + (random() * 60)::INTEGER;
        WHEN 'Low' THEN v_total_hours_required := 40 + (random() * 40)::INTEGER;
        ELSE v_total_hours_required := 60 + (random() * 60)::INTEGER;
    END CASE;

    -- Calculate sprints needed
    v_sprints_needed := CEIL(v_total_hours_required / p_team_capacity_hours);
    v_remaining_hours := v_total_hours_required;
    v_start_date := v_audit_record.planned_start_date;

    -- Create sprints
    WHILE v_sprints_needed > 0 LOOP
        v_end_date := v_start_date + (p_sprint_duration_days - 1);
        v_velocity := LEAST(v_remaining_hours, p_team_capacity_hours);

        INSERT INTO audit_management.agile_audit_sprints (
            audit_id,
            sprint_name,
            start_date,
            end_date,
            goal,
            status,
            velocity_score,
            created_at,
            updated_at
        ) VALUES (
            p_audit_id,
            'Sprint ' || v_sprint_number,
            v_start_date,
            v_end_date,
            CASE
                WHEN v_sprint_number = 1 THEN 'Initial planning and high-risk areas'
                WHEN v_sprint_number = v_sprints_needed THEN 'Final testing and reporting'
                ELSE 'Continued testing and analysis'
            END,
            'Planning',
            v_velocity,
            CURRENT_TIMESTAMP,
            CURRENT_TIMESTAMP
        );

        v_start_date := v_end_date + 1;
        v_remaining_hours := v_remaining_hours - v_velocity;
        v_sprints_needed := v_sprints_needed - 1;
        v_sprint_number := v_sprint_number + 1;
    END LOOP;

    -- Update audit end date based on sprint schedule
    UPDATE audit_management.audit_activities
    SET
        planned_end_date = v_end_date,
        updated_at = CURRENT_TIMESTAMP
    WHERE
        audit_id = p_audit_id;

    RAISE NOTICE 'Created % sprints for audit ID: % with total estimated hours: %',
        v_sprint_number - 1, p_audit_id, v_total_hours_required;
END;
$$;

-- audit value assessment procedures
/**
* PROCEDURE: sp_assess_audit_value
* BUSINESS CASE: Systematically evaluates and quantifies the business value generated
* by completed audits, combining financial metrics with stakeholder feedback to
* demonstrate audit's return on investment.
*/
CREATE OR REPLACE PROCEDURE audit_management.sp_assess_audit_value(
    p_audit_id INTEGER,
    p_assessor_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_cost_savings NUMERIC(12,2) := 0;
    v_revenue_impact NUMERIC(12,2) := 0;
    v_risk_reduction NUMERIC(12,2) := 0;
    v_efficiency_gains NUMERIC(12,2) := 0;
    v_finding RECORD;
    v_audit_cost NUMERIC(12,2);
    v_roi NUMERIC(12,2);
BEGIN
    -- Validate audit is completed
    IF NOT EXISTS (
        SELECT 1 FROM audit_management.audit_activities
        WHERE audit_id = p_audit_id AND status = 'Completed'
    ) THEN
        RAISE EXCEPTION 'Audit must be completed to assess value';
    END IF;

    -- Get audit cost
    SELECT COALESCE(budget_actual, budget_estimated, 0)
    INTO v_audit_cost
    FROM audit_management.audit_activities
    WHERE audit_id = p_audit_id;

    -- Calculate value from findings
    FOR v_finding IN
        SELECT * FROM audit_management.audit_findings
        WHERE audit_id = p_audit_id AND severity IN ('High', 'Critical')
    LOOP
        -- Estimate cost savings from findings (simplified example)
        IF v_finding.finding_category = 'Financial' THEN
            v_cost_savings := v_cost_savings +
                CASE
                    WHEN v_finding.severity = 'Critical' THEN 50000 + (random() * 100000)::NUMERIC(12,2)
                    ELSE 10000 + (random() * 40000)::NUMERIC(12,2)
                END;
        END IF;

        -- Estimate risk reduction (simplified example)
        IF v_finding.finding_category IN ('Compliance', 'IT Security') THEN
            v_risk_reduction := v_risk_reduction +
                CASE
                    WHEN v_finding.severity = 'Critical' THEN 100000 + (random() * 200000)::NUMERIC(12,2)
                    ELSE 25000 + (random() * 75000)::NUMERIC(12,2)
                END;
        END IF;
    END LOOP;

    -- Calculate efficiency gains (simplified example)
    SELECT COUNT(*) * 5000 INTO v_efficiency_gains
    FROM audit_management.corrective_actions ca
    JOIN audit_management.issue_tracking it ON ca.issue_id = it.issue_id
    WHERE it.finding_id IN (
        SELECT finding_id FROM audit_management.audit_findings
        WHERE audit_id = p_audit_id
    )
    AND ca.status = 'Completed'
    AND ca.completion_notes LIKE '%efficiency%';

    -- Store value metrics
    INSERT INTO audit_management.audit_value_metrics (
        audit_id,
        metric_type,
        description,
        amount,
        calculation_methodology,
        validated_by,
        validation_date,
        created_at,
        updated_at
    ) VALUES
    (
        p_audit_id,
        'Cost Savings',
        'Estimated annual cost savings from audit findings',
        v_cost_savings,
        'Based on severity and type of financial findings',
        p_assessor_id,
        CURRENT_DATE,
        CURRENT_TIMESTAMP,
        CURRENT_TIMESTAMP
    ),
    (
        p_audit_id,
        'Risk Reduction',
        'Quantified risk exposure reduction',
        v_risk_reduction,
        'Based on severity of compliance and security findings',
        p_assessor_id,
        CURRENT_DATE,
        CURRENT_TIMESTAMP,
        CURRENT_TIMESTAMP
    ),
    (
        p_audit_id,
        'Efficiency Gain',
        'Process efficiency improvements',
        v_efficiency_gains,
        'Count of efficiency-related corrective actions * $5,000 estimate',
        p_assessor_id,
        CURRENT_DATE,
        CURRENT_TIMESTAMP,
        CURRENT_TIMESTAMP
    );

    -- Calculate ROI
    v_roi := ((v_cost_savings + v_risk_reduction + v_efficiency_gains) - v_audit_cost) / NULLIF(v_audit_cost, 0) * 100;

    RAISE NOTICE 'Audit ID: % value assessment completed. Cost: %, Savings: %, Risk Reduction: %, Efficiency Gains: %, ROI: %',
        p_audit_id, v_audit_cost, v_cost_savings, v_risk_reduction, v_efficiency_gains, v_roi || '%';
END;
$$;

-- Strategic alignment indexes
CREATE INDEX idx_audit_objective_mapping_audit ON audit_management.audit_objective_mapping(audit_id);
CREATE INDEX idx_audit_objective_mapping_objective ON audit_management.audit_objective_mapping(objective_id);

-- Agile audit indexes
CREATE INDEX idx_agile_sprints_audit ON audit_management.agile_audit_sprints(audit_id);
CREATE INDEX idx_sprint_deliverables_sprint ON audit_management.sprint_deliverables(sprint_id);
CREATE INDEX idx_sprint_deliverables_status ON audit_management.sprint_deliverables(status);

-- Value measurement indexes
CREATE INDEX idx_audit_value_metrics_audit ON audit_management.audit_value_metrics(audit_id);
CREATE INDEX idx_audit_value_metrics_type ON audit_management.audit_value_metrics(metric_type);
CREATE INDEX idx_stakeholder_assessments_audit ON audit_management.stakeholder_value_assessments(audit_id);

-- block chain based audit evidence integrity module
/**
* TABLE: blockchain_evidence_anchors
* BUSINESS CASE: Provides cryptographic proof of audit evidence integrity by storing
* hashes in a blockchain, ensuring tamper-proof documentation for regulatory compliance
* and legal defensibility. Critical for high-stakes audits in regulated industries.
*/
CREATE TABLE audit_management.blockchain_evidence_anchors (
    anchor_id SERIAL PRIMARY KEY,
    evidence_id INTEGER REFERENCES audit_management.audit_evidence(evidence_id) NOT NULL,
    hash_algorithm VARCHAR(50) NOT NULL DEFAULT 'SHA-256',
    evidence_hash VARCHAR(64) NOT NULL, -- Hex representation of hash
    blockchain_transaction_id VARCHAR(100), -- Transaction ID from blockchain network
    blockchain_name VARCHAR(50) NOT NULL, -- e.g., 'Ethereum', 'Hyperledger'
    anchor_timestamp TIMESTAMP NOT NULL, -- When hashed/anchored
    verification_count INTEGER DEFAULT 0,
    last_verified TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (evidence_id)
);

/**
* MATERIALIZED VIEW: mv_evidence_integrity_status
* BUSINESS CASE: Provides real-time verification status of all audit evidence anchored
* in blockchain, refreshed hourly to detect any potential tampering attempts.
*/
CREATE MATERIALIZED VIEW audit_management.mv_evidence_integrity_status AS
SELECT
    e.evidence_id,
    w.title AS workpaper_title,
    a.audit_title,
    b.blockchain_name,
    b.anchor_timestamp,
    b.verification_count,
    b.last_verified,
    CASE
        WHEN b.evidence_id IS NULL THEN 'Not Anchored'
        WHEN verify_evidence_hash(e.evidence_id, b.evidence_hash) THEN 'Valid'
        ELSE 'Tamper Alert'
    END AS integrity_status
FROM
    audit_management.audit_evidence e
JOIN
    audit_management.workpapers w ON e.workpaper_id = w.workpaper_id
JOIN
    audit_management.audit_activities a ON w.audit_id = a.audit_id
LEFT JOIN
    audit_management.blockchain_evidence_anchors b ON e.evidence_id = b.evidence_id
ORDER BY
    a.audit_title, w.title;

/**
* PROCEDURE: sp_anchor_evidence_to_blockchain
* BUSINESS CASE: Automates the process of securing audit evidence by generating
* cryptographic hashes and storing them in a blockchain network. Runs nightly to
* batch process new evidence for cost efficiency.
*/
CREATE OR REPLACE PROCEDURE audit_management.sp_anchor_evidence_to_blockchain(
    p_blockchain_name VARCHAR(50) DEFAULT 'Hyperledger'
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_evidence RECORD;
    v_file_content BYTEA;
    v_hash TEXT;
    v_tx_id TEXT;
BEGIN
    -- Process all unanchored evidence
    FOR v_evidence IN
        SELECT e.*
        FROM audit_management.audit_evidence e
        LEFT JOIN audit_management.blockchain_evidence_anchors b ON e.evidence_id = b.evidence_id
        WHERE b.evidence_id IS NULL
        AND e.file_path IS NOT NULL
        LIMIT 100 -- Batch size for nightly processing
    LOOP
        -- In practice, would read file content from storage
        -- For example: v_file_content := read_binary_file(v_evidence.file_path);

        -- Generate hash (simplified example)
        v_hash := encode(sha256('simulated_content_' || v_evidence.evidence_id::TEXT), 'hex');

        -- Simulate blockchain transaction (real implementation would use blockchain API)
        v_tx_id := '0x' || encode(sha256(v_hash || CURRENT_TIMESTAMP::TEXT), 'hex');

        -- Store anchor record
        INSERT INTO audit_management.blockchain_evidence_anchors (
            evidence_id,
            hash_algorithm,
            evidence_hash,
            blockchain_transaction_id,
            blockchain_name,
            anchor_timestamp,
            created_at,
            updated_at
        ) VALUES (
            v_evidence.evidence_id,
            'SHA-256',
            v_hash,
            v_tx_id,
            p_blockchain_name,
            CURRENT_TIMESTAMP,
            CURRENT_TIMESTAMP,
            CURRENT_TIMESTAMP
        );

        RAISE NOTICE 'Anchored evidence ID % to % blockchain (TX: %)',
            v_evidence.evidence_id, p_blockchain_name, v_tx_id;
    END LOOP;

    -- Refresh integrity status view
    REFRESH MATERIALIZED VIEW audit_management.mv_evidence_integrity_status;

    RAISE NOTICE 'Completed blockchain evidence anchoring';
END;
$$;

/**
* FUNCTION: verify_evidence_hash (Helper)
* BUSINESS CASE: Validates evidence integrity by re-computing hash and comparing
* with blockchain-stored value. Essential for regulatory compliance verification.
*/
CREATE OR REPLACE FUNCTION audit_management.verify_evidence_hash(
    p_evidence_id INTEGER,
    p_stored_hash VARCHAR(64)
) RETURNS BOOLEAN
LANGUAGE plpgsql
AS $$
DECLARE
    v_current_hash TEXT;
BEGIN
    -- In practice, would hash actual file content
    -- For example: v_current_hash := encode(sha256(read_binary_file(...)), 'hex');

    -- Simplified simulation
    v_current_hash := encode(sha256('simulated_content_' || p_evidence_id::TEXT), 'hex');

    RETURN v_current_hash = p_stored_hash;
END;
$$;


-- AI powered audit finding classification engine
/**
* TABLE: ai_classification_models
* BUSINESS CASE: Stores trained AI models for automatically classifying audit findings
* by risk, category, and priority, reducing manual effort and improving consistency.
*/
CREATE TABLE audit_management.ai_classification_models (
    model_id SERIAL PRIMARY KEY,
    model_name VARCHAR(100) NOT NULL,
    model_purpose VARCHAR(50) NOT NULL CHECK (model_purpose IN ('Risk', 'Category', 'Priority')),
    model_version VARCHAR(50) NOT NULL,
    training_data_range_start DATE,
    training_data_range_end DATE,
    accuracy_score NUMERIC(5,4),
    precision_score NUMERIC(5,4),
    recall_score NUMERIC(5,4),
    model_data BYTEA NOT NULL, -- Serialized model
    is_production BOOLEAN DEFAULT FALSE,
    created_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

/**
* TABLE: ai_classification_results
* BUSINESS CASE: Stores AI classification outputs with confidence scores and
* human auditor overrides, creating auditable trail of automated decisions.
*/
CREATE TABLE audit_management.ai_classification_results (
    classification_id SERIAL PRIMARY KEY,
    finding_id INTEGER REFERENCES audit_management.audit_findings(finding_id) NOT NULL,
    model_id INTEGER REFERENCES audit_management.ai_classification_models(model_id) NOT NULL,
    predicted_value VARCHAR(50) NOT NULL,
    confidence_score NUMERIC(5,4) NOT NULL,
    override_value VARCHAR(50),
    override_reason TEXT,
    override_by INTEGER REFERENCES core.employees(employee_id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (finding_id, model_id)
);

/**
* VIEW: vw_ai_classification_performance
* BUSINESS CASE: Tracks model accuracy by comparing predictions with actual overrides,
* enabling continuous improvement of classification algorithms.
*/
CREATE OR REPLACE VIEW audit_management.vw_ai_classification_performance AS
SELECT
    m.model_id,
    m.model_name,
    m.model_purpose,
    COUNT(c.classification_id) AS total_classifications,
    COUNT(CASE WHEN c.override_value IS NOT NULL THEN 1 END) AS override_count,
    ROUND(COUNT(CASE WHEN c.override_value IS NULL THEN 1 END) * 100.0 /
          NULLIF(COUNT(c.classification_id), 0), 2) AS auto_acceptance_rate,
    AVG(c.confidence_score) AS avg_confidence,
    AVG(CASE WHEN c.override_value IS NOT NULL THEN c.confidence_score END) AS avg_override_confidence,
    m.accuracy_score AS training_accuracy
FROM
    audit_management.ai_classification_models m
JOIN
    audit_management.ai_classification_results c ON m.model_id = c.model_id
WHERE
    m.is_production = TRUE
GROUP BY
    m.model_id, m.model_name, m.model_purpose, m.accuracy_score;

/**
* PROCEDURE: sp_classify_new_findings
* BUSINESS CASE: Automatically processes new audit findings through AI classification
* models to suggest risk ratings, categories, and priorities. Runs in real-time
* as findings are created.
*/
CREATE OR REPLACE PROCEDURE audit_management.sp_classify_new_findings()
LANGUAGE plpgsql
AS $$
DECLARE
    v_finding RECORD;
    v_model RECORD;
    v_prediction_result VARCHAR(50);
    v_confidence NUMERIC(5,4);
BEGIN
    -- Process findings without classifications
    FOR v_finding IN
        SELECT f.*
        FROM audit_management.audit_findings f
        WHERE NOT EXISTS (
            SELECT 1 FROM audit_management.ai_classification_results c
            WHERE c.finding_id = f.finding_id
        )
        AND f.reported_date >= CURRENT_DATE - INTERVAL '30 days' -- Recent findings only
    LOOP
        -- Process each production model
        FOR v_model IN
            SELECT * FROM audit_management.ai_classification_models
            WHERE is_production = TRUE
        LOOP
            -- In practice, would call ML service with finding text
            -- Simplified simulation:
            CASE v_model.model_purpose
                WHEN 'Risk' THEN
                    v_prediction_result := CASE
                        WHEN random() < 0.7 THEN 'High'
                        WHEN random() < 0.9 THEN 'Medium'
                        ELSE 'Low'
                    END;
                    v_confidence := 0.7 + random() * 0.3;

                WHEN 'Category' THEN
                    v_prediction_result := CASE floor(random() * 5)
                        WHEN 0 THEN 'Financial'
                        WHEN 1 THEN 'Operational'
                        WHEN 2 THEN 'Compliance'
                        WHEN 3 THEN 'IT Security'
                        ELSE 'Other'
                    END;
                    v_confidence := 0.6 + random() * 0.4;

                WHEN 'Priority' THEN
                    v_prediction_result := CASE
                        WHEN random() < 0.6 THEN 'Urgent'
                        WHEN random() < 0.9 THEN 'High'
                        ELSE 'Medium'
                    END;
                    v_confidence := 0.8 + random() * 0.2;
            END CASE;

            -- Store classification result
            INSERT INTO audit_management.ai_classification_results (
                finding_id,
                model_id,
                predicted_value,
                confidence_score,
                created_at,
                updated_at
            ) VALUES (
                v_finding.finding_id,
                v_model.model_id,
                v_prediction_result,
                v_confidence,
                CURRENT_TIMESTAMP,
                CURRENT_TIMESTAMP
            );
        END LOOP;

        RAISE NOTICE 'Classified finding ID % with % models',
            v_finding.finding_id,
            (SELECT COUNT(*) FROM audit_management.ai_classification_models WHERE is_production = TRUE);
    END LOOP;

    RAISE NOTICE 'Completed AI classification of new findings';
END;
$$;

-- audit process mining module
/**
* TABLE: audit_process_events
* BUSINESS CASE: Captures granular event data from audit activities for process mining,
* enabling identification of bottlenecks, deviations, and improvement opportunities.
*/
CREATE TABLE audit_management.audit_process_events (
    event_id BIGSERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id) NOT NULL,
    event_type VARCHAR(50) NOT NULL CHECK (event_type IN (
        'Workpaper Started', 'Workpaper Completed', 'Review Started', 'Review Completed',
        'Testing Started', 'Testing Completed', 'Finding Identified', 'Approval Requested',
        'Approval Received', 'Exception Occurred'
    )),
    event_timestamp TIMESTAMP NOT NULL,
    user_id INTEGER REFERENCES core.employees(employee_id),
    duration_seconds INTEGER, -- For completion events
    additional_data JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

/**
* TABLE: process_mining_models
* BUSINESS CASE: Stores process mining models that analyze audit workflows to identify
* patterns, deviations, and optimization opportunities.
*/
CREATE TABLE audit_management.process_mining_models (
    model_id SERIAL PRIMARY KEY,
    model_name VARCHAR(100) NOT NULL,
    model_type VARCHAR(50) NOT NULL CHECK (model_type IN (
        'Process Discovery', 'Conformance Checking', 'Performance Analysis'
    )),
    audit_type VARCHAR(50),
    risk_level VARCHAR(20),
    model_data BYTEA NOT NULL, -- Serialized process model
    last_updated TIMESTAMP NOT NULL,
    created_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

/**
* MATERIALIZED VIEW: mv_audit_process_flows
* BUSINESS CASE: Pre-computes common audit process flows and their performance
* characteristics, refreshed daily to support continuous process improvement.
*/
CREATE MATERIALIZED VIEW audit_management.mv_audit_process_flows AS
WITH process_paths AS (
    SELECT
        audit_id,
        string_agg(event_type, ' -> ' ORDER BY event_timestamp) AS event_sequence,
        COUNT(*) AS step_count,
        EXTRACT(EPOCH FROM (max(event_timestamp) - min(event_timestamp))) AS total_seconds
    FROM
        audit_management.audit_process_events
    GROUP BY
        audit_id
)
SELECT
    event_sequence,
    COUNT(*) AS occurrence_count,
    AVG(step_count) AS avg_steps,
    AVG(total_seconds)/3600 AS avg_hours,
    MIN(total_seconds)/3600 AS min_hours,
    MAX(total_seconds)/3600 AS max_hours,
    a.audit_type,
    au.inherent_risk_level
FROM
    process_paths pp
JOIN
    audit_management.audit_activities a ON pp.audit_id = a.audit_id
JOIN
    audit_management.audit_universe au ON a.audit_entity_id = au.audit_entity_id
WHERE
    a.status = 'Completed'
GROUP BY
    event_sequence, a.audit_type, au.inherent_risk_level
ORDER BY
    occurrence_count DESC;

/**
* PROCEDURE: sp_analyze_process_deviations
* BUSINESS CASE: Identifies audit processes that deviate significantly from standard
* workflows, flagging potential quality issues or inefficiencies for review.
*/
CREATE OR REPLACE PROCEDURE audit_management.sp_analyze_process_deviations(
    p_audit_type VARCHAR(50) DEFAULT NULL,
    p_risk_level VARCHAR(20) DEFAULT NULL
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_standard_flow TEXT;
    v_deviation_threshold INTEGER := 3; -- Max allowed deviations
BEGIN
    -- Find standard flow for given parameters
    SELECT event_sequence INTO v_standard_flow
    FROM audit_management.mv_audit_process_flows
    WHERE (p_audit_type IS NULL OR audit_type = p_audit_type)
    AND (p_risk_level IS NULL OR inherent_risk_level = p_risk_level)
    ORDER BY occurrence_count DESC
    LIMIT 1;

    IF v_standard_flow IS NULL THEN
        RAISE NOTICE 'No standard process flow found for the given parameters';
        RETURN;
    END IF;

    RAISE NOTICE 'Standard process flow: %', v_standard_flow;

    -- Create table to store deviations (in practice would be permanent table)
    CREATE TEMP TABLE process_deviations AS
    SELECT
        a.audit_id,
        a.audit_title,
        pp.event_sequence,
        pp.step_count,
        pp.total_seconds/3600 AS process_hours,
        levenshtein(pp.event_sequence, v_standard_flow) AS deviation_score
    FROM
        (SELECT
            audit_id,
            string_agg(event_type, ' -> ' ORDER BY event_timestamp) AS event_sequence,
            COUNT(*) AS step_count,
            EXTRACT(EPOCH FROM (max(event_timestamp) - min(event_timestamp))) AS total_seconds
         FROM
            audit_management.audit_process_events
         GROUP BY
            audit_id) pp
    JOIN
        audit_management.audit_activities a ON pp.audit_id = a.audit_id
    WHERE
        a.status = 'Completed'
        AND (p_audit_type IS NULL OR a.audit_type = p_audit_type)
        AND (p_risk_level IS NULL OR EXISTS (
            SELECT 1 FROM audit_management.audit_universe au
            WHERE au.audit_entity_id = a.audit_entity_id
            AND au.inherent_risk_level = p_risk_level
        ))
        AND pp.event_sequence != v_standard_flow
    ORDER BY
        deviation_score DESC;

    -- Report significant deviations
    RAISE NOTICE 'Significant process deviations found:';
    FOR r IN
        SELECT * FROM process_deviations
        WHERE deviation_score > v_deviation_threshold
    LOOP
        RAISE NOTICE 'Audit ID % (%): % deviation steps from standard',
            r.audit_id, r.audit_title, r.deviation_score;
        RAISE NOTICE '  Actual flow: %', r.event_sequence;
    END LOOP;

    DROP TABLE process_deviations;
END;
$$;

--Dynamic Control Testing Engine
/**
* TABLE: adaptive_control_tests
* BUSINESS CASE: Implements intelligent control testing that automatically adjusts
* sample sizes and methods based on risk indicators and prior results, optimizing
* audit effort while maintaining coverage.
*/
CREATE TABLE audit_management.adaptive_control_tests (
    test_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id) NOT NULL,
    control_id INTEGER REFERENCES governance.controls(control_id) NOT NULL,
    original_sample_size INTEGER NOT NULL,
    adjusted_sample_size INTEGER,
    adjustment_reason VARCHAR(100),
    test_method VARCHAR(50) NOT NULL CHECK (test_method IN (
        'Attribute Sampling', 'Variable Sampling', 'Dual-Purpose', 'Data Analytics'
    )),
    confidence_level NUMERIC(5,4) NOT NULL,
    tolerable_deviation_rate NUMERIC(5,4) NOT NULL,
    expected_deviation_rate NUMERIC(5,4) NOT NULL,
    actual_deviation_rate NUMERIC(5,4),
    test_status VARCHAR(20) CHECK (test_status IN ('Planned', 'In Progress', 'Completed', 'Adjusted')),
    created_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

/**
* TABLE: control_test_transactions
* BUSINESS CASE: Tracks individual transactions tested in control samples with
* detailed results, enabling re-use of testing evidence across multiple audits.
*/
CREATE TABLE audit_management.control_test_transactions (
    transaction_id BIGSERIAL PRIMARY KEY,
    test_id INTEGER REFERENCES audit_management.adaptive_control_tests(test_id) NOT NULL,
    transaction_identifier VARCHAR(100) NOT NULL, -- Unique ID for the transaction
    transaction_date DATE NOT NULL,
    test_result VARCHAR(20) CHECK (test_result IN ('Effective', 'Ineffective', 'Exception', 'Not Applicable')),
    result_details TEXT,
    tested_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    tested_date TIMESTAMP NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (test_id, transaction_identifier)
);

/**
* VIEW: vw_control_testing_efficiency
* BUSINESS CASE: Analyzes effectiveness of adaptive testing approach by comparing
* sample sizes, deviation rates, and effort across different risk levels.
*/
CREATE OR REPLACE VIEW audit_management.vw_control_testing_efficiency AS
SELECT
    c.control_name,
    c.control_category,
    au.inherent_risk_level,
    AVG(t.original_sample_size) AS avg_initial_sample,
    AVG(COALESCE(t.adjusted_sample_size, t.original_sample_size)) AS avg_final_sample,
    AVG(t.actual_deviation_rate) AS avg_deviation_rate,
    COUNT(DISTINCT t.test_id) AS test_count,
    COUNT(DISTINCT tt.transaction_id) AS total_transactions_tested,
    ROUND(COUNT(DISTINCT tt.transaction_id) * 100.0 /
          (SELECT COUNT(DISTINCT transaction_id) FROM audit_management.control_test_transactions), 2) AS pct_of_total_testing,
    AVG(CASE WHEN tt.test_result = 'Ineffective' THEN 1 ELSE 0 END) AS ineffectiveness_rate
FROM
    audit_management.adaptive_control_tests t
JOIN
    governance.controls c ON t.control_id = c.control_id
JOIN
    audit_management.audit_activities a ON t.audit_id = a.audit_id
JOIN
    audit_management.audit_universe au ON a.audit_entity_id = au.audit_entity_id
LEFT JOIN
    audit_management.control_test_transactions tt ON t.test_id = tt.test_id
WHERE
    t.test_status = 'Completed'
GROUP BY
    c.control_name, c.control_category, au.inherent_risk_level
ORDER BY
    au.inherent_risk_level, ineffectiveness_rate DESC;

/**
* PROCEDURE: sp_adjust_control_sample
* BUSINESS CASE: Dynamically adjusts control testing samples based on interim results,
* reducing unnecessary testing when controls are effective or expanding coverage when
* issues are found. Implements statistical sampling best practices.
*/
CREATE OR REPLACE PROCEDURE audit_management.sp_adjust_control_sample(
    p_test_id INTEGER,
    p_interim_results JSONB -- Contains interim test results
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_test RECORD;
    v_interim_effective INTEGER := (p_interim_results->>'effective_count')::INTEGER;
    v_interim_ineffective INTEGER := (p_interim_results->>'ineffective_count')::INTEGER;
    v_interim_exceptions INTEGER := (p_interim_results->>'exception_count')::INTEGER;
    v_interim_total INTEGER;
    v_observed_deviation_rate NUMERIC(10,8);
    v_new_sample_size INTEGER;
    v_max_sample_size INTEGER := 200; -- Configurable maximum
BEGIN
    -- Get test details
    SELECT * INTO v_test
    FROM audit_management.adaptive_control_tests
    WHERE test_id = p_test_id;

    IF NOT FOUND THEN
        RAISE EXCEPTION 'Control test % not found', p_test_id;
    END IF;

    v_interim_total := v_interim_effective + v_interim_ineffective + v_interim_exceptions;
    v_observed_deviation_rate := (v_interim_ineffective + v_interim_exceptions)::NUMERIC /
                                 NULLIF(v_interim_total, 0);

    -- Calculate new sample size using statistical sampling approach
    IF v_observed_deviation_rate <= v_test.expected_deviation_rate THEN
        -- Reduce sample size if results are better than expected
        v_new_sample_size := LEAST(
            v_test.original_sample_size,
            CEIL(v_test.original_sample_size * 0.7) -- 30% reduction
        );
    ELSIF v_observed_deviation_rate >= v_test.tolerable_deviation_rate THEN
        -- Expand sample if approaching tolerable limit
        v_new_sample_size := LEAST(
            v_max_sample_size,
            CEIL(v_test.original_sample_size * 1.5) -- 50% increase
        );
    ELSE
        -- Maintain original sample size
        v_new_sample_size := v_test.original_sample_size;
    END IF;

    -- Update test with adjusted sample
    UPDATE audit_management.adaptive_control_tests
    SET
        adjusted_sample_size = v_new_sample_size,
        adjustment_reason = 'Automated adjustment based on interim deviation rate ' ||
                            ROUND(v_observed_deviation_rate*100, 2) || '%',
        actual_deviation_rate = v_observed_deviation_rate,
        updated_at = CURRENT_TIMESTAMP
    WHERE
        test_id = p_test_id;

    RAISE NOTICE 'Adjusted sample size for test % from % to % based on interim results',
        p_test_id, v_test.original_sample_size, v_new_sample_size;
END;
$$;

--Audit Knowledge Graph
/**
* TABLE: audit_knowledge_nodes
* BUSINESS CASE: Creates a semantic network of audit concepts, findings, and relationships
* that enables sophisticated pattern recognition and predictive analytics across audits.
*/
CREATE TABLE audit_management.audit_knowledge_nodes (
    node_id SERIAL PRIMARY KEY,
    node_type VARCHAR(50) NOT NULL CHECK (node_type IN (
        'Control', 'Risk', 'Process', 'System', 'Finding', 'Regulation'
    )),
    node_name VARCHAR(100) NOT NULL,
    node_description TEXT,
    content_hash VARCHAR(64) NOT NULL, -- For version tracking
    is_core_concept BOOLEAN DEFAULT FALSE,
    created_by INTEGER REFERENCES core.employees(employee_id) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (node_type, node_name)
);

/**
* TABLE: audit_knowledge_edges
* BUSINESS CASE: Defines relationships between nodes in the audit knowledge graph,
* capturing how risks, controls, and findings interact across the organization.
*/
CREATE TABLE audit_management.audit_knowledge_edges (
    edge_id SERIAL PRIMARY KEY,
    source_node_id INTEGER REFERENCES audit_management.audit_knowledge_nodes(node_id) NOT NULL,
    target_node_id INTEGER REFERENCES audit_management.audit_knowledge_nodes(node_id) NOT NULL,
    relationship_type VARCHAR(50) NOT NULL CHECK (relationship_type IN (
        'Mitigates', 'Triggers', 'RelatedTo', 'DependsOn', 'FoundIn', 'RequiredBy'
    )),
    strength NUMERIC(5,4) NOT NULL DEFAULT 1.0,
    evidence_count INTEGER DEFAULT 1,
    last_used TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (source_node_id, target_node_id, relationship_type)
);

/**
* MATERIALIZED VIEW: mv_knowledge_graph_insights
* BUSINESS CASE: Pre-computes key insights from the audit knowledge graph,
* identifying strongly connected components and central nodes that represent
* systemic risks or critical controls. Refreshed weekly.
*/
CREATE MATERIALIZED VIEW audit_management.mv_knowledge_graph_insights AS
WITH graph_metrics AS (
    SELECT
        n.node_id,
        n.node_name,
        n.node_type,
        COUNT(e.edge_id) AS connection_count,
        SUM(CASE WHEN e.relationship_type = 'Mitigates' THEN e.strength ELSE 0 END) AS mitigation_strength,
        SUM(CASE WHEN e.relationship_type = 'Triggers' THEN e.strength ELSE 0 END) AS risk_strength,
        COUNT(DISTINCT CASE WHEN e.relationship_type = 'FoundIn' THEN e.target_node_id END) AS finding_connections
    FROM
        audit_management.audit_knowledge_nodes n
    LEFT JOIN
        audit_management.audit_knowledge_edges e ON n.node_id = e.source_node_id
    GROUP BY
        n.node_id, n.node_name, n.node_type
)
SELECT
    *,
    RANK() OVER (PARTITION BY node_type ORDER BY connection_count DESC) AS connectivity_rank,
    CASE
        WHEN node_type = 'Risk' THEN risk_strength - mitigation_strength
        WHEN node_type = 'Control' THEN mitigation_strength
        ELSE 0
    END AS net_effect_score
FROM
    graph_metrics
ORDER BY
    connection_count DESC;

/**
* PROCEDURE: sp_expand_knowledge_graph
* BUSINESS CASE: Automatically processes new audit findings to expand the knowledge graph,
* identifying new nodes and relationships based on natural language analysis.
*/
CREATE OR REPLACE PROCEDURE audit_management.sp_expand_knowledge_graph(
    p_finding_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_finding RECORD;
    v_entity_node_id INTEGER;
    v_risk_node_id INTEGER;
    v_control_node_id INTEGER;
    v_relationship_strength NUMERIC(5,4) := 0.8; -- Default strength for new relationships
BEGIN
    -- Get finding details
    SELECT f.*, a.audit_title, au.entity_name
    INTO v_finding
    FROM audit_management.audit_findings f
    JOIN audit_management.audit_activities a ON f.audit_id = a.audit_id
    JOIN audit_management.audit_universe au ON a.audit_entity_id = au.audit_entity_id
    WHERE f.finding_id = p_finding_id;

    IF NOT FOUND THEN
        RAISE EXCEPTION 'Finding % not found', p_finding_id;
    END IF;

    -- Ensure entity node exists
    INSERT INTO audit_management.audit_knowledge_nodes (
        node_type,
        node_name,
        node_description,
        content_hash,
        created_by,
        created_at,
        updated_at
    ) VALUES (
        'Process',
        v_finding.entity_name,
        'Business process from audit universe',
        encode(sha256(v_finding.entity_name::BYTEA), 'hex'),
        1, -- System user
        CURRENT_TIMESTAMP,
        CURRENT_TIMESTAMP
    ) ON CONFLICT (node_type, node_name) DO UPDATE SET
        updated_at = CURRENT_TIMESTAMP
    RETURNING node_id INTO v_entity_node_id;

    -- Extract risk and control concepts from finding text (simplified example)
    -- In practice would use NLP techniques
    IF v_finding.finding_description LIKE '%security%' THEN
        -- Ensure risk node exists
        INSERT INTO audit_management.audit_knowledge_nodes (
            node_type,
            node_name,
            node_description,
            content_hash,
            created_by,
            created_at,
            updated_at
        ) VALUES (
            'Risk',
            'Data Security Breach',
            'Unauthorized access to sensitive data',
            encode(sha256('Data Security Breach'::BYTEA), 'hex'),
            1, -- System user
            CURRENT_TIMESTAMP,
            CURRENT_TIMESTAMP
        ) ON CONFLICT (node_type, node_name) DO UPDATE SET
            updated_at = CURRENT_TIMESTAMP
        RETURNING node_id INTO v_risk_node_id;

        -- Ensure control node exists
        INSERT INTO audit_management.audit_knowledge_nodes (
            node_type,
            node_name,
            node_description,
            content_hash,
            created_by,
            created_at,
            updated_at
        ) VALUES (
            'Control',
            'Access Controls',
            'User authentication and authorization',
            encode(sha256('Access Controls'::BYTEA), 'hex'),
            1, -- System user
            CURRENT_TIMESTAMP,
            CURRENT_TIMESTAMP
        ) ON CONFLICT (node_type, node_name) DO UPDATE SET
            updated_at = CURRENT_TIMESTAMP
        RETURNING node_id INTO v_control_node_id;

        -- Create relationships
        INSERT INTO audit_management.audit_knowledge_edges (
            source_node_id,
            target_node_id,
            relationship_type,
            strength,
            evidence_count,
            created_at,
            updated_at
        ) VALUES
        -- Process has risk
        (v_entity_node_id, v_risk_node_id, 'Triggers', v_relationship_strength, 1, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP),
        -- Control mitigates risk
        (v_control_node_id, v_risk_node_id, 'Mitigates', v_relationship_strength, 1, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP),
        -- Finding relates to risk
        (v_risk_node_id, (SELECT node_id FROM audit_management.audit_knowledge_nodes
                          WHERE node_type = 'Finding' AND node_name = 'Finding #' || p_finding_id),
         'RelatedTo', v_relationship_strength, 1, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
        ON CONFLICT (source_node_id, target_node_id, relationship_type)
        DO UPDATE SET
            strength = LEAST(1.0, audit_knowledge_edges.strength + 0.1),
            evidence_count = audit_knowledge_edges.evidence_count + 1,
            last_used = CURRENT_TIMESTAMP,
            updated_at = CURRENT_TIMESTAMP;
    END IF;

    -- Refresh materialized view
    REFRESH MATERIALIZED VIEW audit_management.mv_knowledge_graph_insights;

    RAISE NOTICE 'Expanded knowledge graph with finding % about %', p_finding_id, v_finding.entity_name;
END;
$$;

--Real-time Audit Monitoring Dashboard
/**
* TABLE: audit_monitoring_metrics
* BUSINESS CASE: Captures real-time metrics from active audits for operational
* monitoring, enabling audit managers to identify delays or quality issues as they occur.
*/
CREATE TABLE audit_management.audit_monitoring_metrics (
    metric_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id) NOT NULL,
    metric_timestamp TIMESTAMP NOT NULL,
    metric_type VARCHAR(50) NOT NULL CHECK (metric_type IN (
        'Progress', 'BudgetUtilization', 'FindingRate', 'TeamCapacity', 'RiskIndicator'
    )),
    metric_value NUMERIC(12,4) NOT NULL,
    benchmark_value NUMERIC(12,4),
    alert_threshold NUMERIC(12,4),
    is_alert BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

/**
* TABLE: audit_monitoring_alerts
* BUSINESS CASE: Tracks alerts generated from real-time monitoring for immediate
* attention, with escalation paths and resolution tracking.
*/
CREATE TABLE audit_management.audit_monitoring_alerts (
    alert_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES audit_management.audit_activities(audit_id) NOT NULL,
    metric_id INTEGER REFERENCES audit_management.audit_monitoring_metrics(metric_id) NOT NULL,
    alert_type VARCHAR(50) NOT NULL,
    alert_message TEXT NOT NULL,
    severity VARCHAR(20) CHECK (severity IN ('Low', 'Medium', 'High', 'Critical')),
    status VARCHAR(20) CHECK (status IN ('Open', 'Acknowledged', 'Resolved', 'Escalated')),
    assigned_to INTEGER REFERENCES core.employees(employee_id),
    acknowledged_at TIMESTAMP,
    resolved_at TIMESTAMP,
    resolution_notes TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

/**
* VIEW: vw_audit_monitoring_status
* BUSINESS CASE: Aggregates real-time monitoring data for dashboard display,
* providing a comprehensive view of all active audits with alert status.
*/
CREATE OR REPLACE VIEW audit_management.vw_audit_monitoring_status AS
SELECT
    a.audit_id,
    a.audit_title,
    au.entity_name,
    au.inherent_risk_level,
    COUNT(DISTINCT m.metric_id) AS metric_count,
    COUNT(DISTINCT al.alert_id) AS open_alert_count,
    MAX(m.metric_timestamp) AS last_metric_time,
    AVG(CASE WHEN m.metric_type = 'Progress' THEN m.metric_value ELSE NULL END) AS progress_pct,
    AVG(CASE WHEN m.metric_type = 'BudgetUtilization' THEN m.metric_value ELSE NULL END) AS budget_utilization_pct,
    AVG(CASE WHEN m.metric_type = 'FindingRate' THEN m.metric_value ELSE NULL END) AS findings_per_day,
    MAX(CASE WHEN al.severity = 'Critical' THEN 1 ELSE 0 END) AS has_critical_alert
FROM
    audit_management.audit_activities a
JOIN
    audit_management.audit_universe au ON a.audit_entity_id = au.audit_entity_id
LEFT JOIN
    audit_management.audit_monitoring_metrics m ON a.audit_id = m.audit_id
LEFT JOIN
    audit_management.audit_monitoring_alerts al ON a.audit_id = al.audit_id AND al.status != 'Resolved'
WHERE
    a.status = 'In Progress'
GROUP BY
    a.audit_id, a.audit_title, au.entity_name, au.inherent_risk_level
ORDER BY
    has_critical_alert DESC, open_alert_count DESC;

/**
* PROCEDURE: sp_generate_monitoring_alerts
* BUSINESS CASE: Analyzes real-time metrics to generate alerts when audits deviate
* from expected parameters, enabling proactive management of audit execution.
*/
CREATE OR REPLACE PROCEDURE audit_management.sp_generate_monitoring_alerts()
LANGUAGE plpgsql
AS $$
DECLARE
    v_metric RECORD;
    v_alert_message TEXT;
    v_alert_severity VARCHAR(20);
BEGIN
    -- Process metrics that exceed thresholds but don't have active alerts
    FOR v_metric IN
        SELECT m.*, a.audit_title, au.entity_name
        FROM audit_management.audit_monitoring_metrics m
        JOIN audit_management.audit_activities a ON m.audit_id = a.audit_id
        JOIN audit_management.audit_universe au ON a.audit_entity_id = au.audit_entity_id
        WHERE m.is_alert = TRUE
        AND NOT EXISTS (
            SELECT 1 FROM audit_management.audit_monitoring_alerts al
            WHERE al.metric_id = m.metric_id AND al.status != 'Resolved'
        )
        AND m.metric_timestamp >= CURRENT_TIMESTAMP - INTERVAL '1 hour'
    LOOP
        -- Determine alert details based on metric type
        CASE v_metric.metric_type
            WHEN 'Progress' THEN
                IF v_metric.metric_value < v_metric.benchmark_value * 0.7 THEN
                    v_alert_message := 'Audit progress (' || ROUND(v_metric.metric_value, 1) ||
                                       '%) significantly behind benchmark (' ||
                                       ROUND(v_metric.benchmark_value, 1) || '%)';
                    v_alert_severity := 'High';
                ELSIF v_metric.metric_value < v_metric.benchmark_value * 0.9 THEN
                    v_alert_message := 'Audit progress (' || ROUND(v_metric.metric_value, 1) ||
                                       '%) behind benchmark (' ||
                                       ROUND(v_metric.benchmark_value, 1) || '%)';
                    v_alert_severity := 'Medium';
                END IF;

            WHEN 'BudgetUtilization' THEN
                IF v_metric.metric_value > v_metric.alert_threshold * 1.3 THEN
                    v_alert_message := 'Budget utilization (' || ROUND(v_metric.metric_value, 1) ||
                                      '%) significantly exceeds threshold (' ||
                                      ROUND(v_metric.alert_threshold, 1) || '%)';
                    v_alert_severity := 'High';
                ELSIF v_metric.metric_value > v_metric.alert_threshold THEN
                    v_alert_message := 'Budget utilization (' || ROUND(v_metric.metric_value, 1) ||
                                      '%) exceeds threshold (' ||
                                      ROUND(v_metric.alert_threshold, 1) || '%)';
                    v_alert_severity := 'Medium';
                END IF;

            WHEN 'FindingRate' THEN
                IF v_metric.metric_value > v_metric.alert_threshold * 2 THEN
                    v_alert_message := 'Finding rate (' || ROUND(v_metric.metric_value, 1) ||
                                      '/day) significantly exceeds threshold (' ||
                                      ROUND(v_metric.alert_threshold, 1) || '/day)';
                    v_alert_severity := 'Critical';
                ELSIF v_metric.metric_value > v_metric.alert_threshold THEN
                    v_alert_message := 'Finding rate (' || ROUND(v_metric.metric_value, 1) ||
                                      '/day) exceeds threshold (' ||
                                      ROUND(v_metric.alert_threshold, 1) || '/day)';
                    v_alert_severity := 'High';
                END IF;
        END CASE;

        -- Create alert if warranted
        IF v_alert_message IS NOT NULL THEN
            INSERT INTO audit_management.audit_monitoring_alerts (
                audit_id,
                metric_id,
                alert_type,
                alert_message,
                severity,
                status,
                created_at,
                updated_at
            ) VALUES (
                v_metric.audit_id,
                v_metric.metric_id,
                v_metric.metric_type || ' Alert',
                v_alert_message || ' for ' || v_metric.entity_name || ' (' || v_metric.audit_title || ')',
                v_alert_severity,
                'Open',
                CURRENT_TIMESTAMP,
                CURRENT_TIMESTAMP
            );

            RAISE NOTICE 'Generated % alert for audit %: %',
                v_alert_severity, v_metric.audit_id, v_alert_message;
        END IF;
    END LOOP;

    RAISE NOTICE 'Completed monitoring alert generation';
END;
$$;

----------------------

-- Crisis Management and Business Continuity
----------------------
CREATE SCHEMA IF NOT EXISTS cmbc;
COMMENT ON SCHEMA cmbc IS 'Schema for Crisis Management and Business Continuity data, including BCP, risk assessments, incident management, and recovery tracking';

CREATE TABLE cmbc.business_continuity_plans (
    bcp_id SERIAL PRIMARY KEY,
    plan_name VARCHAR(255) NOT NULL,
    description TEXT,
    version VARCHAR(50) NOT NULL,
    effective_date DATE NOT NULL,
    review_frequency_months INTEGER NOT NULL,
    last_review_date DATE,
    next_review_date DATE GENERATED ALWAYS AS (last_review_date + (review_frequency_months * INTERVAL '1 month')) STORED,
    status VARCHAR(50) NOT NULL CHECK (status IN ('Draft', 'Active', 'Under Review', 'Retired')),
    created_by VARCHAR(100) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    approval_date DATE,
    approved_by VARCHAR(100)
);

COMMENT ON TABLE cmbc.business_continuity_plans IS 'Master table for all Business Continuity Plans (BCPs) in the organization';
COMMENT ON COLUMN cmbc.business_continuity_plans.review_frequency_months IS 'Frequency in months for mandatory plan reviews';
COMMENT ON COLUMN cmbc.business_continuity_plans.status IS 'Current lifecycle status of the BCP document';


CREATE TABLE cmbc.critical_business_processes (
    process_id SERIAL PRIMARY KEY,
    process_name VARCHAR(255) NOT NULL,
    description TEXT,
    department VARCHAR(100) NOT NULL,
    owner VARCHAR(100) NOT NULL,
    max_tolerable_downtime_hours INTEGER NOT NULL,
    recovery_priority INTEGER NOT NULL CHECK (recovery_priority BETWEEN 1 AND 5),
    data_criticality VARCHAR(50) NOT NULL CHECK (data_criticality IN ('Low', 'Medium', 'High', 'Critical')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (process_name, department)
);

COMMENT ON TABLE cmbc.critical_business_processes IS 'Identifies and categorizes critical business processes for BIA and recovery prioritization';
COMMENT ON COLUMN cmbc.critical_business_processes.recovery_priority IS 'Priority level for recovery (1=highest, 5=lowest)';

CREATE TABLE cmbc.business_impact_analysis (
    bia_id SERIAL PRIMARY KEY,
    process_id INTEGER NOT NULL REFERENCES cmbc.critical_business_processes(process_id),
    bcp_id INTEGER NOT NULL REFERENCES cmbc.business_continuity_plans(bcp_id),
    financial_impact_per_hour NUMERIC(15,2) NOT NULL,
    operational_impact_level VARCHAR(50) NOT NULL CHECK (operational_impact_level IN ('Low', 'Medium', 'High', 'Critical')),
    legal_compliance_impact TEXT,
    customer_impact TEXT,
    reputation_impact TEXT,
    dependencies TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (process_id, bcp_id)
);

COMMENT ON TABLE cmbc.business_impact_analysis IS 'Detailed impact analysis for each critical business process within a BCP';
COMMENT ON COLUMN cmbc.business_impact_analysis.financial_impact_per_hour IS 'Estimated financial loss per hour of downtime';

CREATE TABLE cmbc.business_impact_analysis (
    bia_id SERIAL PRIMARY KEY,
    process_id INTEGER NOT NULL REFERENCES cmbc.critical_business_processes(process_id),
    bcp_id INTEGER NOT NULL REFERENCES cmbc.business_continuity_plans(bcp_id),
    financial_impact_per_hour NUMERIC(15,2) NOT NULL,
    operational_impact_level VARCHAR(50) NOT NULL CHECK (operational_impact_level IN ('Low', 'Medium', 'High', 'Critical')),
    legal_compliance_impact TEXT,
    customer_impact TEXT,
    reputation_impact TEXT,
    dependencies TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (process_id, bcp_id)
);

COMMENT ON TABLE cmbc.business_impact_analysis IS 'Detailed impact analysis for each critical business process within a BCP';
COMMENT ON COLUMN cmbc.business_impact_analysis.financial_impact_per_hour IS 'Estimated financial loss per hour of downtime';

CREATE TABLE cmbc.recovery_objectives (
    objective_id SERIAL PRIMARY KEY,
    process_id INTEGER NOT NULL REFERENCES cmbc.critical_business_processes(process_id),
    bcp_id INTEGER NOT NULL REFERENCES cmbc.business_continuity_plans(bcp_id),
    rto_hours NUMERIC(6,2) NOT NULL COMMENT 'Recovery Time Objective in hours',
    rpo_minutes INTEGER NOT NULL COMMENT 'Recovery Point Objective in minutes (maximum data loss)',
    mtd_hours NUMERIC(6,2) NOT NULL COMMENT 'Maximum Tolerable Downtime in hours',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (process_id, bcp_id)
);

COMMENT ON TABLE cmbc.recovery_objectives IS 'Defines RTO, RPO and MTD for each critical process in a BCP';

CREATE TABLE cmbc.risk_assessments (
    risk_id SERIAL PRIMARY KEY,
    bcp_id INTEGER NOT NULL REFERENCES cmbc.business_continuity_plans(bcp_id),
    risk_name VARCHAR(255) NOT NULL,
    description TEXT,
    category VARCHAR(100) NOT NULL CHECK (category IN ('Natural', 'Technical', 'Human', 'Organizational', 'External')),
    likelihood VARCHAR(20) NOT NULL CHECK (likelihood IN ('Rare', 'Unlikely', 'Possible', 'Likely', 'Almost Certain')),
    impact VARCHAR(20) NOT NULL CHECK (impact IN ('Insignificant', 'Minor', 'Moderate', 'Major', 'Catastrophic')),
    risk_score INTEGER GENERATED ALWAYS AS (
        CASE
            WHEN likelihood = 'Rare' AND impact = 'Insignificant' THEN 1
            WHEN likelihood = 'Rare' AND impact = 'Minor' THEN 2
            WHEN likelihood = 'Unlikely' AND impact = 'Insignificant' THEN 2
            WHEN likelihood = 'Unlikely' AND impact = 'Minor' THEN 4
            WHEN likelihood = 'Possible' AND impact = 'Moderate' THEN 9
            WHEN likelihood = 'Likely' AND impact = 'Major' THEN 16
            WHEN likelihood = 'Almost Certain' AND impact = 'Catastrophic' THEN 25
            ELSE 0
        END
    ) STORED,
    mitigation_strategy TEXT,
    residual_risk_score INTEGER,
    owner VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.risk_assessments IS 'Identifies and assesses risks to business continuity with mitigation strategies';

CREATE TABLE cmbc.incidents (
    incident_id SERIAL PRIMARY KEY,
    incident_name VARCHAR(255) NOT NULL,
    description TEXT NOT NULL,
    incident_type VARCHAR(100) NOT NULL CHECK (incident_type IN ('Cyber Attack', 'Natural Disaster', 'System Failure', 'Supply Chain', 'Workforce', 'Other')),
    severity VARCHAR(20) NOT NULL CHECK (severity IN ('Low', 'Medium', 'High', 'Critical')),
    status VARCHAR(50) NOT NULL CHECK (status IN ('Reported', 'Assessed', 'Contained', 'Recovering', 'Resolved', 'Closed')),
    start_time TIMESTAMP WITH TIME ZONE NOT NULL,
    end_time TIMESTAMP WITH TIME ZONE,
    bcp_activated BOOLEAN DEFAULT FALSE,
    bcp_id INTEGER REFERENCES cmbc.business_continuity_plans(bcp_id),
    root_cause TEXT,
    lessons_learned TEXT,
    created_by VARCHAR(100) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.incidents IS 'Tracks all disruptive incidents and their resolution status';

CREATE TABLE cmbc.incident_impacts (
    impact_id SERIAL PRIMARY KEY,
    incident_id INTEGER NOT NULL REFERENCES cmbc.incidents(incident_id),
    process_id INTEGER NOT NULL REFERENCES cmbc.critical_business_processes(process_id),
    downtime_hours NUMERIC(6,2) NOT NULL,
    financial_impact NUMERIC(15,2) NOT NULL,
    customer_impact_level VARCHAR(20) CHECK (customer_impact_level IN ('None', 'Low', 'Medium', 'High', 'Critical')),
    reputation_impact_level VARCHAR(20) CHECK (reputation_impact_level IN ('None', 'Low', 'Medium', 'High', 'Critical')),
    notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.incident_impacts IS 'Records the specific impacts of an incident on critical business processes';

CREATE TABLE cmbc.recovery_actions (
    action_id SERIAL PRIMARY KEY,
    incident_id INTEGER NOT NULL REFERENCES cmbc.incidents(incident_id),
    process_id INTEGER NOT NULL REFERENCES cmbc.critical_business_processes(process_id),
    action_description TEXT NOT NULL,
    assigned_to VARCHAR(100) NOT NULL,
    status VARCHAR(50) NOT NULL CHECK (status IN ('Pending', 'In Progress', 'Completed', 'Failed')),
    start_time TIMESTAMP WITH TIME ZONE,
    end_time TIMESTAMP WITH TIME ZONE,
    rto_met BOOLEAN,
    rpo_met BOOLEAN,
    notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.recovery_actions IS 'Tracks specific recovery actions taken during incident response';

CREATE TABLE cmbc.bcp_tests (
    test_id SERIAL PRIMARY KEY,
    bcp_id INTEGER NOT NULL REFERENCES cmbc.business_continuity_plans(bcp_id),
    test_name VARCHAR(255) NOT NULL,
    test_type VARCHAR(100) NOT NULL CHECK (test_type IN ('Tabletop', 'Walkthrough', 'Simulation', 'Full Interruption')),
    test_date DATE NOT NULL,
    test_scenario TEXT NOT NULL,
    status VARCHAR(50) NOT NULL CHECK (status IN ('Planned', 'In Progress', 'Completed', 'Cancelled')),
    results_summary TEXT,
    success_rate_pct NUMERIC(5,2),
    coordinator VARCHAR(100) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.bcp_tests IS 'Records all BCP testing activities and their outcomes';


CREATE TABLE cmbc.bcp_tests (
    test_id SERIAL PRIMARY KEY,
    bcp_id INTEGER NOT NULL REFERENCES cmbc.business_continuity_plans(bcp_id),
    test_name VARCHAR(255) NOT NULL,
    test_type VARCHAR(100) NOT NULL CHECK (test_type IN ('Tabletop', 'Walkthrough', 'Simulation', 'Full Interruption')),
    test_date DATE NOT NULL,
    test_scenario TEXT NOT NULL,
    status VARCHAR(50) NOT NULL CHECK (status IN ('Planned', 'In Progress', 'Completed', 'Cancelled')),
    results_summary TEXT,
    success_rate_pct NUMERIC(5,2),
    coordinator VARCHAR(100) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.bcp_tests IS 'Records all BCP testing activities and their outcomes';


CREATE TABLE cmbc.test_findings (
    finding_id SERIAL PRIMARY KEY,
    test_id INTEGER NOT NULL REFERENCES cmbc.bcp_tests(test_id),
    process_id INTEGER REFERENCES cmbc.critical_business_processes(process_id),
    finding_description TEXT NOT NULL,
    severity VARCHAR(20) NOT NULL CHECK (severity IN ('Low', 'Medium', 'High', 'Critical')),
    corrective_action TEXT,
    assigned_to VARCHAR(100),
    due_date DATE,
    status VARCHAR(50) NOT NULL CHECK (status IN ('Open', 'In Progress', 'Resolved', 'Closed')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.test_findings IS 'Identifies issues discovered during BCP testing and tracks their resolution';

CREATE TABLE cmbc.communication_logs (
    log_id SERIAL PRIMARY KEY,
    incident_id INTEGER REFERENCES cmbc.incidents(incident_id),
    test_id INTEGER REFERENCES cmbc.bcp_tests(test_id),
    communication_type VARCHAR(50) NOT NULL CHECK (communication_type IN ('Alert', 'Update', 'Instruction', 'All Clear')),
    channel VARCHAR(50) NOT NULL CHECK (channel IN ('Email', 'SMS', 'Phone', 'App', 'Website', 'Social Media')),
    recipient_group VARCHAR(100) NOT NULL CHECK (recipient_group IN ('Employees', 'Management', 'Customers', 'Vendors', 'Public', 'Regulators')),
    message TEXT NOT NULL,
    sent_by VARCHAR(100) NOT NULL,
    sent_at TIMESTAMP WITH TIME ZONE NOT NULL,
    delivery_status VARCHAR(50) NOT NULL CHECK (delivery_status IN ('Sent', 'Delivered', 'Failed', 'Read')),
    response_received BOOLEAN DEFAULT FALSE,
    response_details TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.communication_logs IS 'Logs all crisis communications during incidents and tests';

CREATE TABLE cmbc.vendors (
    vendor_id SERIAL PRIMARY KEY,
    vendor_name VARCHAR(255) NOT NULL,
    contact_name VARCHAR(100),
    contact_email VARCHAR(100),
    contact_phone VARCHAR(20),
    service_provided TEXT NOT NULL,
    criticality VARCHAR(20) NOT NULL CHECK (criticality IN ('Low', 'Medium', 'High', 'Critical')),
    bcp_review_date DATE,
    bcp_available BOOLEAN,
    bcp_rating VARCHAR(20) CHECK (bcp_rating IN ('Poor', 'Adequate', 'Good', 'Excellent')),
    contract_end_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.vendors IS 'Tracks third-party vendors and their business continuity preparedness';


CREATE TABLE cmbc.vendor_risk_assessments (
    assessment_id SERIAL PRIMARY KEY,
    vendor_id INTEGER NOT NULL REFERENCES cmbc.vendors(vendor_id),
    assessment_date DATE NOT NULL,
    assessor VARCHAR(100) NOT NULL,
    financial_stability_score INTEGER CHECK (financial_stability_score BETWEEN 1 AND 5),
    operational_resilience_score INTEGER CHECK (operational_resilience_score BETWEEN 1 AND 5),
    bcp_score INTEGER CHECK (bcp_score BETWEEN 1 AND 5),
    recovery_capability_score INTEGER CHECK (recovery_capability_score BETWEEN 1 AND 5),
    overall_risk_score NUMERIC(3,2) GENERATED ALWAYS AS (
        (COALESCE(financial_stability_score, 3) +
        COALESCE(operational_resilience_score, 3) +
        COALESCE(bcp_score, 3) +
        COALESCE(recovery_capability_score, 3)
    ) / 4.0 STORED,
    findings TEXT,
    recommendations TEXT,
    next_review_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.vendor_risk_assessments IS 'Records risk assessments performed on third-party vendors';


CREATE TABLE cmbc.training_programs (
    program_id SERIAL PRIMARY KEY,
    program_name VARCHAR(255) NOT NULL,
    description TEXT,
    target_audience VARCHAR(100) NOT NULL,
    frequency_months INTEGER NOT NULL,
    duration_minutes INTEGER NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.training_programs IS 'Defines BCM training programs for different employee groups';

CREATE TABLE cmbc.training_records (
    record_id SERIAL PRIMARY KEY,
    program_id INTEGER NOT NULL REFERENCES cmbc.training_programs(program_id),
    employee_id VARCHAR(50) NOT NULL,
    employee_name VARCHAR(100) NOT NULL,
    training_date DATE NOT NULL,
    trainer VARCHAR(100),
    completion_status VARCHAR(50) NOT NULL CHECK (completion_status IN ('Completed', 'No Show', 'Partial')),
    score NUMERIC(5,2),
    feedback TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.training_records IS 'Tracks employee participation and performance in BCM training';


CREATE TABLE cmbc.kpis (
    kpi_id SERIAL PRIMARY KEY,
    kpi_name VARCHAR(255) NOT NULL,
    description TEXT NOT NULL,
    category VARCHAR(100) NOT NULL CHECK (category IN ('BCP', 'Risk', 'Recovery', 'Communication', 'Vendor', 'Training')),
    measurement_unit VARCHAR(50) NOT NULL,
    target_value NUMERIC(15,2) NOT NULL,
    is_krm BOOLEAN NOT NULL DEFAULT FALSE,
    frequency VARCHAR(50) NOT NULL CHECK (frequency IN ('Daily', 'Weekly', 'Monthly', 'Quarterly', 'Annual')),
    owner VARCHAR(100) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.kpis IS 'Master list of all KPIs and KRMs for crisis management and business continuity';

CREATE TABLE cmbc.kpi_measurements (
    measurement_id SERIAL PRIMARY KEY,
    kpi_id INTEGER NOT NULL REFERENCES cmbc.kpis(kpi_id),
    measurement_period DATE NOT NULL,
    actual_value NUMERIC(15,2) NOT NULL,
    target_value NUMERIC(15,2) NOT NULL,
    variance NUMERIC(15,2) GENERATED ALWAYS AS (actual_value - target_value) STORED,
    variance_pct NUMERIC(5,2) GENERATED ALWAYS AS (
        CASE
            WHEN target_value = 0 THEN NULL
            ELSE ((actual_value - target_value) / target_value) * 100
        END
    ) STORED,
    notes TEXT,
    created_by VARCHAR(100) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (kpi_id, measurement_period)
);

COMMENT ON TABLE cmbc.kpi_measurements IS 'Records periodic measurements of KPIs and KRMs';


CREATE OR REPLACE VIEW cmbc.vw_bcp_summary AS
SELECT
    b.bcp_id,
    b.plan_name,
    b.version,
    b.status,
    b.effective_date,
    b.next_review_date,
    COUNT(DISTINCT cb.process_id) AS critical_processes_covered,
    COUNT(DISTINCT t.test_id) AS tests_completed,
    MAX(t.test_date) AS last_test_date,
    AVG(t.success_rate_pct) AS avg_test_success_rate
FROM
    cmbc.business_continuity_plans b
LEFT JOIN
    cmbc.business_impact_analysis bia ON b.bcp_id = bia.bcp_id
LEFT JOIN
    cmbc.critical_business_processes cb ON bia.process_id = cb.process_id
LEFT JOIN
    cmbc.bcp_tests t ON b.bcp_id = t.bcp_id
GROUP BY
    b.bcp_id, b.plan_name, b.version, b.status, b.effective_date, b.next_review_date;

COMMENT ON VIEW cmbc.vw_bcp_summary IS 'Provides a summary of each BCP including coverage and testing metrics';


CREATE OR REPLACE VIEW cmbc.vw_risk_heatmap AS
SELECT
    r.risk_id,
    r.risk_name,
    r.category,
    r.likelihood,
    r.impact,
    r.risk_score,
    b.plan_name,
    r.mitigation_strategy,
    r.residual_risk_score,
    r.owner,
    r.updated_at
FROM
    cmbc.risk_assessments r
JOIN
    cmbc.business_continuity_plans b ON r.bcp_id = b.bcp_id
ORDER BY
    r.risk_score DESC;

COMMENT ON VIEW cmbc.vw_risk_heatmap IS 'Displays all risks with their scores for visualization in a risk heatmap';


CREATE OR REPLACE VIEW cmbc.vw_incident_recovery_tracking AS
SELECT
    i.incident_id,
    i.incident_name,
    i.start_time,
    i.end_time,
    i.severity,
    i.status,
    cb.process_name,
    ro.rto_hours,
    ro.rpo_minutes,
    ii.downtime_hours,
    CASE
        WHEN ii.downtime_hours <= ro.rto_hours THEN 'Met'
        ELSE 'Not Met'
    END AS rto_status,
    ra.action_description,
    ra.status AS action_status,
    ra.assigned_to,
    ra.rto_met,
    ra.rpo_met
FROM
    cmbc.incidents i
JOIN
    cmbc.incident_impacts ii ON i.incident_id = ii.incident_id
JOIN
    cmbc.critical_business_processes cb ON ii.process_id = cb.process_id
JOIN
    cmbc.recovery_objectives ro ON cb.process_id = ro.process_id AND i.bcp_id = ro.bcp_id
LEFT JOIN
    cmbc.recovery_actions ra ON i.incident_id = ra.incident_id AND cb.process_id = ra.process_id
ORDER BY
    i.start_time DESC, cb.recovery_priority;

COMMENT ON VIEW cmbc.vw_incident_recovery_tracking IS 'Tracks recovery progress against RTO/RPO for active incidents';


CREATE OR REPLACE VIEW cmbc.vw_vendor_risk_summary AS
SELECT
    v.vendor_id,
    v.vendor_name,
    v.service_provided,
    v.criticality,
    v.bcp_rating,
    a.assessment_date,
    a.overall_risk_score,
    CASE
        WHEN a.overall_risk_score <= 2.0 THEN 'Low Risk'
        WHEN a.overall_risk_score <= 3.5 THEN 'Medium Risk'
        ELSE 'High Risk'
    END AS risk_level,
    a.next_review_date,
    DATEDIFF(DAY, CURRENT_DATE, a.next_review_date) AS days_until_next_review
FROM
    cmbc.vendors v
JOIN
    (SELECT
        vendor_id,
        MAX(assessment_date) AS latest_assessment
     FROM
        cmbc.vendor_risk_assessments
     GROUP BY
        vendor_id) latest ON v.vendor_id = latest.vendor_id
JOIN
    cmbc.vendor_risk_assessments a ON latest.vendor_id = a.vendor_id AND latest.latest_assessment = a.assessment_date
ORDER BY
    a.overall_risk_score DESC;

COMMENT ON VIEW cmbc.vw_vendor_risk_summary IS 'Provides a summary of vendor risk based on latest assessments';

CREATE OR REPLACE VIEW cmbc.vw_kpi_dashboard AS
SELECT
    k.kpi_id,
    k.kpi_name,
    k.description,
    k.category,
    k.measurement_unit,
    k.target_value,
    k.is_krm,
    m.measurement_period,
    m.actual_value,
    m.variance,
    m.variance_pct,
    CASE
        WHEN k.is_krm AND m.variance > 0 THEN 'Risk Increasing'
        WHEN k.is_krm AND m.variance <= 0 THEN 'Risk Controlled'
        WHEN NOT k.is_krm AND m.variance >= 0 THEN 'Target Met/Exceeded'
        ELSE 'Target Not Met'
    END AS status,
    k.frequency,
    k.owner
FROM
    cmbc.kpis k
JOIN
    (SELECT
        kpi_id,
        MAX(measurement_period) AS latest_period
     FROM
        cmbc.kpi_measurements
     GROUP BY
        kpi_id) latest ON k.kpi_id = latest.kpi_id
JOIN
    cmbc.kpi_measurements m ON latest.kpi_id = m.kpi_id AND latest.latest_period = m.measurement_period
ORDER BY
    k.category, k.kpi_name;

COMMENT ON VIEW cmbc.vw_kpi_dashboard IS 'Dashboard view showing latest KPI/KRM measurements against targets';


CREATE MATERIALIZED VIEW cmbc.mv_bcp_testing_performance AS
SELECT
    b.bcp_id,
    b.plan_name,
    COUNT(t.test_id) AS total_tests,
    SUM(CASE WHEN t.status = 'Completed' THEN 1 ELSE 0 END) AS completed_tests,
    AVG(t.success_rate_pct) AS avg_success_rate,
    MAX(t.test_date) AS last_test_date,
    COUNT(f.finding_id) AS total_findings,
    SUM(CASE WHEN f.severity = 'Critical' THEN 1 ELSE 0 END) AS critical_findings,
    SUM(CASE WHEN f.status = 'Open' THEN 1 ELSE 0 END) AS open_findings
FROM
    cmbc.business_continuity_plans b
LEFT JOIN
    cmbc.bcp_tests t ON b.bcp_id = t.bcp_id
LEFT JOIN
    cmbc.test_findings f ON t.test_id = f.test_id
GROUP BY
    b.bcp_id, b.plan_name;

COMMENT ON MATERIALIZED VIEW cmbc.mv_bcp_testing_performance IS 'Aggregated view of BCP testing performance with finding statistics';

CREATE MATERIALIZED VIEW cmbc.mv_incident_trends AS
SELECT
    DATE_TRUNC('month', i.start_time) AS month,
    i.incident_type,
    COUNT(i.incident_id) AS incident_count,
    AVG(EXTRACT(EPOCH FROM (i.end_time - i.start_time))/3600) AS avg_duration_hours,
    SUM(ii.financial_impact) AS total_financial_impact,
    AVG(ii.downtime_hours) AS avg_downtime_hours,
    COUNT(DISTINCT CASE WHEN i.severity = 'Critical' THEN i.incident_id END) AS critical_incidents
FROM
    cmbc.incidents i
JOIN
    cmbc.incident_impacts ii ON i.incident_id = ii.incident_id
WHERE
    i.status = 'Closed'
GROUP BY
    DATE_TRUNC('month', i.start_time), i.incident_type;

COMMENT ON MATERIALIZED VIEW cmbc.mv_incident_trends IS 'Monthly trends of incidents by type with impact metrics';

CREATE MATERIALIZED VIEW cmbc.mv_recovery_compliance AS
SELECT
    cb.process_id,
    cb.process_name,
    cb.department,
    COUNT(DISTINCT i.incident_id) AS total_incidents,
    COUNT(DISTINCT CASE WHEN ii.downtime_hours <= ro.rto_hours THEN i.incident_id END) AS rto_met_count,
    COUNT(DISTINCT CASE WHEN ra.rpo_met = TRUE THEN i.incident_id END) AS rpo_met_count,
    COUNT(DISTINCT i.incident_id) - COUNT(DISTINCT CASE WHEN ii.downtime_hours <= ro.rto_hours THEN i.incident_id END) AS rto_missed_count,
    AVG(ii.downtime_hours) AS avg_downtime_hours,
    ro.rto_hours,
    ro.rpo_minutes
FROM
    cmbc.critical_business_processes cb
JOIN
    cmbc.recovery_objectives ro ON cb.process_id = ro.process_id
LEFT JOIN
    cmbc.incident_impacts ii ON cb.process_id = ii.process_id
LEFT JOIN
    cmbc.incidents i ON ii.incident_id = i.incident_id AND i.status = 'Closed'
LEFT JOIN
    cmbc.recovery_actions ra ON i.incident_id = ra.incident_id AND cb.process_id = ra.process_id
GROUP BY
    cb.process_id, cb.process_name, cb.department, ro.rto_hours, ro.rpo_minutes;

COMMENT ON MATERIALIZED VIEW cmbc.mv_recovery_compliance IS 'Tracks compliance with RTO and RPO objectives by process';


CREATE OR REPLACE PROCEDURE cmbc.sp_create_incident(
    p_incident_name VARCHAR(255),
    p_description TEXT,
    p_incident_type VARCHAR(100),
    p_severity VARCHAR(20),
    p_start_time TIMESTAMP WITH TIME ZONE,
    p_created_by VARCHAR(100),
    OUT p_incident_id INTEGER
)
LANGUAGE plpgsql
AS $$
BEGIN
    INSERT INTO cmbc.incidents (
        incident_name,
        description,
        incident_type,
        severity,
        status,
        start_time,
        created_by
    ) VALUES (
        p_incident_name,
        p_description,
        p_incident_type,
        p_severity,
        'Reported',
        p_start_time,
        p_created_by
    ) RETURNING incident_id INTO p_incident_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_create_incident IS 'Creates a new incident record and returns the generated ID';


CREATE OR REPLACE PROCEDURE cmbc.sp_add_incident_impact(
    p_incident_id INTEGER,
    p_process_id INTEGER,
    p_downtime_hours NUMERIC(6,2),
    p_financial_impact NUMERIC(15,2),
    p_customer_impact_level VARCHAR(20),
    p_reputation_impact_level VARCHAR(20),
    p_notes TEXT
)
LANGUAGE plpgsql
AS $$
BEGIN
    INSERT INTO cmbc.incident_impacts (
        incident_id,
        process_id,
        downtime_hours,
        financial_impact,
        customer_impact_level,
        reputation_impact_level,
        notes
    ) VALUES (
        p_incident_id,
        p_process_id,
        p_downtime_hours,
        p_financial_impact,
        p_customer_impact_level,
        p_reputation_impact_level,
        p_notes
    );

    -- Update incident severity if needed based on impacts
    UPDATE cmbc.incidents
    SET severity = GREATEST(severity,
        CASE
            WHEN p_downtime_hours > 24 OR p_financial_impact > 100000 THEN 'Critical'
            WHEN p_downtime_hours > 8 OR p_financial_impact > 50000 THEN 'High'
            WHEN p_downtime_hours > 4 OR p_financial_impact > 10000 THEN 'Medium'
            ELSE 'Low'
        END)
    WHERE incident_id = p_incident_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_add_incident_impact IS 'Records the impact of an incident on a specific business process and potentially updates incident severity';


CREATE OR REPLACE PROCEDURE cmbc.sp_schedule_bcp_test(
    p_bcp_id INTEGER,
    p_test_name VARCHAR(255),
    p_test_type VARCHAR(100),
    p_test_date DATE,
    p_test_scenario TEXT,
    p_coordinator VARCHAR(100),
    OUT p_test_id INTEGER
)
LANGUAGE plpgsql
AS $$
BEGIN
    INSERT INTO cmbc.bcp_tests (
        bcp_id,
        test_name,
        test_type,
        test_date,
        test_scenario,
        status,
        coordinator
    ) VALUES (
        p_bcp_id,
        p_test_name,
        p_test_type,
        p_test_date,
        p_test_scenario,
        'Planned',
        p_coordinator
    ) RETURNING test_id INTO p_test_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_schedule_bcp_test IS 'Schedules a new BCP test and returns the generated test ID';

CREATE OR REPLACE PROCEDURE cmbc.sp_record_test_finding(
    p_test_id INTEGER,
    p_process_id INTEGER,
    p_finding_description TEXT,
    p_severity VARCHAR(20),
    p_corrective_action TEXT,
    p_assigned_to VARCHAR(100),
    p_due_date DATE
)
LANGUAGE plpgsql
AS $$
BEGIN
    INSERT INTO cmbc.test_findings (
        test_id,
        process_id,
        finding_description,
        severity,
        corrective_action,
        assigned_to,
        due_date,
        status
    ) VALUES (
        p_test_id,
        p_process_id,
        p_finding_description,
        p_severity,
        p_corrective_action,
        p_assigned_to,
        p_due_date,
        'Open'
    );

    -- Update test success rate if severity is high or critical
    IF p_severity IN ('High', 'Critical') THEN
        UPDATE cmbc.bcp_tests
        SET success_rate_pct = COALESCE(success_rate_pct, 100) -
            CASE
                WHEN p_severity = 'High' THEN 10
                ELSE 20
            END
        WHERE test_id = p_test_id;
    END IF;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_record_test_finding IS 'Records a finding from a BCP test and updates test success metrics';

CREATE OR REPLACE PROCEDURE cmbc.sp_update_kpi_measurement(
    p_kpi_id INTEGER,
    p_measurement_period DATE,
    p_actual_value NUMERIC(15,2),
    p_notes TEXT,
    p_created_by VARCHAR(100)
)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Delete existing measurement for this period if it exists
    DELETE FROM cmbc.kpi_measurements
    WHERE kpi_id = p_kpi_id AND measurement_period = p_measurement_period;

    -- Insert new measurement
    INSERT INTO cmbc.kpi_measurements (
        kpi_id,
        measurement_period,
        actual_value,
        target_value,
        notes,
        created_by
    )
    SELECT
        p_kpi_id,
        p_measurement_period,
        p_actual_value,
        k.target_value,
        p_notes,
        p_created_by
    FROM
        cmbc.kpis k
    WHERE
        k.kpi_id = p_kpi_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_update_kpi_measurement IS 'Updates or inserts a KPI measurement for a specific period';


CREATE OR REPLACE PROCEDURE cmbc.sp_perform_vendor_risk_assessment(
    p_vendor_id INTEGER,
    p_assessment_date DATE,
    p_assessor VARCHAR(100),
    p_financial_stability_score INTEGER,
    p_operational_resilience_score INTEGER,
    p_bcp_score INTEGER,
    p_recovery_capability_score INTEGER,
    p_findings TEXT,
    p_recommendations TEXT,
    p_next_review_months INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_next_review_date DATE;
BEGIN
    -- Calculate next review date
    v_next_review_date := p_assessment_date + (p_next_review_months * INTERVAL '1 month');

    -- Insert assessment
    INSERT INTO cmbc.vendor_risk_assessments (
        vendor_id,
        assessment_date,
        assessor,
        financial_stability_score,
        operational_resilience_score,
        bcp_score,
        recovery_capability_score,
        findings,
        recommendations,
        next_review_date
    ) VALUES (
        p_vendor_id,
        p_assessment_date,
        p_assessor,
        p_financial_stability_score,
        p_operational_resilience_score,
        p_bcp_score,
        p_recovery_capability_score,
        p_findings,
        p_recommendations,
        v_next_review_date
    );

    -- Update vendor BCP rating if provided
    IF p_bcp_score IS NOT NULL THEN
        UPDATE cmbc.vendors
        SET
            bcp_rating = CASE
                            WHEN p_bcp_score >= 4.5 THEN 'Excellent'
                            WHEN p_bcp_score >= 3.5 THEN 'Good'
                            WHEN p_bcp_score >= 2.5 THEN 'Adequate'
                            ELSE 'Poor'
                          END,
            bcp_review_date = p_assessment_date,
            updated_at = CURRENT_TIMESTAMP
        WHERE vendor_id = p_vendor_id;
    END IF;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_perform_vendor_risk_assessment IS 'Performs a comprehensive risk assessment of a vendor and updates vendor records';


INSERT INTO cmbc.kpis (kpi_name, description, category, measurement_unit, target_value, is_krm, frequency, owner)
VALUES
('Business Impact Analysis (BIA) Coverage', 'Percentage of critical business processes covered by BIAs', 'BCP', '%', 100, FALSE, 'Annual', 'BCM Manager'),
('Business Continuity Plan (BCP) Testing Frequency', 'Frequency of BCP testing', 'BCP', 'times/year', 1, FALSE, 'Annual', 'BCM Manager'),
('Crisis Communication Effectiveness', 'Effectiveness of crisis communication efforts', 'Communication', '%', 90, FALSE, 'Quarterly', 'Communications Team'),
('Recovery Time Objective (RTO) Achievement', 'Percentage of RTOs met during BCP tests', 'Recovery', '%', 95, TRUE, 'Quarterly', 'BCM Manager'),
('System Availability for Crisis Management', 'Uptime of the Crisis Management module', 'Technical', '%', 99.9, TRUE, 'Monthly', 'IT Team'),
('Incident Response Time', 'Time taken to initiate response after incident detection', 'Recovery', 'minutes', 30, TRUE, 'Monthly', 'Incident Manager'),
('Third-Party Vendor BCM Compliance Rate', 'Percentage of critical vendors with acceptable BCM plans', 'Vendor', '%', 100, TRUE, 'Quarterly', 'Vendor Manager'),
('Employee Training Completion Rate', 'Percentage of employees completing mandatory BCM training', 'Training', '%', 95, FALSE, 'Annual', 'HR Team'),
('Data Breach Containment Time', 'Time to contain a data breach incident', 'Risk', 'hours', 4, TRUE, 'Monthly', 'Security Team'),
('Backup Success Rate', 'Percentage of successful data backup operations', 'Technical', '%', 99.5, TRUE, 'Weekly', 'IT Team');


-- Indexes for business_continuity_plans
CREATE INDEX idx_bcp_status ON cmbc.business_continuity_plans(status);
CREATE INDEX idx_bcp_review_date ON cmbc.business_continuity_plans(next_review_date);

-- Indexes for critical_business_processes
CREATE INDEX idx_process_department ON cmbc.critical_business_processes(department);
CREATE INDEX idx_process_priority ON cmbc.critical_business_processes(recovery_priority);

-- Indexes for incidents
CREATE INDEX idx_incident_status ON cmbc.incidents(status);
CREATE INDEX idx_incident_severity ON cmbc.incidents(severity);
CREATE INDEX idx_incident_date ON cmbc.incidents(start_time);

-- Indexes for recovery_actions
CREATE INDEX idx_action_status ON cmbc.recovery_actions(status);
CREATE INDEX idx_action_incident ON cmbc.recovery_actions(incident_id);

-- Indexes for kpi_measurements
CREATE INDEX idx_kpi_measurement_period ON cmbc.kpi_measurements(measurement_period);
CREATE INDEX idx_kpi_id ON cmbc.kpi_measurements(kpi_id);

-- Indexes for vendors
CREATE INDEX idx_vendor_criticality ON cmbc.vendors(criticality);
CREATE INDEX idx_vendor_bcp_rating ON cmbc.vendors(bcp_rating);


CREATE OR REPLACE PROCEDURE cmbc.sp_refresh_materialized_views()
LANGUAGE plpgsql
AS $$
BEGIN
    REFRESH MATERIALIZED VIEW cmbc.mv_bcp_testing_performance;
    REFRESH MATERIALIZED VIEW cmbc.mv_incident_trends;
    REFRESH MATERIALIZED VIEW cmbc.mv_recovery_compliance;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_refresh_materialized_views IS 'Refreshes all materialized views in the CMBC schema';


--creating a new Business Continutity planning
INSERT INTO cmbc.business_continuity_plans (
    plan_name,
    description,
    version,
    effective_date,
    review_frequency_months,
    status,
    created_by
) VALUES (
    'Corporate Headquarters BCP',
    'Business Continuity Plan for Corporate Headquarters operations',
    '1.0',
    CURRENT_DATE,
    12,
    'Active',
    'jane.doe@company.com'
) RETURNING bcp_id;

--adding a critical business process
INSERT INTO cmbc.critical_business_processes (
    process_name,
    description,
    department,
    owner,
    max_tolerable_downtime_hours,
    recovery_priority,
    data_criticality
) VALUES (
    'Payroll Processing',
    'Monthly payroll processing for all employees',
    'Finance',
    'john.smith@company.com',
    24,
    1,
    'Critical'
) RETURNING process_id;


--recording a new incident
CALL cmbc.sp_create_incident(
    p_incident_name := 'Data Center Power Outage',
    p_description := 'Complete power outage at primary data center due to utility failure',
    p_incident_type := 'System Failure',
    p_severity := 'High',
    p_start_time := '2023-11-15 14:30:00+00',
    p_created_by := 'sara.johnson@company.com',
    p_incident_id := NULL
);


--adding impact to an incident
CALL cmbc.sp_add_incident_impact(
    p_incident_id := 1,
    p_process_id := 1,
    p_downtime_hours := 6.5,
    p_financial_impact := 125000.00,
    p_customer_impact_level := 'Medium',
    p_reputation_impact_level := 'Medium',
    p_notes := 'Payroll delayed by one day causing employee dissatisfaction'
);


--scheduling a BCP test
CALL cmbc.sp_schedule_bcp_test(
    p_bcp_id := 1,
    p_test_name := 'Q1 2024 Tabletop Exercise',
    p_test_type := 'Tabletop',
    p_test_date := '2024-03-15',
    p_test_scenario := 'Simulated ransomware attack on primary systems',
    p_coordinator := 'michael.brown@company.com',
    p_test_id := NULL
);

--recording a test finding
CALL cmbc.sp_record_test_finding(
    p_test_id := 1,
    p_process_id := 1,
    p_finding_description := 'Payroll team lacked access to backup systems during test',
    p_severity := 'High',
    p_corrective_action := 'Implement secondary access controls and train team',
    p_assigned_to := 'it.support@company.com',
    p_due_date := '2025-07-09'
);


-- updating a KPI measurement
CALL cmbc.sp_update_kpi_measurement(
    p_kpi_id := 1,
    p_measurement_period := '2025-07-09',
    p_actual_value := 98.5,
    p_notes := 'Two non-critical processes not yet assessed',
    p_created_by := 'bcm.team@company.com'
);

--performing a vendor risk assessment
CALL cmbc.sp_perform_vendor_risk_assessment(
    p_vendor_id := 1,
    p_assessment_date := '2025-07-09',
    p_assessor := 'vendor.team@company.com',
    p_financial_stability_score := 4,
    p_operational_resilience_score := 3,
    p_bcp_score := 4,
    p_recovery_capability_score := 3,
    p_findings := 'Strong financials but recovery procedures need documentation improvement',
    p_recommendations := 'Request updated BCP documentation by Q1 2024',
    p_next_review_months := 6
);

--alternative work locations
CREATE TABLE cmbc.alternative_work_locations (
    location_id SERIAL PRIMARY KEY,
    location_name VARCHAR(255) NOT NULL,
    address TEXT NOT NULL,
    capacity INTEGER NOT NULL,
    available_from TIMESTAMP WITH TIME ZONE NOT NULL,
    available_to TIMESTAMP WITH TIME ZONE NOT NULL,
    facilities_description TEXT,
    contact_person VARCHAR(100) NOT NULL,
    contact_phone VARCHAR(20) NOT NULL,
    department_coverage VARCHAR(100)[] NOT NULL, -- Array of departments that can use this location
    is_active BOOLEAN NOT NULL DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.alternative_work_locations IS 'Tracks alternative work locations available during disruptions to maintain operations';
COMMENT ON COLUMN cmbc.alternative_work_locations.department_coverage IS 'Array of department names that can utilize this location during a disruption';

--backup systems inventory
CREATE TABLE cmbc.backup_systems (
    system_id SERIAL PRIMARY KEY,
    system_name VARCHAR(255) NOT NULL,
    description TEXT NOT NULL,
    system_type VARCHAR(100) NOT NULL CHECK (system_type IN ('Data Backup', 'Application', 'Infrastructure', 'Network')),
    primary_system_id INTEGER, -- Reference to the primary system this backs up
    location VARCHAR(100) NOT NULL,
    recovery_time_minutes INTEGER NOT NULL,
    recovery_point_minutes INTEGER NOT NULL,
    last_test_date DATE,
    test_status VARCHAR(50) CHECK (test_status IN ('Success', 'Partial Success', 'Failed')),
    owner VARCHAR(100) NOT NULL,
    maintenance_schedule TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (primary_system_id) REFERENCES cmbc.backup_systems(system_id) ON DELETE SET NULL
);

COMMENT ON TABLE cmbc.backup_systems IS 'Inventory of all backup systems supporting business continuity with recovery capabilities';
COMMENT ON COLUMN cmbc.backup_systems.recovery_time_minutes IS 'Estimated time to recover this system in minutes';


--crisis managment team
CREATE TABLE cmbc.crisis_management_team (
    member_id SERIAL PRIMARY KEY,
    employee_id VARCHAR(50) NOT NULL,
    name VARCHAR(100) NOT NULL,
    role VARCHAR(100) NOT NULL,
    department VARCHAR(100) NOT NULL,
    primary_contact VARCHAR(20) NOT NULL,
    secondary_contact VARCHAR(20),
    email VARCHAR(100) NOT NULL,
    is_primary BOOLEAN NOT NULL DEFAULT TRUE,
    backup_member_id INTEGER REFERENCES cmbc.crisis_management_team(member_id),
    skills TEXT[],
    training_completion_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (employee_id)
);

COMMENT ON TABLE cmbc.crisis_management_team IS 'Defines the crisis management team members with contact info and roles';
COMMENT ON COLUMN cmbc.crisis_management_team.skills IS 'Array of relevant skills for crisis management (e.g., Incident Command, First Aid)';


--supply chaiun dependencies
CREATE TABLE cmbc.supply_chain_dependencies (
    dependency_id SERIAL PRIMARY KEY,
    item_name VARCHAR(255) NOT NULL,
    description TEXT,
    category VARCHAR(100) NOT NULL CHECK (category IN ('Raw Material', 'Component', 'Service', 'Software', 'Other')),
    criticality VARCHAR(20) NOT NULL CHECK (criticality IN ('Low', 'Medium', 'High', 'Critical')),
    primary_vendor_id INTEGER REFERENCES cmbc.vendors(vendor_id),
    secondary_vendor_id INTEGER REFERENCES cmbc.vendors(vendor_id),
    lead_time_days INTEGER NOT NULL,
    inventory_level VARCHAR(50),
    max_tolerable_disruption_days INTEGER NOT NULL,
    contingency_plan TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.supply_chain_dependencies IS 'Identifies critical supply chain dependencies and alternative sources';
COMMENT ON COLUMN cmbc.supply_chain_dependencies.max_tolerable_disruption_days IS 'Maximum days the organization can tolerate disruption of this supply item';

--regulatory compliance requirements
CREATE TABLE cmbc.regulatory_requirements (
    requirement_id SERIAL PRIMARY KEY,
    regulation_name VARCHAR(255) NOT NULL,
    jurisdiction VARCHAR(100) NOT NULL,
    description TEXT NOT NULL,
    applicable_departments VARCHAR(100)[] NOT NULL,
    compliance_deadline DATE,
    compliance_status VARCHAR(50) NOT NULL CHECK (compliance_status IN ('Not Started', 'In Progress', 'Compliant', 'Non-Compliant')),
    responsible_party VARCHAR(100) NOT NULL,
    documentation_reference TEXT,
    audit_frequency_months INTEGER,
    last_audit_date DATE,
    next_audit_date DATE GENERATED ALWAYS AS (
        CASE
            WHEN last_audit_date IS NOT NULL THEN last_audit_date + (audit_frequency_months * INTERVAL '1 month')
            ELSE compliance_deadline
        END
    ) STORED,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.regulatory_requirements IS 'Tracks all regulatory requirements related to business continuity and disaster recovery';
COMMENT ON COLUMN cmbc.regulatory_requirements.applicable_departments IS 'Array of departments affected by this regulation';


--crisis team readiness view
CREATE OR REPLACE VIEW cmbc.vw_crisis_team_readiness AS
SELECT
    cmt.member_id,
    cmt.name,
    cmt.role,
    cmt.department,
    cmt.is_primary,
    bm.name AS backup_member_name,
    cmt.training_completion_date,
    CASE
        WHEN cmt.training_completion_date IS NULL THEN 'No Training'
        WHEN cmt.training_completion_date > CURRENT_DATE - INTERVAL '1 year' THEN 'Current'
        ELSE 'Outdated'
    END AS training_status,
    COUNT(DISTINCT tr.record_id) AS training_count,
    COUNT(DISTINCT i.incident_id) AS incident_participation_count
FROM
    cmbc.crisis_management_team cmt
LEFT JOIN
    cmbc.crisis_management_team bm ON cmt.backup_member_id = bm.member_id
LEFT JOIN
    cmbc.training_records tr ON cmt.employee_id = tr.employee_id
LEFT JOIN
    cmbc.recovery_actions ra ON cmt.member_id = ra.assigned_to::INTEGER
LEFT JOIN
    cmbc.incidents i ON ra.incident_id = i.incident_id
GROUP BY
    cmt.member_id, cmt.name, cmt.role, cmt.department, cmt.is_primary,
    bm.name, cmt.training_completion_date;

COMMENT ON VIEW cmbc.vw_crisis_team_readiness IS 'Provides a readiness assessment of crisis management team members including training status and experience';


--supply chain risk exposure view
CREATE OR REPLACE VIEW cmbc.vw_supply_chain_risk AS
SELECT
    scd.dependency_id,
    scd.item_name,
    scd.category,
    scd.criticality,
    v.vendor_name AS primary_vendor,
    v2.vendor_name AS secondary_vendor,
    scd.lead_time_days,
    scd.max_tolerable_disruption_days,
    v.overall_risk_score AS primary_vendor_risk_score,
    v2.overall_risk_score AS secondary_vendor_risk_score,
    CASE
        WHEN v.overall_risk_score IS NULL OR v.overall_risk_score >= 3.5 THEN 'High Risk'
        WHEN scd.secondary_vendor_id IS NULL THEN 'Single Source Risk'
        WHEN scd.max_tolerable_disruption_days > scd.lead_time_days THEN 'Adequate Buffer'
        ELSE 'Potential Disruption Risk'
    END AS risk_assessment
FROM
    cmbc.supply_chain_dependencies scd
LEFT JOIN
    cmbc.vendors v ON scd.primary_vendor_id = v.vendor_id
LEFT JOIN
    cmbc.vendors v2 ON scd.secondary_vendor_id = v2.vendor_id
LEFT JOIN
    (SELECT vendor_id, MAX(assessment_date) AS latest_date
     FROM cmbc.vendor_risk_assessments GROUP BY vendor_id) lv ON v.vendor_id = lv.vendor_id
LEFT JOIN
    cmbc.vendor_risk_assessments vra ON lv.vendor_id = vra.vendor_id AND lv.latest_date = vra.assessment_date
LEFT JOIN
    (SELECT vendor_id, MAX(assessment_date) AS latest_date2
     FROM cmbc.vendor_risk_assessments GROUP BY vendor_id) lv2 ON v2.vendor_id = lv2.vendor_id
LEFT JOIN
    cmbc.vendor_risk_assessments vra2 ON lv2.vendor_id = vra2.vendor_id AND lv2.latest_date2 = vra2.assessment_date;

COMMENT ON VIEW cmbc.vw_supply_chain_risk IS 'Assesses supply chain risks by evaluating vendor reliability and disruption tolerance';

--regulatory compliance dashboard view
CREATE OR REPLACE VIEW cmbc.vw_regulatory_compliance AS
SELECT
    rr.requirement_id,
    rr.regulation_name,
    rr.jurisdiction,
    rr.compliance_status,
    rr.responsible_party,
    rr.compliance_deadline,
    rr.next_audit_date,
    COUNT(DISTINCT d.department) AS departments_affected,
    STRING_AGG(DISTINCT d.department, ', ') AS department_list,
    CASE
        WHEN rr.compliance_status = 'Non-Compliant' AND rr.compliance_deadline < CURRENT_DATE THEN 'Critical'
        WHEN rr.compliance_status != 'Compliant' AND rr.compliance_deadline < CURRENT_DATE + INTERVAL '1 month' THEN 'High'
        WHEN rr.compliance_status = 'Compliant' THEN 'Compliant'
        ELSE 'Medium'
    END AS priority_status
FROM
    cmbc.regulatory_requirements rr,
    UNNEST(rr.applicable_departments) AS d(department)
GROUP BY
    rr.requirement_id, rr.regulation_name, rr.jurisdiction, rr.compliance_status,
    rr.responsible_party, rr.compliance_deadline, rr.next_audit_date;

COMMENT ON VIEW cmbc.vw_regulatory_compliance IS 'Provides a compliance dashboard showing status of all regulatory requirements with priority assessment';

--backup system readiness
CREATE MATERIALIZED VIEW cmbc.mv_backup_system_readiness AS
SELECT
    bs.system_id,
    bs.system_name,
    bs.system_type,
    ps.system_name AS primary_system_name,
    bs.recovery_time_minutes,
    bs.recovery_point_minutes,
    bs.last_test_date,
    bs.test_status,
    CASE
        WHEN bs.last_test_date IS NULL THEN 'Not Tested'
        WHEN bs.last_test_date < CURRENT_DATE - INTERVAL '6 months' THEN 'Testing Stale'
        WHEN bs.test_status != 'Success' THEN 'Needs Attention'
        ELSE 'Operational'
    END AS readiness_status,
    EXTRACT(DAY FROM CURRENT_DATE - bs.last_test_date) AS days_since_last_test
FROM
    cmbc.backup_systems bs
LEFT JOIN
    cmbc.backup_systems ps ON bs.primary_system_id = ps.system_id
WHERE
    bs.is_active = TRUE;

COMMENT ON MATERIALIZED VIEW cmbc.mv_backup_system_readiness IS 'Assesses the readiness of backup systems based on testing history and recovery capabilities';

-- work location coverage
CREATE MATERIALIZED VIEW cmbc.mv_work_location_coverage AS
SELECT
    d.department,
    COUNT(DISTINCT CASE WHEN awl.is_active THEN awl.location_id END) AS available_locations,
    MIN(awl.capacity) AS min_location_capacity,
    MAX(awl.capacity) AS max_location_capacity,
    STRING_AGG(DISTINCT awl.location_name, ', ') AS location_names,
    CASE
        WHEN COUNT(DISTINCT CASE WHEN awl.is_active THEN awl.location_id END) = 0 THEN 'No Coverage'
        WHEN COUNT(DISTINCT CASE WHEN awl.is_active THEN awl.location_id END) > 1 THEN 'Redundant Coverage'
        ELSE 'Single Location'
    END AS coverage_status
FROM
    (SELECT DISTINCT department FROM cmbc.critical_business_processes) d
LEFT JOIN
    cmbc.alternative_work_locations awl ON d.department = ANY(awl.department_coverage)
GROUP BY
    d.department;

COMMENT ON MATERIALIZED VIEW cmbc.mv_work_location_coverage IS 'Analyzes alternative work location coverage by department for business continuity planning';


--advanced crisis management team
CREATE OR REPLACE PROCEDURE cmbc.sp_activate_crisis_team(
    p_incident_id INTEGER,
    p_activation_reason TEXT,
    OUT p_team_activation_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_incident_severity VARCHAR(20);
BEGIN
    -- Get incident severity
    SELECT severity INTO v_incident_severity
    FROM cmbc.incidents
    WHERE incident_id = p_incident_id;

    -- Create team activation record
    INSERT INTO cmbc.crisis_team_activations (
        incident_id,
        activation_time,
        activation_reason,
        status,
        severity_level
    ) VALUES (
        p_incident_id,
        CURRENT_TIMESTAMP,
        p_activation_reason,
        'Active',
        v_incident_severity
    ) RETURNING activation_id INTO p_team_activation_id;

    -- Notify all primary team members
    INSERT INTO cmbc.notifications (
        recipient_id,
        message,
        notification_type,
        related_incident_id,
        status
    )
    SELECT
        member_id,
        'Crisis Team Activation: ' || p_activation_reason,
        'Crisis Activation',
        p_incident_id,
        'Pending'
    FROM
        cmbc.crisis_management_team
    WHERE
        is_primary = TRUE;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_activate_crisis_team IS 'Activates the crisis management team for a specific incident, creates activation record, and notifies team members';

--plan backup system test
CREATE OR REPLACE PROCEDURE cmbc.sp_plan_backup_test(
    p_system_id INTEGER,
    p_test_type VARCHAR(50),
    p_scheduled_date TIMESTAMP WITH TIME ZONE,
    p_test_scenario TEXT,
    p_coordinator VARCHAR(100),
    OUT p_test_id INTEGER
)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Insert backup system test record
    INSERT INTO cmbc.backup_system_tests (
        system_id,
        test_type,
        scheduled_date,
        test_scenario,
        coordinator,
        status
    ) VALUES (
        p_system_id,
        p_test_type,
        p_scheduled_date,
        p_test_scenario,
        p_coordinator,
        'Scheduled'
    ) RETURNING test_id INTO p_test_id;

    -- Update backup system last_test_date if this is an ad-hoc test
    IF p_test_type = 'Ad-hoc' THEN
        UPDATE cmbc.backup_systems
        SET
            last_test_date = p_scheduled_date::DATE,
            updated_at = CURRENT_TIMESTAMP
        WHERE
            system_id = p_system_id;
    END IF;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_plan_backup_test IS 'Schedules a test for a backup system and updates system records accordingly';

--update regulatory compliance status
CREATE OR REPLACE PROCEDURE cmbc.sp_update_regulatory_compliance(
    p_requirement_id INTEGER,
    p_new_status VARCHAR(50),
    p_completion_date DATE DEFAULT NULL,
    p_evidence TEXT DEFAULT NULL,
    p_updated_by VARCHAR(100)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Update compliance status
    UPDATE cmbc.regulatory_requirements
    SET
        compliance_status = p_new_status,
        last_audit_date = CASE WHEN p_new_status = 'Compliant' THEN COALESCE(p_completion_date, CURRENT_DATE) ELSE last_audit_date END,
        documentation_reference = COALESCE(p_evidence, documentation_reference),
        updated_at = CURRENT_TIMESTAMP
    WHERE
        requirement_id = p_requirement_id;

    -- Log the compliance update
    INSERT INTO cmbc.regulatory_compliance_log (
        requirement_id,
        old_status,
        new_status,
        changed_by,
        change_reason,
        evidence
    )
    SELECT
        p_requirement_id,
        compliance_status,
        p_new_status,
        p_updated_by,
        'Status Update',
        p_evidence
    FROM
        cmbc.regulatory_requirements
    WHERE
        requirement_id = p_requirement_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_update_regulatory_compliance IS 'Updates the compliance status of a regulatory requirement and maintains an audit log of changes';


-- assess suply chain risk
CREATE OR REPLACE PROCEDURE cmbc.sp_assess_supply_chain_risk(
    p_dependency_id INTEGER,
    p_assessment_date DATE,
    p_assessor VARCHAR(100),
    p_risk_level VARCHAR(20),
    p_risk_factors TEXT,
    p_mitigation_plan TEXT,
    OUT p_assessment_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_criticality VARCHAR(20);
BEGIN
    -- Get current criticality
    SELECT criticality INTO v_criticality
    FROM cmbc.supply_chain_dependencies
    WHERE dependency_id = p_dependency_id;

    -- Insert new assessment
    INSERT INTO cmbc.supply_chain_risk_assessments (
        dependency_id,
        assessment_date,
        assessor,
        current_criticality,
        risk_level,
        risk_factors,
        mitigation_plan
    ) VALUES (
        p_dependency_id,
        p_assessment_date,
        p_assessor,
        v_criticality,
        p_risk_level,
        p_risk_factors,
        p_mitigation_plan
    ) RETURNING assessment_id INTO p_assessment_id;

    -- Update dependency criticality if risk level is higher
    IF (p_risk_level = 'High' AND v_criticality IN ('Low', 'Medium')) OR
       (p_risk_level = 'Critical' AND v_criticality != 'Critical') THEN
        UPDATE cmbc.supply_chain_dependencies
        SET
            criticality = CASE
                            WHEN p_risk_level = 'High' THEN 'High'
                            ELSE 'Critical'
                          END,
            updated_at = CURRENT_TIMESTAMP
        WHERE
            dependency_id = p_dependency_id;
    END IF;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_assess_supply_chain_risk IS 'Performs a risk assessment on a supply chain dependency and updates criticality if needed';

-- generate BCP test report
CREATE OR REPLACE PROCEDURE cmbc.sp_generate_bcp_test_report(
    p_test_id INTEGER,
    p_report_format VARCHAR(20) DEFAULT 'PDF',
    OUT p_report_path TEXT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_bcp_id INTEGER;
    v_test_name VARCHAR(255);
    v_test_date DATE;
    v_success_rate NUMERIC(5,2);
    v_finding_count INTEGER;
    v_critical_findings INTEGER;
BEGIN
    -- Get test information
    SELECT
        bcp_id, test_name, test_date, success_rate_pct
    INTO
        v_bcp_id, v_test_name, v_test_date, v_success_rate
    FROM
        cmbc.bcp_tests
    WHERE
        test_id = p_test_id;

    -- Get finding statistics
    SELECT
        COUNT(*),
        SUM(CASE WHEN severity = 'Critical' THEN 1 ELSE 0 END)
    INTO
        v_finding_count, v_critical_findings
    FROM
        cmbc.test_findings
    WHERE
        test_id = p_test_id;

    -- Generate report path (in a real implementation, this would generate an actual file)
    p_report_path := '/reports/bcp_tests/' || v_bcp_id || '/' || p_test_id || '_' || REPLACE(LOWER(v_test_name), ' ', '_') || '.' || LOWER(p_report_format);

    -- Insert report record
    INSERT INTO cmbc.bcp_test_reports (
        test_id,
        report_path,
        generated_at,
        success_rate,
        total_findings,
        critical_findings
    ) VALUES (
        p_test_id,
        p_report_path,
        CURRENT_TIMESTAMP,
        v_success_rate,
        v_finding_count,
        v_critical_findings
    );

    -- Update test record
    UPDATE cmbc.bcp_tests
    SET
        last_report_date = CURRENT_DATE,
        updated_at = CURRENT_TIMESTAMP
    WHERE
        test_id = p_test_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_generate_bcp_test_report IS 'Generates a test report for a BCP exercise and updates related records';

-- notification systems tables
CREATE TABLE cmbc.notifications (
    notification_id SERIAL PRIMARY KEY,
    recipient_id INTEGER NOT NULL, -- Could be user_id, team_id, etc.
    message TEXT NOT NULL,
    notification_type VARCHAR(50) NOT NULL CHECK (notification_type IN ('Alert', 'Reminder', 'Approval', 'Crisis Activation')),
    related_incident_id INTEGER REFERENCES cmbc.incidents(incident_id),
    related_test_id INTEGER REFERENCES cmbc.bcp_tests(test_id),
    status VARCHAR(20) NOT NULL CHECK (status IN ('Pending', 'Sent', 'Delivered', 'Read', 'Failed')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    sent_at TIMESTAMP WITH TIME ZONE,
    read_at TIMESTAMP WITH TIME ZONE
);

COMMENT ON TABLE cmbc.notifications IS 'Tracks all system notifications sent to users for incidents, tests, and other events';

CREATE TABLE cmbc.notification_templates (
    template_id SERIAL PRIMARY KEY,
    template_name VARCHAR(100) NOT NULL,
    notification_type VARCHAR(50) NOT NULL,
    subject TEXT NOT NULL,
    body_template TEXT NOT NULL,
    is_active BOOLEAN NOT NULL DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.notification_templates IS 'Stores templates for various notification types to ensure consistent messaging';

--audti logging tables
CREATE TABLE cmbc.audit_logs (
    log_id SERIAL PRIMARY KEY,
    table_name VARCHAR(100) NOT NULL,
    record_id INTEGER NOT NULL,
    action VARCHAR(20) NOT NULL CHECK (action IN ('INSERT', 'UPDATE', 'DELETE')),
    action_by VARCHAR(100) NOT NULL,
    action_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    old_values JSONB,
    new_values JSONB
);

COMMENT ON TABLE cmbc.audit_logs IS 'Comprehensive audit log for all critical data changes in the CMBC system';

CREATE TABLE cmbc.regulatory_compliance_log (
    log_id SERIAL PRIMARY KEY,
    requirement_id INTEGER NOT NULL REFERENCES cmbc.regulatory_requirements(requirement_id),
    old_status VARCHAR(50),
    new_status VARCHAR(50) NOT NULL,
    changed_by VARCHAR(100) NOT NULL,
    changed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    change_reason TEXT,
    evidence TEXT
);

COMMENT ON TABLE cmbc.regulatory_compliance_log IS 'Specialized audit log for tracking changes to regulatory compliance status';

--geographic risk mapping tables
CREATE TABLE cmbc.geographic_risks (
    risk_id SERIAL PRIMARY KEY,
    location_name VARCHAR(255) NOT NULL,
    geo_coordinates POINT NOT NULL,
    risk_type VARCHAR(100) NOT NULL CHECK (risk_type IN ('Earthquake', 'Flood', 'Hurricane', 'Political', 'Other')),
    risk_level VARCHAR(20) NOT NULL CHECK (risk_level IN ('Low', 'Medium', 'High', 'Critical')),
    last_occurrence DATE,
    probability VARCHAR(20),
    affected_facilities TEXT[],
    mitigation_measures TEXT,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.geographic_risks IS 'Tracks geographic-specific risks that could impact business operations';

CREATE TABLE cmbc.facility_risk_mapping (
    mapping_id SERIAL PRIMARY KEY,
    facility_id INTEGER NOT NULL, -- Would reference a facilities table
    facility_name VARCHAR(255) NOT NULL,
    geographic_risk_id INTEGER REFERENCES cmbc.geographic_risks(risk_id),
    risk_exposure VARCHAR(20) NOT NULL CHECK (risk_exposure IN ('None', 'Low', 'Medium', 'High', 'Critical')),
    contingency_plan TEXT,
    last_review_date DATE,
    next_review_date DATE GENERATED ALWAYS AS (last_review_date + INTERVAL '1 year') STORED
);

COMMENT ON TABLE cmbc.facility_risk_mapping IS 'Maps organizational facilities to geographic risks and tracks mitigation plans';

-- AI powered risk prediction tables
CREATE TABLE cmbc.risk_prediction_models (
    model_id SERIAL PRIMARY KEY,
    model_name VARCHAR(255) NOT NULL,
    model_version VARCHAR(50) NOT NULL,
    model_type VARCHAR(100) NOT NULL CHECK (model_type IN ('Time Series', 'Classification', 'Anomaly Detection', 'Clustering')),
    target_risk_category VARCHAR(100) NOT NULL,
    training_data_range DATERANGE NOT NULL,
    accuracy_metric NUMERIC(5,2),
    last_trained_at TIMESTAMP WITH TIME ZONE,
    is_active BOOLEAN NOT NULL DEFAULT TRUE,
    model_owner VARCHAR(100) NOT NULL,
    model_metadata JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.risk_prediction_models IS 'Stores metadata about AI/ML models used for predicting business continuity risks and their performance characteristics';
COMMENT ON COLUMN cmbc.risk_prediction_models.model_metadata IS 'JSON containing model parameters, feature importance, and other technical details';

CREATE TABLE cmbc.risk_predictions (
    prediction_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    model_id INTEGER NOT NULL REFERENCES cmbc.risk_prediction_models(model_id),
    prediction_timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
    prediction_window DATERANGE NOT NULL,
    predicted_risk_type VARCHAR(100) NOT NULL,
    predicted_risk_level VARCHAR(20) NOT NULL CHECK (predicted_risk_level IN ('Low', 'Medium', 'High', 'Critical')),
    confidence_score NUMERIC(5,2) NOT NULL,
    key_influencing_factors JSONB,
    mitigation_recommendations TEXT[],
    reviewed_by VARCHAR(100),
    review_status VARCHAR(20) CHECK (review_status IN ('Pending', 'Accepted', 'Rejected', 'Actioned')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.risk_predictions IS 'Stores risk predictions generated by AI models including confidence scores and recommendations';
COMMENT ON COLUMN cmbc.risk_predictions.key_influencing_factors IS 'JSON structure showing the top factors contributing to the risk prediction';


-- automated workflow engine tables
CREATE TABLE cmbc.workflow_templates (
    template_id SERIAL PRIMARY KEY,
    template_name VARCHAR(255) NOT NULL,
    description TEXT NOT NULL,
    trigger_event VARCHAR(100) NOT NULL CHECK (trigger_event IN ('Incident Created', 'Risk Threshold Exceeded', 'Test Failure', 'Regulatory Change', 'Manual')),
    applicable_risk_types VARCHAR(100)[],
    priority INTEGER NOT NULL DEFAULT 3,
    is_active BOOLEAN NOT NULL DEFAULT TRUE,
    version VARCHAR(50) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.workflow_templates IS 'Defines templates for automated workflows that execute in response to business continuity events';

CREATE TABLE cmbc.workflow_steps (
    step_id SERIAL PRIMARY KEY,
    template_id INTEGER NOT NULL REFERENCES cmbc.workflow_templates(template_id),
    step_order INTEGER NOT NULL,
    step_type VARCHAR(50) NOT NULL CHECK (step_type IN ('Notification', 'Approval', 'Data Collection', 'System Action', 'Manual Task')),
    step_name VARCHAR(255) NOT NULL,
    instructions TEXT,
    assigned_role VARCHAR(100),
    timeout_minutes INTEGER,
    is_parallel BOOLEAN NOT NULL DEFAULT FALSE,
    completion_criteria TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.workflow_steps IS 'Defines the individual steps that comprise each automated workflow template';

CREATE TABLE cmbc.active_workflows (
    workflow_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    template_id INTEGER NOT NULL REFERENCES cmbc.workflow_templates(template_id),
    initiating_event_id INTEGER, -- Could reference incidents, tests, etc.
    initiating_event_type VARCHAR(50) NOT NULL,
    current_status VARCHAR(50) NOT NULL CHECK (current_status IN ('Pending', 'In Progress', 'Completed', 'Failed', 'Terminated')),
    started_at TIMESTAMP WITH TIME ZONE NOT NULL,
    completed_at TIMESTAMP WITH TIME ZONE,
    initiated_by VARCHAR(100),
    timeout_at TIMESTAMP WITH TIME ZONE GENERATED ALWAYS AS (
        started_at + (SELECT MAX(timeout_minutes) FROM cmbc.workflow_steps WHERE template_id = template_id) * INTERVAL '1 minute'
    ) STORED,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.active_workflows IS 'Tracks currently executing workflow instances with their status and timing';

CREATE TABLE cmbc.workflow_execution_log (
    execution_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    workflow_id UUID NOT NULL REFERENCES cmbc.active_workflows(workflow_id),
    step_id INTEGER NOT NULL REFERENCES cmbc.workflow_steps(step_id),
    status VARCHAR(50) NOT NULL CHECK (status IN ('Pending', 'In Progress', 'Completed', 'Failed', 'Skipped')),
    started_at TIMESTAMP WITH TIME ZONE,
    completed_at TIMESTAMP WITH TIME ZONE,
    performed_by VARCHAR(100),
    output_data JSONB,
    error_message TEXT,
    retry_count INTEGER DEFAULT 0,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.workflow_execution_log IS 'Detailed log of each step execution within active workflows for auditing and troubleshooting';

--predictive risk exposure view
CREATE OR REPLACE VIEW cmbc.vw_predictive_risk_exposure AS
WITH latest_predictions AS (
    SELECT
        predicted_risk_type,
        predicted_risk_level,
        confidence_score,
        ROW_NUMBER() OVER (PARTITION BY predicted_risk_type ORDER BY prediction_timestamp DESC) as rn
    FROM
        cmbc.risk_predictions
    WHERE
        prediction_window @> CURRENT_DATE
        AND review_status IN ('Accepted', 'Actioned')
)
SELECT
    r.risk_type AS current_risk_type,
    COUNT(*) AS documented_risk_count,
    MAX(r.risk_score) AS max_current_risk_score,
    p.predicted_risk_level,
    p.confidence_score,
    CASE
        WHEN p.predicted_risk_level = 'Critical' AND MAX(r.risk_score) >= 16 THEN 'Immediate Action Required'
        WHEN p.predicted_risk_level IN ('High', 'Critical') THEN 'Enhanced Monitoring Needed'
        WHEN p.predicted_risk_level = 'Medium' AND MAX(r.risk_score) >= 9 THEN 'Review Recommended'
        ELSE 'Normal Operations'
    END AS recommended_action
FROM
    cmbc.risk_assessments r
LEFT JOIN
    latest_predictions p ON r.category = p.predicted_risk_type AND p.rn = 1
GROUP BY
    r.risk_type, p.predicted_risk_level, p.confidence_score;

COMMENT ON VIEW cmbc.vw_predictive_risk_exposure IS 'Combines current risk assessments with AI predictions to provide forward-looking risk exposure analysis';

--workflow performance analytics view
CREATE OR REPLACE VIEW cmbc.vw_workflow_performance AS
SELECT
    wt.template_id,
    wt.template_name,
    COUNT(DISTINCT aw.workflow_id) AS total_executions,
    AVG(EXTRACT(EPOCH FROM (aw.completed_at - aw.started_at))/60 AS avg_duration_minutes,
    SUM(CASE WHEN aw.current_status = 'Completed' THEN 1 ELSE 0 END) AS success_count,
    SUM(CASE WHEN aw.current_status = 'Failed' THEN 1 ELSE 0 END) AS failure_count,
    MAX(aw.started_at) AS last_executed,
    COUNT(DISTINCT CASE WHEN aw.timeout_at < CURRENT_TIMESTAMP AND aw.current_status = 'In Progress' THEN aw.workflow_id END) AS timed_out_count
FROM
    cmbc.workflow_templates wt
LEFT JOIN
    cmbc.active_workflows aw ON wt.template_id = aw.template_id
GROUP BY
    wt.template_id, wt.template_name;

COMMENT ON VIEW cmbc.vw_workflow_performance IS 'Provides analytics on workflow execution times, success rates, and timeouts for process improvement';

--materialized views for advanced analytics
-- risk prediction accuracy tracking
CREATE MATERIALIZED VIEW cmbc.mv_prediction_accuracy AS
WITH actual_incidents AS (
    SELECT
        incident_type,
        COUNT(*) AS incident_count,
        DATE_TRUNC('month', start_time) AS month
    FROM
        cmbc.incidents
    WHERE
        status = 'Closed'
    GROUP BY
        DATE_TRUNC('month', start_time), incident_type
),
predictions AS (
    SELECT
        predicted_risk_type,
        predicted_risk_level,
        COUNT(*) AS prediction_count,
        DATE_TRUNC('month', prediction_timestamp) AS month
    FROM
        cmbc.risk_predictions
    WHERE
        prediction_window @> (DATE_TRUNC('month', prediction_timestamp) + INTERVAL '1 month')
        AND review_status = 'Accepted'
    GROUP BY
        DATE_TRUNC('month', prediction_timestamp), predicted_risk_type, predicted_risk_level
)
SELECT
    COALESCE(a.month, p.month) AS month,
    COALESCE(a.incident_type, p.predicted_risk_type) AS risk_type,
    a.incident_count,
    p.prediction_count,
    p.predicted_risk_level,
    CASE
        WHEN a.incident_count IS NULL THEN 'False Positive'
        WHEN p.prediction_count IS NULL THEN 'Missed Prediction'
        WHEN p.predicted_risk_level = 'High' AND a.incident_count > 0 THEN 'Accurate Prediction'
        ELSE 'Needs Review'
    END AS prediction_accuracy
FROM
    actual_incidents a
FULL OUTER JOIN
    predictions p ON a.month = p.month AND a.incident_type = p.predicted_risk_type;

COMMENT ON MATERIALIZED VIEW cmbc.mv_prediction_accuracy IS 'Tracks the accuracy of risk predictions by comparing predicted risks to actual incidents';


--workflow step bottleneck analysis
CREATE MATERIALIZED VIEW cmbc.mv_workflow_bottlenecks AS
SELECT
    ws.template_id,
    wt.template_name,
    ws.step_id,
    ws.step_name,
    ws.step_type,
    COUNT(*) AS total_executions,
    AVG(EXTRACT(EPOCH FROM (wel.completed_at - wel.started_at))) AS avg_duration_seconds,
    MAX(EXTRACT(EPOCH FROM (wel.completed_at - wel.started_at))) AS max_duration_seconds,
    SUM(CASE WHEN wel.status = 'Failed' THEN 1 ELSE 0 END) AS failure_count,
    SUM(CASE WHEN wel.retry_count > 0 THEN 1 ELSE 0 END) AS retry_count,
    STRING_AGG(DISTINCT wel.error_message, '|') FILTER (WHERE wel.error_message IS NOT NULL) AS common_errors
FROM
    cmbc.workflow_steps ws
JOIN
    cmbc.workflow_templates wt ON ws.template_id = wt.template_id
JOIN
    cmbc.workflow_execution_log wel ON ws.step_id = wel.step_id
WHERE
    wel.completed_at IS NOT NULL
GROUP BY
    ws.template_id, wt.template_name, ws.step_id, ws.step_name, ws.step_type;

COMMENT ON MATERIALIZED VIEW cmbc.mv_workflow_bottlenecks IS 'Identifies performance bottlenecks and common failure points in automated workflows';


--ai risk prediction generation procedure
CREATE OR REPLACE PROCEDURE cmbc.sp_generate_risk_predictions(
    p_model_id INTEGER,
    p_prediction_window_days INTEGER DEFAULT 30,
    p_confidence_threshold NUMERIC(5,2) DEFAULT 0.7,
    OUT p_prediction_count INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_model_type VARCHAR(100);
    v_target_risk_category VARCHAR(100);
BEGIN
    -- Get model details
    SELECT model_type, target_risk_category INTO v_model_type, v_target_risk_category
    FROM cmbc.risk_prediction_models WHERE model_id = p_model_id;

    -- In a real implementation, this would call an ML service or run a database ML function
    -- This is a simplified simulation of the prediction process

    -- Simulate generating predictions for different risk types
    INSERT INTO cmbc.risk_predictions (
        model_id,
        prediction_timestamp,
        prediction_window,
        predicted_risk_type,
        predicted_risk_level,
        confidence_score,
        key_influencing_factors,
        mitigation_recommendations,
        review_status
    )
    SELECT
        p_model_id,
        CURRENT_TIMESTAMP,
        DATERANGE(CURRENT_DATE, CURRENT_DATE + p_prediction_window_days),
        risk_type,
        CASE
            WHEN random() < 0.1 THEN 'Critical'
            WHEN random() < 0.3 THEN 'High'
            WHEN random() < 0.6 THEN 'Medium'
            ELSE 'Low'
        END,
        random()::NUMERIC(5,2) + 0.5, -- Simulated confidence score between 0.5-1.5
        jsonb_build_object(
            'top_factor', CASE WHEN random() < 0.5 THEN 'Historical Frequency' ELSE 'External Indicators' END,
            'trend', CASE WHEN random() < 0.5 THEN 'Increasing' ELSE 'Stable' END
        ),
        ARRAY[
            CASE
                WHEN random() < 0.3 THEN 'Review contingency plans'
                WHEN random() < 0.6 THEN 'Increase monitoring'
                ELSE 'Conduct stress test'
            END
        ],
        'Pending'
    FROM
        (SELECT DISTINCT category AS risk_type FROM cmbc.risk_assessments
         WHERE v_target_risk_category = 'All' OR category = v_target_risk_category) t;

    -- Get count of generated predictions
    GET DIAGNOSTICS p_prediction_count = ROW_COUNT;

    -- Update model last used timestamp
    UPDATE cmbc.risk_prediction_models
    SET updated_at = CURRENT_TIMESTAMP
    WHERE model_id = p_model_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_generate_risk_predictions IS 'Simulates generating risk predictions using AI models (in production would integrate with actual ML services)';


--automated workflow executione ngine

CREATE OR REPLACE PROCEDURE cmbc.sp_execute_workflow_step(
    p_workflow_id UUID,
    p_step_id INTEGER,
    p_triggered_by VARCHAR(100) DEFAULT 'system'
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_step_type VARCHAR(50);
    v_instructions TEXT;
    v_assigned_role VARCHAR(100);
    v_template_id INTEGER;
    v_step_order INTEGER;
    v_is_parallel BOOLEAN;
    v_next_step_id INTEGER;
    v_workflow_status VARCHAR(50);
    v_notification_id INTEGER;
BEGIN
    -- Get step details
    SELECT
        ws.step_type, ws.instructions, ws.assigned_role,
        ws.template_id, ws.step_order, ws.is_parallel
    INTO
        v_step_type, v_instructions, v_assigned_role,
        v_template_id, v_step_order, v_is_parallel
    FROM
        cmbc.workflow_steps ws
    WHERE
        ws.step_id = p_step_id;

    -- Log step execution start
    INSERT INTO cmbc.workflow_execution_log (
        workflow_id,
        step_id,
        status,
        started_at,
        performed_by
    ) VALUES (
        p_workflow_id,
        p_step_id,
        'In Progress',
        CURRENT_TIMESTAMP,
        p_triggered_by
    ) RETURNING execution_id INTO v_notification_id;

    -- Execute step based on type
    IF v_step_type = 'Notification' THEN
        -- Create notification for assigned role
        INSERT INTO cmbc.notifications (
            recipient_id,
            message,
            notification_type,
            related_workflow_id,
            status
        )
        SELECT
            member_id,
            'Workflow Action Required: ' || v_instructions,
            'Workflow Step',
            p_workflow_id,
            'Pending'
        FROM
            cmbc.crisis_management_team
        WHERE
            role = v_assigned_role;

        -- Mark step as completed (notifications are fire-and-forget)
        UPDATE cmbc.workflow_execution_log
        SET
            status = 'Completed',
            completed_at = CURRENT_TIMESTAMP,
            output_data = jsonb_build_object('notification_count', 1)
        WHERE
            execution_id = v_notification_id;

    ELSIF v_step_type = 'System Action' THEN
        -- In a real implementation, this would trigger an API call or database operation
        -- Simulate system action with a delay
        PERFORM pg_sleep(1);

        -- Randomly fail 10% of system actions for demonstration
        IF random() < 0.1 THEN
            UPDATE cmbc.workflow_execution_log
            SET
                status = 'Failed',
                completed_at = CURRENT_TIMESTAMP,
                error_message = 'Simulated system action failure'
            WHERE
                execution_id = v_notification_id;
        ELSE
            UPDATE cmbc.workflow_execution_log
            SET
                status = 'Completed',
                completed_at = CURRENT_TIMESTAMP,
                output_data = jsonb_build_object('result', 'success')
            WHERE
                execution_id = v_notification_id;
        END IF;
    END IF;

    -- Check if workflow is complete
    SELECT status INTO v_workflow_status
    FROM cmbc.active_workflows
    WHERE workflow_id = p_workflow_id;

    -- Get next step if current step completed successfully
    IF v_workflow_status = 'In Progress' THEN
        SELECT step_id INTO v_next_step_id
        FROM cmbc.workflow_steps
        WHERE template_id = v_template_id
          AND step_order > v_step_order
          AND is_parallel = FALSE
        ORDER BY step_order
        LIMIT 1;

        IF v_next_step_id IS NOT NULL THEN
            CALL cmbc.sp_execute_workflow_step(p_workflow_id, v_next_step_id, p_triggered_by);
        ELSE
            -- No more steps, mark workflow as completed
            UPDATE cmbc.active_workflows
            SET
                current_status = 'Completed',
                completed_at = CURRENT_TIMESTAMP
            WHERE
                workflow_id = p_workflow_id;
        END IF;
    END IF;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_execute_workflow_step IS 'Executes an individual workflow step and handles transitions to subsequent steps';


--automated incident response workflow trigger
CREATE OR REPLACE PROCEDURE cmbc.sp_trigger_incident_response(
    p_incident_id INTEGER,
    p_severity VARCHAR(20)
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_template_id INTEGER;
    v_workflow_id UUID;
BEGIN
    -- Determine appropriate workflow template based on incident severity
    IF p_severity = 'Critical' THEN
        SELECT template_id INTO v_template_id
        FROM cmbc.workflow_templates
        WHERE template_name = 'Critical Incident Response'
        LIMIT 1;
    ELSIF p_severity = 'High' THEN
        SELECT template_id INTO v_template_id
        FROM cmbc.workflow_templates
        WHERE template_name = 'High Severity Incident Response'
        LIMIT 1;
    ELSE
        SELECT template_id INTO v_template_id
        FROM cmbc.workflow_templates
        WHERE template_name = 'Standard Incident Response'
        LIMIT 1;
    END IF;

    -- Create workflow instance
    INSERT INTO cmbc.active_workflows (
        template_id,
        initiating_event_id,
        initiating_event_type,
        current_status,
        started_at,
        initiated_by
    ) VALUES (
        v_template_id,
        p_incident_id,
        'Incident',
        'In Progress',
        CURRENT_TIMESTAMP,
        'system'
    ) RETURNING workflow_id INTO v_workflow_id;

    -- Start workflow execution with first step
    PERFORM cmbc.sp_execute_workflow_step(
        v_workflow_id,
        (SELECT step_id FROM cmbc.workflow_steps
         WHERE template_id = v_template_id
         ORDER BY step_order LIMIT 1),
        'system'
    );

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_trigger_incident_response IS 'Automatically triggers the appropriate response workflow based on incident severity';


-- real-time monitoring integrationt ables
CREATE TABLE cmbc.monitoring_sensors (
    sensor_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    sensor_name VARCHAR(255) NOT NULL,
    sensor_type VARCHAR(100) NOT NULL CHECK (sensor_type IN ('Network', 'Server', 'Facility', 'Environmental', 'Application')),
    location VARCHAR(255),
    monitoring_metric VARCHAR(100) NOT NULL,
    normal_range NUMERIC[],
    critical_threshold NUMERIC,
    update_frequency_seconds INTEGER NOT NULL,
    is_active BOOLEAN NOT NULL DEFAULT TRUE,
    last_reading_time TIMESTAMP WITH TIME ZONE,
    last_reading_value NUMERIC,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.monitoring_sensors IS 'Defines sensors and monitoring points for real-time operational monitoring';

CREATE TABLE cmbc.sensor_alert_rules (
    rule_id SERIAL PRIMARY KEY,
    sensor_type VARCHAR(100) NOT NULL,
    condition_expression TEXT NOT NULL,
    severity VARCHAR(20) NOT NULL CHECK (severity IN ('Warning', 'Critical', 'Emergency')),
    notification_template_id INTEGER REFERENCES cmbc.notification_templates(template_id),
    auto_trigger_workflow_id INTEGER REFERENCES cmbc.workflow_templates(template_id),
    is_active BOOLEAN NOT NULL DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.sensor_alert_rules IS 'Defines rules for generating alerts from sensor data and automatic response actions';

--blockchain based audit treail trables
CREATE TABLE cmbc.immutable_audit_log (
    log_id SERIAL PRIMARY KEY,
    transaction_hash VARCHAR(64) NOT NULL,
    table_name VARCHAR(100) NOT NULL,
    record_id INTEGER NOT NULL,
    action VARCHAR(20) NOT NULL CHECK (action IN ('INSERT', 'UPDATE', 'DELETE')),
    action_by VARCHAR(100) NOT NULL,
    action_at TIMESTAMP WITH TIME ZONE NOT NULL,
    data_hash VARCHAR(64) NOT NULL,
    blockchain_confirmed BOOLEAN NOT NULL DEFAULT FALSE,
    confirmation_timestamp TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (transaction_hash)
);

COMMENT ON TABLE cmbc.immutable_audit_log IS 'Stores cryptographically hashed audit records that can be verified against blockchain storage';

CREATE TABLE cmbc.blockchain_verification (
    verification_id SERIAL PRIMARY KEY,
    log_id INTEGER NOT NULL REFERENCES cmbc.immutable_audit_log(log_id),
    verification_method VARCHAR(50) NOT NULL CHECK (verification_method IN ('Ethereum', 'Hyperledger', 'Quorum')),
    contract_address VARCHAR(42),
    block_number INTEGER,
    verification_timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
    verified_by VARCHAR(100) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.blockchain_verification IS 'Tracks blockchain verification details for immutable audit records';


--predictive employee location safety
CREATE TABLE cmbc.employee_safety_zones (
    zone_id SERIAL PRIMARY KEY,
    zone_name VARCHAR(255) NOT NULL,
    geo_boundary GEOGRAPHY(POLYGON, 4326) NOT NULL,
    risk_category VARCHAR(100) NOT NULL CHECK (risk_category IN ('Safe', 'Caution', 'High Risk', 'Restricted')),
    risk_factors JSONB COMMENT 'JSON structure detailing risk factors like crime rates, environmental hazards',
    daytime_risk_adjustment NUMERIC(3,2) DEFAULT 1.0,
    nighttime_risk_adjustment NUMERIC(3,2) DEFAULT 1.5,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.employee_safety_zones IS 'Defines geographic zones with safety ratings used to assess employee risk during disruptions';

CREATE TABLE cmbc.employee_location_risk (
    risk_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    employee_id VARCHAR(50) NOT NULL,
    timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
    location GEOGRAPHY(POINT, 4326) NOT NULL,
    calculated_risk_score NUMERIC(5,2) NOT NULL,
    risk_zone_id INTEGER REFERENCES cmbc.employee_safety_zones(zone_id),
    time_of_day VARCHAR(10) GENERATED ALWAYS AS (
        CASE
            WHEN EXTRACT(HOUR FROM timestamp) BETWEEN 6 AND 18 THEN 'Day'
            ELSE 'Night'
        END
    ) STORED,
    recommended_action VARCHAR(255),
    notification_sent BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.employee_location_risk IS 'Tracks real-time location-based risk assessments for employees during crises';


---employee risk exposure trends
CREATE MATERIALIZED VIEW cmbc.mv_employee_risk_trends AS
SELECT
    DATE_TRUNC('day', timestamp) AS day,
    risk_zone_id,
    ez.zone_name,
    time_of_day,
    COUNT(DISTINCT employee_id) AS employees_at_risk,
    AVG(calculated_risk_score) AS avg_risk_score,
    MAX(calculated_risk_score) AS max_risk_score,
    COUNT(DISTINCT CASE WHEN calculated_risk_score > 70 THEN employee_id END) AS high_risk_employees,
    STRING_AGG(DISTINCT recommended_action, '|') AS common_actions
FROM
    cmbc.employee_location_risk elr
LEFT JOIN
    cmbc.employee_safety_zones ez ON elr.risk_zone_id = ez.zone_id
WHERE
    timestamp > CURRENT_DATE - INTERVAL '30 days'
GROUP BY
    DATE_TRUNC('day', timestamp), risk_zone_id, ez.zone_name, time_of_day;

COMMENT ON MATERIALIZED VIEW cmbc.mv_employee_risk_trends IS 'Aggregates daily employee risk exposure data for trend analysis and safety planning';


--satellite imagery integration for site risk analysis
CREATE TABLE cmbc.satellite_risk_monitoring (
    monitor_id SERIAL PRIMARY KEY,
    facility_id INTEGER NOT NULL,
    image_date DATE NOT NULL,
    image_resolution_meters NUMERIC(5,2) NOT NULL,
    risk_indicators JSONB NOT NULL COMMENT 'Flood levels, vegetation encroachment, security vulnerabilities',
    change_from_previous NUMERIC(5,2) COMMENT 'Percentage change in risk indicators',
    analyst_notes TEXT,
    automated_alert_generated BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.satellite_risk_monitoring IS 'Stores analyzed satellite imagery data for detecting environmental risks to facilities';

--neural network based incident pattern recognition
CREATE TABLE cmbc.incident_patterns (
    pattern_id SERIAL PRIMARY KEY,
    model_version VARCHAR(50) NOT NULL,
    pattern_type VARCHAR(100) NOT NULL CHECK (pattern_type IN ('Temporal', 'Geospatial', 'Systemic', 'Behavioral')),
    pattern_expression JSONB NOT NULL COMMENT 'Mathematical representation of the pattern',
    confidence_score NUMERIC(5,2) NOT NULL,
    first_detected_date DATE NOT NULL,
    last_observed_date DATE NOT NULL,
    predicted_future_occurrences JSONB COMMENT 'Dates/locations of predicted recurrences',
    recommended_preventative_actions TEXT[],
    is_active BOOLEAN NOT NULL DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.incident_patterns IS 'Stores complex incident patterns identified by AI models to enable proactive disruption prevention';


-- predictive capacity planning
CREATE TABLE cmbc.capacity_simulations (
    simulation_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    scenario_name VARCHAR(255) NOT NULL,
    simulation_parameters JSONB NOT NULL COMMENT 'Workload, resource constraints, etc.',
    predicted_bottlenecks JSONB NOT NULL,
    recommended_actions TEXT[] NOT NULL,
    confidence_score NUMERIC(5,2) NOT NULL CHECK (confidence_score BETWEEN 0 AND 1),
    run_by VARCHAR(100) NOT NULL,
    run_at TIMESTAMP WITH TIME ZONE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.capacity_simulations IS 'Stores results of predictive simulations for resource capacity during crises';

--AI powered threat intelligence integration
CREATE TABLE cmbc.threat_intelligence_feeds (
    feed_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    feed_name VARCHAR(255) NOT NULL,
    feed_type VARCHAR(50) NOT NULL CHECK (feed_type IN ('Cyber', 'GeoPolitical', 'SupplyChain', 'Health', 'Environmental')),
    source_name VARCHAR(255) NOT NULL,
    update_frequency_minutes INTEGER NOT NULL,
    last_update TIMESTAMP WITH TIME ZONE,
    is_active BOOLEAN NOT NULL DEFAULT TRUE,
    confidence_score NUMERIC(3,2) CHECK (confidence_score BETWEEN 0 AND 1),
    coverage_scope VARCHAR(100)[],
    authentication_details JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.threat_intelligence_feeds IS 'Tracks external threat intelligence sources that provide real-time risk data to the organization';

CREATE TABLE cmbc.threat_indicators (
    indicator_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    feed_id UUID REFERENCES cmbc.threat_intelligence_feeds(feed_id),
    indicator_type VARCHAR(50) NOT NULL CHECK (indicator_type IN ('IP', 'Domain', 'Hash', 'Pattern', 'Behavior')),
    indicator_value TEXT NOT NULL,
    first_seen TIMESTAMP WITH TIME ZONE NOT NULL,
    last_seen TIMESTAMP WITH TIME ZONE NOT NULL,
    severity VARCHAR(20) NOT NULL CHECK (severity IN ('Low', 'Medium', 'High', 'Critical')),
    confidence NUMERIC(3,2) NOT NULL CHECK (confidence BETWEEN 0 AND 1),
    related_incidents UUID[],
    recommended_actions TEXT[],
    expiry_time TIMESTAMP WITH TIME ZONE,
    is_active BOOLEAN GENERATED ALWAYS AS (expiry_time > CURRENT_TIMESTAMP) STORED,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.threat_indicators IS 'Stores specific threat indicators from intelligence feeds with contextual metadata';

-- blockchain based audit trails
CREATE TABLE cmbc.blockchain_audit (
    audit_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    transaction_hash VARCHAR(66) NOT NULL,
    block_number BIGINT NOT NULL,
    event_type VARCHAR(50) NOT NULL CHECK (event_type IN ('Configuration', 'Access', 'Incident', 'Recovery', 'Decision')),
    event_details JSONB NOT NULL,
    actor_id VARCHAR(100) NOT NULL,
    actor_role VARCHAR(50) NOT NULL,
    network VARCHAR(50) NOT NULL CHECK (network IN ('Ethereum', 'Hyperledger', 'Quorum')),
    smart_contract_address VARCHAR(42),
    timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
    verified BOOLEAN NOT NULL DEFAULT FALSE,
    verification_timestamp TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (transaction_hash)
);

COMMENT ON TABLE cmbc.blockchain_audit IS 'Immutable record of critical CMBC actions stored on blockchain for non-repudiation';

CREATE TABLE cmbc.audit_verification (
    verification_id SERIAL PRIMARY KEY,
    audit_id UUID NOT NULL REFERENCES cmbc.blockchain_audit(audit_id),
    verification_method VARCHAR(50) NOT NULL CHECK (verification_method IN ('OnChain', 'SmartContract', 'ThirdParty')),
    verification_result JSONB NOT NULL,
    verified_by VARCHAR(100) NOT NULL,
    verified_at TIMESTAMP WITH TIME ZONE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.audit_verification IS 'Tracks verification of blockchain audit records for compliance purposes';


--IoT Sensor Integration for Physical Risk Monitoring
CREATE TABLE cmbc.iot_sensors (
    sensor_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    sensor_name VARCHAR(255) NOT NULL,
    sensor_type VARCHAR(50) NOT NULL CHECK (sensor_type IN ('Temperature', 'Humidity', 'Motion', 'AirQuality', 'Water', 'Power')),
    location_id INTEGER NOT NULL,
    asset_id INTEGER,
    normal_range NUMERIC[] NOT NULL,
    critical_threshold NUMERIC NOT NULL,
    update_frequency_seconds INTEGER NOT NULL,
    last_reading TIMESTAMP WITH TIME ZONE,
    last_value NUMERIC,
    status VARCHAR(20) NOT NULL CHECK (status IN ('Active', 'Calibrating', 'Maintenance', 'Retired')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.iot_sensors IS 'Registry of IoT sensors monitoring physical environments for potential disruptions';

CREATE TABLE cmbc.iot_alert_rules (
    rule_id SERIAL PRIMARY KEY,
    sensor_type VARCHAR(50) NOT NULL,
    condition_expression TEXT NOT NULL,
    severity VARCHAR(20) NOT NULL CHECK (severity IN ('Warning', 'Severe', 'Critical')),
    notification_template_id INTEGER,
    auto_trigger_workflow_id INTEGER,
    cooldown_minutes INTEGER NOT NULL DEFAULT 5,
    is_active BOOLEAN NOT NULL DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.iot_alert_rules IS 'Defines rules for generating alerts from IoT sensor data thresholds';


-- Threat Intelligence Dashboard
CREATE OR REPLACE VIEW cmbc.vw_threat_intelligence_dashboard AS
SELECT
    ti.feed_id,
    tf.feed_name,
    tf.feed_type,
    COUNT(ti.indicator_id) AS indicator_count,
    SUM(CASE WHEN ti.severity = 'Critical' THEN 1 ELSE 0 END) AS critical_indicators,
    SUM(CASE WHEN ti.severity = 'High' THEN 1 ELSE 0 END) AS high_indicators,
    MAX(ti.last_seen) AS latest_indicator,
    AVG(ti.confidence) AS avg_confidence,
    COUNT(DISTINCT ti.indicator_type) AS indicator_types,
    ARRAY_AGG(DISTINCT i.incident_name) FILTER (WHERE i.incident_name IS NOT NULL) AS linked_incidents
FROM
    cmbc.threat_indicators ti
JOIN
    cmbc.threat_intelligence_feeds tf ON ti.feed_id = tf.feed_id
LEFT JOIN
    cmbc.incidents i ON i.incident_id = ANY(ti.related_incidents)
WHERE
    ti.is_active = TRUE
    AND tf.is_active = TRUE
GROUP BY
    ti.feed_id, tf.feed_name, tf.feed_type;

COMMENT ON VIEW cmbc.vw_threat_intelligence_dashboard IS 'Aggregates threat intelligence data for security operations center dashboards';

--Blockchain Audit Trail Verificaiton
CREATE OR REPLACE VIEW cmbc.vw_blockchain_verification_status AS
SELECT
    ba.event_type,
    COUNT(*) AS total_events,
    SUM(CASE WHEN ba.verified = TRUE THEN 1 ELSE 0 END) AS verified_events,
    SUM(CASE WHEN ba.verified = FALSE THEN 1 ELSE 0 END) AS unverified_events,
    MIN(ba.timestamp) AS earliest_event,
    MAX(ba.timestamp) AS latest_event,
    COUNT(DISTINCT ba.actor_id) AS unique_actors,
    ARRAY_AGG(DISTINCT ba.network) AS blockchain_networks
FROM
    cmbc.blockchain_audit ba
GROUP BY
    ba.event_type;

COMMENT ON VIEW cmbc.vw_blockchain_verification_status IS 'Provides verification status of blockchain audit records by event type';

--Iot Sensor Anomaly Detection
CREATE MATERIALIZED VIEW cmbc.mv_iot_anomalies AS
SELECT
    s.sensor_id,
    s.sensor_name,
    s.sensor_type,
    s.location_id,
    COUNT(r.reading_id) AS reading_count,
    AVG(r.value) AS avg_value,
    STDDEV(r.value) AS stddev_value,
    MIN(r.value) AS min_value,
    MAX(r.value) AS max_value,
    COUNT(CASE WHEN r.value < s.normal_range[1] OR r.value > s.normal_range[2] THEN 1 END) AS anomaly_count,
    COUNT(CASE WHEN r.value >= s.critical_threshold THEN 1 END) AS critical_count
FROM
    cmbc.iot_sensors s
JOIN
    cmbc.iot_sensor_readings r ON s.sensor_id = r.sensor_id
WHERE
    r.timestamp > CURRENT_TIMESTAMP - INTERVAL '7 days'
GROUP BY
    s.sensor_id, s.sensor_name, s.sensor_type, s.location_id;

COMMENT ON MATERIALIZED VIEW cmbc.mv_iot_anomalies IS 'Aggregates IoT sensor data to identify abnormal patterns and potential facility risks';


--Threat indicator correlation
CREATE MATERIALIZED VIEW cmbc.mv_threat_correlations AS
WITH indicator_stats AS (
    SELECT
        indicator_type,
        severity,
        COUNT(*) AS indicator_count,
        AVG(confidence) AS avg_confidence
    FROM
        cmbc.threat_indicators
    WHERE
        is_active = TRUE
    GROUP BY
        indicator_type, severity
),
incident_correlations AS (
    SELECT
        ti.indicator_type,
        COUNT(DISTINCT i.incident_id) AS incident_count
    FROM
        cmbc.threat_indicators ti,
        UNNEST(ti.related_incidents) AS incident_id
    JOIN
        cmbc.incidents i ON i.incident_id = incident_id
    GROUP BY
        ti.indicator_type
)
SELECT
    is.indicator_type,
    is.severity,
    is.indicator_count,
    is.avg_confidence,
    COALESCE(ic.incident_count, 0) AS incident_count,
    CASE
        WHEN ic.incident_count IS NULL THEN 0
        ELSE ic.incident_count::NUMERIC / is.indicator_count
    END AS incident_ratio
FROM
    indicator_stats is
LEFT JOIN
    incident_correlations ic ON is.indicator_type = ic.indicator_type;

COMMENT ON MATERIALIZED VIEW cmbc.mv_threat_correlations IS 'Analyzes relationships between threat indicators and actual incidents to improve threat scoring';


-- Threat Intelligence Enrichment Procedure
CREATE OR REPLACE PROCEDURE cmbc.sp_enrich_threat_indicators(
    p_feed_id UUID DEFAULT NULL,
    p_min_severity VARCHAR(20) DEFAULT 'Medium',
    OUT p_indicators_processed INTEGER
)
LANGUAGE plpgsql
AS $$
BEGIN
    -- In production, this would call external threat intelligence APIs
    -- This simulates enrichment by adding related incident data

    -- Update indicators with related incidents
    WITH matched_incidents AS (
        SELECT
            ti.indicator_id,
            ARRAY_AGG(i.incident_id) AS related_incidents
        FROM
            cmbc.threat_indicators ti
        JOIN
            cmbc.incidents i ON
                i.incident_type = 'Cyber' AND
                i.description LIKE '%' || ti.indicator_value || '%'
        WHERE
            (p_feed_id IS NULL OR ti.feed_id = p_feed_id)
            AND ti.severity >= p_min_severity
        GROUP BY
            ti.indicator_id
    )
    UPDATE cmbc.threat_indicators ti
    SET
        related_incidents = mi.related_incidents,
        updated_at = CURRENT_TIMESTAMP
    FROM
        matched_incidents mi
    WHERE
        ti.indicator_id = mi.indicator_id;

    -- Get count of processed indicators
    GET DIAGNOSTICS p_indicators_processed = ROW_COUNT;

    -- Update feed last processed time
    IF p_feed_id IS NOT NULL THEN
        UPDATE cmbc.threat_intelligence_feeds
        SET last_update = CURRENT_TIMESTAMP
        WHERE feed_id = p_feed_id;
    END IF;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_enrich_threat_indicators IS 'Enhances threat indicators with contextual data from internal incidents and external sources';

-- Blockchain Audit Verification Procedure
CREATE OR REPLACE PROCEDURE cmbc.sp_verify_blockchain_audit(
    p_audit_id UUID,
    p_verification_method VARCHAR(50),
    p_verified_by VARCHAR(100),
    OUT p_verification_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_transaction_hash VARCHAR(66);
    v_block_number BIGINT;
    v_network VARCHAR(50);
BEGIN
    -- Get audit record details
    SELECT transaction_hash, block_number, network
    INTO v_transaction_hash, v_block_number, v_network
    FROM cmbc.blockchain_audit
    WHERE audit_id = p_audit_id;

    -- In production, this would verify against actual blockchain
    -- This simulates verification with a 95% success rate

    -- Simulate verification
    INSERT INTO cmbc.audit_verification (
        audit_id,
        verification_method,
        verification_result,
        verified_by,
        verified_at
    ) VALUES (
        p_audit_id,
        p_verification_method,
        jsonb_build_object(
            'transactionHash', v_transaction_hash,
            'blockNumber', v_block_number,
            'status', CASE WHEN random() < 0.95 THEN 'Verified' ELSE 'Failed' END,
            'network', v_network
        ),
        p_verified_by,
        CURRENT_TIMESTAMP
    ) RETURNING verification_id INTO p_verification_id;

    -- Update audit record if verified
    UPDATE cmbc.blockchain_audit
    SET
        verified = (SELECT verification_result->>'status' = 'Verified'
                   FROM cmbc.audit_verification
                   WHERE verification_id = p_verification_id),
        verification_timestamp = CURRENT_TIMESTAMP
    WHERE
        audit_id = p_audit_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_verify_blockchain_audit IS 'Performs verification of blockchain audit records against the distributed ledger';

--Iot Alert Processing engine
CREATE OR REPLACE PROCEDURE cmbc.sp_process_iot_alerts(
    p_cooldown_minutes INTEGER DEFAULT 5,
    OUT p_alerts_processed INTEGER
)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Process new sensor readings that exceed thresholds
    WITH alert_candidates AS (
        SELECT
            r.reading_id,
            r.sensor_id,
            r.value,
            r.timestamp,
            s.sensor_name,
            s.sensor_type,
            s.location_id,
            s.critical_threshold,
            s.normal_range,
            ar.rule_id,
            ar.severity,
            ar.notification_template_id,
            ar.auto_trigger_workflow_id
        FROM
            cmbc.iot_sensor_readings r
        JOIN
            cmbc.iot_sensors s ON r.sensor_id = s.sensor_id
        JOIN
            cmbc.iot_alert_rules ar ON
                ar.sensor_type = s.sensor_type AND
                ar.is_active = TRUE
        WHERE
            r.processed = FALSE AND
            (r.value < s.normal_range[1] OR r.value > s.normal_range[2] OR r.value >= s.critical_threshold) AND
            (r.timestamp > CURRENT_TIMESTAMP - (p_cooldown_minutes * INTERVAL '1 minute'))
    ),
    new_alerts AS (
        INSERT INTO cmbc.iot_alerts (
            sensor_id,
            reading_id,
            alert_value,
            normal_range,
            threshold_exceeded,
            severity,
            alert_time,
            location_id,
            rule_id
        )
        SELECT
            sensor_id,
            reading_id,
            value,
            normal_range,
            CASE
                WHEN value >= critical_threshold THEN 'CriticalThreshold'
                WHEN value > normal_range[2] THEN 'UpperBound'
                ELSE 'LowerBound'
            END,
            severity,
            timestamp,
            location_id,
            rule_id
        FROM
            alert_candidates
        RETURNING alert_id, sensor_id, severity, rule_id, location_id
    )
    SELECT COUNT(*) INTO p_alerts_processed FROM new_alerts;

    -- Mark readings as processed
    UPDATE cmbc.iot_sensor_readings r
    SET processed = TRUE
    FROM alert_candidates ac
    WHERE r.reading_id = ac.reading_id;

    -- Trigger notifications for critical alerts
    INSERT INTO cmbc.notifications (
        recipient_id,
        message,
        notification_type,
        related_alert_id,
        status
    )
    SELECT
        cmt.member_id,
        'IoT Alert: ' || s.sensor_name || ' at location ' || l.location_name ||
        ' exceeded ' || a.threshold_exceeded || ' with value ' || a.alert_value,
        'IoT Alert',
        a.alert_id,
        'Pending'
    FROM
        cmbc.iot_alerts a
    JOIN
        cmbc.iot_sensors s ON a.sensor_id = s.sensor_id
    JOIN
        cmbc.locations l ON a.location_id = l.location_id
    JOIN
        cmbc.crisis_management_team cmt ON cmt.role = 'Facility Manager'
    WHERE
        a.notification_sent = FALSE AND
        a.severity IN ('Severe', 'Critical');

    -- Update alert notification status
    UPDATE cmbc.iot_alerts
    SET notification_sent = TRUE
    WHERE alert_id IN (
        SELECT related_alert_id FROM cmbc.notifications
        WHERE notification_type = 'IoT Alert' AND status = 'Pending'
    );

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_process_iot_alerts IS 'Processes IoT sensor data to generate alerts when thresholds are exceeded and triggers notifications';

---dynamic capacity planning engine
CREATE TABLE cmbc.capacity_plans (
    plan_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    plan_name VARCHAR(255) NOT NULL,
    scenario_type VARCHAR(100) NOT NULL CHECK (scenario_type IN ('Baseline', 'Peak', 'Disruption', 'Recovery')),
    time_horizon_days INTEGER NOT NULL CHECK (time_horizon_days BETWEEN 1 AND 365),
    resource_type VARCHAR(100) NOT NULL CHECK (resource_type IN ('Personnel', 'IT', 'Facility', 'SupplyChain')),
    critical_process_ids UUID[] NOT NULL COMMENT 'Reference to critical business processes',
    created_by VARCHAR(100) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.capacity_plans IS 'Master table for capacity planning scenarios that map resources to critical processes';

CREATE TABLE cmbc.capacity_requirements (
    requirement_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    plan_id UUID NOT NULL REFERENCES cmbc.capacity_plans(plan_id),
    process_id UUID NOT NULL COMMENT 'Reference to business process',
    resource_type VARCHAR(100) NOT NULL,
    normal_capacity NUMERIC(15,2) NOT NULL,
    min_required_capacity NUMERIC(15,2) NOT NULL,
    peak_capacity NUMERIC(15,2) NOT NULL,
    lead_time_hours INTEGER NOT NULL COMMENT 'Time to ramp up capacity',
    dependencies UUID[] COMMENT 'Other requirements that must be met first',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.capacity_requirements IS 'Defines specific capacity needs for each process in different scenarios';

CREATE TABLE cmbc.capacity_sources (
    source_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    requirement_id UUID NOT NULL REFERENCES cmbc.capacity_requirements(requirement_id),
    source_type VARCHAR(100) NOT NULL CHECK (source_type IN ('Internal', 'Vendor', 'Cloud', 'Shared')),
    source_name VARCHAR(255) NOT NULL,
    available_capacity NUMERIC(15,2) NOT NULL,
    activation_time_hours INTEGER NOT NULL,
    cost_per_unit NUMERIC(15,2) NOT NULL,
    contractual_terms TEXT,
    is_preferred BOOLEAN NOT NULL DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.capacity_sources IS 'Identifies available sources to meet capacity requirements with activation details';

--capacity planning view --capacity gap analysis view
CREATE OR REPLACE VIEW cmbc.vw_capacity_gaps AS
SELECT
    cp.plan_id,
    cp.plan_name,
    cp.scenario_type,
    cr.resource_type,
    SUM(cr.min_required_capacity) AS required_capacity,
    SUM(COALESCE(cs.available_capacity, 0)) AS secured_capacity,
    SUM(cr.min_required_capacity) - SUM(COALESCE(cs.available_capacity, 0)) AS capacity_gap,
    MAX(cr.lead_time_hours) AS max_lead_time,
    COUNT(DIST cr.process_id) AS affected_processes,
    STRING_AGG(DIST cp2.process_name, ', ') AS process_names
FROM
    cmbc.capacity_plans cp
JOIN
    cmbc.capacity_requirements cr ON cp.plan_id = cr.plan_id
LEFT JOIN
    cmbc.capacity_sources cs ON cr.requirement_id = cs.requirement_id
LEFT JOIN
    cmbc.critical_processes cp2 ON cr.process_id = cp2.process_id
GROUP BY
    cp.plan_id, cp.plan_name, cp.scenario_type, cr.resource_type;

COMMENT ON VIEW cmbc.vw_capacity_gaps IS 'Identifies capacity shortfalls across all planning scenarios with impact analysis';

--resource utilization forecast view
CREATE OR REPLACE VIEW cmbc.vw_resource_forecast AS
WITH time_periods AS (
    SELECT generate_series(
        CURRENT_DATE,
        CURRENT_DATE + INTERVAL '30 days',
        INTERVAL '1 day'
    ) AS period_date
)
SELECT
    r.resource_type,
    r.resource_id,
    r.resource_name,
    tp.period_date,
    r.base_capacity,
    COALESCE(d.demand, 0) AS projected_demand,
    r.base_capacity - COALESCE(d.demand, 0) AS surplus_deficit,
    CASE
        WHEN COALESCE(d.demand, 0) = 0 THEN 0
        WHEN r.base_capacity = 0 THEN 100
        ELSE (COALESCE(d.demand, 0) / r.base_capacity) * 100
    END AS utilization_pct
FROM
    cmbc.resources r
CROSS JOIN
    time_periods tp
LEFT JOIN (
    SELECT
        resource_type,
        resource_id,
        DATE_TRUNC('day', demand_date) AS demand_date,
        SUM(demand_amount) AS demand
    FROM
        cmbc.resource_demand_forecasts
    GROUP BY
        resource_type, resource_id, DATE_TRUNC('day', demand_date)
) d ON r.resource_type = d.resource_type
    AND r.resource_id = d.resource_id
    AND tp.period_date = d.demand_date;

COMMENT ON VIEW cmbc.vw_resource_forecast IS 'Projects resource utilization over 30-day horizon to identify future constraints';

--capacity planning stored procedures
CREATE OR REPLACE PROCEDURE cmbc.sp_allocate_capacity(
    p_incident_id UUID,
    p_plan_id UUID DEFAULT NULL,
    OUT p_resources_allocated INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_severity VARCHAR(20);
    v_impacted_processes UUID[];
BEGIN
    -- Get incident details
    SELECT severity, impacted_processes
    INTO v_severity, v_impacted_processes
    FROM cmbc.incidents
    WHERE incident_id = p_incident_id;

    -- Determine appropriate capacity plan if not specified
    IF p_plan_id IS NULL THEN
        SELECT plan_id INTO p_plan_id
        FROM cmbc.capacity_plans
        WHERE scenario_type = CASE
            WHEN v_severity = 'Critical' THEN 'Disruption'
            ELSE 'Peak'
        END
        LIMIT 1;
    END IF;

    -- Allocate resources for impacted processes
    WITH required_resources AS (
        SELECT
            cr.resource_type,
            cr.min_required_capacity,
            cs.source_id,
            cs.source_name,
            cs.available_capacity,
            cs.activation_time_hours
        FROM
            cmbc.capacity_requirements cr
        JOIN
            cmbc.capacity_sources cs ON cr.requirement_id = cs.requirement_id
        WHERE
            cr.plan_id = p_plan_id
            AND cr.process_id = ANY(v_impacted_processes)
    )
    INSERT INTO cmbc.resource_allocations (
        incident_id,
        resource_type,
        source_id,
        source_name,
        allocated_amount,
        activation_time,
        expected_ready_time
    )
    SELECT
        p_incident_id,
        resource_type,
        source_id,
        source_name,
        LEAST(min_required_capacity, available_capacity),
        activation_time_hours,
        CURRENT_TIMESTAMP + (activation_time_hours * INTERVAL '1 hour')
    FROM
        required_resources;

    -- Get count of allocated resources
    GET DIAGNOSTICS p_resources_allocated = ROW_COUNT;

    -- Update incident record
    UPDATE cmbc.incidents
    SET
        capacity_plan_id = p_plan_id,
        resource_allocation_time = CURRENT_TIMESTAMP
    WHERE
        incident_id = p_incident_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_allocate_capacity IS 'Automatically allocates resources based on incident severity and impacted processes using predefined capacity plans';

---capacity optimization procedure
CREATE OR REPLACE PROCEDURE cmbc.sp_optimize_capacity(
    p_plan_id UUID,
    p_max_cost NUMERIC(15,2) DEFAULT NULL,
    p_max_lead_time_hours INTEGER DEFAULT NULL,
    OUT p_recommendations JSONB
)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Generate optimized capacity sourcing recommendations
    WITH ranked_sources AS (
        SELECT
            cr.requirement_id,
            cr.process_id,
            cr.resource_type,
            cr.min_required_capacity,
            cs.source_id,
            cs.source_name,
            cs.available_capacity,
            cs.activation_time_hours,
            cs.cost_per_unit,
            ROW_NUMBER() OVER (
                PARTITION BY cr.requirement_id
                ORDER BY
                    CASE WHEN p_max_lead_time_hours IS NOT NULL AND cs.activation_time_hours > p_max_lead_time_hours THEN 1 ELSE 0 END,
                    CASE WHEN p_max_cost IS NOT NULL AND (cs.cost_per_unit * cr.min_required_capacity) > p_max_cost THEN 1 ELSE 0 END,
                    cs.activation_time_hours,
                    cs.cost_per_unit
            ) AS source_rank
        FROM
            cmbc.capacity_requirements cr
        JOIN
            cmbc.capacity_sources cs ON cr.requirement_id = cs.requirement_id
        WHERE
            cr.plan_id = p_plan_id
    )
    SELECT
        jsonb_agg(
            jsonb_build_object(
                'requirement_id', rs.requirement_id,
                'process_id', rs.process_id,
                'resource_type', rs.resource_type,
                'recommended_source', jsonb_build_object(
                    'source_id', rs.source_id,
                    'source_name', rs.source_name,
                    'allocated_amount', LEAST(rs.min_required_capacity, rs.available_capacity),
                    'activation_time', rs.activation_time_hours,
                    'total_cost', rs.cost_per_unit * LEAST(rs.min_required_capacity, rs.available_capacity)
                ),
                'alternative_sources', (
                    SELECT jsonb_agg(
                        jsonb_build_object(
                            'source_id', alt.source_id,
                            'source_name', alt.source_name,
                            'available_capacity', alt.available_capacity,
                            'activation_time', alt.activation_time_hours,
                            'cost_per_unit', alt.cost_per_unit
                        )
                    )
                    FROM ranked_sources alt
                    WHERE alt.requirement_id = rs.requirement_id AND alt.source_rank > 1
                )
            )
        ) INTO p_recommendations
    FROM
        ranked_sources rs
    WHERE
        rs.source_rank = 1;

    -- Store optimization results
    INSERT INTO cmbc.capacity_optimizations (
        plan_id,
        optimization_time,
        parameters,
        recommendations
    ) VALUES (
        p_plan_id,
        CURRENT_TIMESTAMP,
        jsonb_build_object('max_cost', p_max_cost, 'max_lead_time', p_max_lead_time_hours),
        p_recommendations
    );

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_optimize_capacity IS 'Generates cost and time optimized capacity sourcing recommendations within constraints';

--materialized views for capacity planning
CREATE MATERIALIZED VIEW cmbc.mv_capacity_utilization AS
SELECT
    cs.source_id,
    cs.source_name,
    cs.source_type,
    cs.resource_type,
    SUM(cr.min_required_capacity) AS total_committed,
    cs.available_capacity,
    cs.available_capacity - SUM(cr.min_required_capacity) AS remaining_capacity,
    COUNT(DIST cr.plan_id) AS supporting_plans,
    COUNT(DIST cr.process_id) AS supported_processes,
    MIN(cs.activation_time_hours) AS min_activation_time,
    MAX(cs.activation_time_hours) AS max_activation_time
FROM
    cmbc.capacity_sources cs
JOIN
    cmbc.capacity_requirements cr ON cs.requirement_id = cr.requirement_id
GROUP BY
    cs.source_id, cs.source_name, cs.source_type, cs.resource_type, cs.available_capacity;

COMMENT ON MATERIALIZED VIEW cmbc.mv_capacity_utilization IS 'Tracks utilization of capacity sources across all plans to identify overallocation risks';


--process resilience scoring
CREATE MATERIALIZED VIEW cmbc.mv_process_resilience AS
WITH process_capacity AS (
    SELECT
        cp.process_id,
        cp.process_name,
        COUNT(DIST cr.requirement_id) AS capacity_requirements,
        COUNT(DIST cs.source_id) AS capacity_sources,
        MIN(cs.activation_time_hours) AS fastest_recovery,
        MAX(cs.activation_time_hours) AS slowest_recovery,
        SUM(cs.available_capacity) AS total_available,
        SUM(cr.min_required_capacity) AS total_required
    FROM
        cmbc.critical_processes cp
    LEFT JOIN
        cmbc.capacity_requirements cr ON cp.process_id = cr.process_id
    LEFT JOIN
        cmbc.capacity_sources cs ON cr.requirement_id = cs.requirement_id
    GROUP BY
        cp.process_id, cp.process_name
)
SELECT
    pc.*,
    CASE
        WHEN pc.capacity_sources = 0 THEN 0
        WHEN pc.total_available >= pc.total_required * 1.5 THEN 100
        WHEN pc.total_available >= pc.total_required THEN 75
        WHEN pc.total_available >= pc.total_required * 0.5 THEN 50
        ELSE 25
    END AS capacity_score,
    CASE
        WHEN pc.fastest_recovery IS NULL THEN 0
        WHEN pc.fastest_recovery <= 1 THEN 100
        WHEN pc.fastest_recovery <= 4 THEN 75
        WHEN pc.fastest_recovery <= 24 THEN 50
        ELSE 25
    END AS speed_score,
    CASE
        WHEN pc.capacity_sources = 0 THEN 0
        WHEN pc.capacity_sources >= 3 THEN 100
        WHEN pc.capacity_sources >= 2 THEN 75
        ELSE 50
    END AS redundancy_score
FROM
    process_capacity pc;

COMMENT ON MATERIALIZED VIEW cmbc.mv_process_resilience IS 'Calculates comprehensive resilience scores for processes based on capacity, recovery speed, and redundancy';


-- resource pool management
CREATE TABLE cmbc.resource_pools (
    pool_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    pool_name VARCHAR(255) NOT NULL,
    resource_type VARCHAR(100) NOT NULL,
    total_capacity NUMERIC(15,2) NOT NULL,
    allocated_capacity NUMERIC(15,2) NOT NULL DEFAULT 0,
    available_capacity NUMERIC(15,2) GENERATED ALWAYS AS (total_capacity - allocated_capacity) STORED,
    utilization_pct NUMERIC(5,2) GENERATED ALWAYS AS (
        CASE
            WHEN total_capacity = 0 THEN 0
            ELSE (allocated_capacity / total_capacity) * 100
        END
    ) STORED,
    auto_scaling_capability BOOLEAN NOT NULL DEFAULT FALSE,
    max_scaled_capacity NUMERIC(15,2),
    scaling_time_minutes INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.resource_pools IS 'Manages pools of shared resources with dynamic allocation and scaling capabilities';

CREATE TABLE cmbc.pool_allocations (
    allocation_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    pool_id UUID NOT NULL REFERENCES cmbc.resource_pools(pool_id),
    incident_id UUID REFERENCES cmbc.incidents(incident_id),
    plan_id UUID REFERENCES cmbc.capacity_plans(plan_id),
    allocated_amount NUMERIC(15,2) NOT NULL,
    start_time TIMESTAMP WITH TIME ZONE NOT NULL,
    end_time TIMESTAMP WITH TIME ZONE,
    status VARCHAR(20) NOT NULL CHECK (status IN ('Reserved', 'Active', 'Released', 'Expired')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.pool_allocations IS 'Tracks allocation of resources from pools to incidents and plans';


--capacity planning stored procedures
CREATE OR REPLACE PROCEDURE cmbc.sp_scale_resource_pool(
    p_pool_id UUID,
    p_target_capacity NUMERIC(15,2),
    OUT p_success BOOLEAN,
    OUT p_message TEXT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_max_capacity NUMERIC(15,2);
    v_scaling_capable BOOLEAN;
BEGIN
    -- Check pool capabilities
    SELECT auto_scaling_capability, max_scaled_capacity
    INTO v_scaling_capable, v_max_capacity
    FROM cmbc.resource_pools
    WHERE pool_id = p_pool_id;

    IF NOT v_scaling_capable THEN
        p_success := FALSE;
        p_message := 'Pool does not support auto-scaling';
        RETURN;
    END IF;

    IF p_target_capacity > v_max_capacity THEN
        p_success := FALSE;
        p_message := 'Target capacity exceeds maximum scalable capacity';
        RETURN;
    END IF;

    -- In production, this would trigger cloud APIs or vendor systems
    -- Here we simulate scaling with a 90% success rate

    IF random() < 0.9 THEN
        UPDATE cmbc.resource_pools
        SET
            total_capacity = p_target_capacity,
            updated_at = CURRENT_TIMESTAMP
        WHERE
            pool_id = p_pool_id;

        p_success := TRUE;
        p_message := 'Pool successfully scaled to ' || p_target_capacity;
    ELSE
        p_success := FALSE;
        p_message := 'Scaling operation failed due to simulated provider issue';
    END IF;

    -- Log scaling attempt
    INSERT INTO cmbc.pool_scaling_events (
        pool_id,
        target_capacity,
        success,
        message,
        initiated_by
    ) VALUES (
        p_pool_id,
        p_target_capacity,
        p_success,
        p_message,
        'system'
    );

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_scale_resource_pool IS 'Manages dynamic scaling of resource pools to meet changing capacity demands';


-- ai-powered decision support system
CREATE TABLE cmbc.decision_support_models (
    model_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    model_name VARCHAR(255) NOT NULL,
    model_version VARCHAR(50) NOT NULL,
    model_type VARCHAR(100) NOT NULL CHECK (model_type IN ('ImpactAnalysis', 'ResourceAllocation', 'CommunicationStrategy')),
    training_data_range DATERANGE NOT NULL,
    accuracy_metrics JSONB NOT NULL,
    last_retrained TIMESTAMP WITH TIME ZONE,
    input_parameters JSONB NOT NULL,
    output_schema JSONB NOT NULL,
    is_active BOOLEAN NOT NULL DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.decision_support_models IS 'Stores metadata for AI models that provide real-time decision support during crises';

CREATE TABLE cmbc.model_recommendations (
    recommendation_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    incident_id UUID REFERENCES cmbc.incidents(incident_id),
    model_id UUID REFERENCES cmbc.decision_support_models(model_id),
    recommendation_time TIMESTAMP WITH TIME ZONE NOT NULL,
    confidence_score NUMERIC(5,2) NOT NULL CHECK (confidence_score BETWEEN 0 AND 1),
    recommended_actions JSONB NOT NULL,
    expected_outcomes JSONB,
    accepted BOOLEAN DEFAULT NULL,
    acceptance_time TIMESTAMP WITH TIME ZONE,
    accepted_by VARCHAR(100),
    actual_outcome JSONB,
    outcome_accuracy NUMERIC(5,2),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.model_recommendations IS 'Tracks AI-generated recommendations and their acceptance/outcomes for continuous improvement';


--virtual command center integration
CREATE TABLE cmbc.virtual_command_centers (
    vcc_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    vcc_name VARCHAR(255) NOT NULL,
    status VARCHAR(20) NOT NULL CHECK (status IN ('Active', 'Standby', 'Maintenance')),
    activation_time TIMESTAMP WITH TIME ZONE,
    deactivation_time TIMESTAMP WITH TIME ZONE,
    max_participants INTEGER NOT NULL,
    current_participants INTEGER NOT NULL DEFAULT 0,
    incident_id UUID REFERENCES cmbc.incidents(incident_id),
    collaboration_tools JSONB NOT NULL COMMENT 'Integrated tools like chat, video, document sharing',
    access_controls JSONB NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.virtual_command_centers IS 'Manages virtual collaboration spaces for crisis response teams';

CREATE TABLE cmbc.vcc_participants (
    participant_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    vcc_id UUID NOT NULL REFERENCES cmbc.virtual_command_centers(vcc_id),
    user_id VARCHAR(100) NOT NULL,
    role VARCHAR(100) NOT NULL,
    join_time TIMESTAMP WITH TIME ZONE NOT NULL,
    leave_time TIMESTAMP WITH TIME ZONE,
    active_duration INTERVAL GENERATED ALWAYS AS (
        CASE WHEN leave_time IS NULL THEN NULL ELSE leave_time - join_time END
    ) STORED,
    contributions JSONB COMMENT 'Chat messages, shared files, etc.',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.vcc_participants IS 'Tracks participants in virtual command centers and their contributions';

--automated impact assessment engine
CREATE TABLE cmbc.impact_assessment_templates (
    template_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    template_name VARCHAR(255) NOT NULL,
    incident_type VARCHAR(100) NOT NULL,
    severity_level VARCHAR(20) NOT NULL,
    assessment_parameters JSONB NOT NULL COMMENT 'Fields and metrics to assess',
    weighting_factors JSONB NOT NULL COMMENT 'Relative importance of each parameter',
    scoring_algorithm TEXT NOT NULL,
    version VARCHAR(50) NOT NULL,
    is_active BOOLEAN NOT NULL DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.impact_assessment_templates IS 'Standardized templates for assessing incident impacts consistently';

CREATE TABLE cmbc.automated_assessments (
    assessment_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    incident_id UUID NOT NULL REFERENCES cmbc.incidents(incident_id),
    template_id UUID REFERENCES cmbc.impact_assessment_templates(template_id),
    assessment_time TIMESTAMP WITH TIME ZONE NOT NULL,
    raw_data_sources JSONB NOT NULL,
    calculated_impacts JSONB NOT NULL,
    overall_score NUMERIC(10,2) NOT NULL,
    confidence_score NUMERIC(5,2) NOT NULL,
    manual_override BOOLEAN NOT NULL DEFAULT FALSE,
    override_reason TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.automated_assessments IS 'Stores results of automated impact assessments with data provenance';


-- AI recommendation performance view
CREATE OR REPLACE VIEW cmbc.vw_ai_recommendation_performance AS
SELECT
    dsm.model_name,
    dsm.model_type,
    COUNT(mr.recommendation_id) AS total_recommendations,
    AVG(mr.confidence_score) AS avg_confidence,
    SUM(CASE WHEN mr.accepted = TRUE THEN 1 ELSE 0 END) AS accepted_count,
    SUM(CASE WHEN mr.accepted = FALSE THEN 1 ELSE 0 END) AS rejected_count,
    AVG(mr.outcome_accuracy) FILTER (WHERE mr.outcome_accuracy IS NOT NULL) AS avg_accuracy,
    COUNT(DISTINCT mr.incident_id) AS incidents_served,
    MAX(mr.recommendation_time) AS latest_recommendation
FROM
    cmbc.model_recommendations mr
JOIN
    cmbc.decision_support_models dsm ON mr.model_id = dsm.model_id
GROUP BY
    dsm.model_name, dsm.model_type;

COMMENT ON VIEW cmbc.vw_ai_recommendation_performance IS 'Tracks performance metrics for AI decision support models to guide improvements';

-- virtual command center activity view
CREATE OR REPLACE VIEW cmbc.vw_vcc_activity AS
SELECT
    vcc.vcc_id,
    vcc.vcc_name,
    vcc.status,
    vcc.activation_time,
    vcc.current_participants,
    i.incident_name,
    i.severity,
    COUNT(DISTINCT vp.user_id) AS unique_participants,
    MAX(vp.active_duration) AS max_session_duration,
    AVG(EXTRACT(EPOCH FROM vp.active_duration)/60) AS avg_session_minutes,
    COUNT(DIST CASE WHEN vp.contributions IS NOT NULL THEN vp.participant_id END) AS active_contributors
FROM
    cmbc.virtual_command_centers vcc
LEFT JOIN
    cmbc.incidents i ON vcc.incident_id = i.incident_id
LEFT JOIN
    cmbc.vcc_participants vp ON vcc.vcc_id = vp.vcc_id
GROUP BY
    vcc.vcc_id, vcc.vcc_name, vcc.status, vcc.activation_time,
    vcc.current_participants, i.incident_name, i.severity;

COMMENT ON VIEW cmbc.vw_vcc_activity IS 'Provides insights into virtual command center utilization and collaboration patterns';

--incident impact trend analysis
CREATE MATERIALIZED VIEW cmbc.mv_impact_trends AS
SELECT
    DATE_TRUNC('month', ia.assessment_time) AS month,
    iat.incident_type,
    AVG(ia.overall_score) AS avg_impact_score,
    STDDEV(ia.overall_score) AS impact_volatility,
    COUNT(DISTINCT ia.incident_id) AS incidents_assessed,
    PERCENTILE_CONT(0.9) WITHIN GROUP (ORDER BY ia.overall_score) AS p90_impact,
    ARRAY_AGG(DISTINCT i.severity) AS severity_levels,
    COUNT(DIST CASE WHEN ia.manual_override = TRUE THEN ia.assessment_id END) AS manual_overrides
FROM
    cmbc.automated_assessments ia
JOIN
    cmbc.impact_assessment_templates iat ON ia.template_id = iat.template_id
JOIN
    cmbc.incidents i ON ia.incident_id = i.incident_id
WHERE
    ia.assessment_time > CURRENT_DATE - INTERVAL '1 year'
GROUP BY
    DATE_TRUNC('month', ia.assessment_time), iat.incident_type;

COMMENT ON MATERIALIZED VIEW cmbc.mv_impact_trends IS 'Analyzes trends in incident impacts over time by type and severity';

--decision modele ffectiveness
CREATE MATERIALIZED VIEW cmbc.mv_model_effectiveness AS
WITH outcome_analysis AS (
    SELECT
        mr.model_id,
        dsm.model_name,
        dsm.model_type,
        COUNT(*) FILTER (WHERE mr.accepted = TRUE AND mr.outcome_accuracy > 0.8) AS high_quality_acceptances,
        COUNT(*) FILTER (WHERE mr.accepted = TRUE) AS total_acceptances,
        COUNT(*) FILTER (WHERE mr.accepted = FALSE AND mr.outcome_accuracy < 0.5) AS good_rejections,
        COUNT(*) FILTER (WHERE mr.accepted = FALSE) AS total_rejections,
        AVG(mr.outcome_accuracy) FILTER (WHERE mr.outcome_accuracy IS NOT NULL) AS avg_accuracy
    FROM
        cmbc.model_recommendations mr
    JOIN
        cmbc.decision_support_models dsm ON mr.model_id = dsm.model_id
    GROUP BY
        mr.model_id, dsm.model_name, dsm.model_type
)
SELECT
    *,
    CASE
        WHEN total_acceptances = 0 THEN 0
        ELSE (high_quality_acceptances::FLOAT / total_acceptances) * 100
    END AS acceptance_quality_pct,
    CASE
        WHEN total_rejections = 0 THEN 0
        ELSE (good_rejections::FLOAT / total_rejections) * 100
    END AS rejection_quality_pct
FROM
    outcome_analysis;

COMMENT ON MATERIALIZED VIEW cmbc.mv_model_effectiveness IS 'Evaluates the effectiveness of AI decision models by analyzing acceptance patterns and outcomes';

--AI decision support orchestrator
CREATE OR REPLACE PROCEDURE cmbc.sp_generate_ai_recommendations(
    p_incident_id UUID,
    p_model_types VARCHAR(100)[] DEFAULT NULL,
    OUT p_recommendations_generated INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_incident_type VARCHAR(100);
    v_severity VARCHAR(20);
BEGIN
    -- Get incident details
    SELECT incident_type, severity INTO v_incident_type, v_severity
    FROM cmbc.incidents WHERE incident_id = p_incident_id;

    -- Generate recommendations from relevant models
    INSERT INTO cmbc.model_recommendations (
        incident_id,
        model_id,
        recommendation_time,
        confidence_score,
        recommended_actions,
        expected_outcomes
    )
    SELECT
        p_incident_id,
        m.model_id,
        CURRENT_TIMESTAMP,
        -- Simulate confidence score based on incident severity
        CASE
            WHEN v_severity = 'Critical' THEN 0.7 + random()*0.3
            WHEN v_severity = 'High' THEN 0.5 + random()*0.3
            ELSE 0.3 + random()*0.4
        END,
        -- Simulate recommended actions based on model type
        CASE m.model_type
            WHEN 'ImpactAnalysis' THEN
                jsonb_build_array(
                    jsonb_build_object(
                        'action', 'Conduct detailed impact assessment',
                        'priority', 'High',
                        'resources', jsonb_build_array('AssessmentTeam')
                )
            WHEN 'ResourceAllocation' THEN
                jsonb_build_array(
                    jsonb_build_object(
                        'action', 'Activate backup resources',
                        'priority', 'Critical',
                        'resources', jsonb_build_array('BackupSite', 'CloudCapacity')
                )
            WHEN 'CommunicationStrategy' THEN
                jsonb_build_array(
                    jsonb_build_object(
                        'action', 'Issue stakeholder notification',
                        'priority', 'Medium',
                        'channels', jsonb_build_array('Email', 'SMS')
                )
        END,
        -- Simulate expected outcomes
        jsonb_build_object(
            'time_to_resolution',
            CASE
                WHEN v_severity = 'Critical' THEN '4-8 hours'
                WHEN v_severity = 'High' THEN '24-48 hours'
                ELSE '3-5 days'
            END,
            'success_probability',
            CASE
                WHEN v_severity = 'Critical' THEN '60-70%'
                WHEN v_severity = 'High' THEN '75-85%'
                ELSE '90-95%'
            END
        )
    FROM
        cmbc.decision_support_models m
    WHERE
        m.is_active = TRUE
        AND (p_model_types IS NULL OR m.model_type = ANY(p_model_types));

    -- Get count of generated recommendations
    GET DIAGNOSTICS p_recommendations_generated = ROW_COUNT;

    -- Log recommendation generation
    INSERT INTO cmbc.incident_actions (
        incident_id,
        action_type,
        action_details,
        performed_by,
        performed_at
    ) VALUES (
        p_incident_id,
        'AIRecommendationGenerated',
        jsonb_build_object('count', p_recommendations_generated, 'model_types', COALESCE(p_model_types, ARRAY['All'])),
        'system',
        CURRENT_TIMESTAMP
    );

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_generate_ai_recommendations IS 'Orchestrates the generation of AI-powered recommendations for incident response based on incident characteristics';


--virtual command center auto-scaler
CREATE OR REPLACE PROCEDURE cmbc.sp_scale_virtual_command_center(
    p_vcc_id UUID,
    OUT p_action_taken BOOLEAN,
    OUT p_message TEXT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_current_participants INTEGER;
    v_max_participants INTEGER;
    v_incident_severity VARCHAR(20);
    v_participant_growth_rate NUMERIC(10,2);
    v_recommended_capacity INTEGER;
BEGIN
    -- Get current VCC status
    SELECT
        current_participants,
        max_participants,
        i.severity
    INTO
        v_current_participants,
        v_max_participants,
        v_incident_severity
    FROM
        cmbc.virtual_command_centers vcc
    JOIN
        cmbc.incidents i ON vcc.incident_id = i.incident_id
    WHERE
        vcc.vcc_id = p_vcc_id;

    -- Calculate participant growth rate (simplified)
    SELECT
        COUNT(*)::NUMERIC / EXTRACT(EPOCH FROM (MAX(join_time) - MIN(join_time))/60
    INTO
        v_participant_growth_rate
    FROM
        cmbc.vcc_participants
    WHERE
        vcc_id = p_vcc_id
        AND join_time > CURRENT_TIMESTAMP - INTERVAL '1 hour';

    -- Determine recommended capacity based on severity and growth
    v_recommended_capacity :=
        CASE
            WHEN v_incident_severity = 'Critical' THEN LEAST(v_current_participants * 2, 500)
            WHEN v_incident_severity = 'High' THEN LEAST(v_current_participants * 1.5, 300)
            ELSE LEAST(v_current_participants * 1.2, 200)
        END;

    -- Only scale if needed and different from current max
    IF v_recommended_capacity > v_max_participants THEN
        UPDATE cmbc.virtual_command_centers
        SET
            max_participants = v_recommended_capacity,
            updated_at = CURRENT_TIMESTAMP
        WHERE
            vcc_id = p_vcc_id;

        p_action_taken := TRUE;
        p_message := 'Scaled VCC capacity from ' || v_max_participants || ' to ' || v_recommended_capacity;

        -- Log scaling event
        INSERT INTO cmbc.vcc_scaling_events (
            vcc_id,
            old_capacity,
            new_capacity,
            reason,
            metrics
        ) VALUES (
            p_vcc_id,
            v_max_participants,
            v_recommended_capacity,
            'Auto-scale based on participant growth',
            jsonb_build_object(
                'current_participants', v_current_participants,
                'growth_rate', v_participant_growth_rate,
                'incident_severity', v_incident_severity
            )
        );
    ELSE
        p_action_taken := FALSE;
        p_message := 'No scaling needed - current capacity sufficient';
    END IF;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE cmbc.sp_scale_virtual_command_center IS 'Automatically adjusts virtual command center capacity based on participant growth and incident severity';

--predictive maintenance integration
CREATE TABLE cmbc.predictive_maintenance_alerts (
    alert_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    equipment_id VARCHAR(100) NOT NULL,
    equipment_type VARCHAR(100) NOT NULL,
    failure_mode VARCHAR(255) NOT NULL,
    predicted_failure_time TIMESTAMP WITH TIME ZONE NOT NULL,
    confidence_score NUMERIC(5,2) NOT NULL,
    recommended_actions TEXT[] NOT NULL,
    maintenance_window_start TIMESTAMP WITH TIME ZONE,
    maintenance_window_end TIMESTAMP WITH TIME ZONE,
    status VARCHAR(20) NOT NULL CHECK (status IN ('Pending', 'Scheduled', 'Completed', 'Dismissed')),
    actual_failure_time TIMESTAMP WITH TIME ZONE,
    prediction_accuracy VARCHAR(20),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE cmbc.predictive_maintenance_alerts IS 'Tracks predicted equipment failures from AI models to prevent operational disruptions';

----------------------ual
-- ESG Risk Management
----------------------

CREATE SCHEMA esg;


CREATE TABLE esg.risk_categories (
    category_id SERIAL PRIMARY KEY,
    category_name VARCHAR(100) NOT NULL,
    category_type VARCHAR(50) NOT NULL CHECK (category_type IN ('Environmental', 'Social', 'Governance')),
    description TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.risk_categories IS 'Master table for ESG risk categories (Environmental, Social, Governance)';
COMMENT ON COLUMN esg.risk_categories.category_type IS 'Type of ESG risk: Environmental, Social, or Governance';


CREATE TABLE esg.risks (
    risk_id SERIAL PRIMARY KEY,
    category_id INTEGER REFERENCES esg.risk_categories(category_id),
    risk_name VARCHAR(200) NOT NULL,
    risk_description TEXT,
    risk_owner VARCHAR(100),
    materiality_level VARCHAR(20) CHECK (materiality_level IN ('Low', 'Medium', 'High')),
    risk_exposure_score INTEGER CHECK (risk_exposure_score BETWEEN 1 AND 10),
    mitigation_strategy TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.risks IS 'Detailed ESG risks with categorization and scoring';
COMMENT ON COLUMN esg.risks.materiality_level IS 'Materiality level of the risk (Low, Medium, High)';
COMMENT ON COLUMN esg.risks.risk_exposure_score IS 'Risk exposure score from 1 (low) to 10 (high)';


CREATE TABLE esg.risk_assessments (
    assessment_id SERIAL PRIMARY KEY,
    risk_id INTEGER REFERENCES esg.risks(risk_id),
    assessment_date DATE NOT NULL,
    assessed_by VARCHAR(100) NOT NULL,
    likelihood_score INTEGER CHECK (likelihood_score BETWEEN 1 AND 5),
    impact_score INTEGER CHECK (impact_score BETWEEN 1 AND 5),
    overall_score INTEGER GENERATED ALWAYS AS (likelihood_score * impact_score) STORED,
    assessment_notes TEXT,
    next_review_date DATE,
    status VARCHAR(20) CHECK (status IN ('Open', 'In Progress', 'Mitigated', 'Closed')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.risk_assessments IS 'Detailed assessments of ESG risks with scoring';
COMMENT ON COLUMN esg.risk_assessments.likelihood_score IS 'Likelihood score from 1 (rare) to 5 (almost certain)';
COMMENT ON COLUMN esg.risk_assessments.impact_score IS 'Impact score from 1 (minor) to 5 (severe)';


CREATE TABLE esg.data_collection (
    data_id SERIAL PRIMARY KEY,
    risk_id INTEGER REFERENCES esg.risks(risk_id),
    data_source VARCHAR(200) NOT NULL,
    collection_method VARCHAR(100),
    collection_frequency VARCHAR(50),
    reporting_standard VARCHAR(50) CHECK (reporting_standard IN ('GRI', 'SASB', 'TCFD', 'CSRD', 'Other')),
    data_accuracy DECIMAL(5,2) CHECK (data_accuracy BETWEEN 0 AND 100),
    last_collection_date DATE,
    next_collection_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.data_collection IS 'Tracking of ESG data collection processes and standards';
COMMENT ON COLUMN esg.data_collection.data_accuracy IS 'Estimated accuracy percentage of the collected data';


CREATE TABLE esg.stakeholders (
    stakeholder_id SERIAL PRIMARY KEY,
    stakeholder_name VARCHAR(200) NOT NULL,
    stakeholder_type VARCHAR(50) CHECK (stakeholder_type IN ('Investor', 'Employee', 'Customer', 'Supplier', 'Regulator', 'Community', 'Other')),
    contact_email VARCHAR(100),
    contact_phone VARCHAR(20),
    engagement_level VARCHAR(20) CHECK (engagement_level IN ('Low', 'Medium', 'High')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.stakeholders IS 'List of stakeholders involved in ESG matters';

CREATE TABLE esg.stakeholder_engagement (
    engagement_id SERIAL PRIMARY KEY,
    stakeholder_id INTEGER REFERENCES esg.stakeholders(stakeholder_id),
    engagement_date DATE NOT NULL,
    engagement_type VARCHAR(100) NOT NULL,
    engagement_topic VARCHAR(200),
    satisfaction_score INTEGER CHECK (satisfaction_score BETWEEN 1 AND 10),
    notes TEXT,
    follow_up_required BOOLEAN DEFAULT FALSE,
    follow_up_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.stakeholder_engagement IS 'Records of stakeholder engagement activities';
COMMENT ON COLUMN esg.stakeholder_engagement.satisfaction_score IS 'Stakeholder satisfaction score from 1 (low) to 10 (high)';


CREATE TABLE esg.performance_metrics (
    metric_id SERIAL PRIMARY KEY,
    metric_name VARCHAR(200) NOT NULL,
    metric_description TEXT,
    category_id INTEGER REFERENCES esg.risk_categories(category_id),
    unit_of_measure VARCHAR(50),
    target_value DECIMAL(15,2),
    reporting_frequency VARCHAR(50),
    benchmark_value DECIMAL(15,2),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.performance_metrics IS 'Definition of ESG performance metrics and KPIs';


CREATE TABLE esg.performance_data (
    performance_id SERIAL PRIMARY KEY,
    metric_id INTEGER REFERENCES esg.performance_metrics(metric_id),
    reporting_period DATE NOT NULL,
    actual_value DECIMAL(15,2) NOT NULL,
    data_source VARCHAR(200),
    notes TEXT,
    verification_status VARCHAR(20) CHECK (verification_status IN ('Unverified', 'In Review', 'Verified')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_metric_period UNIQUE (metric_id, reporting_period)
);

COMMENT ON TABLE esg.performance_data IS 'Actual performance data for ESG metrics';


CREATE TABLE esg.initiatives (
    initiative_id SERIAL PRIMARY KEY,
    initiative_name VARCHAR(200) NOT NULL,
    description TEXT,
    start_date DATE,
    end_date DATE,
    budget DECIMAL(15,2),
    status VARCHAR(20) CHECK (status IN ('Planned', 'In Progress', 'Completed', 'On Hold')),
    responsible_party VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.initiatives IS 'ESG improvement initiatives and projects';


CREATE TABLE esg.initiative_metrics (
    initiative_metric_id SERIAL PRIMARY KEY,
    initiative_id INTEGER REFERENCES esg.initiatives(initiative_id),
    metric_id INTEGER REFERENCES esg.performance_metrics(metric_id),
    target_improvement DECIMAL(15,2),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_initiative_metric UNIQUE (initiative_id, metric_id)
);

COMMENT ON TABLE esg.initiative_metrics IS 'Links between initiatives and the metrics they aim to improve';


CREATE TABLE esg.initiative_metrics (
    initiative_metric_id SERIAL PRIMARY KEY,
    initiative_id INTEGER REFERENCES esg.initiatives(initiative_id),
    metric_id INTEGER REFERENCES esg.performance_metrics(metric_id),
    target_improvement DECIMAL(15,2),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_initiative_metric UNIQUE (initiative_id, metric_id)
);

COMMENT ON TABLE esg.initiative_metrics IS 'Links between initiatives and the metrics they aim to improve';


CREATE TABLE esg.suppliers (
    supplier_id SERIAL PRIMARY KEY,
    supplier_name VARCHAR(200) NOT NULL,
    industry_sector VARCHAR(100),
    country_code VARCHAR(2),
    esg_rating VARCHAR(10),
    last_assessment_date DATE,
    risk_level VARCHAR(20) CHECK (risk_level IN ('Low', 'Medium', 'High')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.suppliers IS 'Third-party suppliers with ESG information';


CREATE TABLE esg.supplier_assessments (
    assessment_id SERIAL PRIMARY KEY,
    supplier_id INTEGER REFERENCES esg.suppliers(supplier_id),
    assessment_date DATE NOT NULL,
    assessment_type VARCHAR(100) NOT NULL,
    overall_score DECIMAL(5,2) CHECK (overall_score BETWEEN 0 AND 100),
    labor_practices_score DECIMAL(5,2),
    environmental_score DECIMAL(5,2),
    ethics_score DECIMAL(5,2),
    notes TEXT,
    next_assessment_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.supplier_assessments IS 'Detailed ESG assessments of suppliers';


CREATE TABLE esg.regulatory_requirements (
    regulation_id SERIAL PRIMARY KEY,
    regulation_name VARCHAR(200) NOT NULL,
    jurisdiction VARCHAR(100) NOT NULL,
    effective_date DATE,
    description TEXT,
    compliance_deadline DATE,
    reporting_requirements TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.regulatory_requirements IS 'ESG-related regulations and compliance requirements';


CREATE TABLE esg.compliance_status (
    compliance_id SERIAL PRIMARY KEY,
    regulation_id INTEGER REFERENCES esg.regulatory_requirements(regulation_id),
    status VARCHAR(20) CHECK (status IN ('Not Started', 'In Progress', 'Compliant', 'Non-Compliant')),
    compliance_date DATE,
    evidence TEXT,
    notes TEXT,
    next_review_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_regulation_status UNIQUE (regulation_id)
);

COMMENT ON TABLE esg.compliance_status IS 'Current compliance status for each regulation';


CREATE TABLE esg.resilience_metrics (
    resilience_id SERIAL PRIMARY KEY,
    metric_name VARCHAR(200) NOT NULL,
    description TEXT,
    category_id INTEGER REFERENCES esg.risk_categories(category_id),
    target_value DECIMAL(15,2),
    measurement_frequency VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.resilience_metrics IS 'Metrics for assessing organizational resilience to ESG risks';

CREATE TABLE esg.resilience_data (
    data_id SERIAL PRIMARY KEY,
    resilience_id INTEGER REFERENCES esg.resilience_metrics(resilience_id),
    measurement_date DATE NOT NULL,
    actual_value DECIMAL(15,2) NOT NULL,
    notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_resilience_measurement UNIQUE (resilience_id, measurement_date)
);

COMMENT ON TABLE esg.resilience_data IS 'Actual measurements of resilience metrics';


--ESG Risk dashboard view
CREATE OR REPLACE VIEW esg.vw_risk_dashboard AS
SELECT
    r.risk_id,
    rc.category_name,
    r.risk_name,
    r.materiality_level,
    r.risk_exposure_score,
    ra.likelihood_score,
    ra.impact_score,
    ra.overall_score,
    ra.status,
    ra.next_review_date
FROM
    esg.risks r
JOIN
    esg.risk_categories rc ON r.category_id = rc.category_id
LEFT JOIN
    (SELECT risk_id, likelihood_score, impact_score, overall_score, status, next_review_date
     FROM esg.risk_assessments
     WHERE assessment_date = (SELECT MAX(assessment_date)
                             FROM esg.risk_assessments ra2
                             WHERE ra2.risk_id = esg.risk_assessments.risk_id)) ra
    ON r.risk_id = ra.risk_id;

COMMENT ON VIEW esg.vw_risk_dashboard IS 'Provides a comprehensive view of all ESG risks with their latest assessment scores';


--ESG Performance Tracking view
CREATE OR REPLACE VIEW esg.vw_performance_tracking AS
SELECT
    pm.metric_id,
    pm.metric_name,
    rc.category_name,
    pm.unit_of_measure,
    pm.target_value,
    pm.benchmark_value,
    pd.reporting_period,
    pd.actual_value,
    ((pd.actual_value - LAG(pd.actual_value) OVER (PARTITION BY pm.metric_id ORDER BY pd.reporting_period)) /
     NULLIF(LAG(pd.actual_value) OVER (PARTITION BY pm.metric_id ORDER BY pd.reporting_period), 0)) * 100 AS yoy_change_percent,
    CASE
        WHEN pm.target_value IS NOT NULL THEN
            CASE
                WHEN pd.actual_value >= pm.target_value THEN 'Met'
                ELSE 'Not Met'
            END
        ELSE NULL
    END AS target_status
FROM
    esg.performance_metrics pm
JOIN
    esg.risk_categories rc ON pm.category_id = rc.category_id
JOIN
    esg.performance_data pd ON pm.metric_id = pd.metric_id
WHERE
    pd.reporting_period >= (CURRENT_DATE - INTERVAL '2 years');

COMMENT ON VIEW esg.vw_performance_tracking IS 'Tracks performance metrics against targets with year-over-year comparison';


--stakeholder engagement summary view
CREATE OR REPLACE VIEW esg.vw_stakeholder_engagement_summary AS
SELECT
    s.stakeholder_id,
    s.stakeholder_name,
    s.stakeholder_type,
    s.engagement_level,
    COUNT(e.engagement_id) AS total_engagements,
    AVG(e.satisfaction_score) AS avg_satisfaction_score,
    MAX(e.engagement_date) AS last_engagement_date,
    SUM(CASE WHEN e.follow_up_required THEN 1 ELSE 0 END) AS pending_follow_ups
FROM
    esg.stakeholders s
LEFT JOIN
    esg.stakeholder_engagement e ON s.stakeholder_id = e.stakeholder_id
GROUP BY
    s.stakeholder_id, s.stakeholder_name, s.stakeholder_type, s.engagement_level;

COMMENT ON VIEW esg.vw_stakeholder_engagement_summary IS 'Summarizes engagement activities and satisfaction by stakeholder';

--supplier esg risk view
CREATE OR REPLACE VIEW esg.vw_supplier_esg_risk AS
SELECT
    s.supplier_id,
    s.supplier_name,
    s.industry_sector,
    s.country_code,
    s.esg_rating,
    s.risk_level,
    sa.assessment_date AS last_assessment_date,
    sa.overall_score,
    sa.labor_practices_score,
    sa.environmental_score,
    sa.ethics_score,
    CASE
        WHEN sa.overall_score >= 80 THEN 'Low Risk'
        WHEN sa.overall_score >= 50 THEN 'Medium Risk'
        ELSE 'High Risk'
    END AS risk_category
FROM
    esg.suppliers s
LEFT JOIN
    (SELECT supplier_id, assessment_date, overall_score, labor_practices_score, environmental_score, ethics_score
     FROM esg.supplier_assessments
     WHERE assessment_date = (SELECT MAX(assessment_date)
                             FROM esg.supplier_assessments sa2
                             WHERE sa2.supplier_id = esg.supplier_assessments.supplier_id)) sa
    ON s.supplier_id = sa.supplier_id;

COMMENT ON VIEW esg.vw_supplier_esg_risk IS 'Provides a risk assessment of suppliers based on ESG factors';

-- ESG regualatory compliance status view
CREATE OR REPLACE VIEW esg.vw_regulatory_compliance AS
SELECT
    rr.regulation_id,
    rr.regulation_name,
    rr.jurisdiction,
    rr.effective_date,
    rr.compliance_deadline,
    cs.status,
    cs.compliance_date,
    CASE
        WHEN cs.status = 'Compliant' THEN 'Compliant'
        WHEN cs.status = 'Non-Compliant' THEN 'Non-Compliant'
        WHEN CURRENT_DATE > rr.compliance_deadline THEN 'Overdue'
        ELSE 'Pending'
    END AS compliance_status,
    CASE
        WHEN cs.status = 'Compliant' THEN NULL
        WHEN cs.status = 'Non-Compliant' THEN NULL
        ELSE AGE(rr.compliance_deadline, CURRENT_DATE)
    END AS days_remaining
FROM
    esg.regulatory_requirements rr
LEFT JOIN
    esg.compliance_status cs ON rr.regulation_id = cs.regulation_id;

COMMENT ON VIEW esg.vw_regulatory_compliance IS 'Shows compliance status for all ESG-related regulations';

--materialized view for high-risk ESG items
CREATE MATERIALIZED VIEW esg.mv_high_risk_items AS
SELECT
    r.risk_id,
    rc.category_name,
    r.risk_name,
    ra.likelihood_score,
    ra.impact_score,
    ra.overall_score,
    ra.assessment_date,
    ra.next_review_date
FROM
    esg.risks r
JOIN
    esg.risk_categories rc ON r.category_id = rc.category_id
JOIN
    esg.risk_assessments ra ON r.risk_id = ra.risk_id
WHERE
    ra.overall_score >= 12  -- High risk threshold (likelihood 3 * impact 4)
    AND ra.assessment_date = (SELECT MAX(assessment_date)
                             FROM esg.risk_assessments ra2
                             WHERE ra2.risk_id = ra.risk_id)
WITH DATA;

COMMENT ON MATERIALIZED VIEW esg.mv_high_risk_items IS 'Pre-aggregated view of high-risk ESG items for quick reference';

--materialized view for ESG performacne trends
CREATE MATERIALIZED VIEW esg.mv_performance_trends AS
SELECT
    pm.metric_id,
    pm.metric_name,
    rc.category_name,
    DATE_TRUNC('quarter', pd.reporting_period) AS quarter,
    AVG(pd.actual_value) AS avg_value,
    MIN(pd.actual_value) AS min_value,
    MAX(pd.actual_value) AS max_value,
    pm.target_value,
    pm.benchmark_value
FROM
    esg.performance_metrics pm
JOIN
    esg.risk_categories rc ON pm.category_id = rc.category_id
JOIN
    esg.performance_data pd ON pm.metric_id = pd.metric_id
WHERE
    pd.reporting_period >= (CURRENT_DATE - INTERVAL '3 years')
GROUP BY
    pm.metric_id, pm.metric_name, rc.category_name, DATE_TRUNC('quarter', pd.reporting_period), pm.target_value, pm.benchmark_value
WITH DATA;

COMMENT ON MATERIALIZED VIEW esg.mv_performance_trends IS 'Quarterly aggregated performance metrics for trend analysis';

-- stored procedures to add new ESG Risk Assessment
CREATE OR REPLACE PROCEDURE esg.sp_add_risk_assessment(
    p_risk_id INTEGER,
    p_assessed_by VARCHAR(100),
    p_likelihood_score INTEGER,
    p_impact_score INTEGER,
    p_assessment_notes TEXT,
    p_next_review_date DATE
)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Validate scores
    IF p_likelihood_score < 1 OR p_likelihood_score > 5 THEN
        RAISE EXCEPTION 'Likelihood score must be between 1 and 5';
    END IF;

    IF p_impact_score < 1 OR p_impact_score > 5 THEN
        RAISE EXCEPTION 'Impact score must be between 1 and 5';
    END IF;

    -- Insert new assessment
    INSERT INTO esg.risk_assessments (
        risk_id,
        assessment_date,
        assessed_by,
        likelihood_score,
        impact_score,
        assessment_notes,
        next_review_date,
        status
    ) VALUES (
        p_risk_id,
        CURRENT_DATE,
        p_assessed_by,
        p_likelihood_score,
        p_impact_score,
        p_assessment_notes,
        p_next_review_date,
        'Open'
    );

    -- Update risk exposure score in risks table based on new assessment
    UPDATE esg.risks
    SET risk_exposure_score = CASE
                                WHEN (p_likelihood_score * p_impact_score) >= 16 THEN 10
                                WHEN (p_likelihood_score * p_impact_score) >= 12 THEN 8
                                WHEN (p_likelihood_score * p_impact_score) >= 8 THEN 6
                                WHEN (p_likelihood_score * p_impact_score) >= 4 THEN 4
                                ELSE 2
                              END,
        updated_at = CURRENT_TIMESTAMP
    WHERE risk_id = p_risk_id;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE esg.sp_add_risk_assessment IS 'Adds a new risk assessment and updates the risk exposure score';

-- procedure to update ESG performance data
CREATE OR REPLACE PROCEDURE esg.sp_update_performance_data(
    p_metric_id INTEGER,
    p_reporting_period DATE,
    p_actual_value DECIMAL(15,2),
    p_data_source VARCHAR(200),
    p_notes TEXT,
    p_verification_status VARCHAR(20)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Validate verification status
    IF p_verification_status NOT IN ('Unverified', 'In Review', 'Verified') THEN
        RAISE EXCEPTION 'Invalid verification status';
    END IF;

    -- Check if record exists
    IF EXISTS (SELECT 1 FROM esg.performance_data
               WHERE metric_id = p_metric_id AND reporting_period = p_reporting_period) THEN
        -- Update existing record
        UPDATE esg.performance_data
        SET
            actual_value = p_actual_value,
            data_source = p_data_source,
            notes = p_notes,
            verification_status = p_verification_status,
            updated_at = CURRENT_TIMESTAMP
        WHERE
            metric_id = p_metric_id AND reporting_period = p_reporting_period;
    ELSE
        -- Insert new record
        INSERT INTO esg.performance_data (
            metric_id,
            reporting_period,
            actual_value,
            data_source,
            notes,
            verification_status
        ) VALUES (
            p_metric_id,
            p_reporting_period,
            p_actual_value,
            p_data_source,
            p_notes,
            p_verification_status
        );
    END IF;

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE esg.sp_update_performance_data IS 'Updates or inserts performance data for a specific metric and period';


-- procedure to generate ESG report
CREATE OR REPLACE PROCEDURE esg.sp_generate_esg_report(
    p_report_year INTEGER,
    p_report_type VARCHAR(50),
    OUT p_report_html TEXT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_report_start DATE;
    v_report_end DATE;
    v_summary_html TEXT := '';
    v_performance_html TEXT := '';
    v_risk_html TEXT := '';
    v_compliance_html TEXT := '';
BEGIN
    -- Set report date range
    v_report_start := TO_DATE(p_report_year || '-01-01', 'YYYY-MM-DD');
    v_report_end := TO_DATE(p_report_year || '-12-31', 'YYYY-MM-DD');

    -- Build summary section
    SELECT
        '<h2>ESG Performance Summary</h2>' ||
        '<p><strong>Year:</strong> ' || p_report_year || '</p>' ||
        '<p><strong>Report Generated:</strong> ' || CURRENT_DATE || '</p>' ||
        '<p><strong>Total Risks Tracked:</strong> ' || COUNT(*) || '</p>' ||
        '<p><strong>High Risks:</strong> ' || SUM(CASE WHEN overall_score >= 12 THEN 1 ELSE 0 END) || '</p>' ||
        '<p><strong>Medium Risks:</strong> ' || SUM(CASE WHEN overall_score BETWEEN 8 AND 11 THEN 1 ELSE 0 END) || '</p>' ||
        '<p><strong>Low Risks:</strong> ' || SUM(CASE WHEN overall_score < 8 THEN 1 ELSE 0 END) || '</p>'
    INTO v_summary_html
    FROM esg.vw_risk_dashboard;

    -- Build performance section
    SELECT
        '<h2>Key Performance Indicators</h2>' ||
        STRING_AGG(
            '<div class="kpi">' ||
            '<h3>' || metric_name || '</h3>' ||
            '<p>Category: ' || category_name || '</p>' ||
            '<p>Target: ' || COALESCE(target_value::TEXT, 'N/A') || ' ' || COALESCE(unit_of_measure, '') || '</p>' ||
            '<p>Actual: ' || actual_value || ' ' || COALESCE(unit_of_measure, '') || '</p>' ||
            '<p>Status: ' || target_status || '</p>' ||
            '</div>',
            ''
        )
    INTO v_performance_html
    FROM esg.vw_performance_tracking
    WHERE EXTRACT(YEAR FROM reporting_period) = p_report_year;

    -- Build risk section
    SELECT
        '<h2>Top ESG Risks</h2>' ||
        '<table border="1">' ||
        '<tr><th>Category</th><th>Risk</th><th>Likelihood</th><th>Impact</th><th>Score</th><th>Status</th></tr>' ||
        STRING_AGG(
            '<tr>' ||
            '<td>' || category_name || '</td>' ||
            '<td>' || risk_name || '</td>' ||
            '<td>' || likelihood_score || '</td>' ||
            '<td>' || impact_score || '</td>' ||
            '<td>' || overall_score || '</td>' ||
            '<td>' || status || '</td>' ||
            '</tr>',
            ''
        ) ||
        '</table>'
    INTO v_risk_html
    FROM esg.vw_risk_dashboard
    WHERE overall_score >= 8  -- Medium and high risks
    ORDER BY overall_score DESC
    LIMIT 10;

    -- Build compliance section
    SELECT
        '<h2>Regulatory Compliance</h2>' ||
        '<table border="1">' ||
        '<tr><th>Regulation</th><th>Jurisdiction</th><th>Status</th><th>Deadline</th></tr>' ||
        STRING_AGG(
            '<tr>' ||
            '<td>' || regulation_name || '</td>' ||
            '<td>' || jurisdiction || '</td>' ||
            '<td>' || compliance_status || '</td>' ||
            '<td>' || compliance_deadline || '</td>' ||
            '</tr>',
            ''
        ) ||
        '</table>'
    INTO v_compliance_html
    FROM esg.vw_regulatory_compliance;

    -- Combine all sections
    p_report_html :=
        '<!DOCTYPE html>' ||
        '<html>' ||
        '<head><title>ESG Report ' || p_report_year || '</title></head>' ||
        '<body>' ||
        '<h1>ESG Annual Report - ' || p_report_year || '</h1>' ||
        v_summary_html ||
        v_performance_html ||
        v_risk_html ||
        v_compliance_html ||
        '</body>' ||
        '</html>';

    COMMIT;
END;
$$;

COMMENT ON PROCEDURE esg.sp_generate_esg_report IS 'Generates an HTML ESG report for the specified year';


-- procedure to refresh materialized views
CREATE OR REPLACE PROCEDURE esg.sp_refresh_materialized_views()
LANGUAGE plpgsql
AS $$
BEGIN
    REFRESH MATERIALIZED VIEW esg.mv_high_risk_items;
    REFRESH MATERIALIZED VIEW esg.mv_performance_trends;
    COMMIT;
END;
$$;

COMMENT ON PROCEDURE esg.sp_refresh_materialized_views IS 'Refreshes all materialized views in the ESG schema';

-- Indexes for performance optimization
CREATE INDEX idx_risks_category ON esg.risks(category_id);
CREATE INDEX idx_risk_assessments_risk ON esg.risk_assessments(risk_id);
CREATE INDEX idx_risk_assessments_date ON esg.risk_assessments(assessment_date);
CREATE INDEX idx_performance_data_metric ON esg.performance_data(metric_id);
CREATE INDEX idx_performance_data_period ON esg.performance_data(reporting_period);
CREATE INDEX idx_stakeholder_engagement_stakeholder ON esg.stakeholder_engagement(stakeholder_id);
CREATE INDEX idx_supplier_assessments_supplier ON esg.supplier_assessments(supplier_id);
CREATE INDEX idx_compliance_status_regulation ON esg.compliance_status(regulation_id);
CREATE INDEX idx_resilience_data_metric ON esg.resilience_data(resilience_id);


---examples
-- adding a new risk assessment
CALL esg.sp_add_risk_assessment(
    p_risk_id := 5,
    p_assessed_by := 'John Smith',
    p_likelihood_score := 4,
    p_impact_score := 3,
    p_assessment_notes := 'Risk has increased due to new regulations',
    p_next_review_date := '2025-05-31'
);
--updating performance data
CALL esg.sp_update_performance_data(
    p_metric_id := 10,
    p_reporting_period := '2025-05-31',
    p_actual_value := 85.5,
    p_data_source := 'Internal Sustainability Report',
    p_notes := 'Improved due to new efficiency measures',
    p_verification_status := 'Verified'
);

--generating ESG report
DO $$
DECLARE
    v_report_html TEXT;
BEGIN
    CALL esg.sp_generate_esg_report(
        p_report_year := 2024,
        p_report_type := 'Annual',
        p_report_html := v_report_html
    );

    -- Save the report to a file or display it
    RAISE NOTICE 'Report generated: %', LEFT(v_report_html, 100) || '...';
END;
$$;


--querying high-risk items
SELECT * FROM esg.mv_high_risk_items
ORDER BY overall_score DESC;

--checking regulatory compliance status
SELECT * FROM esg.vw_regulatory_compliance
WHERE compliance_status != 'Compliant'
ORDER BY days_remaining;


---Climate Scenario Analysis
--Business Case: Organizations need to assess climate-related risks under different warming scenarios (1.5Â°C, 2Â°C, 3Â°C) to comply with TCFD recommendations and prepare transition plans.
CREATE TABLE esg.climate_scenarios (
    scenario_id SERIAL PRIMARY KEY,
    scenario_name VARCHAR(100) NOT NULL,
    warming_level DECIMAL(3,1) NOT NULL, -- Degrees Celsius
    time_horizon INTEGER NOT NULL, -- Year
    description TEXT,
    source VARCHAR(200),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.climate_scenario_impacts (
    impact_id SERIAL PRIMARY KEY,
    scenario_id INTEGER REFERENCES esg.climate_scenarios(scenario_id),
    risk_id INTEGER REFERENCES esg.risks(risk_id),
    impact_description TEXT NOT NULL,
    financial_impact DECIMAL(15,2), -- Monetary value
    operational_impact VARCHAR(100), -- High/Medium/Low
    probability DECIMAL(5,2) CHECK (probability BETWEEN 0 AND 1),
    adaptation_strategy TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.climate_scenarios IS 'Stores different climate change scenarios for risk assessment';
COMMENT ON TABLE esg.climate_scenario_impacts IS 'Links climate scenarios to specific risks and quantifies potential impacts';

--ESG incident tracking
-- Business Case: Companies must systematically track ESG-related incidents (spills, labor violations, governance failures) for regulatory reporting, remediation, and trend analysis.

CREATE TABLE esg.incidents (
    incident_id SERIAL PRIMARY KEY,
    incident_date TIMESTAMP WITH TIME ZONE NOT NULL,
    discovery_date TIMESTAMP WITH TIME ZONE NOT NULL,
    risk_id INTEGER REFERENCES esg.risks(risk_id),
    category_id INTEGER REFERENCES esg.risk_categories(category_id),
    severity VARCHAR(20) CHECK (severity IN ('Minor', 'Moderate', 'Major', 'Critical')),
    description TEXT NOT NULL,
    location VARCHAR(200),
    root_cause TEXT,
    status VARCHAR(20) CHECK (status IN ('Open', 'Investigating', 'Remediating', 'Closed', 'Litigation')),
    reporter VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.incident_remediation (
    remediation_id SERIAL PRIMARY KEY,
    incident_id INTEGER REFERENCES esg.incidents(incident_id),
    action_description TEXT NOT NULL,
    responsible_party VARCHAR(100) NOT NULL,
    due_date DATE,
    completion_date DATE,
    cost DECIMAL(15,2),
    effectiveness_rating INTEGER CHECK (effectiveness_rating BETWEEN 1 AND 5),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.incidents IS 'Tracks ESG-related incidents with full lifecycle management';
COMMENT ON TABLE esg.incident_remediation IS 'Records corrective actions taken to address ESG incidents';

--ESG Target Setting and Tracking
-- Business Case: Companies set science-based targets (SBTi) and need to track progress against commitments like net-zero, DEI goals, and governance improvements.
CREATE TABLE esg.targets (
    target_id SERIAL PRIMARY KEY,
    target_name VARCHAR(200) NOT NULL,
    metric_id INTEGER REFERENCES esg.performance_metrics(metric_id),
    target_type VARCHAR(50) CHECK (target_type IN ('Absolute', 'Intensity', 'Binary')),
    baseline_value DECIMAL(15,2),
    baseline_year INTEGER,
    target_value DECIMAL(15,2) NOT NULL,
    target_year INTEGER NOT NULL,
    science_based BOOLEAN DEFAULT FALSE,
    approved_by VARCHAR(100), -- e.g., "SBTi"
    commitment_public BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.target_progress (
    progress_id SERIAL PRIMARY KEY,
    target_id INTEGER REFERENCES esg.targets(target_id),
    reporting_date DATE NOT NULL,
    current_value DECIMAL(15,2) NOT NULL,
    on_track_status VARCHAR(20) CHECK (on_track_status IN ('On Track', 'Off Track', 'At Risk')),
    comments TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_target_reporting UNIQUE (target_id, reporting_date)
);

COMMENT ON TABLE esg.targets IS 'Stores ESG targets including science-based climate targets';
COMMENT ON TABLE esg.target_progress IS 'Tracks progress against ESG targets over time';

--ESG Audit Framework
-- Business Case: Independent verification of ESG performance and controls is critical for investor confidence and regulatory compliance.
CREATE TABLE esg.audit_programs (
    program_id SERIAL PRIMARY KEY,
    program_name VARCHAR(200) NOT NULL,
    audit_standard VARCHAR(100) NOT NULL, -- e.g., "AA1000", "ISO 14001"
    scope TEXT NOT NULL,
    frequency VARCHAR(50) NOT NULL,
    responsible_party VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.audits (
    audit_id SERIAL PRIMARY KEY,
    program_id INTEGER REFERENCES esg.audit_programs(program_id),
    audit_date DATE NOT NULL,
    lead_auditor VARCHAR(100) NOT NULL,
    status VARCHAR(20) CHECK (status IN ('Planned', 'In Progress', 'Completed', 'Closed')),
    conclusion VARCHAR(20) CHECK (conclusion IN ('Satisfactory', 'Minor Findings', 'Major Findings', 'Unsatisfactory')),
    report_location VARCHAR(200),
    next_audit_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.audit_findings (
    finding_id SERIAL PRIMARY KEY,
    audit_id INTEGER REFERENCES esg.audits(audit_id),
    risk_id INTEGER REFERENCES esg.risks(risk_id),
    finding_description TEXT NOT NULL,
    severity VARCHAR(20) CHECK (severity IN ('Minor', 'Moderate', 'Major', 'Critical')),
    root_cause TEXT,
    recommendation TEXT,
    due_date DATE,
    status VARCHAR(20) CHECK (status IN ('Open', 'In Progress', 'Resolved', 'Verified')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.audit_programs IS 'Defines ESG audit programs and standards';
COMMENT ON TABLE esg.audits IS 'Records individual audit instances and outcomes';
COMMENT ON TABLE esg.audit_findings IS 'Tracks specific findings from ESG audits with remediation tracking';


-- Climate Risk Exposure view
--Business Case: Executives need a consolidated view of physical and transition risks across different climate scenarios.
CREATE OR REPLACE VIEW esg.vw_climate_risk_exposure AS
SELECT
    r.risk_id,
    r.risk_name,
    rc.category_name,
    cs.scenario_name,
    cs.warming_level,
    cs.time_horizon,
    csi.financial_impact,
    csi.operational_impact,
    csi.probability,
    csi.adaptation_strategy,
    ra.overall_score AS current_risk_score
FROM
    esg.risks r
JOIN
    esg.risk_categories rc ON r.category_id = rc.category_id
JOIN
    esg.climate_scenario_impacts csi ON r.risk_id = csi.risk_id
JOIN
    esg.climate_scenarios cs ON csi.scenario_id = cs.scenario_id
LEFT JOIN
    (SELECT risk_id, overall_score
     FROM esg.risk_assessments
     WHERE assessment_date = (SELECT MAX(assessment_date) FROM esg.risk_assessments ra2 WHERE ra2.risk_id = esg.risk_assessments.risk_id)) ra
    ON r.risk_id = ra.risk_id
WHERE
    rc.category_type = 'Environmental';

COMMENT ON VIEW esg.vw_climate_risk_exposure IS 'Provides climate scenario analysis showing how risks evolve under different warming scenarios';

--ESG Incident Trend Analysis View

--Business Case: Risk managers need to identify patterns in ESG incidents to prioritize preventive measures.
CREATE OR REPLACE VIEW esg.vw_incident_trends AS
SELECT
    rc.category_name,
    DATE_TRUNC('quarter', i.incident_date) AS quarter,
    COUNT(*) AS incident_count,
    SUM(CASE WHEN severity = 'Critical' THEN 1 ELSE 0 END) AS critical_count,
    SUM(CASE WHEN severity = 'Major' THEN 1 ELSE 0 END) AS major_count,
    AVG(EXTRACT(EPOCH FROM (i.discovery_date - i.incident_date))/86400 AS avg_days_to_detect,
    AVG(EXTRACT(EPOCH FROM (r.completion_date - i.incident_date))/86400 AS avg_days_to_remediate
FROM
    esg.incidents i
JOIN
    esg.risk_categories rc ON i.category_id = rc.category_id
LEFT JOIN
    (SELECT incident_id, MAX(completion_date) AS completion_date
     FROM esg.incident_remediation
     WHERE completion_date IS NOT NULL
     GROUP BY incident_id) r ON i.incident_id = r.incident_id
WHERE
    i.incident_date >= CURRENT_DATE - INTERVAL '2 years'
GROUP BY
    rc.category_name, DATE_TRUNC('quarter', i.incident_date)
ORDER BY
    rc.category_name, quarter;

COMMENT ON VIEW esg.vw_incident_trends IS 'Analyzes ESG incident trends by category with detection and remediation metrics';


--ESG Target Performance View
--Business Case: Sustainability teams need to monitor progress against all ESG targets in one place with visual indicators.
CREATE OR REPLACE VIEW esg.vw_target_performance AS
SELECT
    t.target_id,
    t.target_name,
    pm.metric_name,
    pm.unit_of_measure,
    t.target_type,
    t.baseline_value,
    t.baseline_year,
    t.target_value,
    t.target_year,
    tp.reporting_date,
    tp.current_value,
    CASE
        WHEN t.target_type = 'Binary' THEN
            CASE WHEN tp.current_value >= t.target_value THEN 'Achieved' ELSE 'Not Achieved' END
        ELSE
            ROUND(((tp.current_value - t.baseline_value) / NULLIF((t.target_value - t.baseline_value), 0) * 100, 1) || '%'
    END AS progress_percentage,
    tp.on_track_status,
    t.science_based,
    t.commitment_public
FROM
    esg.targets t
JOIN
    esg.performance_metrics pm ON t.metric_id = pm.metric_id
JOIN
    (SELECT target_id, reporting_date, current_value, on_track_status
     FROM esg.target_progress
     WHERE reporting_date = (SELECT MAX(reporting_date) FROM esg.target_progress tp2 WHERE tp2.target_id = esg.target_progress.target_id)) tp
    ON t.target_id = tp.target_id;

COMMENT ON VIEW esg.vw_target_performance IS 'Shows current status of all ESG targets with progress calculations';


-- ESG Risk Heatmap Materialized View
--Business Case: The board requires a pre-aggregated risk matrix showing likelihood vs impact for all material ESG risks.
CREATE MATERIALIZED VIEW esg.mv_risk_heatmap AS
SELECT
    rc.category_name,
    ra.likelihood_score,
    ra.impact_score,
    COUNT(*) AS risk_count,
    STRING_AGG(r.risk_name, '; ' ORDER BY r.risk_name) AS risk_names
FROM
    esg.risks r
JOIN
    esg.risk_categories rc ON r.category_id = rc.category_id
JOIN
    (SELECT risk_id, likelihood_score, impact_score
     FROM esg.risk_assessments
     WHERE assessment_date = (SELECT MAX(assessment_date) FROM esg.risk_assessments ra2 WHERE ra2.risk_id = esg.risk_assessments.risk_id)) ra
    ON r.risk_id = ra.risk_id
WHERE
    r.materiality_level IN ('High', 'Medium')
GROUP BY
    rc.category_name, ra.likelihood_score, ra.impact_score
WITH DATA;

COMMENT ON MATERIALIZED VIEW esg.mv_risk_heatmap IS 'Pre-aggregated risk matrix for heatmap visualization by category';


--ESG Audit Findings Summary Materialized View
-- Business Case: Internal audit needs quick access to open findings by risk category to prioritize remediation.
CREATE MATERIALIZED VIEW esg.mv_audit_findings_summary AS
SELECT
    rc.category_name,
    af.severity,
    a.audit_date,
    COUNT(*) AS finding_count,
    SUM(CASE WHEN af.status != 'Resolved' THEN 1 ELSE 0 END) AS open_finding_count,
    MIN(af.due_date) AS earliest_due_date,
    STRING_AGG(DISTINCT ap.program_name, ', ') AS audit_programs
FROM
    esg.audit_findings af
JOIN
    esg.audits a ON af.audit_id = a.audit_id
JOIN
    esg.audit_programs ap ON a.program_id = ap.program_id
JOIN
    esg.risks r ON af.risk_id = r.risk_id
JOIN
    esg.risk_categories rc ON r.category_id = rc.category_id
WHERE
    af.status != 'Resolved'
GROUP BY
    rc.category_name, af.severity, a.audit_date
WITH DATA;

COMMENT ON MATERIALIZED VIEW esg.mv_audit_findings_summary IS 'Summarizes open audit findings by category and severity for remediation tracking';

-- ESG Risk Simulation procedure
--Business Case: Risk managers need to model how risk scores would change under different mitigation scenarios.

CREATE OR REPLACE PROCEDURE esg.sp_simulate_risk_impact(
    p_risk_id INTEGER,
    p_new_likelihood INTEGER,
    p_new_impact INTEGER,
    p_mitigation_cost DECIMAL(15,2),
    OUT p_current_score INTEGER,
    OUT p_proposed_score INTEGER,
    OUT p_risk_reduction INTEGER,
    OUT p_cost_per_risk_point DECIMAL(15,2)
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_current_ra RECORD;
BEGIN
    -- Get current risk assessment
    SELECT likelihood_score, impact_score, overall_score
    INTO v_current_ra
    FROM esg.risk_assessments
    WHERE risk_id = p_risk_id
    ORDER BY assessment_date DESC
    LIMIT 1;

    -- Validate inputs
    IF v_current_ra IS NULL THEN
        RAISE EXCEPTION 'No current assessment found for risk ID %', p_risk_id;
    END IF;

    IF p_new_likelihood < 1 OR p_new_likelihood > 5 THEN
        RAISE EXCEPTION 'Likelihood score must be between 1 and 5';
    END IF;

    IF p_new_impact < 1 OR p_new_impact > 5 THEN
        RAISE EXCEPTION 'Impact score must be between 1 and 5';
    END IF;

    -- Calculate outputs
    p_current_score := v_current_ra.overall_score;
    p_proposed_score := p_new_likelihood * p_new_impact;
    p_risk_reduction := p_current_score - p_proposed_score;

    IF p_risk_reduction > 0 THEN
        p_cost_per_risk_point := p_mitigation_cost / p_risk_reduction;
    ELSE
        p_cost_per_risk_point := NULL;
    END IF;
END;
$$;

COMMENT ON PROCEDURE esg.sp_simulate_risk_impact IS 'Simulates the effect of risk mitigation measures by calculating potential score improvements and cost per risk point reduced';

--ESG Data Quality Procedure Checks
--Business Case: Before ESG reporting, teams need to validate data completeness and accuracy across all metrics.
CREATE OR REPLACE PROCEDURE esg.sp_validate_esg_data(
    p_reporting_period DATE,
    OUT p_total_metrics INTEGER,
    OUT p_missing_data INTEGER,
    OUT p_unverified_data INTEGER,
    OUT p_completeness_percent DECIMAL(5,2),
    OUT p_accuracy_issues TEXT
)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Count total metrics
    SELECT COUNT(*) INTO p_total_metrics
    FROM esg.performance_metrics
    WHERE reporting_frequency IS NOT NULL;

    -- Count metrics missing data for the period
    SELECT COUNT(*) INTO p_missing_data
    FROM esg.performance_metrics pm
    WHERE pm.reporting_frequency IS NOT NULL
    AND NOT EXISTS (
        SELECT 1 FROM esg.performance_data pd
        WHERE pd.metric_id = pm.metric_id
        AND pd.reporting_period = p_reporting_period
    );

    -- Count unverified data points
    SELECT COUNT(*) INTO p_unverified_data
    FROM esg.performance_data
    WHERE reporting_period = p_reporting_period
    AND verification_status != 'Verified';

    -- Calculate completeness percentage
    p_completeness_percent := ((p_total_metrics - p_missing_data) * 100.0) / NULLIF(p_total_metrics, 0);

    -- Identify accuracy issues
    SELECT STRING_AGG(pm.metric_name, ', ' ORDER BY pm.metric_name) INTO p_accuracy_issues
    FROM esg.performance_data pd
    JOIN esg.performance_metrics pm ON pd.metric_id = pm.metric_id
    WHERE pd.reporting_period = p_reporting_period
    AND (
        (pd.actual_value < 0 AND pm.metric_name NOT LIKE '%Reduction%') OR
        (pd.actual_value > pm.target_value * 5 AND pm.target_value IS NOT NULL) OR
        (pd.data_source IS NULL)
    );
END;
$$;

COMMENT ON PROCEDURE esg.sp_validate_esg_data IS 'Validates ESG data quality for a reporting period by checking completeness, verification status, and potential accuracy issues';

--ESG Regulatory Change Impact Assessment
--Business Case: Legal teams need to assess which risks and controls are affected by new ESG regulations.
CREATE OR REPLACE PROCEDURE esg.sp_assess_regulatory_impact(
    p_regulation_id INTEGER,
    OUT p_affected_risks INTEGER,
    OUT p_high_impact_risks INTEGER,
    OUT p_existing_controls INTEGER,
    OUT p_gap_analysis TEXT
)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Count risks potentially affected by the regulation
    SELECT COUNT(*) INTO p_affected_risks
    FROM esg.risks r
    JOIN esg.risk_categories rc ON r.category_id = rc.category_id
    JOIN esg.regulatory_requirements rr ON
        (rr.regulation_id = p_regulation_id AND
         (rr.regulation_name ILIKE '%' || rc.category_name || '%' OR
          rr.description ILIKE '%' || rc.category_name || '%'));

    -- Count high materiality risks affected
    SELECT COUNT(*) INTO p_high_impact_risks
    FROM esg.risks r
    JOIN esg.risk_categories rc ON r.category_id = rc.category_id
    JOIN esg.regulatory_requirements rr ON
        (rr.regulation_id = p_regulation_id AND
         (rr.regulation_name ILIKE '%' || rc.category_name || '%' OR
          rr.description ILIKE '%' || rc.category_name || '%'))
    WHERE r.materiality_level = 'High';

    -- Count existing controls mapped to affected risks
    SELECT COUNT(DISTINCT r.risk_id) INTO p_existing_controls
    FROM esg.risks r
    JOIN esg.risk_categories rc ON r.category_id = rc.category_id
    JOIN esg.regulatory_requirements rr ON
        (rr.regulation_id = p_regulation_id AND
         (rr.regulation_name ILIKE '%' || rc.category_name || '%' OR
          rr.description ILIKE '%' || rc.category_name || '%'))
    WHERE r.mitigation_strategy IS NOT NULL;

    -- Generate gap analysis
    SELECT STRING_AGG(
        'Risk: ' || r.risk_name || ' | Current Control: ' ||
        COALESCE(NULLIF(r.mitigation_strategy, ''), 'None') || ' | Regulation Requirement: ' ||
        LEFT(rr.description, 100),
        E'\n'
    ) INTO p_gap_analysis
    FROM esg.risks r
    JOIN esg.risk_categories rc ON r.category_id = rc.category_id
    JOIN esg.regulatory_requirements rr ON
        (rr.regulation_id = p_regulation_id AND
         (rr.regulation_name ILIKE '%' || rc.category_name || '%' OR
          rr.description ILIKE '%' || rc.category_name || '%'))
    WHERE r.mitigation_strategy IS NULL OR r.mitigation_strategy = '';
END;
$$;

COMMENT ON PROCEDURE esg.sp_assess_regulatory_impact IS 'Assesses which risks and controls are affected by a new ESG regulation and identifies gaps';

-- Indexes
-- Indexes for new tables
CREATE INDEX idx_climate_scenario_impacts_risk ON esg.climate_scenario_impacts(risk_id);
CREATE INDEX idx_climate_scenario_impacts_scenario ON esg.climate_scenario_impacts(scenario_id);
CREATE INDEX idx_incidents_risk ON esg.incidents(risk_id);
CREATE INDEX idx_incidents_category ON esg.incidents(category_id);
CREATE INDEX idx_incidents_date ON esg.incidents(incident_date);
CREATE INDEX idx_incident_remediation_incident ON esg.incident_remediation(incident_id);
CREATE INDEX idx_targets_metric ON esg.targets(metric_id);
CREATE INDEX idx_target_progress_target ON esg.target_progress(target_id);
CREATE INDEX idx_target_progress_date ON esg.target_progress(reporting_date);
CREATE INDEX idx_audits_program ON esg.audits(program_id);
CREATE INDEX idx_audit_findings_audit ON esg.audit_findings(audit_id);
CREATE INDEX idx_audit_findings_risk ON esg.audit_findings(risk_id);
CREATE INDEX idx_audit_findings_status ON esg.audit_findings(status);

-- Functional indexes for text search
CREATE INDEX idx_risks_name_search ON esg.risks USING gin (risk_name gin_trgm_ops);
CREATE INDEX idx_regulations_name_search ON esg.regulatory_requirements USING gin (regulation_name gin_trgm_ops);

--climate Scenario Analysis
-- Analyze physical risk exposure under 2Â°C scenario
SELECT * FROM esg.vw_climate_risk_exposure
WHERE warming_level = 2.0 AND time_horizon = 2030
ORDER BY financial_impact DESC NULLS LAST;

--Incident Trend Monitoring
-- Identify categories with worsening detection times
SELECT * FROM esg.vw_incident_trends
WHERE avg_days_to_detect > 30
ORDER BY quarter, category_name;

--Target Performance Review
-- Check all off-track targets
SELECT * FROM esg.vw_target_performance
WHERE on_track_status != 'On Track' AND target_year >= EXTRACT(YEAR FROM CURRENT_DATE);



-- Risk Simulation
-- Evaluate a mitigation investment
-- Evaluate a mitigation investment
CALL esg.sp_simulate_risk_impact(
    p_risk_id := 42,
    p_new_likelihood := 2,
    p_new_impact := 3,
    p_mitigation_cost := 500000,
    p_current_score := NULL,
    p_proposed_score := NULL,
    p_risk_reduction := NULL,
    p_cost_per_risk_point := NULL
);


--regulatory impact assessment
-- Assess new EU CSRD regulation impact
DO $$
DECLARE
    v_reg_id INTEGER;
    v_affected INTEGER;
    v_high_risk INTEGER;
    v_controls INTEGER;
    v_gaps TEXT;
BEGIN
    SELECT regulation_id INTO v_reg_id
    FROM esg.regulatory_requirements
    WHERE regulation_name LIKE '%Corporate Sustainability Reporting Directive%';

    CALL esg.sp_assess_regulatory_impact(
        p_regulation_id := v_reg_id,
        p_affected_risks := v_affected,
        p_high_impact_risks := v_high_risk,
        p_existing_controls := v_controls,
        p_gap_analysis := v_gaps
    );

    RAISE NOTICE 'CSRD impacts % risks (% high), with % existing controls. Gaps: %',
        v_affected, v_high_risk, v_controls, v_gaps;
END;
$$;


-- ESG Capacity Planning Framework
--
-- Business Case: Organizations need to forecast resource requirements (people, budget, technology) for ESG initiatives and ensure adequate capacity for risk mitigation programs.

CREATE TABLE esg.capacity_requirements (
    requirement_id SERIAL PRIMARY KEY,
    initiative_id INTEGER REFERENCES esg.initiatives(initiative_id),
    resource_type VARCHAR(50) NOT NULL CHECK (resource_type IN ('Personnel', 'Budget', 'Technology', 'Training')),
    quantity DECIMAL(15,2) NOT NULL,
    unit_of_measure VARCHAR(20),
    required_by_date DATE NOT NULL,
    current_availability DECIMAL(15,2),
    gap DECIMAL(15,2) GENERATED ALWAYS AS (quantity - COALESCE(current_availability, 0)) STORED,
    priority VARCHAR(20) CHECK (priority IN ('Critical', 'High', 'Medium', 'Low')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.capacity_allocations (
    allocation_id SERIAL PRIMARY KEY,
    requirement_id INTEGER REFERENCES esg.capacity_requirements(requirement_id),
    allocated_amount DECIMAL(15,2) NOT NULL,
    allocation_date DATE NOT NULL,
    allocation_source VARCHAR(100),
    notes TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.capacity_requirements IS 'Tracks resource needs for ESG initiatives with gap analysis';
COMMENT ON TABLE esg.capacity_allocations IS 'Records actual resource allocations to meet ESG requirements';

-- ESG Technology Stack Inventory
--
-- Business Case: Proper tracking of ESG software and tools is essential for budget planning, vendor management, and avoiding redundant solutions.

CREATE TABLE esg.technology_stack (
    technology_id SERIAL PRIMARY KEY,
    technology_name VARCHAR(200) NOT NULL,
    vendor VARCHAR(100),
    category VARCHAR(50) CHECK (category IN ('Data Collection', 'Analytics', 'Reporting', 'Disclosure', 'Risk Management')),
    license_type VARCHAR(50),
    annual_cost DECIMAL(15,2),
    implementation_date DATE,
    renewal_date DATE,
    users INTEGER,
    data_capacity_gb DECIMAL(15,2),
    uptime_sla DECIMAL(5,2) CHECK (uptime_sla BETWEEN 0 AND 100),
    compliance_standards VARCHAR(200), -- e.g., 'SOC2, ISO27001'
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.technology_usage (
    usage_id SERIAL PRIMARY KEY,
    technology_id INTEGER REFERENCES esg.technology_stack(technology_id),
    measurement_date DATE NOT NULL,
    active_users INTEGER,
    storage_used_gb DECIMAL(15,2),
    api_calls INTEGER,
    uptime_percentage DECIMAL(5,2),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_tech_measurement UNIQUE (technology_id, measurement_date)
);

COMMENT ON TABLE esg.technology_stack IS 'Inventory of ESG technology solutions with capacity details';
COMMENT ON TABLE esg.technology_usage IS 'Tracks utilization metrics for ESG technologies over time';

-- ESG Training Competency Framework
--
-- Business Case: Building internal ESG capabilities requires tracking employee training and competency levels across different risk domains.

CREATE TABLE esg.competency_domains (
    domain_id SERIAL PRIMARY KEY,
    domain_name VARCHAR(100) NOT NULL,
    description TEXT,
    criticality VARCHAR(20) CHECK (criticality IN ('Mandatory', 'Recommended', 'Optional')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.training_programs (
    program_id SERIAL PRIMARY KEY,
    program_name VARCHAR(200) NOT NULL,
    domain_id INTEGER REFERENCES esg.competency_domains(domain_id),
    duration_hours DECIMAL(5,1),
    delivery_method VARCHAR(50) CHECK (delivery_method IN ('In-Person', 'Virtual', 'E-Learning', 'Hybrid')),
    max_capacity INTEGER,
    annual_frequency INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.employee_competencies (
    competency_id SERIAL PRIMARY KEY,
    employee_id VARCHAR(50) NOT NULL, -- Reference to HR system
    domain_id INTEGER REFERENCES esg.competency_domains(domain_id),
    proficiency_level VARCHAR(20) CHECK (proficiency_level IN ('Beginner', 'Intermediate', 'Advanced', 'Expert')),
    last_training_date DATE,
    next_refresh_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_employee_domain UNIQUE (employee_id, domain_id)
);

COMMENT ON TABLE esg.competency_domains IS 'Defines ESG competency areas required for effective risk management';
COMMENT ON TABLE esg.training_programs IS 'Catalog of available ESG training programs with capacity constraints';
COMMENT ON TABLE esg.employee_competencies IS 'Tracks employee ESG skills and training history';

--ESG Resource Gap Analysis View
--Business Case: Leadership needs visibility into resource constraints that may impact ESG goal achievement.

CREATE OR REPLACE VIEW esg.vw_resource_gaps AS
SELECT
    i.initiative_name,
    cr.resource_type,
    cr.quantity AS required,
    cr.current_availability AS available,
    cr.gap,
    cr.priority,
    COALESCE(SUM(ca.allocated_amount), 0) AS allocated,
    (cr.gap - COALESCE(SUM(ca.allocated_amount), 0)) AS remaining_gap,
    cr.required_by_date,
    CASE
        WHEN (cr.gap - COALESCE(SUM(ca.allocated_amount), 0)) <= 0 THEN 'Fully Resourced'
        WHEN CURRENT_DATE > cr.required_by_date THEN 'Overdue'
        WHEN (cr.gap - COALESCE(SUM(ca.allocated_amount), 0)) > 0
             AND CURRENT_DATE > (cr.required_by_date - INTERVAL '30 days') THEN 'Urgent'
        ELSE 'Monitoring'
    END AS status
FROM
    esg.capacity_requirements cr
JOIN
    esg.initiatives i ON cr.initiative_id = i.initiative_id
LEFT JOIN
    esg.capacity_allocations ca ON cr.requirement_id = ca.requirement_id
GROUP BY
    i.initiative_name, cr.resource_type, cr.quantity, cr.current_availability,
    cr.gap, cr.priority, cr.required_by_date, cr.requirement_id
ORDER BY
    status, priority, required_by_date;

COMMENT ON VIEW esg.vw_resource_gaps IS 'Identifies resource shortfalls for ESG initiatives with allocation tracking';

---Technology Utilizaiton trends view
---Business Case: IT teams need to monitor ESG tool usage to optimize licenses and plan upgrades.
CREATE OR REPLACE VIEW esg.vw_technology_utilization AS
SELECT
    ts.technology_name,
    ts.vendor,
    ts.category,
    ts.annual_cost,
    ts.users AS licensed_users,
    tu.measurement_date,
    tu.active_users,
    ROUND((tu.active_users::DECIMAL / NULLIF(ts.users, 0)) * 100, 1) AS user_utilization_pct,
    tu.storage_used_gb,
    ts.data_capacity_gb,
    ROUND((tu.storage_used_gb / NULLIF(ts.data_capacity_gb, 0)) * 100, 1) AS storage_utilization_pct,
    tu.uptime_percentage,
    ts.uptime_sla,
    CASE
        WHEN tu.uptime_percentage < ts.uptime_sla THEN 'SLA Breach'
        WHEN (tu.active_users::DECIMAL / NULLIF(ts.users, 0)) > 0.9 THEN 'Capacity Limit'
        ELSE 'Normal'
    END AS alert_status
FROM
    esg.technology_stack ts
JOIN
    (SELECT technology_id, measurement_date, active_users, storage_used_gb, uptime_percentage
     FROM esg.technology_usage
     WHERE measurement_date = (SELECT MAX(measurement_date) FROM esg.technology_usage tu2
                              WHERE tu2.technology_id = esg.technology_usage.technology_id)) tu
    ON ts.technology_id = tu.technology_id;

COMMENT ON VIEW esg.vw_technology_utilization IS 'Monitors usage trends and capacity thresholds for ESG technologies';

--Training capacity planning view
-- Business Case: HR needs to schedule ESG training sessions based on competency gaps and room/venue availability.

CREATE OR REPLACE VIEW esg.vw_training_capacity AS
SELECT
    cd.domain_name,
    tp.program_name,
    tp.delivery_method,
    tp.duration_hours,
    tp.max_capacity,
    COUNT(ec.employee_id) FILTER (WHERE ec.proficiency_level IN ('Beginner', 'Intermediate')) AS employees_needing_training,
    COUNT(ec.employee_id) FILTER (WHERE ec.next_refresh_date <= CURRENT_DATE + INTERVAL '90 days')) AS employees_due_refresh,
    CEILING((COUNT(ec.employee_id) FILTER (WHERE ec.proficiency_level IN ('Beginner', 'Intermediate')) +
           COUNT(ec.employee_id) FILTER (WHERE ec.next_refresh_date <= CURRENT_DATE + INTERVAL '90 days')) /
           NULLIF(tp.max_capacity, 0)) AS sessions_required,
    (tp.annual_frequency * tp.max_capacity) AS annual_capacity,
    (COUNT(ec.employee_id) FILTER (WHERE ec.proficiency_level IN ('Beginner', 'Intermediate')) +
     COUNT(ec.employee_id) FILTER (WHERE ec.next_refresh_date <= CURRENT_DATE + INTERVAL '90 days')) AS total_demand,
    CASE
        WHEN (tp.annual_frequency * tp.max_capacity) >=
             (COUNT(ec.employee_id) FILTER (WHERE ec.proficiency_level IN ('Beginner', 'Intermediate')) +
              COUNT(ec.employee_id) FILTER (WHERE ec.next_refresh_date <= CURRENT_DATE + INTERVAL '90 days')) THEN 'Adequate'
        ELSE 'Insufficient'
    END AS capacity_status
FROM
    esg.competency_domains cd
JOIN
    esg.training_programs tp ON cd.domain_id = tp.domain_id
LEFT JOIN
    esg.employee_competencies ec ON cd.domain_id = ec.domain_id
GROUP BY
    cd.domain_name, tp.program_name, tp.delivery_method, tp.duration_hours,
    tp.max_capacity, tp.annual_frequency;

COMMENT ON VIEW esg.vw_training_capacity IS 'Analyzes training demand vs capacity for ESG competency development';

--ESG Initiative Resource Forecast MV
--Business Case: Financial planning requires aggregated resource projections across all ESG initiatives.
CREATE MATERIALIZED VIEW esg.mv_initiative_resource_forecast AS
SELECT
    i.initiative_id,
    i.initiative_name,
    i.status,
    DATE_TRUNC('quarter', cr.required_by_date) AS quarter,
    cr.resource_type,
    SUM(cr.quantity) AS total_required,
    SUM(cr.current_availability) AS total_available,
    SUM(cr.gap) AS total_gap,
    SUM(COALESCE(ca.allocated_amount, 0)) AS total_allocated,
    SUM(cr.gap - COALESCE(ca.allocated_amount, 0)) AS remaining_gap,
    COUNT(DISTINCT cr.requirement_id) AS requirement_count
FROM
    esg.initiatives i
JOIN
    esg.capacity_requirements cr ON i.initiative_id = cr.initiative_id
LEFT JOIN
    esg.capacity_allocations ca ON cr.requirement_id = ca.requirement_id
WHERE
    cr.required_by_date BETWEEN CURRENT_DATE AND CURRENT_DATE + INTERVAL '2 years'
GROUP BY
    i.initiative_id, i.initiative_name, i.status, DATE_TRUNC('quarter', cr.required_by_date), cr.resource_type
WITH DATA;

COMMENT ON MATERIALIZED VIEW esg.mv_initiative_resource_forecast IS 'Quarterly aggregated resource forecasts for ESG initiatives with gap analysis';

-- Technology stack capacity mv
-- Business Case: IT capacity planning requires historical trends to predict future ESG technology needs.
CREATE MATERIALIZED VIEW esg.mv_technology_capacity_trends AS
SELECT
    ts.technology_id,
    ts.technology_name,
    ts.category,
    DATE_TRUNC('month', tu.measurement_date) AS month,
    AVG(tu.active_users) AS avg_active_users,
    MAX(tu.active_users) AS peak_active_users,
    AVG(tu.storage_used_gb) AS avg_storage_used,
    MAX(tu.storage_used_gb) AS peak_storage_used,
    ts.users AS license_capacity,
    ts.data_capacity_gb,
    ROUND((AVG(tu.active_users) / NULLIF(ts.users, 0)) * 100, 1) AS avg_user_utilization,
    ROUND((MAX(tu.active_users) / NULLIF(ts.users, 0)) * 100, 1) AS peak_user_utilization,
    ROUND((AVG(tu.storage_used_gb) / NULLIF(ts.data_capacity_gb, 0)) * 100, 1) AS avg_storage_utilization,
    ROUND((MAX(tu.storage_used_gb) / NULLIF(ts.data_capacity_gb, 0)) * 100, 1) AS peak_storage_utilization
FROM
    esg.technology_stack ts
JOIN
    esg.technology_usage tu ON ts.technology_id = tu.technology_id
WHERE
    tu.measurement_date >= CURRENT_DATE - INTERVAL '18 months'
GROUP BY
    ts.technology_id, ts.technology_name, ts.category, DATE_TRUNC('month', tu.measurement_date),
    ts.users, ts.data_capacity_gb
WITH DATA;

COMMENT ON MATERIALIZED VIEW esg.mv_technology_capacity_trends IS 'Monthly trends in ESG technology utilization for capacity planning';


-- ESG Initiative Capacity Planning Stored Procedure
--Business Case: Program managers need to simulate different resourcing scenarios for ESG initiatives.
CREATE OR REPLACE PROCEDURE esg.sp_simulate_initiative_capacity(
    p_initiative_id INTEGER,
    p_resource_type VARCHAR(50),
    p_scenario_name VARCHAR(100),
    p_additional_amount DECIMAL(15,2),
    OUT p_original_gap DECIMAL(15,2),
    OUT p_new_gap DECIMAL(15,2),
    OUT p_gap_reduction_pct DECIMAL(5,2),
    OUT p_cost_implications TEXT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_requirement RECORD;
    v_total_cost DECIMAL(15,2) := 0;
BEGIN
    -- Get current gap for the initiative and resource type
    SELECT SUM(quantity) AS total_required,
           SUM(current_availability) AS total_available,
           SUM(gap) AS total_gap
    INTO v_requirement
    FROM esg.capacity_requirements
    WHERE initiative_id = p_initiative_id
    AND resource_type = p_resource_type
    GROUP BY initiative_id, resource_type;

    IF v_requirement IS NULL THEN
        RAISE EXCEPTION 'No capacity requirements found for initiative % and resource type %',
                        p_initiative_id, p_resource_type;
    END IF;

    p_original_gap := v_requirement.total_gap;
    p_new_gap := GREATEST(0, v_requirement.total_gap - p_additional_amount);
    p_gap_reduction_pct := CASE WHEN v_requirement.total_gap > 0
                                THEN (p_additional_amount / v_requirement.total_gap) * 100
                                ELSE 0 END;

    -- Estimate cost implications based on resource type
    IF p_resource_type = 'Personnel' THEN
        v_total_cost := p_additional_amount * 100000; -- Assuming $100k avg fully loaded cost per FTE
        p_cost_implications := 'Approx. $' || ROUND(v_total_cost) || ' annual cost for ' ||
                              p_additional_amount || ' additional FTEs';
    ELSIF p_resource_type = 'Budget' THEN
        v_total_cost := p_additional_amount;
        p_cost_implications := 'Direct budget impact of $' || ROUND(v_total_cost);
    ELSIF p_resource_type = 'Technology' THEN
        v_total_cost := p_additional_amount * 5000; -- Assuming $5k avg annual cost per license
        p_cost_implications := 'Approx. $' || ROUND(v_total_cost) || ' annual cost for ' ||
                              p_additional_amount || ' additional licenses';
    ELSE
        p_cost_implications := 'Cost implications vary based on specific requirements';
    END IF;

    -- Log the simulation scenario for audit purposes
    INSERT INTO esg.capacity_simulations (initiative_id, resource_type, scenario_name,
                                         original_gap, additional_amount, new_gap, cost_estimate)
    VALUES (p_initiative_id, p_resource_type, p_scenario_name,
            p_original_gap, p_additional_amount, p_new_gap, v_total_cost);
END;
$$;

COMMENT ON PROCEDURE esg.sp_simulate_initiative_capacity IS 'Simulates the impact of additional resources on ESG initiative capacity gaps with cost estimates';


-- ESG Technology Right-Sizing Procedure
--
-- Business Case: IT needs data-driven recommendations for optimizing ESG software licenses and infrastructure.

CREATE OR REPLACE PROCEDURE esg.sp_recommend_technology_rightsizing(
    p_threshold_pct DECIMAL(5,2) DEFAULT 80.0,
    p_forecast_months INTEGER DEFAULT 12,
    OUT p_recommendations TEXT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_recommendation RECORD;
BEGIN
    p_recommendations := '';

    FOR v_recommendation IN
        WITH growth_rates AS (
            SELECT
                technology_id,
                technology_name,
                category,
                users AS current_licenses,
                data_capacity_gb,
                AVG(peak_user_utilization) AS avg_peak_utilization,
                AVG(peak_storage_utilization) AS avg_storage_utilization,
                -- Calculate 6-month growth rate for users
                REGR_SLOPE(peak_active_users, EXTRACT(EPOCH FROM month)) OVER
                    (PARTITION BY technology_id) * 2592000 AS monthly_user_growth,
                -- Calculate 6-month growth rate for storage
                REGR_SLOPE(peak_storage_used, EXTRACT(EPOCH FROM month)) OVER
                    (PARTITION BY technology_id) AS monthly_storage_growth
            FROM
                esg.mv_technology_capacity_trends
            WHERE
                month >= CURRENT_DATE - INTERVAL '6 months'
            GROUP BY
                technology_id, technology_name, category, users, data_capacity_gb
        )
        SELECT
            technology_id,
            technology_name,
            category,
            current_licenses,
            data_capacity_gb,
            avg_peak_utilization,
            avg_storage_utilization,
            -- Projected user license needs
            CASE
                WHEN avg_peak_utilization < p_threshold_pct THEN
                    'Reduce licenses by ' || (current_licenses - CEIL(current_licenses * (avg_peak_utilization / p_threshold_pct))) ||
                    ' to ' || CEIL(current_licenses * (avg_peak_utilization / p_threshold_pct))
                WHEN monthly_user_growth > 0 THEN
                    'Increase licenses by ' || CEIL(monthly_user_growth * p_forecast_months) ||
                    ' to ' || (current_licenses + CEIL(monthly_user_growth * p_forecast_months))
                ELSE 'Maintain current licenses'
            END AS user_recommendation,
            -- Projected storage needs
            CASE
                WHEN avg_storage_utilization < p_threshold_pct THEN
                    'Reduce storage by ' || (data_capacity_gb - CEIL(data_capacity_gb * (avg_storage_utilization / p_threshold_pct))) || 'GB'
                WHEN monthly_storage_growth > 0 THEN
                    'Increase storage by ' || CEIL(monthly_storage_growth * p_forecast_months) || 'GB'
                ELSE 'Maintain current storage'
            END AS storage_recommendation
        FROM
            growth_rates
        WHERE
            avg_peak_utilization < p_threshold_pct OR
            avg_peak_utilization > p_threshold_pct * 0.9 OR
            avg_storage_utilization < p_threshold_pct OR
            avg_storage_utilization > p_threshold_pct * 0.9
    LOOP
        p_recommendations := p_recommendations ||
            E'\nTechnology: ' || v_recommendation.technology_name || ' (' || v_recommendation.category || ')' ||
            E'\n  Current Licenses: ' || v_recommendation.current_licenses ||
            ' (Peak Utilization: ' || ROUND(v_recommendation.avg_peak_utilization, 1) || '%)' ||
            E'\n  Recommendation: ' || v_recommendation.user_recommendation ||
            E'\n  Current Storage: ' || v_recommendation.data_capacity_gb || 'GB' ||
            ' (Peak Utilization: ' || ROUND(v_recommendation.avg_storage_utilization, 1) || '%)' ||
            E'\n  Recommendation: ' || v_recommendation.storage_recommendation || E'\n';
    END LOOP;
END;
$$;

COMMENT ON PROCEDURE esg.sp_recommend_technology_rightsizing IS 'Analyzes technology utilization trends and provides license/storage optimization recommendations';

--ESG Training schedule optimizer
-- Business Case: HR needs to optimize training schedules considering employee availability, room capacity, and priority skills.
CREATE OR REPLACE PROCEDURE esg.sp_optimize_training_schedule(
    p_planning_horizon_months INTEGER DEFAULT 6,
    OUT p_schedule TEXT
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_month DATE;
    v_domain RECORD;
    v_sessions_needed INTEGER;
    v_available_capacity INTEGER;
BEGIN
    p_schedule := 'Training Schedule Optimization (' || p_planning_horizon_months || ' months):\n\n';

    FOR i IN 0..p_planning_horizon_months-1 LOOP
        v_month := DATE_TRUNC('month', CURRENT_DATE) + (i * INTERVAL '1 month');
        p_schedule := p_schedule || 'Month: ' || TO_CHAR(v_month, 'YYYY-MM') || E'\n';

        FOR v_domain IN
            SELECT
                cd.domain_id,
                cd.domain_name,
                cd.criticality,
                tp.program_id,
                tp.program_name,
                tp.duration_hours,
                tp.max_capacity,
                COUNT(ec.employee_id) FILTER (WHERE ec.proficiency_level IN ('Beginner', 'Intermediate')) AS new_trainees,
                COUNT(ec.employee_id) FILTER (WHERE ec.next_refresh_date <= v_month + INTERVAL '1 month')) AS refreshers
            FROM
                esg.competency_domains cd
            JOIN
                esg.training_programs tp ON cd.domain_id = tp.domain_id
            LEFT JOIN
                esg.employee_competencies ec ON cd.domain_id = ec.domain_id
            GROUP BY
                cd.domain_id, cd.domain_name, cd.criticality, tp.program_id, tp.program_name,
                tp.duration_hours, tp.max_capacity
            ORDER BY
                cd.criticality DESC,
                (COUNT(ec.employee_id) FILTER (WHERE ec.proficiency_level IN ('Beginner', 'Intermediate')) +
                COUNT(ec.employee_id) FILTER (WHERE ec.next_refresh_date <= v_month + INTERVAL '1 month')) DESC
        LOOP
            v_sessions_needed := CEIL((v_domain.new_trainees + v_domain.refreshers) / NULLIF(v_domain.max_capacity, 0));

            IF v_sessions_needed > 0 THEN
                p_schedule := p_schedule ||
                    '  ' || v_domain.domain_name || ' (' || v_domain.criticality || ')' ||
                    E'\n    Program: ' || v_domain.program_name ||
                    ' (' || v_domain.duration_hours || ' hrs, Capacity: ' || v_domain.max_capacity || ')' ||
                    E'\n    Needed: ' || v_sessions_needed || ' session(s) for ' ||
                    (v_domain.new_trainees + v_domain.refreshers) || ' employees (' ||
                    v_domain.new_trainees || ' new + ' || v_domain.refreshers || ' refresh)' || E'\n';
            END IF;
        END LOOP;

        p_schedule := p_schedule || E'\n';
    END LOOP;
END;
$$;

COMMENT ON PROCEDURE esg.sp_optimize_training_schedule IS 'Generates optimized training schedule based on competency gaps, priorities, and capacity constraints';



-- intiative capacity simulation
CALL esg.sp_simulate_initiative_capacity(
    p_initiative_id := 15, -- Renewable Energy Transition
    p_resource_type := 'Budget',
    p_scenario_name := 'Accelerated Timeline',
    p_additional_amount := 500000,
    p_original_gap := NULL,
    p_new_gap := NULL,
    p_gap_reduction_pct := NULL,
    p_cost_implications := NULL
);

-- technology right sizing

DO $$
DECLARE
    v_recommendations TEXT;
BEGIN
    CALL esg.sp_recommend_technology_rightsizing(
        p_threshold_pct := 75.0, -- Target 75% utilization
        p_forecast_months := 12,
        p_recommendations := v_recommendations
    );
    RAISE NOTICE '%', v_recommendations;
END;
$$;

--training schedule optimization
DO $$
DECLARE
    v_schedule TEXT;
BEGIN
    CALL esg.sp_optimize_training_schedule(
        p_planning_horizon_months := 6,
        p_schedule := v_schedule
    );
    RAISE NOTICE '%', v_schedule;
END;
$$;

-- Additional indexes for capacity planning
CREATE INDEX idx_capacity_requirements_initiative ON esg.capacity_requirements(initiative_id);
CREATE INDEX idx_capacity_requirements_resource ON esg.capacity_requirements(resource_type);
CREATE INDEX idx_technology_usage_dates ON esg.technology_usage(measurement_date);
CREATE INDEX idx_employee_competencies_refresh ON esg.employee_competencies(next_refresh_date);



-- ESG Risk Heatmap visualization framework
-- Business Case: Executives need dynamic risk visualization to prioritize mitigation efforts and allocate resources effectively.
CREATE TABLE esg.risk_heatmap_settings (
    setting_id SERIAL PRIMARY KEY,
    user_role VARCHAR(50) NOT NULL,
    likelihood_bands VARCHAR(100) NOT NULL DEFAULT '1-2,3-4,5',
    impact_bands VARCHAR(100) NOT NULL DEFAULT '1-2,3-4,5',
    color_scheme VARCHAR(50) NOT NULL DEFAULT 'TrafficLight',
    default_view VARCHAR(50) NOT NULL DEFAULT 'Materiality',
    refresh_frequency INTEGER NOT NULL DEFAULT 24, -- hours
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.risk_heatmap_settings IS 'Stores user preferences for ESG risk heatmap visualizations to support decision-making';

CREATE MATERIALIZED VIEW esg.mv_dynamic_risk_heatmap AS
SELECT
    r.risk_id,
    r.risk_name,
    rc.category_name,
    rc.category_type,
    r.materiality_level,
    ra.likelihood_score,
    ra.impact_score,
    ra.overall_score,
    ra.assessment_date,
    i.initiative_name AS mitigation_initiative,
    i.status AS initiative_status,
    CASE
        WHEN ra.overall_score >= 16 THEN 'Extreme'
        WHEN ra.overall_score >= 12 THEN 'High'
        WHEN ra.overall_score >= 8 THEN 'Medium'
        ELSE 'Low'
    END AS risk_band,
    DENSE_RANK() OVER (ORDER BY ra.overall_score DESC) AS risk_rank
FROM
    esg.risks r
JOIN
    esg.risk_categories rc ON r.category_id = rc.category_id
JOIN
    esg.risk_assessments ra ON r.risk_id = ra.risk_id
LEFT JOIN
    esg.initiatives i ON r.mitigation_strategy LIKE '%' || i.initiative_name || '%'
WHERE
    ra.assessment_date = (SELECT MAX(assessment_date)
                         FROM esg.risk_assessments ra2
                         WHERE ra2.risk_id = ra.risk_id)
WITH DATA;

COMMENT ON MATERIALIZED VIEW esg.mv_dynamic_risk_heatmap IS 'Pre-aggregated risk data optimized for heatmap visualization with dynamic banding';



-- ESG Regulatory Change Impact Tracker
--
-- Business Case: Compliance teams need to monitor evolving ESG regulations and assess their operational impact.

CREATE TABLE esg.regulatory_changes (
    change_id SERIAL PRIMARY KEY,
    regulation_id INTEGER REFERENCES esg.regulatory_requirements(regulation_id),
    change_date DATE NOT NULL,
    change_type VARCHAR(50) CHECK (change_type IN ('New Requirement', 'Amendment', 'Enforcement', 'Guidance')),
    change_description TEXT NOT NULL,
    impact_level VARCHAR(20) CHECK (impact_level IN ('Low', 'Medium', 'High', 'Critical')),
    implementation_deadline DATE,
    responsible_department VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.regulatory_change_actions (
    action_id SERIAL PRIMARY KEY,
    change_id INTEGER REFERENCES esg.regulatory_changes(change_id),
    action_description TEXT NOT NULL,
    action_type VARCHAR(50) CHECK (action_type IN ('Policy Update', 'Process Change', 'Training', 'Disclosure', 'Technology')),
    status VARCHAR(20) CHECK (status IN ('Not Started', 'In Progress', 'Completed', 'Verified')),
    due_date DATE,
    completion_date DATE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.regulatory_changes IS 'Tracks specific changes to ESG regulations with impact assessment';
COMMENT ON TABLE esg.regulatory_change_actions IS 'Records actions taken to implement regulatory changes';

CREATE OR REPLACE VIEW esg.vw_regulatory_change_impact AS
SELECT
    rr.regulation_name,
    rr.jurisdiction,
    rc.change_date,
    rc.change_type,
    rc.impact_level,
    rc.implementation_deadline,
    COUNT(ca.action_id) AS total_actions,
    COUNT(ca.action_id) FILTER (WHERE ca.status = 'Completed')) AS completed_actions,
    COUNT(ca.action_id) FILTER (WHERE ca.status != 'Completed' AND ca.due_date < CURRENT_DATE)) AS overdue_actions,
    rc.responsible_department,
    CASE
        WHEN COUNT(ca.action_id) = 0 THEN 'No Action'
        WHEN COUNT(ca.action_id) FILTER (WHERE ca.status != 'Completed')) = 0 THEN 'Fully Implemented'
        WHEN COUNT(ca.action_id) FILTER (WHERE ca.due_date < CURRENT_DATE AND ca.status != 'Completed')) > 0 THEN 'Critical Path'
        ELSE 'In Progress'
    END AS implementation_status
FROM
    esg.regulatory_changes rc
JOIN
    esg.regulatory_requirements rr ON rc.regulation_id = rr.regulation_id
LEFT JOIN
    esg.regulatory_change_actions ca ON rc.change_id = ca.change_id
GROUP BY
    rr.regulation_name, rr.jurisdiction, rc.change_date, rc.change_type,
    rc.impact_level, rc.implementation_deadline, rc.responsible_department, rc.change_id;

COMMENT ON VIEW esg.vw_regulatory_change_impact IS 'Tracks progress of implementing regulatory changes with status indicators';


--
--
-- ESG Data Lineage & Provenance Tracking
--
-- Business Case: Auditors require full traceability of ESG data from source to disclosure to ensure reliability.

CREATE TABLE esg.data_lineage (
    lineage_id SERIAL PRIMARY KEY,
    metric_id INTEGER REFERENCES esg.performance_metrics(metric_id),
    data_source VARCHAR(200) NOT NULL,
    extraction_method VARCHAR(100),
    transformation_rules TEXT,
    quality_checks TEXT,
    steward VARCHAR(100) NOT NULL,
    last_refresh TIMESTAMP WITH TIME ZONE,
    next_refresh TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.data_lineage_versions (
    version_id SERIAL PRIMARY KEY,
    lineage_id INTEGER REFERENCES esg.data_lineage(lineage_id),
    version_number INTEGER NOT NULL,
    change_description TEXT NOT NULL,
    changed_by VARCHAR(100) NOT NULL,
    changed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_lineage_version UNIQUE (lineage_id, version_number)
);

COMMENT ON TABLE esg.data_lineage IS 'Documents the origin, transformation, and stewardship of ESG data elements';
COMMENT ON TABLE esg.data_lineage_versions IS 'Tracks changes to data lineage definitions over time for audit purposes';

CREATE OR REPLACE PROCEDURE esg.sp_track_data_change(
    p_metric_id INTEGER,
    p_change_description TEXT,
    p_changed_by VARCHAR(100)
LANGUAGE plpgsql
AS $$
DECLARE
    v_lineage_id INTEGER;
    v_next_version INTEGER;
BEGIN
    -- Find or create lineage record
    SELECT lineage_id INTO v_lineage_id
    FROM esg.data_lineage
    WHERE metric_id = p_metric_id;

    IF v_lineage_id IS NULL THEN
        INSERT INTO esg.data_lineage (metric_id, steward)
        VALUES (p_metric_id, p_changed_by)
        RETURNING lineage_id INTO v_lineage_id;
    END IF;

    -- Get next version number
    SELECT COALESCE(MAX(version_number), 0) + 1 INTO v_next_version
    FROM esg.data_lineage_versions
    WHERE lineage_id = v_lineage_id;

    -- Record the change
    INSERT INTO esg.data_lineage_versions (
        lineage_id, version_number, change_description, changed_by
    ) VALUES (
        v_lineage_id, v_next_version, p_change_description, p_changed_by
    );

    -- Update last refresh timestamp
    UPDATE esg.data_lineage
    SET last_refresh = CURRENT_TIMESTAMP
    WHERE lineage_id = v_lineage_id;
END;
$$;

COMMENT ON PROCEDURE esg.sp_track_data_change IS 'Records changes to ESG data definitions and transformations for audit trails';


-- ESG Stakeholder Sentiment Analysis
--
-- Business Case: Investor relations teams need to monitor ESG-related sentiment from stakeholders to identify emerging concerns.

CREATE TABLE esg.sentiment_sources (
    source_id SERIAL PRIMARY KEY,
    source_name VARCHAR(100) NOT NULL,
    source_type VARCHAR(50) CHECK (source_type IN ('Social Media', 'News', 'Investor Call', 'Survey', 'Direct Feedback')),
    extraction_method VARCHAR(100),
    refresh_frequency VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.stakeholder_sentiment (
    sentiment_id SERIAL PRIMARY KEY,
    source_id INTEGER REFERENCES esg.sentiment_sources(source_id),
    analysis_date DATE NOT NULL,
    stakeholder_group VARCHAR(50) CHECK (stakeholder_group IN ('Investors', 'Employees', 'Customers', 'Regulators', 'Communities')),
    topic VARCHAR(100) NOT NULL,
    sentiment_score DECIMAL(5,2) CHECK (sentiment_score BETWEEN -1 AND 1),
    volume INTEGER,
    key_phrases TEXT[],
    alert_flag BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.sentiment_sources IS 'Defines sources for ESG-related sentiment analysis';
COMMENT ON TABLE esg.stakeholder_sentiment IS 'Stores analyzed sentiment data from various stakeholder groups';

CREATE MATERIALIZED VIEW esg.mv_sentiment_trends AS
SELECT
    stakeholder_group,
    topic,
    DATE_TRUNC('week', analysis_date) AS week_start,
    AVG(sentiment_score) AS avg_sentiment,
    SUM(volume) AS total_volume,
    COUNT(*) FILTER (WHERE sentiment_score < -0.5) AS negative_count,
    COUNT(*) FILTER (WHERE sentiment_score > 0.5) AS positive_count,
    ARRAY(
        SELECT DISTINCT unnest(key_phrases)
        FROM esg.stakeholder_sentiment ss2
        WHERE ss2.stakeholder_group = ss.stakeholder_group
        AND ss2.topic = ss.topic
        AND DATE_TRUNC('week', ss2.analysis_date) = DATE_TRUNC('week', ss.analysis_date)
    ) AS trending_phrases
FROM
    esg.stakeholder_sentiment ss
WHERE
    analysis_date >= CURRENT_DATE - INTERVAL '3 months'
GROUP BY
    stakeholder_group, topic, DATE_TRUNC('week', analysis_date)
WITH DATA;

COMMENT ON MATERIALIZED VIEW esg.mv_sentiment_trends IS 'Aggregates sentiment data by stakeholder group and topic for trend analysis';

CREATE OR REPLACE PROCEDURE esg.sp_generate_sentiment_alerts(
    p_threshold_change DECIMAL(5,2) DEFAULT 0.3
)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Update alert flags based on significant sentiment changes
    UPDATE esg.stakeholder_sentiment ss
    SET alert_flag = TRUE
    FROM (
        SELECT
            sentiment_id,
            ABS(sentiment_score - LAG(sentiment_score) OVER (
                PARTITION BY stakeholder_group, topic
                ORDER BY analysis_date
            )) AS score_change,
            volume - LAG(volume) OVER (
                PARTITION BY stakeholder_group, topic
                ORDER BY analysis_date
            ) AS volume_change
        FROM
            esg.stakeholder_sentiment
        WHERE
            analysis_date >= CURRENT_DATE - INTERVAL '7 days'
    ) changes
    WHERE ss.sentiment_id = changes.sentiment_id
    AND (changes.score_change >= p_threshold_change OR changes.volume_change >= 100);

    -- Notify stakeholders of new alerts
    INSERT INTO esg.notifications (notification_type, message, priority)
    SELECT
        'Sentiment Alert',
        'Significant change detected for ' || stakeholder_group || ' on topic: ' || topic ||
        ' (New Score: ' || sentiment_score || ', Volume: ' || volume || ')',
        'High'
    FROM
        esg.stakeholder_sentiment
    WHERE
        alert_flag = TRUE
        AND created_at >= CURRENT_TIMESTAMP - INTERVAL '1 hour';
END;
$$;

COMMENT ON PROCEDURE esg.sp_generate_sentiment_alerts IS 'Identifies and flags significant changes in stakeholder sentiment for immediate attention';



-- ESG Control Effectiveness Monitoring
-- Business Case: Internal audit needs to evaluate whether risk mitigation controls are operating as intended.
CREATE TABLE esg.risk_controls (
    control_id SERIAL PRIMARY KEY,
    risk_id INTEGER REFERENCES esg.risks(risk_id),
    control_name VARCHAR(200) NOT NULL,
    control_description TEXT,
    control_type VARCHAR(50) CHECK (control_type IN ('Preventive', 'Detective', 'Corrective', 'Compensating')),
    frequency VARCHAR(50) NOT NULL,
    owner VARCHAR(100) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.control_testing (
    test_id SERIAL PRIMARY KEY,
    control_id INTEGER REFERENCES esg.risk_controls(control_id),
    test_date DATE NOT NULL,
    tested_by VARCHAR(100) NOT NULL,
    test_method VARCHAR(100),
    sample_size INTEGER,
    sample_description TEXT,
    defects_found INTEGER,
    effectiveness_rating INTEGER CHECK (effectiveness_rating BETWEEN 1 AND 5),
    test_results TEXT,
    remediation_plan TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.risk_controls IS 'Documents controls implemented to mitigate ESG risks';
COMMENT ON TABLE esg.control_testing IS 'Records testing of control effectiveness with results';

CREATE OR REPLACE VIEW esg.vw_control_effectiveness AS
SELECT
    r.risk_id,
    r.risk_name,
    rc.category_name,
    c.control_id,
    c.control_name,
    c.control_type,
    c.owner,
    COUNT(t.test_id) AS tests_conducted,
    MAX(t.test_date) AS last_test_date,
    AVG(t.effectiveness_rating) AS avg_effectiveness,
    SUM(t.defects_found) AS total_defects,
    CASE
        WHEN COUNT(t.test_id) = 0 THEN 'Not Tested'
        WHEN AVG(t.effectiveness_rating) >= 4 THEN 'Effective'
        WHEN AVG(t.effectiveness_rating) >= 2.5 THEN 'Partially Effective'
        ELSE 'Ineffective'
    END AS effectiveness_status
FROM
    esg.risks r
JOIN
    esg.risk_controls c ON r.risk_id = c.risk_id
JOIN
    esg.risk_categories rc ON r.category_id = rc.category_id
LEFT JOIN
    esg.control_testing t ON c.control_id = t.control_id
GROUP BY
    r.risk_id, r.risk_name, rc.category_name, c.control_id, c.control_name, c.control_type, c.owner;

COMMENT ON VIEW esg.vw_control_effectiveness IS 'Evaluates the effectiveness of controls mitigating ESG risks based on testing results';

CREATE OR REPLACE PROCEDURE esg.sp_schedule_control_tests()
LANGUAGE plpgsql
AS $$
BEGIN
    -- Schedule tests for controls based on their frequency
    INSERT INTO esg.control_testing (
        control_id,
        test_date,
        tested_by,
        test_method
    )
    SELECT
        c.control_id,
        CASE
            WHEN c.frequency = 'Monthly' THEN CURRENT_DATE + INTERVAL '1 month'
            WHEN c.frequency = 'Quarterly' THEN CURRENT_DATE + INTERVAL '3 months'
            WHEN c.frequency = 'Annual' THEN CURRENT_DATE + INTERVAL '1 year'
            ELSE CURRENT_DATE + INTERVAL '6 months' -- Default for other frequencies
        END AS test_date,
        c.owner AS tested_by,
        CASE
            WHEN c.control_type = 'Preventive' THEN 'Sample Testing'
            WHEN c.control_type = 'Detective' THEN 'Full Population'
            ELSE 'Targeted Review'
        END AS test_method
    FROM
        esg.risk_controls c
    WHERE
        NOT EXISTS (
            SELECT 1
            FROM esg.control_testing t
            WHERE t.control_id = c.control_id
            AND t.test_date >= CURRENT_DATE - INTERVAL '1 month'
        );
END;
$$;

COMMENT ON PROCEDURE esg.sp_schedule_control_tests IS 'Automatically schedules control tests based on defined frequencies and recent testing history';


-- Example integration with HR system for competency tracking
CREATE FOREIGN TABLE hr.employees (
    employee_id VARCHAR(50),
    name VARCHAR(100),
    department VARCHAR(50),
    position VARCHAR(50)
) SERVER hr_system;

-- Create view that combines ESG competencies with HR data
CREATE VIEW esg.vw_employee_esg_skills AS
SELECT
    e.employee_id,
    e.name,
    e.department,
    cd.domain_name,
    ec.proficiency_level,
    ec.last_training_date
FROM
    hr.employees e
JOIN
    esg.employee_competencies ec ON e.employee_id = ec.employee_id
JOIN
    esg.competency_domains cd ON ec.domain_id = cd.domain_id;



    -- Add indexes for new tables
CREATE INDEX idx_sentiment_stakeholder_topic ON esg.stakeholder_sentiment(stakeholder_group, topic);
CREATE INDEX idx_control_testing_dates ON esg.control_testing(test_date);
CREATE INDEX idx_regulatory_changes_deadline ON esg.regulatory_changes(implementation_deadline);

-- Partition large tables by time ranges
CREATE TABLE esg.sentiment_data_partitioned (
    LIKE esg.stakeholder_sentiment INCLUDING INDEXES
) PARTITION BY RANGE (analysis_date);

-- Create monthly partitions
CREATE TABLE esg.sentiment_data_2025_11 PARTITION OF esg.sentiment_data_partitioned
    FOR VALUES FROM ('2025-1-01') TO ('2023-05-01');


    CREATE OR REPLACE PROCEDURE esg.sp_maintain_partitions()
    LANGUAGE plpgsql
    AS $$
    BEGIN
        -- Create next month's partition for sentiment data
        EXECUTE format('
            CREATE TABLE IF NOT EXISTS esg.sentiment_data_%s PARTITION OF esg.sentiment_data_partitioned
            FOR VALUES FROM (%L) TO (%L)',
            to_char(CURRENT_DATE + INTERVAL '1 month', 'YYYY_MM'),
            date_trunc('month', CURRENT_DATE + INTERVAL '1 month'),
            date_trunc('month', CURRENT_DATE + INTERVAL '2 month')
        );

        -- Archive old data (retention policy: 3 years)
        EXECUTE format('
            DROP TABLE IF EXISTS esg.sentiment_data_%s',
            to_char(CURRENT_DATE - INTERVAL '3 years', 'YYYY_MM')
        );
    END;
    $$;

    COMMENT ON PROCEDURE esg.sp_maintain_partitions IS 'Manages time-based partitions for large tables according to retention policies';

-- ESG Transition Risk Modeling Framework
-- Business Case: Financial institutions need to assess portfolio exposure to carbon-intensive assets under different transition scenarios to comply with TCFD and meet net-zero commitments.

CREATE TABLE esg.transition_scenarios (
    scenario_id SERIAL PRIMARY KEY,
    scenario_name VARCHAR(100) NOT NULL,
    scenario_type VARCHAR(50) CHECK (scenario_type IN ('Net Zero 2050', 'Delayed Transition', 'Disorderly Transition')),
    warming_target DECIMAL(3,1), -- Degrees Celsius
    reference_model VARCHAR(100),
    publication_year INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.portfolio_exposure (
    exposure_id SERIAL PRIMARY KEY,
    scenario_id INTEGER REFERENCES esg.transition_scenarios(scenario_id),
    asset_class VARCHAR(50) NOT NULL,
    sector VARCHAR(50) NOT NULL,
    exposure_value DECIMAL(15,2) NOT NULL,
    carbon_intensity DECIMAL(15,2), -- tCO2e/$M revenue
    stranding_risk DECIMAL(5,2) CHECK (stranding_risk BETWEEN 0 AND 1),
    transition_risk_score DECIMAL(5,2) CHECK (transition_risk_score BETWEEN 0 AND 10),
    analysis_date DATE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_scenario_exposure UNIQUE (scenario_id, asset_class, sector, analysis_date)
);

COMMENT ON TABLE esg.transition_scenarios IS 'Stores climate transition scenarios used for portfolio risk assessment';
COMMENT ON TABLE esg.portfolio_exposure IS 'Tracks financial exposure to transition risks by asset class and sector';

CREATE MATERIALIZED VIEW esg.mv_transition_risk_summary AS
SELECT
    ts.scenario_name,
    ts.warming_target,
    pe.asset_class,
    pe.sector,
    SUM(pe.exposure_value) AS total_exposure,
    AVG(pe.carbon_intensity) AS avg_carbon_intensity,
    SUM(pe.exposure_value * pe.stranding_risk) AS at_risk_exposure,
    RANK() OVER (PARTITION BY ts.scenario_id ORDER BY SUM(pe.exposure_value * pe.stranding_risk) DESC) AS risk_rank
FROM
    esg.portfolio_exposure pe
JOIN
    esg.transition_scenarios ts ON pe.scenario_id = ts.scenario_id
WHERE
    pe.analysis_date = (SELECT MAX(analysis_date) FROM esg.portfolio_exposure)
GROUP BY
    ts.scenario_id, ts.scenario_name, ts.warming_target, pe.asset_class, pe.sector
WITH DATA;

COMMENT ON MATERIALIZED VIEW esg.mv_transition_risk_summary IS 'Aggregates portfolio exposure to transition risks by scenario for strategic decision-making';

CREATE OR REPLACE PROCEDURE esg.sp_calculate_transition_risk(
    p_scenario_id INTEGER,
    p_as_of_date DATE DEFAULT CURRENT_DATE
)
LANGUAGE plpgsql
AS $$
BEGIN
    -- Calculate stranding risk based on sector benchmarks
    UPDATE esg.portfolio_exposure pe
    SET
        stranding_risk = CASE
            WHEN s.risk_category = 'High' THEN 0.7
            WHEN s.risk_category = 'Medium' THEN 0.4
            ELSE 0.1
        END,
        transition_risk_score = CASE
            WHEN s.risk_category = 'High' THEN carbon_intensity * 0.8
            WHEN s.risk_category = 'Medium' THEN carbon_intensity * 0.5
            ELSE carbon_intensity * 0.2
        END
    FROM
        esg.sector_benchmarks s
    WHERE
        pe.scenario_id = p_scenario_id
        AND pe.sector = s.sector_name
        AND pe.analysis_date = p_as_of_date;

    -- Refresh materialized view
    REFRESH MATERIALIZED VIEW esg.mv_transition_risk_summary;
END;
$$;

COMMENT ON PROCEDURE esg.sp_calculate_transition_risk IS 'Computes transition risk metrics for portfolio exposures based on sector benchmarks and scenario parameters';


-- ESG Controversy Monitoring System
--Business Case: Companies must track and respond to ESG-related controversies that could impact reputation and stock price.
CREATE TABLE esg.controversy_types (
    type_id SERIAL PRIMARY KEY,
    type_name VARCHAR(100) NOT NULL,
    severity_level VARCHAR(20) CHECK (severity_level IN ('Minor', 'Moderate', 'Major', 'Critical')),
    common_sources TEXT,
    response_protocol TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.controversy_incidents (
    incident_id SERIAL PRIMARY KEY,
    type_id INTEGER REFERENCES esg.controversy_types(type_id),
    incident_date TIMESTAMP WITH TIME ZONE NOT NULL,
    title VARCHAR(200) NOT NULL,
    description TEXT,
    source_url VARCHAR(500),
    affected_entities TEXT[],
    severity_rating INTEGER CHECK (severity_rating BETWEEN 1 AND 10),
    stock_impact DECIMAL(5,2), -- Percentage change
    media_coverage INTEGER, -- Number of articles
    status VARCHAR(20) CHECK (status IN ('Active', 'Contained', 'Resolved', 'Litigation')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.controversy_responses (
    response_id SERIAL PRIMARY KEY,
    incident_id INTEGER REFERENCES esg.controversy_incidents(incident_id),
    response_date TIMESTAMP WITH TIME ZONE NOT NULL,
    response_type VARCHAR(50) CHECK (response_type IN ('Statement', 'Policy Change', 'Remediation', 'Settlement')),
    response_text TEXT,
    response_url VARCHAR(500),
    effectiveness INTEGER CHECK (effectiveness BETWEEN 1 AND 5),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.controversy_types IS 'Catalog of potential ESG controversy types with severity classifications';
COMMENT ON TABLE esg.controversy_incidents IS 'Tracks actual ESG controversy events with impact metrics';
COMMENT ON TABLE esg.controversy_responses IS 'Records organizational responses to ESG controversies';

CREATE OR REPLACE VIEW esg.vw_controversy_impact AS
SELECT
    ci.incident_id,
    ct.type_name,
    ci.incident_date,
    ci.title,
    ci.severity_rating,
    ci.stock_impact,
    ci.media_coverage,
    cr.response_type,
    cr.effectiveness,
    CASE
        WHEN ci.status = 'Resolved' AND cr.effectiveness >= 4 THEN 'Well Managed'
        WHEN ci.status = 'Active' AND ci.severity_rating >= 7 THEN 'Critical Situation'
        WHEN ci.media_coverage > 100 THEN 'High Visibility'
        ELSE 'Standard'
    END AS impact_category
FROM
    esg.controversy_incidents ci
JOIN
    esg.controversy_types ct ON ci.type_id = ct.type_id
LEFT JOIN
    esg.controversy_responses cr ON ci.incident_id = cr.incident_id
WHERE
    ci.incident_date >= CURRENT_DATE - INTERVAL '1 year';

COMMENT ON VIEW esg.vw_controversy_impact IS 'Analyzes ESG controversies by impact level and response effectiveness';

CREATE OR REPLACE PROCEDURE esg.sp_monitor_controversies()
LANGUAGE plpgsql
AS $$
BEGIN
    -- Flag controversies needing immediate attention
    UPDATE esg.controversy_incidents
    SET status = 'Critical'
    WHERE severity_rating >= 8
    AND status NOT IN ('Resolved', 'Litigation')
    AND NOT EXISTS (
        SELECT 1 FROM esg.controversy_responses
        WHERE incident_id = esg.controversy_incidents.incident_id
    );

    -- Generate alerts for high media coverage
    INSERT INTO esg.notifications (notification_type, message, priority)
    SELECT
        'Controversy Alert',
        'High media coverage (' || media_coverage || ' articles) for: ' || title,
        CASE
            WHEN severity_rating >= 7 THEN 'Critical'
            ELSE 'High'
        END
    FROM
        esg.controversy_incidents
    WHERE
        media_coverage > 50
        AND created_at >= CURRENT_TIMESTAMP - INTERVAL '1 day';
END;
$$;

COMMENT ON PROCEDURE esg.sp_monitor_controversies IS 'Automatically flags high-risk ESG controversies and generates alerts for rapid response';


-- ESG Data Marketplace Integration
--Business Case: Organizations need to aggregate ESG data from multiple providers (MSCI, Sustainalytics, Bloomberg) into a unified model.

CREATE TABLE esg.data_providers (
    provider_id SERIAL PRIMARY KEY,
    provider_name VARCHAR(100) NOT NULL,
    coverage_areas TEXT[] NOT NULL,
    data_frequency VARCHAR(50) NOT NULL,
    api_endpoint VARCHAR(500),
    auth_method VARCHAR(50),
    last_sync TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.external_esg_data (
    data_id SERIAL PRIMARY KEY,
    provider_id INTEGER REFERENCES esg.data_providers(provider_id),
    entity_id VARCHAR(100) NOT NULL, -- ISIN, LEI, or internal ID
    entity_name VARCHAR(200) NOT NULL,
    metric_name VARCHAR(100) NOT NULL,
    metric_value DECIMAL(15,2),
    metric_date DATE NOT NULL,
    confidence_score DECIMAL(5,2) CHECK (confidence_score BETWEEN 0 AND 1),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT unique_provider_metric UNIQUE (provider_id, entity_id, metric_name, metric_date)
);

CREATE TABLE esg.data_normalization_rules (
    rule_id SERIAL PRIMARY KEY,
    internal_metric_id INTEGER REFERENCES esg.performance_metrics(metric_id),
    provider_metric_name VARCHAR(100) NOT NULL,
    transformation_rule TEXT NOT NULL,
    provider_id INTEGER REFERENCES esg.data_providers(provider_id),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.data_providers IS 'Registry of external ESG data providers and connection details';
COMMENT ON TABLE esg.external_esg_data IS 'Stores ESG data obtained from external providers';
COMMENT ON TABLE esg.data_normalization_rules IS 'Defines rules for transforming provider-specific data into internal format';

CREATE MATERIALIZED VIEW esg.mv_unified_esg_scores AS
SELECT
    e.entity_id,
    e.entity_name,
    pm.metric_name AS internal_metric,
    AVG(CASE
        WHEN pm.unit_of_measure = 'Percentage' THEN e.metric_value * 100
        WHEN dn.transformation_rule IS NOT NULL THEN eval_expr(dn.transformation_rule, e.metric_value)
        ELSE e.metric_value
    END) AS normalized_value,
    COUNT(DISTINCT e.provider_id) AS provider_count,
    AVG(e.confidence_score) AS avg_confidence
FROM
    esg.external_esg_data e
JOIN
    esg.data_normalization_rules dn ON e.provider_id = dn.provider_id
    AND e.metric_name = dn.provider_metric_name
JOIN
    esg.performance_metrics pm ON dn.internal_metric_id = pm.metric_id
WHERE
    e.metric_date >= CURRENT_DATE - INTERVAL '1 year'
GROUP BY
    e.entity_id, e.entity_name, pm.metric_name
WITH DATA;

COMMENT ON MATERIALIZED VIEW esg.mv_unified_esg_scores IS 'Consolidates and normalizes ESG data from multiple providers into comparable metrics';

CREATE OR REPLACE PROCEDURE esg.sp_sync_provider_data(
    p_provider_id INTEGER
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_endpoint VARCHAR(500);
    v_auth_method VARCHAR(50);
BEGIN
    -- Get provider API details
    SELECT api_endpoint, auth_method INTO v_endpoint, v_auth_method
    FROM esg.data_providers
    WHERE provider_id = p_provider_id;

    -- Placeholder for actual API integration  (to do -- modify this --Awase)
    -- In practice, this would:
    -- 1. Authenticate with provider
    -- 2. Fetch latest data
    -- 3. Transform according to normalization rules
    -- 4. Store in external_esg_data

    -- Update sync timestamp
    UPDATE esg.data_providers
    SET last_sync = CURRENT_TIMESTAMP
    WHERE provider_id = p_provider_id;

    -- Refresh unified view
    REFRESH MATERIALIZED VIEW esg.mv_unified_esg_scores;
END;
$$;

COMMENT ON PROCEDURE esg.sp_sync_provider_data IS 'Synchronizes ESG data from external providers and normalizes into internal format';

-- ESG--linked Compensation Analytics
-- Business Case: Companies are increasingly tying executive compensation to ESG performance metrics and need robust tracking.
CREATE TABLE esg.compensation_plans (
    plan_id SERIAL PRIMARY KEY,
    plan_name VARCHAR(100) NOT NULL,
    target_group VARCHAR(50) CHECK (target_group IN ('Executives', 'Management', 'All Employees')),
    effective_date DATE NOT NULL,
    expiry_date DATE,
    performance_period VARCHAR(50) NOT NULL, -- 'Annual', 'Multi-Year'
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.compensation_metrics (
    metric_link_id SERIAL PRIMARY KEY,
    plan_id INTEGER REFERENCES esg.compensation_plans(plan_id),
    metric_id INTEGER REFERENCES esg.performance_metrics(metric_id),
    weight DECIMAL(5,2) CHECK (weight BETWEEN 0 AND 1),
    threshold_value DECIMAL(15,2),
    target_value DECIMAL(15,2),
    stretch_value DECIMAL(15,2),
    payout_curve TEXT, -- JSON defining payout formula
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.compensation_outcomes (
    outcome_id SERIAL PRIMARY KEY,
    plan_id INTEGER REFERENCES esg.compensation_plans(plan_id),
    employee_id VARCHAR(50) NOT NULL,
    assessment_date DATE NOT NULL,
    base_salary DECIMAL(15,2),
    total_potential_payout DECIMAL(15,2),
    actual_payout DECIMAL(15,2),
    payout_percentage DECIMAL(5,2),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.compensation_metric_results (
    result_id SERIAL PRIMARY KEY,
    outcome_id INTEGER REFERENCES esg.compensation_outcomes(outcome_id),
    metric_link_id INTEGER REFERENCES esg.compensation_metrics(metric_link_id),
    actual_value DECIMAL(15,2),
    target_value DECIMAL(15,2),
    performance_percentage DECIMAL(5,2),
    weighted_contribution DECIMAL(5,2),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.compensation_plans IS 'Defines ESG-linked compensation programs and their parameters';
COMMENT ON TABLE esg.compensation_metrics IS 'Links ESG performance metrics to compensation plans with weighting';
COMMENT ON TABLE esg.compensation_outcomes IS 'Records actual compensation outcomes tied to ESG performance';
COMMENT ON TABLE esg.compensation_metric_results IS 'Detailed results of individual metric performance for compensation calculations';

CREATE OR REPLACE VIEW esg.vw_esg_payout_analysis AS
SELECT
    cp.plan_name,
    cp.target_group,
    co.employee_id,
    co.assessment_date,
    co.base_salary,
    co.total_potential_payout,
    co.actual_payout,
    co.payout_percentage,
    STRING_AGG(pm.metric_name || ': ' || ROUND(cmr.performance_percentage, 1) || '%', ', ' ORDER BY cm.weight DESC) AS metric_performance,
    SUM(cmr.weighted_contribution) AS calculated_payout_percentage,
    co.actual_payout - (co.total_potential_payout * SUM(cmr.weighted_contribution) / 100) AS variance
FROM
    esg.compensation_outcomes co
JOIN
    esg.compensation_plans cp ON co.plan_id = cp.plan_id
JOIN
    esg.compensation_metric_results cmr ON co.outcome_id = cmr.outcome_id
JOIN
    esg.compensation_metrics cm ON cmr.metric_link_id = cm.metric_link_id
JOIN
    esg.performance_metrics pm ON cm.metric_id = pm.metric_id
GROUP BY
    cp.plan_name, cp.target_group, co.employee_id, co.assessment_date,
    co.base_salary, co.total_potential_payout, co.actual_payout, co.payout_percentage;

COMMENT ON VIEW esg.vw_esg_payout_analysis IS 'Analyzes actual compensation payouts against ESG performance metrics to identify variances';

CREATE OR REPLACE PROCEDURE esg.sp_calculate_esg_payouts(
    p_plan_id INTEGER,
    p_assessment_date DATE
)
LANGUAGE plpgsql
AS $$
DECLARE
    v_payout_curve JSON;
    v_metric_record RECORD;
    v_performance_pct DECIMAL(5,2);
    v_weighted_total DECIMAL(5,2) := 0;
    v_employee_id VARCHAR(50);
BEGIN
    -- Create outcome records for all plan participants
    FOR v_employee_id IN
        SELECT employee_id FROM hr.employees
        WHERE employee_type = (SELECT target_group FROM esg.compensation_plans WHERE plan_id = p_plan_id)
    LOOP
        -- Insert base outcome record
        INSERT INTO esg.compensation_outcomes (
            plan_id, employee_id, assessment_date, base_salary, total_potential_payout
        )
        VALUES (
            p_plan_id,
            v_employee_id,
            p_assessment_date,
            (SELECT base_salary FROM hr.employee_compensation WHERE employee_id = v_employee_id),
            (SELECT esg_bonus_potential FROM hr.employee_compensation WHERE employee_id = v_employee_id)
        )
        RETURNING outcome_id INTO v_outcome_id;

        -- Calculate performance for each metric
        FOR v_metric_record IN
            SELECT
                cm.metric_link_id,
                cm.metric_id,
                cm.weight,
                cm.threshold_value,
                cm.target_value,
                cm.stretch_value,
                cm.payout_curve
            FROM
                esg.compensation_metrics cm
            WHERE
                cm.plan_id = p_plan_id
        LOOP
            -- Get actual performance value
            SELECT actual_value INTO v_actual_value
            FROM esg.performance_data
            WHERE metric_id = v_metric_record.metric_id
            AND reporting_period = p_assessment_date
            LIMIT 1;

            -- Calculate performance percentage
            v_performance_pct := CASE
                WHEN v_actual_value <= v_metric_record.threshold_value THEN 0
                WHEN v_actual_value >= v_metric_record.stretch_value THEN 150
                ELSE 100 * (v_actual_value - v_metric_record.threshold_value) /
                     (v_metric_record.target_value - v_metric_record.threshold_value)
            END;

            -- Store metric result
            INSERT INTO esg.compensation_metric_results (
                outcome_id, metric_link_id, actual_value, target_value,
                performance_percentage, weighted_contribution
            )
            VALUES (
                v_outcome_id, v_metric_record.metric_link_id, v_actual_value,
                v_metric_record.target_value, v_performance_pct,
                v_performance_pct * v_metric_record.weight
            );

            v_weighted_total := v_weighted_total + (v_performance_pct * v_metric_record.weight);
        END LOOP;

        -- Update outcome with final payout calculation
        UPDATE esg.compensation_outcomes
        SET
            payout_percentage = v_weighted_total,
            actual_payout = total_potential_payout * LEAST(v_weighted_total, 150) / 100
        WHERE
            outcome_id = v_outcome_id;
    END LOOP;
END;
$$;

COMMENT ON PROCEDURE esg.sp_calculate_esg_payouts IS 'Calculates compensation payouts based on ESG performance against targets and payout curves';


-- ESG AI Governance Framework
-- Business case: : Organizations using AI for ESG analytics need to ensure models are fair, transparent, and aligned with ESG principles

CREATE TABLE esg.ai_models (
    model_id SERIAL PRIMARY KEY,
    model_name VARCHAR(100) NOT NULL,
    model_purpose TEXT NOT NULL,
    model_type VARCHAR(50) CHECK (model_type IN ('Predictive', 'Classification', 'Optimization', 'Generative')),
    deployment_date DATE,
    input_data_sources TEXT[],
    output_metrics TEXT[],
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.ai_model_audits (
    audit_id SERIAL PRIMARY KEY,
    model_id INTEGER REFERENCES esg.ai_models(model_id),
    audit_date DATE NOT NULL,
    auditor VARCHAR(100) NOT NULL,
    fairness_score DECIMAL(5,2) CHECK (fairness_score BETWEEN 0 AND 1),
    transparency_score DECIMAL(5,2) CHECK (transparency_score BETWEEN 0 AND 1),
    alignment_score DECIMAL(5,2) CHECK (alignment_score BETWEEN 0 AND 1),
    bias_metrics JSONB,
    performance_metrics JSONB,
    recommendations TEXT,
    status VARCHAR(20) CHECK (status IN ('Approved', 'Conditional', 'Rejected', 'Remediation')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE esg.ai_model_deployments (
    deployment_id SERIAL PRIMARY KEY,
    model_id INTEGER REFERENCES esg.ai_models(model_id),
    environment VARCHAR(50) CHECK (environment IN ('Development', 'Staging', 'Production')),
    version VARCHAR(50) NOT NULL,
    deployment_date TIMESTAMP WITH TIME ZONE NOT NULL,
    rollback_procedure TEXT,
    monitoring_frequency VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

COMMENT ON TABLE esg.ai_models IS 'Registry of AI models used for ESG analytics with purpose and specifications';
COMMENT ON TABLE esg.ai_model_audits IS 'Records audits of AI models for fairness, transparency, and ESG alignment';
COMMENT ON TABLE esg.ai_model_deployments IS 'Tracks deployment history and environments for ESG AI models';

CREATE OR REPLACE VIEW esg.vw_ai_governance_status AS
SELECT
    m.model_name,
    m.model_type,
    d.environment,
    d.version,
    a.audit_date,
    ROUND((a.fairness_score + a.transparency_score + a.alignment_score) / 3, 2) AS overall_esg_score,
    a.status AS audit_status,
    CASE
        WHEN (a.fairness_score + a.transparency_score + a.alignment_score) / 3 >= 0.8 THEN 'High Confidence'
        WHEN (a.fairness_score + a.transparency_score + a.alignment_score) / 3 >= 0.6 THEN 'Moderate Confidence'
        ELSE 'Low Confidence'
    END AS usage_recommendation,
    d.deployment_date
FROM
    esg.ai_models m
JOIN
    esg.ai_model_deployments d ON m.model_id = d.model_id
LEFT JOIN
    (SELECT model_id, audit_date, fairness_score, transparency_score, alignment_score, status
     FROM esg.ai_model_audits
     WHERE audit_date = (SELECT MAX(audit_date) FROM esg.ai_model_audits a2
                        WHERE a2.model_id = esg.ai_model_audits.model_id)) a
    ON m.model_id = a.model_id
WHERE
    d.environment = 'Production';

COMMENT ON VIEW esg.vw_ai_governance_status IS 'Provides a comprehensive view of AI model governance status for ESG compliance';

CREATE OR REPLACE PROCEDURE esg.sp_conduct_ai_audit(
    p_model_id INTEGER,
    p_auditor VARCHAR(100)
LANGUAGE plpgsql
AS $$
DECLARE
    v_fairness_score DECIMAL(5,2);
    v_transparency_score DECIMAL(5,2);
    v_alignment_score DECIMAL(5,2);
    v_bias_metrics JSONB;
    v_performance_metrics JSONB;
    v_recommendations TEXT;
BEGIN
    -- Placeholder for actual audit logic
    -- In practice, this would:
    -- 1. Run fairness tests on model outputs
    -- 2. Evaluate explainability/transparency
    -- 3. Assess alignment with ESG principles
    -- 4. Generate recommendations

    -- For demonstration, using simplified scoring
    v_fairness_score := 0.7 + random() * 0.3;
    v_transparency_score := 0.6 + random() * 0.4;
    v_alignment_score := 0.8 + random() * 0.2;

    v_bias_metrics := jsonb_build_object(
        'demographic_parity', 0.85,
        'equal_opportunity', 0.92,
        'disparate_impact', 1.08
    );

    v_performance_metrics := jsonb_build_object(
        'accuracy', 0.88,
        'precision', 0.91,
        'recall', 0.85,
        'f1_score', 0.88
    );

    v_recommendations := CASE
        WHEN v_fairness_score < 0.7 THEN 'Implement bias mitigation techniques'
        WHEN v_transparency_score < 0.7 THEN 'Enhance model documentation and explainability'
        ELSE 'No major remediation required'
    END;

    -- Record audit results
    INSERT INTO esg.ai_model_audits (
        model_id, audit_date, auditor, fairness_score, transparency_score,
        alignment_score, bias_metrics, performance_metrics, recommendations,
        status
    )
    VALUES (
        p_model_id, CURRENT_DATE, p_auditor, v_fairness_score, v_transparency_score,
        v_alignment_score, v_bias_metrics, v_performance_metrics, v_recommendations,
        CASE
            WHEN (v_fairness_score + v_transparency_score + v_alignment_score) / 3 >= 0.7 THEN 'Approved'
            WHEN (v_fairness_score + v_transparency_score + v_alignment_score) / 3 >= 0.5 THEN 'Conditional'
            ELSE 'Remediation'
        END
    );
END;
$$;

COMMENT ON PROCEDURE esg.sp_conduct_ai_audit IS 'Performs comprehensive audit of AI models used for ESG analytics against fairness, transparency and alignment criteria';

-- integration
-- Example integration with financial systems for transition risk
CREATE FOREIGN TABLE finance.portfolio_holdings (
    asset_id VARCHAR(50),
    asset_class VARCHAR(50),
    sector VARCHAR(50),
    market_value DECIMAL(15,2),
    currency VARCHAR(3)
) SERVER finance_system;

-- Create unified view of financial and ESG data
CREATE VIEW esg.vw_portfolio_esg_risk AS
SELECT
    p.asset_id,
    p.asset_class,
    p.sector,
    p.market_value,
    e.transition_risk_score,
    e.carbon_intensity,
    e.stranding_risk * p.market_value AS potential_value_at_risk
FROM
    finance.portfolio_holdings p
JOIN
    esg.mv_transition_risk_summary e ON p.sector = e.sector
WHERE
    e.scenario_name = 'Net Zero 2050';


-- performance optimization
-- Add indexes for new tables
CREATE INDEX idx_portfolio_exposure_scenario ON esg.portfolio_exposure(scenario_id, analysis_date);
CREATE INDEX idx_controversy_incidents_date ON esg.controversy_incidents(incident_date);
CREATE INDEX idx_external_data_entity ON esg.external_esg_data(entity_id, metric_date);
CREATE INDEX idx_ai_audits_model ON esg.ai_model_audits(model_id, audit_date);

-- Partition large tables
CREATE TABLE esg.controversy_incidents_partitioned (
    LIKE esg.controversy_incidents INCLUDING INDEXES
) PARTITION BY RANGE (incident_date);

CREATE TABLE esg.controversy_incidents_2023 PARTITION OF esg.controversy_incidents_partitioned
    FOR VALUES FROM ('2025-01-01') TO ('2025-06-01');


--maintenance procedure
CREATE OR REPLACE PROCEDURE esg.sp_maintain_esg_data()
LANGUAGE plpgsql
AS $$
BEGIN
    -- Refresh all materialized views
    REFRESH MATERIALIZED VIEW esg.mv_transition_risk_summary;
    REFRESH MATERIALIZED VIEW esg.mv_unified_esg_scores;

    -- Sync data from providers
    CALL esg.sp_sync_provider_data(1); -- MSCI
    CALL esg.sp_sync_provider_data(2); -- Sustainalytics
    CALL esg.sp_sync_provider_data(3); -- Bloomberg

    -- Monitor for controversies
    CALL esg.sp_monitor_controversies();

    -- Audit AI models on schedule
    PERFORM esg.sp_conduct_ai_audit(model_id, 'Scheduled Audit')
    FROM esg.ai_models
    WHERE model_id IN (
        SELECT model_id FROM esg.ai_model_deployments
        WHERE environment = 'Production'
        AND deployment_date >= CURRENT_DATE - INTERVAL '6 months'
    );
END;
$$;

COMMENT ON PROCEDURE esg.sp_maintain_esg_data IS 'Orchestrates regular maintenance tasks for ESG data and analytics';

----------------------
-- -- Data Ingestion Framework
-- ----------------------



-- --------------------------------
--  Resiliency Framework
-- ----------------------



-- ----------------------
-- Reporting and Analytics
-- ----------------------



-- ----------------------
--
--  Data Governance
-- ----------------------
